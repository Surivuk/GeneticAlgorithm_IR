0) 53.64571, Computer - Wikipedia, the free encyclopedia.txt#84, term: computer, content:There are many types of computer architectures:
1) 47.416553, Computer programming - Wikipedia, the free encyclopedia.txt#29, term: computer, content:Computer programmers are those who write computer software. Their jobs usually involve:
2) 46.939995, EDVAC - Wikipedia, the free encyclopedia.txt#7, term: computer, content:Physically, the computer comprised the following components:
3) 46.939995, User interface - Wikipedia, the free encyclopedia.txt#33, term: computer, content:HMI for a Computer numerical control (CNC)
4) 46.458546, Computer program - Wikipedia, the free encyclopedia.txt#33, term: computer, content:An operating system is a computer program that acts as an intermediary between a user of a computer and the computer hardware. [23]
5) 44.9833, Computing - Wikipedia, the free encyclopedia.txt#31, term: computer, content:The computer industry is made up of all of the businesses involved in developing computer software, designing computer hardware and computer networking infrastructures, the manufacture of computer components and the provision of information technology services including system administration and maintenance.
6) 40.234283, Analog computer - Wikipedia, the free encyclopedia.txt#73, term: computer, content:The core mathematical operations used in an electric analog computer are:
7) 40.234283, Computation - Wikipedia, the free encyclopedia.txt#1, term: computer, content:The study of computation is paramount to the discipline of computer science.
8) 40.234283, Computer - Wikipedia, the free encyclopedia.txt#70, term: computer, content:Computer main memory comes in two principal varieties:
9) 40.234283, Computer engineering - Wikipedia, the free encyclopedia.txt#4, term: computer, content:There are two major specialties in computer engineering: software and hardware.
10) 40.234283, Computer engineering - Wikipedia, the free encyclopedia.txt#7, term: computer, content:There are many specialty areas in the field of computer engineering.
11) 40.234283, Computer graphics - Wikipedia, the free encyclopedia.txt#60, term: computer, content:Computer graphics may be used in the following areas:
12) 40.234283, Computer hardware - Wikipedia, the free encyclopedia.txt#4, term: computer, content:There are a number of different types of computer system in use today.
13) 40.234283, Computer network - Wikipedia, the free encyclopedia.txt#4, term: computer, content:The chronology of significant computer-network developments includes:
14) 40.234283, Computer security - Wikipedia, the free encyclopedia.txt#104, term: computer, content: Media related to Computer security at Wikimedia Commons
15) 40.234283, Computer simulation - Wikipedia, the free encyclopedia.txt#19, term: computer, content:Computer simulations are used in a wide variety of practical contexts, such as:
16) 40.234283, Computer speaker - Wikipedia, the free encyclopedia.txt#1, term: computer, content:Computer speakers range widely in quality and in price. Computer speakers sometimes packaged with computer systems are small, plastic, and have mediocre sound quality. Some computer speakers have equalization features such as bass and treble controls.
17) 40.234283, Exclusive or - Wikipedia, the free encyclopedia.txt#26, term: computer, content:In computer science, exclusive disjunction has several uses:
18) 40.234283, High-level programming language - Wikipedia, the free encyclopedia.txt#11, term: computer, content:Alternatively, it is possible for a high-level language to be directly implemented by a computer  the computer directly executes the HLL code. This is known as a high-level language computer architecture  the computer architecture itself is designed to be targeted by a specific high-level language.
19) 40.234283, Personal computer - Wikipedia, the free encyclopedia.txt#20, term: computer, content:In 1982 "The Computer" was named Machine of the Year by Time Magazine.
20) 40.234283, Portable computer - Wikipedia, the free encyclopedia.txt#8, term: computer, content:Another early portable computer released in 1982 was the Kaypro.
21) 40.234283, Software - Wikipedia, the free encyclopedia.txt#0, term: computer, content:Computer software, or simply software, is that part of a computer system that consists of encoded information or computer instructions, in contrast to the physical hardware from which the system is built. The term is roughly synonymous with computer program.
22) 40.234283, Software - Wikipedia, the free encyclopedia.txt#10, term: computer, content:Based on the goal, computer software can be divided into:
23) 37.933243, Cite This Page - Wikipedia, the free encyclopedia.txt#8, term: computer, content:Computer, https://en.wikipedia.org/w/index.php?title=Computer&oldid=724624530 (last visited June 22, 2016). 
24) 37.933243, Computer architecture - Wikipedia, the free encyclopedia.txt#8, term: computer, content:There are other types of computer architecture. The following types are used in bigger companies like Intel, and count for 1% of all of computer architecture
25) 37.933243, Computer graphics - Wikipedia, the free encyclopedia.txt#37, term: computer, content:2D computer graphics are the computer-based generation of digital imagesmostly from models, such as digital image, and by techniques specific to them.
26) 37.933243, Computer science - Wikipedia, the free encyclopedia.txt#23, term: computer, content:Applied computer science aims at identifying certain computer science concepts that can be used directly in solving real world problems.
27) 37.933243, E6B - Wikipedia, the free encyclopedia.txt#6, term: computer, content:In addition, computer programs to emulate the flight computer functions are also available, both for computers and smartphones.
28) 37.933243, File Transfer Protocol - Wikipedia, the free encyclopedia.txt#0, term: computer, content:The File Transfer Protocol (FTP) is a standard network protocol used to transfer computer files between a client and server on a computer network.
29) 37.933243, Harwell CADET - Wikipedia, the free encyclopedia.txt#0, term: computer, content:The Harwell CADET was the first fully transistorized computer in Europe, and may have been the first fully transistorized computer in the world.
30) 37.933243, IBM 604 - Wikipedia, the free encyclopedia.txt#5, term: computer, content:An IBM 604 is preserved at the American Computer Museum and another at the University of Amsterdam Computer Museum.
31) 37.933243, Programmer - Wikipedia, the free encyclopedia.txt#2, term: computer, content:The first person to run a program on a functioning modern electronically based computer was computer scientist Konrad Zuse, in 1941.
32) 37.48608, Computer architecture - Wikipedia, the free encyclopedia.txt#0, term: computer, content:In computer engineering, computer architecture is a set of rules and methods that describe the functionality, organization, and implementation of computer systems. Some definitions of architecture define it as describing the capabilities and programming model of a computer but not a particular implementation.[1] In other descriptions computer architecture involves instruction set architecture design, microarchitecture design, logic design, and implementation.[2]
33) 35.4833, Computer animation - Wikipedia, the free encyclopedia.txt#0, term: computer, content:Computer animation, or CGI animation, is the process used for generating animated images. The more general term computer-generated imagery encompasses both static scenes and dynamic images, while computer animation only refers to the moving images. Modern computer animation usually uses 3D computer graphics, although 2D computer graphics are still used for stylistic, low bandwidth, and faster real-time renderings. Sometimes, the target of the animation is the computer itself, but sometimes film as well.
34) 35.4833, Computer graphics - Wikipedia, the free encyclopedia.txt#46, term: computer, content:Computer animation is the art of creating moving images via the use of computers. It is a subfield of computer graphics and animation. Increasingly it is created by means of 3D computer graphics, though 2D computer graphics are still widely used for stylistic, low bandwidth, and faster real-time rendering needs. Sometimes the target of the animation is the computer itself, but sometimes the target is another medium, such as film. It is also referred to as CGI (Computer-generated imagery or computer-generated imaging), especially when used in films.
35) 34.84391, Computer graphics - Wikipedia, the free encyclopedia.txt#18, term: computer, content:The 1980s began to see the modernization and commercialization of computer graphics. As the home computer proliferated, a subject which had previously been an academics-only discipline was adopted by a much larger audience, and the number of computer graphics developers increased significantly.
36) 34.84391, Computer hardware - Wikipedia, the free encyclopedia.txt#17, term: computer, content:A mainframe computer is a much larger computer that typically fills a room and may cost many hundreds or thousands of times as much as a personal computer. They are designed to perform large numbers of calculations for governments and large enterprises.
37) 34.84391, Computer network - Wikipedia, the free encyclopedia.txt#5, term: computer, content:Computer networking may be considered a branch of electrical engineering, telecommunications, computer science, information technology or computer engineering, since it relies upon the theoretical and practical application of the related disciplines.
38) 34.84391, Computer program - Wikipedia, the free encyclopedia.txt#0, term: computer, content:A computer program is a collection of instructions[1] that performs a specific task when executed by a computer. A computer requires programs to function, and typically executes the program's instructions in a central processing unit.[2]
39) 34.84391, Computer program - Wikipedia, the free encyclopedia.txt#2, term: computer, content:A part of a computer program that performs a well-defined task is known as an algorithm. A collection of computer programs, libraries and related data are referred to as software. Computer programs may be categorized along functional lines, such as application software or system software.
40) 34.84391, Computing - Wikipedia, the free encyclopedia.txt#23, term: computer, content:Computer networking is sometimes considered a sub-discipline of electrical engineering, telecommunications, computer science, information technology or computer engineering, since it relies upon the theoretical and practical application of these disciplines.
41) 34.84391, Information system - Wikipedia, the free encyclopedia.txt#14, term: computer, content:A computer(-based) information system is essentially an IS using computer technology to carry out some or all of its planned tasks. The basic components of computer-based information systems are:
42) 34.84391, Software - Wikipedia, the free encyclopedia.txt#1, term: computer, content:The term "software" was first proposed by Alan Turing and used in this sense by John W. Tukey in 1957. In computer science and software engineering, computer software is all information processed by computer systems, programs and data.
43) 34.84391, Software - Wikipedia, the free encyclopedia.txt#2, term: computer, content:Computer software includes computer programs, libraries and related non-executable data, such as online documentation or digital media. Computer hardware and software require each other and neither can be realistically used on its own.
44) 33.52857, 3D computer graphics - Wikipedia, the free encyclopedia.txt#4, term: computer, content:3D computer graphics creation falls into three basic phases:
45) 33.52857, 3D computer graphics software - Wikipedia, the free encyclopedia.txt#6, term: computer, content:Computer aided design software may employ the same fundamental 3D modeling techniques that 3D modeling software use but their goal differs. They are used in computer-aided engineering, computer-aided manufacturing, Finite element analysis, product lifecycle management, 3D printing and Computer-aided architectural design.
46) 33.52857, Abstract machine - Wikipedia, the free encyclopedia.txt#0, term: computer, content:An abstract machine, also called an abstract computer, is a theoretical model of a computer hardware or software system used in automata theory. Abstraction of computing processes is used in both the computer science and computer engineering disciplines and usually assumes a discrete time paradigm.
47) 33.52857, Algorithm - Wikipedia, the free encyclopedia.txt#87, term: computer, content:Emil Post (1936) described the actions of a "computer" (human being) as follows:
48) 33.52857, Bourne shell - Wikipedia, the free encyclopedia.txt#0, term: computer, content:The Bourne shell (sh) is a shell, or command-line interpreter, for computer operating systems.
49) 33.52857, British Computer Society - Wikipedia, the free encyclopedia.txt#4, term: computer, content:Members are sent the quarterly IT professional magazine ITNOW (formerly The Computer Bulletin).
50) 33.52857, Computer animation - Wikipedia, the free encyclopedia.txt#30, term: computer, content:Two examples of films using computer-assisted animation are Beauty and the Beast and Antz.
51) 33.52857, Computer architecture - Wikipedia, the free encyclopedia.txt#7, term: computer, content:The discipline of computer architecture has three main subcategories:[5]
52) 33.52857, Computer monitor - Wikipedia, the free encyclopedia.txt#33, term: computer, content:Computer monitors are provided with a variety of methods for mounting them depending on the application and environment.
53) 33.52857, Computer mouse - Wikipedia, the free encyclopedia.txt#36, term: computer, content:Logitech spacemouse 3D. On display at the Bolo computer Museum, EPFL, Lausanne.
54) 33.52857, Computer network - Wikipedia, the free encyclopedia.txt#20, term: computer, content:The NIC responds to traffic addressed to a network address for either the NIC or the computer as a whole.
55) 33.52857, Computer program - Wikipedia, the free encyclopedia.txt#1, term: computer, content:A computer program is usually written by a computer programmer in a programming language. From the program in its human-readable form of source code, a compiler can derive machine codea form consisting of instructions that the computer can directly execute. Alternatively, a computer program may be executed with the aid of an interpreter.
56) 33.52857, Computer program - Wikipedia, the free encyclopedia.txt#7, term: computer, content:The Z3 computer, invented by Konrad Zuse (1941) in Germany, was a digital and programmable computer.[10] A digital computer uses electricity as the calculating component. The Z3 contained 2,400 relays to create the circuits. The circuits provided a binary, floating-point, nine-instruction computer. Programming the Z3 was through a specially designed keyboard and punched tape.
57) 33.52857, Computer program - Wikipedia, the free encyclopedia.txt#32, term: computer, content:Utility programs are application programs designed to aid system administrators and computer programmers.
58) 33.52857, Computer programming - Wikipedia, the free encyclopedia.txt#27, term: computer, content:Allen Downey, in his book How To Think Like A Computer Scientist, writes:
59) 33.52857, Computer science - Wikipedia, the free encyclopedia.txt#30, term: computer, content:This branch of computer science aims to manage networks between computers worldwide.
60) 33.52857, Computer security - Wikipedia, the free encyclopedia.txt#57, term: computer, content:Some illustrative examples of different types of computer security breaches are given below.
61) 33.52857, Computer simulation - Wikipedia, the free encyclopedia.txt#10, term: computer, content:Computer models can be classified according to several independent pairs of attributes, including:
62) 33.52857, Computer Technology Limited - Wikipedia, the free encyclopedia.txt#2, term: computer, content:The first CTL computer (the Modular One) appeared for sale in 1968.
63) 33.52857, Desktop computer - Wikipedia, the free encyclopedia.txt#1, term: computer, content:An all-in-one desktop computer typically combines the case and monitor in one unit.
64) 33.52857, Digital camera - Wikipedia, the free encyclopedia.txt#64, term: computer, content:Many digital cameras can connect directly to a computer to transfer data:-
65) 33.52857, E6B - Wikipedia, the free encyclopedia.txt#11, term: computer, content:The mathematical formulas that equate to the results of the flight computer wind calculator are as follows:
66) 33.52857, Ferranti Pegasus - Wikipedia, the free encyclopedia.txt#9, term: computer, content:In 1957 a Pegasus computer was used to calculate 7480 digits of pi, a record at the time.
67) 33.52857, Flash memory - Wikipedia, the free encyclopedia.txt#90, term: computer, content:As of 2012, there are attempts to use flash memory as the main computer memory, DRAM.[67]
68) 33.52857, Floating point - Wikipedia, the free encyclopedia.txt#85, term: computer, content:Floating-point computation in a computer can run into three kinds of problems:
69) 33.52857, Home computer - Wikipedia, the free encyclopedia.txt#54, term: computer, content:The following computers also introduced significant advancements to the home computer segment:
70) 33.52857, Human–computer interaction - Wikipedia, the free encyclopedia.txt#7, term: computer, content:Much of the research in the field of human-computer interaction takes an interest in:
71) 33.52857, IBM 650 - Wikipedia, the free encyclopedia.txt#15, term: computer, content:Donald Knuth's series of books The Art of Computer Programming is famously dedicated to a 650.
72) 33.52857, Information technology - Wikipedia, the free encyclopedia.txt#21, term: computer, content:Employment distribution of computer systems design and related services, 2011[42]
73) 33.52857, Interrupt - Wikipedia, the free encyclopedia.txt#26, term: computer, content:PCI Express, a serial computer bus, uses message-signalled interrupts exclusively.
74) 33.52857, Jacquard loom - Wikipedia, the free encyclopedia.txt#18, term: computer, content:Punched paper tape used to instruct the 1944 Harvard Mark I computer
75) 33.52857, List of programming languages by type - Wikipedia, the free encyclopedia.txt#51, term: computer, content:Computer scientist Niklaus Wirth designed and implemented several influential languages.
76) 33.52857, Negation - Wikipedia, the free encyclopedia.txt#20, term: computer, content:As in mathematics, negation is used in computer science to construct logical statements.
77) 33.52857, Peripheral - Wikipedia, the free encyclopedia.txt#0, term: computer, content:A peripheral is a "device that is used to put information into or get information out of the computer."[1]
78) 33.52857, Portable computer - Wikipedia, the free encyclopedia.txt#10, term: computer, content:The first full-color portable computer was the Commodore SX-64 in January 1984.
79) 33.52857, Software - Wikipedia, the free encyclopedia.txt#9, term: computer, content:On virtually all computer platforms, software can be grouped into a few broad categories.
80) 33.52857, Stack machine - Wikipedia, the free encyclopedia.txt#0, term: computer, content:In computer science, computer engineering and programming language implementations, a stack machine is a real or emulated computer that uses a pushdown stack rather than individual machine registers to evaluate each sub-expression in the program. A stack computer is programmed with a reverse Polish notation instruction set.
81) 33.52857, Turing completeness - Wikipedia, the free encyclopedia.txt#3, term: computer, content:In colloquial usage, the terms "Turing complete" or "Turing equivalent" are used to mean that any real-world general-purpose computer or computer language can approximately simulate the computational aspects of any other real-world general-purpose computer or computer language.
82) 33.52857, User interface - Wikipedia, the free encyclopedia.txt#31, term: computer, content:Voice user interface of a wearable computer (here: Google Glass)
83) 33.52857, Vector processor - Wikipedia, the free encyclopedia.txt#5, term: computer, content:A computer for operations with functions was presented and developed by Kartsev in 1967.[1]
84) 33.52857, Video card - Wikipedia, the free encyclopedia.txt#21, term: computer, content:The most common connection systems between the video card and the computer display are:
85) 33.52857, Wearable computer - Wikipedia, the free encyclopedia.txt#6, term: computer, content:Wearable computer items have been developed and applied in the following:
86) 33.52857, Word (computer architecture) - Wikipedia, the free encyclopedia.txt#4, term: computer, content:Depending on how a computer is organized, word-size units may be used for:
87) 33.52857, Z22 (computer) - Wikipedia, the free encyclopedia.txt#4, term: computer, content:In the 1970s, clones of the Z22 using TTL were built by the company Thiemicke Computer.
88) 33.19159, Cite This Page - Wikipedia, the free encyclopedia.txt#6, term: computer, content:Wikipedia contributors, "Computer,"  Wikipedia, The Free Encyclopedia, https://en.wikipedia.org/w/index.php?title=Computer&oldid=724624530 (accessed June 22, 2016). 
89) 33.19159, Command-line interface - Wikipedia, the free encyclopedia.txt#2, term: computer, content:Command-line interfaces to computer operating systems are less widely used by casual computer users, who favor graphical user interfaces.
90) 33.19159, Computational science - Wikipedia, the free encyclopedia.txt#1, term: computer, content:In practical use, it is typically the application of computer simulation and other forms of computation from numerical analysis and theoretical computer science to solve problems in various scientific disciplines.
91) 33.19159, Computer hardware - Wikipedia, the free encyclopedia.txt#14, term: computer, content:Input and output devices are typically housed externally to the main computer chassis. The following are either standard or very common to many computer systems.
92) 33.19159, Computer mouse - Wikipedia, the free encyclopedia.txt#2, term: computer, content:The earliest known publication of the term mouse as a computer pointing device is in Bill English's 1965 publication "Computer-Aided Display Control".[1]
93) 33.19159, Computer speaker - Wikipedia, the free encyclopedia.txt#4, term: computer, content:Instead of using a computer speaker for better sound, a computer can be connected to any external sound system, typically a high-power high-quality setup.
94) 33.19159, Computer Technology Limited - Wikipedia, the free encyclopedia.txt#0, term: computer, content:Computer Technology Limited or CTL, was a British computer company founded slightly later than Digital Equipment Corporation (DEC) in the United States.
95) 33.19159, Computing - Wikipedia, the free encyclopedia.txt#3, term: computer, content:and it defines five sub-disciplines of the computing field: Computer Science, Computer Engineering, Information Systems, Information Technology, and Software Engineering.[2]
96) 33.19159, Digital camera - Wikipedia, the free encyclopedia.txt#66, term: computer, content:Many modern cameras support the PictBridge standard, which allows them to send data directly to a PictBridge-capable computer printer without the need for a computer.
97) 33.19159, History of computing hardware - Wikipedia, the free encyclopedia.txt#56, term: computer, content:With the proposal of the stored-program computer this changed. A stored-program computer includes by design an instruction set and can store in memory a set of instructions (a program) that details the computation.
98) 33.19159, Logic in computer science - Wikipedia, the free encyclopedia.txt#0, term: computer, content:Logic in computer science covers the overlap between the field of logic and that of computer science. The topic can essentially be divided into three main areas:
99) 33.19159, Microprocessor - Wikipedia, the free encyclopedia.txt#21, term: computer, content:The first use of the term "microprocessor" is attributed to Viatron Computer Systems describing the custom integrated circuit used in their System 21 small computer system announced in 1968.
100) 33.19159, Printer (computing) - Wikipedia, the free encyclopedia.txt#7, term: computer, content:A virtual printer is a piece of computer software whose user interface and API resembles that of a printer driver, but which is not connected with a physical computer printer.
101) 33.19159, Von Neumann architecture - Wikipedia, the free encyclopedia.txt#4, term: computer, content:With the proposal of the stored-program computer, this changed. A stored-program computer includes, by design, an instruction set and can store in memory a set of instructions (a program) that details the computation.
102) 32.851154, Analog computer - Wikipedia, the free encyclopedia.txt#35, term: computer, content:In the early 1970s analog computer manufacturers tried to tie together their analog computer with a digital computer to get the advantages of the two techniques. In such systems, the digital computer controlled the analog computer, providing initial set-up, initiating multiple analog runs, and automatically feeding and collecting data. The digital computer may also participate to the calculation itself using analog-to-digital and digital-to-analog converters.
103) 32.851154, Chemical computer - Wikipedia, the free encyclopedia.txt#0, term: computer, content:A chemical computer, also called reaction-diffusion computer, BZ computer (stands for BelousovZhabotinsky computer) or gooware computer is an unconventional computer based on a semi-solid chemical "soup" where data is represented by varying concentrations of chemicals.[1] The computations are performed by naturally occurring chemical reactions. Chemical computing in today's world mainly refers to the BZ reaction-diffusion model.
104) 32.851154, Digital electronics - Wikipedia, the free encyclopedia.txt#53, term: computer, content:Computer architecture is a specialized engineering activity that tries to arrange the registers, calculation logic, buses and other parts of the computer in the best way for some purpose. Computer architects have applied large amounts of ingenuity to computer design to reduce the cost and increase the speed and immunity to programming errors of computers. An increasingly common goal is to reduce the power used in a battery-powered computer system, such as a cell-phone. Many computer architects serve an extended apprenticeship as microprogrammers.
105) 31.047886, Computer engineering - Wikipedia, the free encyclopedia.txt#18, term: computer, content:Most entry-level computer engineering jobs require at least a bachelor's degree in computer engineering. Sometimes a degree in electronic engineering is accepted, due to the similarity of the two fields. Because hardware engineers commonly work with computer software systems, a background in computer programming usually is needed. According to BLS, "a computer engineering major is similar to electrical engineering but with some computer science courses added to the curriculum".[8] Some large firms or specialized jobs require a master's degree. It is also important for computer engineers to keep up with rapid advances in technology. Therefore, many continue learning throughout their careers.
106) 31.047886, Computer music - Wikipedia, the free encyclopedia.txt#23, term: computer, content:Computer-aided algorithmic composition (CAAC, pronounced "sea-ack") is the implementation and use of algorithmic composition techniques in software. This label is derived from the combination of two labels, each too vague for continued use. The label computer-aided composition lacks the specificity of using generative algorithms. Music produced with notation or sequencing software could easily be considered computer-aided composition. The label algorithmic composition is likewise too broad, particularly in that it does not specify the use of a computer. The term computer-aided, rather than computer-assisted, is used in the same manner as computer-aided design.[citation needed]
107) 29.988865, Antivirus software - Wikipedia, the free encyclopedia.txt#5, term: computer, content:In 1983, the term "computer virus" was coined by Fred Cohen in one of the first ever published academic papers on computer viruses.[14] Cohen used the term "computer virus" to describe a program that: "affect other computer programs by modifying them in such a way as to include a (possibly evolved) copy of itself."[15] (note that a more recent, and precise, definition of computer virus has been given by the Hungarian security researcher Pter Szr: "a code that recursively replicates a possibly evolved copy of itself"[16][17])
108) 29.988865, Computer - Wikipedia, the free encyclopedia.txt#88, term: computer, content:A computer does not need to be electronic, nor even have a processor, nor RAM, nor even a hard disk. While popular usage of the word "computer" is synonymous with a personal electronic computer, the modern[73] definition of a computer is literally: "A device that computes, especially a programmable [usually] electronic machine that performs high-speed mathematical or logical operations or that assembles, stores, correlates, or otherwise processes information."[74] Any device which processes information qualifies as a computer, especially if the processing is purposeful.[citation needed]
109) 29.988865, Computer architecture - Wikipedia, the free encyclopedia.txt#6, term: computer, content:The earliest computer architectures were designed on paper and then directly built into the final hardware form.[4] Later, computer architecture prototypes were physically built in the form of a transistortransistor logic (TTL) computersuch as the prototypes of the 6800 and the PA-RISCtested, and tweaked, before committing to the final hardware form. As of the 1990s, new computer architectures are typically "built", tested, and tweakedinside some other computer architecture in a computer architecture simulator; or inside a FPGA as a soft microprocessor; or bothbefore committing to the final hardware form.
110) 29.988865, Computer engineering - Wikipedia, the free encyclopedia.txt#5, term: computer, content:Computer software engineers develop, design, and test software. Some software engineers design, construct, and maintain computer programs for companies. Some set up networks such as "intranets" for companies. Others make or install new software or upgrade computer systems. Computer software engineers can also work in application design. This involves designing or coding new programs and applications to meet the needs of a business or individual. Computer software engineers can also work as freelancers and sell their software products/applications to an enterprise/individual.[7]
111) 29.988865, Computer graphics - Wikipedia, the free encyclopedia.txt#0, term: computer, content:Computer graphics are pictures and movies created using computers - usually referring to image data created by a computer specifically with help from specialized graphical hardware and software. It is a vast and recent area in computer science.The phrase was coined by computer graphics researchers Verne Hudson and William Fetter of Boeing in 1960. Another name for the field is computer-generated imagery, or simply CGI.
112) 29.988865, Computer hardware - Wikipedia, the free encyclopedia.txt#0, term: computer, content:Computer hardware (or simply hardware in computing contexts) is the collection of physical elements that constitutes a computer system. Computer hardware is the physical parts or components of a computer, such as the monitor, keyboard, computer data storage, hard disk drive (HDD), graphic cards, sound cards, memory (RAM), motherboard, and so on, all of which are tangible physical objects.[1] By contrast, software is instructions that can be stored and run by hardware.
113) 29.988865, Computer music - Wikipedia, the free encyclopedia.txt#9, term: computer, content:Despite the ubiquity of computer music in contemporary culture, there is considerable activity in the field of computer music, as researchers continue to pursue new and interesting computer-based synthesis, composition, and performance approaches. Throughout the world there are many organizations and institutions dedicated to the area of computer and electronic music study and research, including the ICMA (International Computer Music Association), IRCAM, GRAME, SEAMUS (Society for Electro Acoustic Music in the United States), CEC (Canadian Electroacoustic Community), and a great number of institutions of higher learning around the world.
114) 29.988865, Image processing - Wikipedia, the free encyclopedia.txt#2, term: computer, content:Closely related to image processing are computer graphics and computer vision. In computer graphics, images are manually made from physical models of objects, environments, and lighting, instead of being acquired (via imaging devices such as cameras) from natural scenes, as in most animated movies. Computer vision, on the other hand, is often considered high-level image processing out of which a machine/computer/software intends to decipher the physical contents of an image or a sequence of images (e.g., videos or 3D full-body magnetic resonance scans).
115) 29.988865, Programming language - Wikipedia, the free encyclopedia.txt#5, term: computer, content:The term computer language is sometimes used interchangeably with programming language.[21] However, the usage of both terms varies among authors, including the exact scope of each. One usage describes programming languages as a subset of computer languages.[22] In this vein, languages used in computing that have a different goal than expressing computer programs are generically designated computer languages. For instance, markup languages are sometimes referred to as computer languages to emphasize that they are not meant to be used for programming.[23]
116) 29.03659, 3D computer graphics - Wikipedia, the free encyclopedia.txt#0, term: computer, content:3D computer graphics (in contrast to 2D computer graphics) are graphics that use a three-dimensional representation of geometric data (often Cartesian) that is stored in the computer for the purposes of performing calculations and rendering 2D images. Such images may be stored for viewing later or displayed in real-time.
117) 29.03659, Analog computer - Wikipedia, the free encyclopedia.txt#4, term: computer, content:Running an electronic analog computer, assuming a satisfactory setup, started with the computer held with some variables fixed at their initial values. Moving a switch released the holds and permitted the problem to run. In some instances, the computer could, after a certain running time interval, repeatedly return to the initial-conditions state to reset the problem, and run it again.
118) 29.03659, Computer - Simple English Wikipedia, the free encyclopedia.txt#4, term: computer, content:Modern computers are electronic machines. A computer is only useful if it has both hardware and software. Hardware is the physical parts the computer is made of - for example keyboard, mouse, screen, tower, and the circuits inside it. Software is the computer programs (mathematical instructions). The software uses the hardware, when the user gives it instructions and changing it in to useful output.
119) 29.03659, Computer - Wikipedia, the free encyclopedia.txt#49, term: computer, content:Machine languages and the assembly languages that represent them (collectively termed low-level programming languages) tend to be unique to a particular type of computer. For instance, an ARM architecture computer (such as may be found in a PDA or a hand-held videogame) cannot understand the machine language of an Intel Pentium or the AMD Athlon 64 computer that might be in a PC.[56]
120) 29.03659, Computer cluster - Wikipedia, the free encyclopedia.txt#16, term: computer, content:Computer clusters are used for computation-intensive purposes, rather than handling IO-oriented operations such as web service or databases.[14] For instance, a computer cluster might support computational simulations of vehicle crashes or weather. Very tightly coupled computer clusters are designed for work that may approach "supercomputing".
121) 29.03659, Computer data storage - Wikipedia, the free encyclopedia.txt#27, term: computer, content:When a computer needs to read information from the tertiary storage, it will first consult a catalog database to determine which tape or disc contains the information. Next, the computer will instruct a robotic arm to fetch the medium and place it in a drive. When the computer has finished reading the information, the robotic arm will return the medium to its place in the library.
122) 29.03659, Computer engineering - Wikipedia, the free encyclopedia.txt#13, term: computer, content:Engineers working in computer systems work on research projects that allow for reliable, secure, and high-performance computer systems. Projects such as designing processors for multi-threading and parallel processing are included in this field. Other examples of work in this field include development of new theories, algorithms, and other tools that add performance to computer systems.[9]
123) 29.03659, Computer graphics - Wikipedia, the free encyclopedia.txt#58, term: computer, content:The study of computer graphics is a sub-field of computer science which studies methods for digitally synthesizing and manipulating visual content. Although the term often refers to three-dimensional computer graphics, it also encompasses two-dimensional graphics and image processing.
124) 29.03659, Computer network - Wikipedia, the free encyclopedia.txt#0, term: computer, content:A computer network or data network is a telecommunications network which allows computers to exchange data. In computer networks, networked computing devices exchange data with each other using a data link. The connections between nodes are established using either cable media or wireless media. The best-known computer network is the Internet.
125) 29.03659, Computer science - Wikipedia, the free encyclopedia.txt#27, term: computer, content:Computer graphics is the study of digital visual contents, and involves synthesis and manipulation of image data. The study is connected to many other fields in computer science, including computer vision, image processing, and computational geometry, and is heavily applied in the fields of special effects and video games.
126) 29.03659, Database - Wikipedia, the free encyclopedia.txt#14, term: computer, content:Following the technology progress in the areas of processors, computer memory, computer storage and computer networks, the sizes, capabilities, and performance of databases and their respective DBMSs have grown in orders of magnitude. The development of database technology can be divided into three eras based on data model or structure: navigational,[9] SQL/relational, and post-relational.
127) 29.03659, Execution (computing) - Wikipedia, the free encyclopedia.txt#0, term: computer, content:Execution in computer and software engineering is the process by which a computer or a virtual machine performs the instructions of a computer program. The instructions in the program trigger sequences of simple actions on the executing machine. Those actions produce effects according to the semantics of the instructions in the program.
128) 29.03659, Home computer - Wikipedia, the free encyclopedia.txt#49, term: computer, content:In the U.S.A., an Apple II is a home computer; the IBM PC in its smaller configurations is a home computer; the Macintosh is a home computer. Home computers use floppy disks for mass storage and perform useful functions like word processing and income tax preparation as well as playing games.
129) 29.03659, Information technology - Wikipedia, the free encyclopedia.txt#1, term: computer, content:The term is commonly used as a synonym for computers and computer networks, but it also encompasses other information distribution technologies such as television and telephones. Several industries are associated with information technology, including computer hardware, software, electronics, semiconductors, internet, telecom equipment, engineering, healthcare, e-commerce and computer services.[4][a]
130) 29.03659, Operating system - Wikipedia, the free encyclopedia.txt#0, term: computer, content:An operating system (OS) is system software that manages computer hardware and software resources and provides common services for computer programs. The operating system is a component of the system software in a computer system. Application programs usually require an operating system to function.
131) 29.03659, Peripheral - Wikipedia, the free encyclopedia.txt#3, term: computer, content:Common input peripherals include keyboards, computer mice, graphic tablets, touchscreens, barcode readers, image scanners, microphones, webcams, game controllers, light pens, and digital cameras. Common output peripherals include computer displays, printers, projectors, and computer speakers.
132) 29.03659, Personal computer - Wikipedia, the free encyclopedia.txt#0, term: computer, content:A personal computer (PC) is a general-purpose computer whose size, capabilities, and original sale price make it useful for individuals, and is intended to be operated directly by an end-user with no intervening computer time-sharing models that allowed larger, more expensive minicomputer and mainframe systems to be used by many people, usually at the same time.
133) 29.03659, Personal digital assistant - Wikipedia, the free encyclopedia.txt#17, term: computer, content:Most PDAs can synchronize their data with applications on a user's computer. This allows the user to update contact, schedule, or other information on their computer, using software such as Microsoft Outlook or ACT!, and have that same data transferred to PDAor transfer updated information from the PDA back to the computer. This eliminates the need for the user to update their data in two places.
134) 29.03659, Quantum computing - Wikipedia, the free encyclopedia.txt#23, term: computer, content:There are a number of technical challenges in building a large-scale quantum computer, and thus far quantum computers have yet to solve a problem faster than a classical computer. David DiVincenzo, of IBM, listed the following requirements for a practical quantum computer:[23]
135) 29.03659, Software - Wikipedia, the free encyclopedia.txt#7, term: computer, content:This eventually led to the creation of the twin academic fields of computer science and software engineering, which both study software and its creation. Computer science is more theoretical (Turing's essay is an example of computer science), whereas software engineering focuses on more practical concerns.
136) 28.74476, Computer - Simple English Wikipedia, the free encyclopedia.txt#28, term: computer, content:There is another type of computer, called an embedded computer. An embedded computer is a computer that does one thing and one thing only, and usually does it very well. For example, an alarm clock is a embedded computer: it tells the time. Unlike your personal computer, you cannot use your clock to play Tetris. Because of this, we say that embedded computers cannot be programmed, because you cannot install programs like Tetris on your clock. Some mobile phones, automatic teller machines, microwave ovens, CD players and cars are examples of embedded computers.
137) 28.74476, Computer engineering - Wikipedia, the free encyclopedia.txt#0, term: computer, content:Computer engineering is a discipline that integrates several fields of electrical engineering and computer science required to develop computer hardware and software.[1] Computer engineers usually have training in electronic engineering (or electrical engineering), software design, and hardware-software integration instead of only software engineering or electronic engineering. Computer engineers are involved in many hardware and software aspects of computing, from the design of individual microcontrollers, microprocessors, personal computers, and supercomputers, to circuit design. This field of engineering not only focuses on how computer systems themselves work, but also how they integrate into the larger picture.[2]
138) 28.74476, Computing - Wikipedia, the free encyclopedia.txt#33, term: computer, content:Computer engineering is a discipline that integrates several fields of electrical engineering and computer science required to develop computer hardware and software.[7] Computer engineers usually have training in electronic engineering (or electrical engineering), software design, and hardware-software integration instead of only software engineering or electronic engineering. Computer engineers are involved in many hardware and software aspects of computing, from the design of individual microprocessors, personal computers, and supercomputers, to circuit design. This field of engineering not only focuses on how computer systems themselves work, but also how they integrate into the larger picture.[8]
139) 28.74476, Digital electronics - Wikipedia, the free encyclopedia.txt#50, term: computer, content:The most general-purpose register-transfer logic machine is a computer. This is basically an automatic binary abacus. The control unit of a computer is usually designed as a microprogram run by a microsequencer. A microprogram is much like a player-piano roll. Each table entry or "word" of the microprogram commands the state of every bit that controls the computer. The sequencer then counts, and the count addresses the memory or combinational logic machine that contains the microprogram. The bits from the microprogram control the arithmetic logic unit, memory and other parts of the computer, including the microsequencer itself.A "specialized computer" is usually a conventional computer with special-purpose control logic or microprogram.
140) 28.74476, Quantum computing - Wikipedia, the free encyclopedia.txt#6, term: computer, content:For example: Consider first a classical computer that operates on a three-bit register. The state of the computer at any time is a probability distribution over the      2  3   = 8   {\displaystyle 2^{3}=8}   different three-bit strings 000, 001, 010, 011, 100, 101, 110, 111. If it is a deterministic computer, then it is in exactly one of these states with probability 1. However, if it is a probabilistic computer, then there is a possibility of it being in any one of a number of different states. We can describe this probabilistic state by eight nonnegative numbers A,B,C,D,E,F,G,H (where A = is the probability that the computer is in state 000, B = is the probability that the computer is in state 001, etc.). There is a restriction that these probabilities sum to 1.
141) 28.449932, 3D computer graphics software - Wikipedia, the free encyclopedia.txt#0, term: computer, content:3D computer graphics software produces computer-generated imagery (CGI) through 3D modeling and 3D rendering or produces 3D models for analytic, scientific and industrial purposes.
142) 28.449932, Analog computer - Wikipedia, the free encyclopedia.txt#80, term: computer, content:Analog (audio) synthesizers can also be viewed as a form of analog computer, and their technology was originally based in part on electronic analog computer technology. The ARP 2600's Ring Modulator was actually a moderate-accuracy analog multiplier.
143) 28.449932, Boolean algebra - Wikipedia, the free encyclopedia.txt#91, term: computer, content:Boolean algebra as the calculus of two values is fundamental to computer circuits, computer programming, and mathematical logic, and is also used in other areas of mathematics such as set theory and statistics.[3]
144) 28.449932, Chemical computer - Wikipedia, the free encyclopedia.txt#5, term: computer, content:In 2015, Stanford University graduate students created a computer using magnetic fields and water droplets infused with magnetic nanoparticles, illustrating some of the basic principles behind a chemical computer.[8][9]
145) 28.449932, Cite This Page - Wikipedia, the free encyclopedia.txt#3, term: computer, content:Computer. (2016, June 10).  In Wikipedia, The Free Encyclopedia. Retrieved 12:18, June 22, 2016, from https://en.wikipedia.org/w/index.php?title=Computer&oldid=724624530 
146) 28.449932, Cite This Page - Wikipedia, the free encyclopedia.txt#5, term: computer, content:Wikipedia contributors, 'Computer',  Wikipedia, The Free Encyclopedia, 10 June 2016, 12:44 UTC, <https://en.wikipedia.org/w/index.php?title=Computer&oldid=724624530> [accessed 22 June 2016] 
147) 28.449932, Cite This Page - Wikipedia, the free encyclopedia.txt#7, term: computer, content:Wikipedia contributors. Computer [Internet].  Wikipedia, The Free Encyclopedia;  2016 Jun 10, 12:44 UTC [cited 2016 Jun 22].  Available from:  https://en.wikipedia.org/w/index.php?title=Computer&oldid=724624530. 
148) 28.449932, Cite This Page - Wikipedia, the free encyclopedia.txt#9, term: computer, content:Wikipedia contributors. Computer. Wikipedia, The Free Encyclopedia. June 10, 2016, 12:44 UTC. Available at: https://en.wikipedia.org/w/index.php?title=Computer&oldid=724624530. Accessed June 22, 2016. 
149) 28.449932, Computer - Simple English Wikipedia, the free encyclopedia.txt#7, term: computer, content:A computer is an electronic machine which helps in solving problems quickly and easily. It solves problems according to instructions given to it by the computer user called programs or software. It is a digital machine(that uses binary digits) used in all fields.
150) 28.449932, Computer - Wikipedia, the free encyclopedia.txt#0, term: computer, content:A computer is a general purpose device that can be programmed to carry out a set of arithmetic or logical operations automatically. Since a sequence of operations can be readily changed, the computer can solve more than one kind of problem.
151) 28.449932, Computer - Wikipedia, the free encyclopedia.txt#91, term: computer, content:A computer will solve problems in exactly the way it is programmed to, without regard to efficiency, alternative solutions, possible shortcuts, or possible errors in the code. Computer programs that learn and adapt are part of the emerging field of artificial intelligence and machine learning.
152) 28.449932, Computer cluster - Wikipedia, the free encyclopedia.txt#10, term: computer, content:The history of early computer clusters is more or less directly tied into the history of early networks, as one of the primary motivations for the development of a network was to link computing resources, creating a de facto computer cluster.
153) 28.449932, Computer data storage - Wikipedia, the free encyclopedia.txt#0, term: computer, content:Computer data storage, often called storage or memory, is a technology consisting of computer components and recording media used to retain digital data. It is a core function and fundamental component of computers.
154) 28.449932, Computer data storage - Wikipedia, the free encyclopedia.txt#48, term: computer, content:A secondary or tertiary storage may connect to a computer utilizing computer networks. This concept does not pertain to the primary storage, which is shared between multiple processors to a lesser degree.
155) 28.449932, Computer engineering - Wikipedia, the free encyclopedia.txt#17, term: computer, content:Computer engineers in this area develop improvements in humancomputer interaction, including speech recognition and synthesis, medical and scientific imaging, or communications systems. Other work in this area includes computer vision development such as recognition of human facial features.[9]
156) 28.449932, Computer graphics - Wikipedia, the free encyclopedia.txt#2, term: computer, content:The term computer graphics has been used a broad sense to describe "almost everything on computers that is not text or sound".[1] Typically, the term computer graphics refers to several different things:
157) 28.449932, Computer hardware - Wikipedia, the free encyclopedia.txt#5, term: computer, content:The personal computer, also known as the PC, is one of the most common types of computer due to its versatility and relatively low price. Laptops are generally very similar, although they may use lower-power or reduced size components.
158) 28.449932, Computer program - Wikipedia, the free encyclopedia.txt#19, term: computer, content:A computer program in the form of a human-readable, computer programming language is called source code. Source code may be converted into an executable image by a compiler or executed immediately with the aid of an interpreter.
159) 28.449932, Computer program - Wikipedia, the free encyclopedia.txt#23, term: computer, content:Just in time compilers pre-compile computer programs ahead of time and interpret them later. For example, Java computer programs are pre-compiled into a file containing bytecode. Bytecode is then executed by an interpreter called a virtual machine.
160) 28.449932, Computer science - Wikipedia, the free encyclopedia.txt#10, term: computer, content:A folkloric quotation, often attributed tobut almost certainly not first formulated byEdsger Dijkstra, states that "computer science is no more about computers than astronomy is about telescopes."[note 3] The design and deployment of computers and computer systems is generally considered the province of disciplines other than computer science. For example, the study of computer hardware is usually considered part of computer engineering, while the study of commercial computer systems and their deployment is often called information technology or information systems. However, there has been much cross-fertilization of ideas between the various computer-related disciplines. Computer science research also often intersects other disciplines, such as philosophy, cognitive science, linguistics, mathematics, physics, biology, statistics, and logic.
161) 28.449932, Computer security - Wikipedia, the free encyclopedia.txt#81, term: computer, content:Computer Emergency Response Team is a name given to expert groups that handle computer security incidents. In the US, two distinct organization exist, although they do work closely together.
162) 28.449932, Computing - Wikipedia, the free encyclopedia.txt#14, term: computer, content:The execution process carries out the instructions in a computer program. Instructions express the computations performed by the computer. They trigger sequences of simple actions on the executing machine. Those actions produce effects according to the semantics of the instructions.
163) 28.449932, Computing - Wikipedia, the free encyclopedia.txt#35, term: computer, content:Computer science or computing science (abbreviated CS or Comp Sci) is the scientific and practical approach to computation and its applications. A computer scientist specializes in the theory of computation and the design of computational systems.[16]
164) 28.449932, DARPA - Wikipedia, the free encyclopedia.txt#16, term: computer, content:The resulting "brain drain" is also credited with boosting the development of the fledgling personal computer industry. Some young computer scientists left the universities to startups and private research labs such as Xerox PARC.
165) 28.449932, Digital electronics - Wikipedia, the free encyclopedia.txt#6, term: computer, content:The Z3 was an electromechanical computer designed by Konrad Zuse, finished in 1941. It was the world's first working programmable, fully automatic digital computer.[4] Its operation was facilitated by the invention of the vacuum tube in 1904 by John Ambrose Fleming.
166) 28.449932, Digital electronics - Wikipedia, the free encyclopedia.txt#37, term: computer, content:For automated analysis, these representations have digital file formats that can be processed by computer programs. Most digital engineers are very careful to select computer programs ("tools") with compatible file formats.
167) 28.449932, E6B - Wikipedia, the free encyclopedia.txt#7, term: computer, content:Instructions for ratio calculations and wind problems are printed on either side of the computer for reference and are also found in a booklet sold with the computer. Also, many computers have Fahrenheit to Celsius conversion charts and various reference tables.
168) 28.449932, EDVAC - Wikipedia, the free encyclopedia.txt#0, term: computer, content:EDVAC (Electronic Discrete Variable Automatic Computer) was one of the earliest electronic computers. Unlike its predecessor the ENIAC, it was binary rather than decimal, and was a stored program computer.
169) 28.449932, ENIAC - Wikipedia, the free encyclopedia.txt#0, term: computer, content:ENIAC (/ini.k/ or /ni.k/; Electronic Numerical Integrator And Computer)[1][2] was the first electronic general-purpose computer. It was Turing-complete, digital, and could solve "a large class of numerical problems" through reprogramming.[3][4]
170) 28.449932, Floating point - Wikipedia, the free encyclopedia.txt#22, term: computer, content:The first commercial computer with floating-point hardware was Zuse's Z4 computer, designed in 19421945. In 1946, Bell Laboratories introduced the MarkV, which implements decimal floating-point numbers.[6]
171) 28.449932, Frederic Calland Williams - Wikipedia, the free encyclopedia.txt#3, term: computer, content:In 1946 he was appointed as head of the Electrical Engineering Department of the University of Manchester. There, with Tom Kilburn, he pioneered the first stored-program digital computer, the Manchester Mark 1 computer.[14]
172) 28.449932, Graphical user interface - Wikipedia, the free encyclopedia.txt#33, term: computer, content:Three-dimensional graphics are currently mostly used in computer games, art, and computer-aided design (CAD). A three-dimensional computing environment can also be useful in other uses, like molecular graphics and aircraft design.
173) 28.449932, Human–computer interaction - Wikipedia, the free encyclopedia.txt#34, term: computer, content:The humancomputer interface can be described as the point of communication between the human user and the computer. The flow of information between the human and computer is defined as the loop of interaction. The loop of interaction has several aspects to it, including:
174) 28.449932, IBM 7080 - Wikipedia, the free encyclopedia.txt#0, term: computer, content:The IBM 7080 was a variable word length BCD transistor computer in the IBM 700/7000 series commercial architecture line, introduced in August 1961, that provided an upgrade path from the vacuum tube IBM 705 computer.
175) 28.449932, Image scanner - Wikipedia, the free encyclopedia.txt#40, term: computer, content:Battery-powered portable scanners store scans on internal memory; they can later be transferred to a computer either by direct connection, typically USB, or in some cases a memory card may be removed from the scanner and plugged into the computer.
176) 28.449932, John Mauchly - Wikipedia, the free encyclopedia.txt#24, term: computer, content:Critics note that while the court said that the ABC was the first electronic digital computer, it did not define the term computer. It had originally referred to a person who computes, but was adapted to apply to a machine.
177) 28.449932, Logarithm - Wikipedia, the free encyclopedia.txt#84, term: computer, content:Analysis of algorithms is a branch of computer science that studies the performance of algorithms (computer programs solving a certain problem).[76] Logarithms are valuable for describing algorithms that divide a problem into smaller ones, and join the solutions of the subproblems.[77]
178) 28.449932, Mac OS - Wikipedia, the free encyclopedia.txt#38, term: computer, content:The Macintosh Application Environment (MAE) was a software package introduced by Apple Computer in 1994 which allowed users of certain Unix-based computer workstations to run Apple Macintosh application software.
179) 28.449932, MIPS instruction set - Wikipedia, the free encyclopedia.txt#57, term: computer, content:Among the manufacturers which have made computer workstation systems using MIPS processors are SGI, MIPS Computer Systems, Inc., Whitechapel Workstations, Olivetti, Siemens-Nixdorf, Acer, Digital Equipment Corporation, NEC, and DeskStation.
180) 28.449932, Operating system - Wikipedia, the free encyclopedia.txt#98, term: computer, content:Most of the modern computer systems support graphical user interfaces (GUI), and often include them. In some computer systems, such as the original implementation of Mac OS, the GUI is integrated into the kernel.
181) 28.449932, Personal computer - Wikipedia, the free encyclopedia.txt#57, term: computer, content:The keyboard and the mouse are external devices plugged into the computer through connectors on an I/O panel on the back of the computer case. The monitor is also connected to the I/O panel, either through an onboard port on the motherboard, or a port on the graphics card.
182) 28.449932, Programming language - Wikipedia, the free encyclopedia.txt#0, term: computer, content:A programming language is a formal computer language or constructed language designed to communicate instructions to a machine, particularly a computer. Programming languages can be used to create programs to control the behavior of a machine or to express algorithms.
183) 28.449932, Raster graphics editor - Wikipedia, the free encyclopedia.txt#0, term: computer, content:A raster graphics editor is a computer program that allows users to create and edit images interactively on the computer screen and save them in one of many popular "bitmap" or "raster" formats such as JPEG, PNG, GIF and TIFF.
184) 28.449932, Server (computing) - Wikipedia, the free encyclopedia.txt#9, term: computer, content:When referring to hardware, the word server typically designates computer models specialized for their role. In general, a server performs its role better than a generic personal computer.
185) 28.449932, Software bug - Wikipedia, the free encyclopedia.txt#7, term: computer, content:The term "bug" was used in an account by computer pioneer Grace Hopper, who publicized the cause of a malfunction in an early electromechanical computer.[8] A typical version of the story is given by this quote:[9]
186) 28.449932, Spreadsheet - Wikipedia, the free encyclopedia.txt#55, term: computer, content:When the computer calculates a formula in one cell to update the displayed value of that cell, cell reference(s) in that cell, naming some other cell(s), cause the computer to fetch the value of the named cell(s).
187) 28.449932, Stored-program computer - Wikipedia, the free encyclopedia.txt#1, term: computer, content:A computer with a Von Neumann architecture stores program data and instruction data in the same memory; a computer with a Harvard architecture has separate memories for storing program and data.[5][6] Both are stored-program designs.
188) 28.449932, Telecommunications engineering - Wikipedia, the free encyclopedia.txt#31, term: computer, content:A network engineer is a computer engineer who is in charge of designing, deploying and maintaining computer networks. In addition, they oversee network operations from a network operations center, designs backbone infrastructure, or supervises interconnections in a data center.
189) 28.449932, Vector graphics editor - Wikipedia, the free encyclopedia.txt#0, term: computer, content:A vector graphics editor is a computer program that allows users to compose and edit vector graphics images interactively on a computer and save them in one of many popular vector graphics formats, such as EPS, PDF, WMF, SVG, or VML.
190) 28.449932, Wearable computer - Wikipedia, the free encyclopedia.txt#9, term: computer, content:However, a computer is not merely a time-keeping or calculating device, but rather a user-programmable item for complex algorithms, interfacing, and data management. By this definition, the wearable computer was invented by Steve Mann, in the late 1970s:[8][9][10]
191) 26.822855, 3D computer graphics - Wikipedia, the free encyclopedia.txt#1, term: computer, content:3D computer graphics rely on many of the same algorithms as 2D computer vector graphics in the wire-frame model and 2D computer raster graphics in the final rendered display. In computer graphics software, the distinction between 2D and 3D is occasionally blurred; 2D applications may use 3D techniques to achieve effects such as lighting, and 3D may use 2D rendering techniques.
192) 26.822855, 3D computer graphics software - Wikipedia, the free encyclopedia.txt#8, term: computer, content:Use of real-time computer graphics engines to create a cinematic production is called machinima.
193) 26.822855, Alan Turing - Wikipedia, the free encyclopedia.txt#0, term: computer, content:Alan Mathison Turing OBE FRS (/tjr/; 23 June 1912 7 June 1954) was a pioneering English computer scientist, mathematician, logician, cryptanalyst and theoretical biologist. He was highly influential in the development of theoretical computer science, providing a formalisation of the concepts of algorithm and computation with the Turing machine, which can be considered a model of a general purpose computer.[2][3][4] Turing is widely considered to be the father of theoretical computer science and artificial intelligence.[5]
194) 26.822855, Alan Turing - Wikipedia, the free encyclopedia.txt#84, term: computer, content:At the University of Oxford, a new course in Computer Science and Philosophy was established to coincide with the centenary of Turing's birth.[198]
195) 26.822855, Algorithm - Wikipedia, the free encyclopedia.txt#31, term: computer, content:This means that the programmer must know a "language" that is effective relative to the target computing agent (computer/computor).
196) 26.822855, Analog computer - Wikipedia, the free encyclopedia.txt#18, term: computer, content:The FERMIAC was an analog computer invented by physicist Enrico Fermi in 1947 to aid in his studies of neutron transport.[12] Project Cyclone was an analog computer developed by Reeves in 1950 for the analysis and design of dynamic systems.[13] Project Typhoon was an analog computer developed by RCA in 1952. It consisted of over 4000 electron tubes and used 100 dials and 6000 plug-in connectors to program.[14] The MONIAC Computer was a hydraulic model of a national economy first unveiled in 1949.[citation needed]
197) 26.822855, Analog computer - Wikipedia, the free encyclopedia.txt#74, term: computer, content:In some analog computer designs, multiplication is much preferred to division. Division is carried out with a multiplier in the feedback path of an Operational Amplifier.
198) 26.822855, Antivirus software - Wikipedia, the free encyclopedia.txt#11, term: computer, content:In 1987, Fred Cohen wrote that there is no algorithm that can perfectly detect all possible computer viruses.[30]
199) 26.822855, Arithmetic logic unit - Wikipedia, the free encyclopedia.txt#15, term: computer, content:Mathematician John von Neumann proposed the ALU concept in 1945 in a report on the foundations for a new computer called the EDVAC.[1]
200) 26.822855, Assembly language - Wikipedia, the free encyclopedia.txt#15, term: computer, content:This binary computer code can be made more human-readable by expressing it in hexadecimal as follows.
201) 26.822855, Association for Computing Machinery - Wikipedia, the free encyclopedia.txt#27, term: computer, content:The ACM presents or copresents a number of awards for outstanding technical and professional achievements and contributions in computer science and information technology.[25]
202) 26.822855, Asynchronous circuit - Wikipedia, the free encyclopedia.txt#19, term: computer, content:The ILLIAC II was the first completely asynchronous, speed independent processor design ever built; it was the most powerful computer at the time.[16]
203) 26.822855, Atanasoff–Berry computer - Wikipedia, the free encyclopedia.txt#2, term: computer, content:Atanasoff and Berry's computer work was not widely known until it was rediscovered in the 1960s, amidst conflicting claims about the first instance of an electronic computer. At that time, the ENIAC was considered to be the first computer in the modern sense, but in 1973 a U.S. District Court invalidated the ENIAC patent and concluded that the ENIAC inventors had derived the subject matter of the electronic digital computer from Atanasoff (see Patent dispute).
204) 26.822855, Atanasoff–Berry computer - Wikipedia, the free encyclopedia.txt#6, term: computer, content:The machine was, however, the first to implement three critical ideas that are still part of every modern computer:
205) 26.822855, Bus (computing) - Wikipedia, the free encyclopedia.txt#9, term: computer, content:The external bus, or expansion bus, is made up of the electronic pathways that connect the different external devices, such as printer etc., to the computer.
206) 26.822855, Central processing unit - Wikipedia, the free encyclopedia.txt#0, term: computer, content:A central processing unit (CPU) is the electronic circuitry within a computer that carries out the instructions of a computer program by performing the basic arithmetic, logical, control and input/output (I/O) operations specified by the instructions. The term has been used in the computer industry at least since the early 1960s.[1] Traditionally, the term "CPU" refers to a processor, more specifically to its processing unit and control unit (CU), distinguishing these core elements of a computer from external components such as main memory and I/O circuitry.[2]
207) 26.822855, Cite This Page - Wikipedia, the free encyclopedia.txt#4, term: computer, content:Wikipedia contributors. "Computer." Wikipedia, The Free Encyclopedia. Wikipedia, The Free Encyclopedia, 10 Jun. 2016. Web. 22 Jun. 2016. 
208) 26.822855, Computer - Wikipedia, the free encyclopedia.txt#73, term: computer, content:I/O is the means by which a computer exchanges information with the outside world.[65] Devices that provide input or output to the computer are called peripherals.[66] On a typical personal computer, peripherals include input devices like the keyboard and mouse, and output devices such as the display and printer. Hard disk drives, floppy disk drives and optical disc drives serve as both input and output devices. Computer networking is another form of I/O.
209) 26.822855, Computer - Wikipedia, the free encyclopedia.txt#77, term: computer, content:Before the era of cheap computers, the principal use for multitasking was to allow many people to share the same computer.
210) 26.822855, Computer - Wikipedia, the free encyclopedia.txt#85, term: computer, content:Of all these abstract machines, a quantum computer holds the most promise for revolutionizing computing.[72]
211) 26.822855, Computer - Wikipedia, the free encyclopedia.txt#98, term: computer, content:The means through which computer gives output are known as output devices. Some examples of output devices are:
212) 26.822855, Computer animation - Wikipedia, the free encyclopedia.txt#9, term: computer, content:The very first full length computer animated television series was ReBoot, which debuted in September 1994; the series followed the adventures of characters who lived inside a computer.[8] The first feature-length computer animated film was Toy Story (1995), which was made by Pixar.[9][10][11] It followed an adventure centered around toys and their owners. This groundbreaking film was also the first of many fully computer-animated movies.[10]
213) 26.822855, Computer animation - Wikipedia, the free encyclopedia.txt#24, term: computer, content:CGI short films have been produced as independent animation since 1976,[34] although the popularity of computer animation (especially in the field of special effects) skyrocketed during the modern era of U.S. animation.[35] The first completely computer-animated television series was ReBoot in 1994,[8][36] and the first completely computer-animated movie was Toy Story (1995), but VeggieTales is the 1st American fully 3-D Computer Animated christian direct-to-video series that started all in 1993.
214) 26.822855, Computer animation - Wikipedia, the free encyclopedia.txt#32, term: computer, content:A few examples of computer-generated animation movies are Toy Story, Tangled, Frozen, Inside Out, Shrek, and Finding Nemo.
215) 26.822855, Computer architecture - Wikipedia, the free encyclopedia.txt#12, term: computer, content:The ISA of a computer is usually described in a small instruction manual, which describes how the instructions are encoded. Also, it may define short (vaguely) mnemonic names for the instructions. The names can be recognized by a software development tool called an assembler. An assembler is a computer program that translates a human-readable form of the ISA into a computer-readable form. Disassemblers are also widely available, usually in debuggers and software programs to isolate and correct malfunctions in binary computer programs.
216) 26.822855, Computer cluster - Wikipedia, the free encyclopedia.txt#13, term: computer, content:Within the same time frame, while computer clusters used parallelism outside the computer on a commodity network, supercomputers began to use them within the same computer. Following the success of the CDC 6600 in 1964, the Cray 1 was delivered in 1976, and introduced internal parallelism via vector processing.[12] While early supercomputers excluded clusters and relied on shared memory, in time some of the fastest supercomputers (e.g. the K computer) relied on cluster architectures.
217) 26.822855, Computer memory - Wikipedia, the free encyclopedia.txt#0, term: computer, content:In computing, memory refers to the computer hardware devices used to store information for immediate use in a computer; it is synonymous with the term "primary storage". Computer memory operates at a high speed, for example random-access memory (RAM), as a distinction from storage that provides slow-to-access program and data storage but offers higher capacities. If needed, contents of the computer memory can be transferred to secondary storage, through a memory management technique called "virtual memory". An archaic synonym for memory is store.[1]
218) 26.822855, Computer monitor - Wikipedia, the free encyclopedia.txt#9, term: computer, content:TFT-LCD is a variant of LCD which is now the dominant technology used for computer monitors.[3]
219) 26.822855, Computer monitor - Wikipedia, the free encyclopedia.txt#36, term: computer, content:For Computer Monitors, the VESA Mount typically consists of four threaded holes on the rear of the display that will mate with an adapter bracket.
220) 26.822855, Computer monitor - Wikipedia, the free encyclopedia.txt#37, term: computer, content:Rack mount computer monitors are available in two styles and are intended to be mounted into a 19-inch rack:
221) 26.822855, Computer music - Wikipedia, the free encyclopedia.txt#10, term: computer, content:Computer-generated music is music composed by, or with the extensive aid of, a computer. Although any music which uses computers in its composition or realisation is computer-generated to some extent, the use of computers is now so widespread (in the editing of pop songs, for instance) that the phrase computer-generated music is generally used to mean a kind of music which could not have been created without the use of computers.[citation needed]
222) 26.822855, Computer network - Wikipedia, the free encyclopedia.txt#64, term: computer, content:A Metropolitan area network (MAN) is a large computer network that usually spans a city or a large campus.
223) 26.822855, Computer network - Wikipedia, the free encyclopedia.txt#73, term: computer, content:An internetwork is the connection of multiple computer networks via a common routing technology using routers.
224) 26.822855, Computer network - Wikipedia, the free encyclopedia.txt#96, term: computer, content:Computer and network surveillance programs are widespread today, and almost all Internet traffic is or could potentially be monitored for clues to illegal activity.
225) 26.822855, Computer program - Wikipedia, the free encyclopedia.txt#15, term: computer, content:Computer programs can be categorized by the programming language paradigm used to produce them. Two of the main paradigms are imperative and declarative.
226) 26.822855, Computer science - Wikipedia, the free encyclopedia.txt#25, term: computer, content:Computer architecture, or digital computer organization, is the conceptual design and fundamental operational structure of a computer system. It focuses largely on the way by which the central processing unit performs internally and accesses addresses in memory.[46] The field often involves disciplines of computer engineering and electrical engineering, selecting and interconnecting hardware components to create computers that meet functional, performance, and cost goals.
227) 26.822855, Computer science - Wikipedia, the free encyclopedia.txt#34, term: computer, content:The philosopher of computing Bill Rapaport noted three Great Insights of Computer Science:[48]
228) 26.822855, Computer science - Wikipedia, the free encyclopedia.txt#37, term: computer, content:In most countries, there is a significant gender gap in computer science education. For example, in the US about 20% of computer science degrees in 2012 were conferred to women.[58] This gender gap also exists in other Western countries.[59] However, in some parts of the world, the gap is small or nonexistent. In 2011, approximately half of all computer science degrees in Malaysia were conferred to women.[60] In 2001, women made up 54.5% of computer science graduates in Guyana.[59]
229) 26.822855, Computer security - Wikipedia, the free encyclopedia.txt#4, term: computer, content:To secure a computer system, it is important to understand the attacks that can be made against it, and these threats can typically be classified into one of the categories below:
230) 26.822855, Computer security - Wikipedia, the free encyclopedia.txt#84, term: computer, content:Here are the main computer emergency response teams around the world. Most countries have their own team to protect network security.
231) 26.822855, Computer simulation - Wikipedia, the free encyclopedia.txt#2, term: computer, content:Because of the computational cost of simulation, computer experiments are used to perform inference such as uncertainty quantification.[6]
232) 26.822855, Computer simulation - Wikipedia, the free encyclopedia.txt#16, term: computer, content:Generic examples of types of computer simulations in science, which are derived from an underlying mathematical description:
233) 26.822855, Computer Technology Limited - Wikipedia, the free encyclopedia.txt#5, term: computer, content:The various building blocks could be assembled and configured to produce a fault-tolerant computer system.[1]
234) 26.822855, Computer-aided design - Wikipedia, the free encyclopedia.txt#5, term: computer, content:The design of geometric models for object shapes, in particular, is occasionally called computer-aided geometric design (CAGD).[7]
235) 26.822855, Control flow - Wikipedia, the free encyclopedia.txt#13, term: computer, content:Goto statements have been considered harmful by many computer scientists, notably Dijkstra.
236) 26.822855, CSIRAC - Wikipedia, the free encyclopedia.txt#0, term: computer, content:CSIRAC (/sark/; Council for Scientific and Industrial Research Automatic Computer), originally known as CSIR Mk 1, was Australia's first digital computer, and the fifth stored program computer in the world.[1] It is the oldest surviving first-generation electronic computer[2] (The Zuse Z4 at the Deutsches Museum is older, but was electro-mechanical, not electronic), and was the first in the world to play digital music.[3][4]
237) 26.822855, Digital electronics - Wikipedia, the free encyclopedia.txt#51, term: computer, content:In this way, the complex task of designing the controls of a computer is reduced to a simpler task of programming a collection of much simpler logic machines.
238) 26.822855, Digital electronics - Wikipedia, the free encyclopedia.txt#62, term: computer, content:It is common for the function tables of such computer-generated state-machines to be optimized with logic-minimization software such as Minilog.
239) 26.822855, DOS - Wikipedia, the free encyclopedia.txt#0, term: computer, content:DOS /ds/, short for disk operating system,[1] is an acronym for several computer operating systems that are operated by using the command line.
240) 26.822855, E6B - Wikipedia, the free encyclopedia.txt#0, term: computer, content:The E6B flight computer, or simply the "whiz wheel", is a form of circular slide rule used in aviation.
241) 26.822855, E6B - Wikipedia, the free encyclopedia.txt#13, term: computer, content:Wind Correction Angle, in degrees, as it might be programmed into a computer (which includes conversion of degrees to radians and back):
242) 26.822855, Electronic engineering - Wikipedia, the free encyclopedia.txt#20, term: computer, content:Computer engineering deals with the design of computers and computer systems. This may involve the design of new computer hardware, the design of PDAs or the use of computers to control an industrial plant. Development of embedded systemssystems made for specific tasks (e.g., mobile phones)is also included in this field. This field includes the micro controller and its applications. Computer engineers may also work on a system's software. However, the design of complex software systems is often the domain of software engineering, which is usually considered a separate discipline.
243) 26.822855, Exploit (computer security) - Wikipedia, the free encyclopedia.txt#5, term: computer, content:Typically, the proxy or VPN applications enabling pivoting are executed on the target computer as the payload (software) of an exploit.
244) 26.822855, Ferranti Pegasus - Wikipedia, the free encyclopedia.txt#0, term: computer, content:Pegasus was an early vacuum tube (valve) computer built by Ferranti, Ltd of Great Britain.[1][2]
245) 26.822855, Flash memory - Wikipedia, the free encyclopedia.txt#0, term: computer, content:Flash memory is an electronic (solid-state) non-volatile computer storage medium that can be electrically erased and reprogrammed.
246) 26.822855, FLOPS - Wikipedia, the free encyclopedia.txt#26, term: computer, content:In 2008, James Bamford's book The Shadow Factory reported that NSA told the Pentagon it would need an exaflop computer by 2018.[41]
247) 26.822855, Grace Hopper - Wikipedia, the free encyclopedia.txt#26, term: computer, content:A named professorship in the Department of Computer Sciences was established at Yale University in her honor. Joan Feigenbaum was named to this chair in 2008.[43]
248) 26.822855, Graphics tablet - Wikipedia, the free encyclopedia.txt#2, term: computer, content:Some tablets are intended as a replacement for the computer mouse as the primary pointing and navigation device for desktop computers.
249) 26.822855, Hang (computing) - Wikipedia, the free encyclopedia.txt#13, term: computer, content:On embedded devices where human interaction is limited, a watchdog timer can reboot the computer in the event of a hang.
250) 26.822855, Harvard Mark II - Wikipedia, the free encyclopedia.txt#0, term: computer, content:The Harvard Mark II was an electromechanical computer built at under the direction of Howard Aiken and was finished in 1947. It was financed by the United States Navy.
251) 26.822855, Hertz - Wikipedia, the free encyclopedia.txt#11, term: computer, content:Various computer buses, such as the front-side bus connecting the CPU and northbridge, also operate at various frequencies in the megahertz range.
252) 26.822855, History of computing hardware - Wikipedia, the free encyclopedia.txt#31, term: computer, content:A fully electronic general-purpose analog computer was built by Helmut Hlzer in 1941 at Peenemnde Army Research Center .[46]
253) 26.822855, History of computing hardware - Wikipedia, the free encyclopedia.txt#62, term: computer, content:Although the computer was considered "small and primitive" by the standards of its time, it was the first working machine to contain all of the elements essential to a modern electronic computer.[81] As soon as the SSEM had demonstrated the feasibility of its design, a project was initiated at the university to develop it into a more usable computer, the Manchester Mark 1. The Mark 1 in turn quickly became the prototype for the Ferranti Mark 1, the world's first commercially available general-purpose computer.[82]
254) 26.822855, Honeywell, Inc. v. Sperry Rand Corp. - Wikipedia, the free encyclopedia.txt#8, term: computer, content:Finding 3 was the most controversial, as it assigned the invention of the electronic digital computer by judicial fiat to John V. Atanasoff:
255) 26.822855, Human computer - Wikipedia, the free encyclopedia.txt#1, term: computer, content:The first time the term "Computer" appeared in The New York Times was February 3, 1853; an obituary stated:
256) 26.822855, Human computer - Wikipedia, the free encyclopedia.txt#7, term: computer, content:Human computers were used to predict the effects of building the Afsluitdijk in the Zuiderzee. The computer simulation was set up by Hendrik Lorentz.[4]
257) 26.822855, IBM 701 - Wikipedia, the free encyclopedia.txt#8, term: computer, content:The 701 can claim to be the first computer displaying the potential of artificial intelligence in Arthur Samuel's Checkers-playing Program.[5]
258) 26.822855, Image scanner - Wikipedia, the free encyclopedia.txt#37, term: computer, content:Scanners communicate to their host computer using one of the following physical interfaces, listing roughly from slow to fast:
259) 26.822855, Information system - Wikipedia, the free encyclopedia.txt#18, term: computer, content:A computer-based information system, following a definition of Langefors,[22] is a technologically implemented medium for:
260) 26.822855, Information technology - Wikipedia, the free encyclopedia.txt#22, term: computer, content:Employment in the computer systems and design related services industry, in thousands, 1990-2011[42]
261) 26.822855, Information technology - Wikipedia, the free encyclopedia.txt#23, term: computer, content:Occupational growth and wages in computer systems design and related services, 2010-2020[42]
262) 26.822855, Information technology - Wikipedia, the free encyclopedia.txt#24, term: computer, content:Projected percent change in employment in selected occupations in computer systems design and related services, 2010-2020[42]
263) 26.822855, J. Presper Eckert - Wikipedia, the free encyclopedia.txt#0, term: computer, content:John Adam Presper "Pres" Eckert, Jr. (April 9, 1919  June 3, 1995) was an American electrical engineer and computer pioneer. With John Mauchly he invented the first general-purpose electronic digital computer (ENIAC), presented the first course in computing topics (the Moore School Lectures), founded the EckertMauchly Computer Corporation, and designed the first commercial computer in the U.S., the UNIVAC, which incorporated Eckert's invention of the mercury delay line memory.
264) 26.822855, Jack Kilby - Wikipedia, the free encyclopedia.txt#16, term: computer, content:The Jack Kilby Computer Centre at the Merchiston Campus of Edinburgh Napier University in Edinburgh is also named in his honor.[12]
265) 26.822855, John Mauchly - Wikipedia, the free encyclopedia.txt#1, term: computer, content:Together they started the first computer company, the Eckert-Mauchly Computer Corporation (EMCC), and pioneered fundamental computer concepts including the stored program, subroutines, and programming languages. Their work, as exposed in the widely read First Draft of a Report on the EDVAC (1945) and as taught in the Moore School Lectures (1946), influenced an explosion of computer development in the late 1940s all over the world.
266) 26.822855, John Mauchly - Wikipedia, the free encyclopedia.txt#17, term: computer, content:In 1947 Eckert and Mauchly formed the first computer company, the Eckert-Mauchly Computer Corporation (EMCC); Mauchly was president. They secured a contract with the National Bureau of Standards to build an "EDVAC II", later named UNIVAC. UNIVAC, the first computer designed for business applications, had many significant technical advantages such as magnetic tape for mass storage. As an interim product, the company created and delivered a smaller computer, BINAC, but were still in a shaky financial situation. They were purchased by Remington Rand and became the UNIVAC division.
267) 26.822855, Laptop - Wikipedia, the free encyclopedia.txt#4, term: computer, content:As the personal computer (PC) became feasible in 1971, the idea of a portable personal computer soon followed. A "personal, portable information manipulator" was imagined by Alan Kay at Xerox PARC in 1968,[5] and described in his 1972 paper as the "Dynabook".[6] The IBM Special Computer APL Machine Portable (SCAMP) was demonstrated in 1973. This prototype was based on the IBM PALM processor.[7] The IBM 5100, the first commercially available portable computer, appeared in September 1975, and was based on the SCAMP prototype.[8]
268) 26.822855, LEO (computer) - Wikipedia, the free encyclopedia.txt#0, term: computer, content:The LEO I (Lyons Electronic Office I) was the first computer used for commercial business applications.
269) 26.822855, List of programming languages by type - Wikipedia, the free encyclopedia.txt#19, term: computer, content:An esoteric programming language is a programming language designed as a test of the boundaries of computer programming language design, as a proof of concept, or as a joke.
270) 26.822855, Mac OS - Wikipedia, the free encyclopedia.txt#1, term: computer, content:Mac OS is a series of graphical user interfacebased operating systems developed by Apple Inc. for their Macintosh line of computer systems.
271) 26.822855, MIPS instruction set - Wikipedia, the free encyclopedia.txt#94, term: computer, content:MARS[42] is another GUI-based MIPS emulator designed for use in education, specifically for use with Hennessy's Computer Organization and Design.
272) 26.822855, MOS Technology 6502 - Wikipedia, the free encyclopedia.txt#19, term: computer, content:In the 1980s a popular electronics magazine Elektor/Elektuur used the processor in its microprocessor development board Junior Computer.
273) 26.822855, Negation - Wikipedia, the free encyclopedia.txt#25, term: computer, content:This convention occasionally surfaces in written speech, as computer-related slang for not. The phrase !voting, for example, means "not voting".
274) 26.822855, Netbook - Wikipedia, the free encyclopedia.txt#49, term: computer, content:Another NPD study indicated that by September 2009 netbooks accounted for 20% of all portable computer shipments.[125]
275) 26.822855, Operating system - Wikipedia, the free encyclopedia.txt#70, term: computer, content:"Virtual memory" provides the programmer or the user with the perception that there is a much larger amount of RAM in the computer than is really there.[30]
276) 26.822855, Perl - Wikipedia, the free encyclopedia.txt#39, term: computer, content:The design of Perl can be understood as a response to three broad trends in the computer industry: falling hardware costs, rising labor costs, and improvements in compiler technology. Many earlier computer languages, such as Fortran and C, aimed to make efficient use of expensive computer hardware. In contrast, Perl was designed so that computer programmers could write programs more quickly and easily.
277) 26.822855, Personal computer - Wikipedia, the free encyclopedia.txt#55, term: computer, content:Mass-market consumer computers use highly standardized components and so are simple for an end user to assemble into a working system. A typical desktop computer consists of a computer case that holds the power supply, motherboard, hard disk drive, and often an optical disc drive. External devices such as a computer monitor or visual display unit, keyboard, and a pointing device are usually found in a personal computer.
278) 26.822855, Personal computer - Wikipedia, the free encyclopedia.txt#89, term: computer, content:Computer software is any kind of computer program, procedure, or documentation that performs some task on a computer system.[67] The term includes application software such as word processors that perform productive tasks for users, system software such as operating systems that interface with computer hardware to provide the necessary services for application software, and middleware that controls and co-ordinates distributed systems.
279) 26.822855, Python (programming language) - Wikipedia, the free encyclopedia.txt#69, term: computer, content:The Raspberry Pi single-board computer project has adopted Python as its main user-programming language.
280) 26.822855, Quantum computing - Wikipedia, the free encyclopedia.txt#33, term: computer, content:In 2001, researchers demonstrated Shor's algorithm to factor 15 using a 7-qubit NMR computer.[42]
281) 26.822855, Register machine - Wikipedia, the free encyclopedia.txt#2, term: computer, content:There are at least 4 sub-classes found in literature, here listed from most primitive to the most like a computer:
282) 26.822855, Register machine - Wikipedia, the free encyclopedia.txt#11, term: computer, content:Wang expressed hope that his model would be "a rapprochement" (p.63) between the theory of Turing machines and the practical world of the computer.
283) 26.822855, RS-232 - Wikipedia, the free encyclopedia.txt#34, term: computer, content:The Ring Indicator signal is used by some older uninterruptible power supplies (UPSs) to signal a power failure state to the computer.
284) 26.822855, RS-232 - Wikipedia, the free encyclopedia.txt#35, term: computer, content:Certain personal computers can be configured for wake-on-ring, allowing a computer that is suspended to answer a phone call.
285) 26.822855, SCSI - Wikipedia, the free encyclopedia.txt#5, term: computer, content:The "small" part in SCSI is historical; since the mid-1990s, SCSI has been available on even the largest of computer systems.
286) 26.822855, Stan Frankel - Wikipedia, the free encyclopedia.txt#9, term: computer, content:Frankel, Stanley P, A Logic Design for a Microwave Computer, IRE Transactions on Electronic Computers, September 1959, p 271-276.
287) 26.822855, Stored-program computer - Wikipedia, the free encyclopedia.txt#7, term: computer, content:Several computers could be considered the first stored-program computer, depending on the criteria.[22]
288) 26.822855, Telecommunication - Wikipedia, the free encyclopedia.txt#55, term: computer, content:The Internet is a worldwide network of computers and computer networks that communicate with each other using the Internet Protocol.[79] Any computer on the Internet has a unique IP address that can be used by other computers to route information to it. Hence, any computer on the Internet can send a message to any other computer using its IP address. These messages carry with them the originating computer's IP address allowing for two-way communication. The Internet is thus an exchange of messages between computers.[80]
289) 26.822855, Texas Instruments - Wikipedia, the free encyclopedia.txt#52, term: computer, content:In 2007, TI released the TI-Nspire family of calculators, as well as computer software that has similar capabilities to the calculators.
290) 26.822855, Tommy Flowers - Wikipedia, the free encyclopedia.txt#13, term: computer, content:In 1993, he received a certificate from Hendon College, having completed a basic course in information processing on a personal computer.[17]
291) 26.822855, Torpedo Data Computer - Wikipedia, the free encyclopedia.txt#11, term: computer, content:In 1943, the Torpedo Data Computer Mark IV was developed to support the Mark 18 torpedo.[26][27]
292) 26.822855, Transistor computer - Wikipedia, the free encyclopedia.txt#8, term: computer, content:In Italy, Olivetti's first commercial fully transistorized computer was the Olivetti Elea 9003, being sold from 1959.[16]
293) 26.822855, Wearable computer - Wikipedia, the free encyclopedia.txt#36, term: computer, content:The 6th-generation iPod Nano, released in September 2010, has a wristband attachment available to convert it into a wearable wristwatch computer.
294) 26.822855, Web browser - Wikipedia, the free encyclopedia.txt#31, term: computer, content:A browser extension is a computer program that extends the functionality of a web browser. Every major web browser supports the development of browser extensions.
295) 26.822855, Webcam - Wikipedia, the free encyclopedia.txt#0, term: computer, content:A webcam is a video camera that feeds or streams its image in real time to or through a computer to computer network. When "captured" by the computer, the video stream may be saved, viewed or sent on to other networks via systems such as the internet, and email as an attachment. When sent to a remote location, the video stream may be saved, viewed or on sent there. Unlike an IP camera (which connects using Ethernet or Wi-Fi), a webcam is generally connected by a USB cable, or similar cable, or built into computer hardware, such as laptops.
296) 26.822855, Webcam - Wikipedia, the free encyclopedia.txt#11, term: computer, content:Webcams can be used as security cameras. Software is available to allow PC-connected cameras to watch for movement and sound,[6] recording both when they are detected. These recordings can then be saved to the computer, e-mailed, or uploaded to the Internet. In one well-publicised case,[7] a computer e-mailed images of the burglar during the theft of the computer, enabling the owner to give police a clear picture of the burglar's face even after the computer had been stolen.
297) 26.822855, Webcam - Wikipedia, the free encyclopedia.txt#22, term: computer, content:In 2010, Time Magazine named the QuickCam as one of the top computer devices of all time.[13]
298) 26.822855, Word processor - Wikipedia, the free encyclopedia.txt#0, term: computer, content:A word processor is an electronic device or computer software application, that performs the task of composition, editing, formatting, printing of documents.
299) 26.822855, Z3 (computer) - Wikipedia, the free encyclopedia.txt#2, term: computer, content:Thanks to this machine and its predecessors, Konrad Zuse is often regarded as the inventor of the computer.[9][10][11][12]
300) 26.822855, Zilog Z8000 - Wikipedia, the free encyclopedia.txt#9, term: computer, content:Z8000-based computer systems included Zilog's own System 8000 series, as well as other manufacturers:
301) 26.612474, Computer engineering - Wikipedia, the free encyclopedia.txt#3, term: computer, content:The first computer engineering degree program in the United States was established at Case Western Reserve University in 1972. As of 2015[update], there were 238 ABET-accredited computer engineering programs in the US.[6] In Europe, accreditation of computer engineering schools is done by a variety of agencies part of the EQANIE network. Due to increasing job requirements for engineers who can concurrently design hardware, software, firmware, and manage all forms of computer systems used in industry, some tertiary institutions around the world offer a bachelor's degree generally called computer engineering. Both computer engineering and electronic engineering programs include analog and digital circuit design in their curriculum. As with most engineering disciplines, having a sound knowledge of mathematics and science is necessary for computer engineers.
302) 26.612474, Computer engineering - Wikipedia, the free encyclopedia.txt#6, term: computer, content:Most computer hardware engineers research, develop, design, and test various computer equipment. This can range from circuit boards and microprocessors to routers. Some update existing computer equipment to be more efficient and work with newer software. Most computer hardware engineers work in research laboratories and high-tech manufacturing firms. Some also work for the federal government. According to BLS, 95% of computer hardware engineers work in metropolitan areas.[citation needed] They generally work full-time. Approximately 33% of their work requires more than 40 hours a week. The median salary for employed qualified computer hardware engineers (2012) was $100,920 per year or $48.52 per hour. Computer hardware engineers held 83,300 jobs in 2012 in the USA.[8]
303) 26.612474, Computer science - Wikipedia, the free encyclopedia.txt#4, term: computer, content:During the 1940s, as new and more powerful computing machines were developed, the term computer came to refer to the machines rather than their human predecessors.[9] As it became clear that computers could be used for more than just mathematical calculations, the field of computer science broadened to study computation in general. Computer science began to be established as a distinct academic discipline in the 1950s and early 1960s.[10][11] The world's first computer science degree program, the Cambridge Diploma in Computer Science, began at the University of Cambridge Computer Laboratory in 1953. The first computer science degree program in the United States was formed at Purdue University in 1962.[12] Since practical computers became available, many applications of computing have become distinct areas of study in their own rights.
304) 26.612474, Computer science - Wikipedia, the free encyclopedia.txt#15, term: computer, content:As a discipline, computer science spans a range of topics from theoretical studies of algorithms and the limits of computation to the practical issues of implementing computing systems in hardware and software.[41][42] CSAB, formerly called Computing Sciences Accreditation Boardwhich is made up of representatives of the Association for Computing Machinery (ACM), and the IEEE Computer Society (IEEE CS)[43]identifies four areas that it considers crucial to the discipline of computer science: theory of computation, algorithms and data structures, programming methodology and languages, and computer elements and architecture. In addition to these four areas, CSAB also identifies fields such as software engineering, artificial intelligence, computer networking and communication, database systems, parallel computation, distributed computation, humancomputer interaction, computer graphics, operating systems, and numerical and symbolic computation as being important areas of computer science.[41]
305) 26.240257, Analytical Engine - Wikipedia, the free encyclopedia.txt#0, term: computer, content:The Analytical Engine was a proposed mechanical general-purpose computer designed by English mathematician and computer pioneer Charles Babbage.[2][3] It was first described in 1837 as the successor to Babbage's difference engine, a design for a mechanical computer.[4] The Analytical Engine incorporated an arithmetic logic unit, control flow in the form of conditional branching and loops, and integrated memory, making it the first design for a general-purpose computer that could be described in modern terms as Turing-complete.[5][6] In other words, the logical structure of the Analytical Engine was essentially the same as that which has dominated computer design in the electronic era.[3]
306) 26.240257, BASIC - Wikipedia, the free encyclopedia.txt#9, term: computer, content:During this period a number of simple computer games were written in BASIC, most notably Mike Mayfield's Star Trek. A number of these were collected by DEC employee David H. Ahl and published in a newsletter he compiled. He later collected a number of these into book form, 101 BASIC Computer Games, published in 1973.[7][8] During the same period, Ahl was involved in the creation of a small computer for education use, an early personal computer. When management refused to support the concept, Ahl left DEC in 1974 to found the seminal computer magazine, Creative Computing. The book remained popular, and was re-published on several occasions.[9]
307) 26.240257, Computer graphics - Wikipedia, the free encyclopedia.txt#1, term: computer, content:Important topics in computer graphics include user interface design, sprite graphics, vector graphics, 3D modeling, shaders, GPU design, and computer vision, among others. The overall methodology depends heavily on the underlying sciences of geometry, optics, and physics. Computer graphics is responsible for displaying art and image data effectively and beautifully to the user, and processing image data received from the physical world. The interaction and understanding of computers and interpretation of data has been made easier because of computer graphics. Computer graphic development has had a significant impact on many types of media and has revolutionized animation, movies, advertising, video games, and graphic design generally.
308) 26.240257, Computer music - Wikipedia, the free encyclopedia.txt#5, term: computer, content:In Japan, experiments in computer music date back to 1962, when Keio University professor Sekine and Toshiba engineer Hayashi experimented with the TOSBAC computer. This resulted in a piece entitled TOSBAC Suite, influenced by the Illiac Suite. Later Japanese computer music compositions include a piece by Kenjiro Ezaki presented during Osaka Expo '70 and "Panoramic Sonore" (1974) by music critic Akimichi Takeda. Ezaki also published an article called "Contemporary Music and Computers" in 1970. Since then, Japanese research in computer music has largely been carried out for commercial purposes in popular music, though some of the more serious Japanese musicians used large computer systems such as the Fairlight in the 1970s.[5]
309) 26.240257, Computer science - Wikipedia, the free encyclopedia.txt#14, term: computer, content:A number of computer scientists have argued for the distinction of three separate paradigms in computer science. Peter Wegner argued that those paradigms are science, technology, and mathematics.[38] Peter Denning's working group argued that they are theory, abstraction (modeling), and design.[39] Amnon H. Eden described them as the "rationalist paradigm" (which treats computer science as a branch of mathematics, which is prevalent in theoretical computer science, and mainly employs deductive reasoning), the "technocratic paradigm" (which might be found in engineering approaches, most prominently in software engineering), and the "scientific paradigm" (which approaches computer-related artifacts from the empirical perspective of natural sciences, identifiable in some branches of artificial intelligence).[40]
310) 26.240257, Electrical engineering - Wikipedia, the free encyclopedia.txt#37, term: computer, content:Computer engineering deals with the design of computers and computer systems. This may involve the design of new hardware, the design of PDAs, tablets, and supercomputers, or the use of computers to control an industrial plant.[46] Computer engineers may also work on a system's software. However, the design of complex software systems is often the domain of software engineering, which is usually considered a separate discipline.[47] Desktop computers represent a tiny fraction of the devices a computer engineer might work on, as computer-like architectures are now found in a range of devices including video game consoles and DVD players.
311) 26.240257, Jack Dongarra - Wikipedia, the free encyclopedia.txt#0, term: computer, content:Jack J. Dongarra (born July 18, 1950) is an American University Distinguished Professor of Computer Science in the Electrical Engineering and Computer Science Department[10] at the University of Tennessee. He holds the position of a Distinguished Research Staff member in the Computer Science and Mathematics Division at Oak Ridge National Laboratory, and is an Adjunct Professor in the Computer Science Department at Rice University. Dongarra holds the Turing Fellowship in the schools of Computer Science and Mathematics at the University of Manchester. He is the founding director of Innovative Computing Laboratory.[11][12][1][13][14][15]
312) 26.240257, Laptop - Wikipedia, the free encyclopedia.txt#0, term: computer, content:A laptop, often called a notebook, or notebook computer, is a portable personal computer with a "clamshell" form factor, with a keyboard on the lower part of the "clamshell" and a thin LCD/LED computer screen on the upper portion, which is opened up to use the computer. Laptops are folded shut for transportation, and thus are suitable for mobile use.[1] Although originally there was a distinction between laptops and notebooks, the former being bigger and heavier than the latter, as of 2014, there is often no longer any difference.[2] Laptops are commonly used in a variety of settings, such as at work, in education, and for personal multimedia and home computer use.
313) 26.240257, Manchester Small-Scale Experimental Machine - Wikipedia, the free encyclopedia.txt#1, term: computer, content:The machine was not intended to be a practical computer but was instead designed as a testbed for the Williams tube, an early form of computer memory. Although considered "small and primitive" by the standards of its time, it was the first working machine to contain all the elements essential to a modern electronic computer.[2] As soon as the SSEM had demonstrated the feasibility of its design, a project was initiated at the university to develop it into a more usable computer, the Manchester Mark 1. The Mark 1 in turn quickly became the prototype for the Ferranti Mark 1, the world's first commercially available general-purpose computer.[3]
314) 26.240257, Personal computer - Wikipedia, the free encyclopedia.txt#13, term: computer, content:In 1976 Steve Jobs and Steve Wozniak sold the Apple I computer circuit board, which was fully prepared and contained about 30 chips. The Apple I computer differed from the other hobby computers of the time at the beckoning of Paul Terrell owner of the Byte Shop who gave Steve Jobs his first purchase order for 50 Apple I computers only if the computers were assembled and tested and not a kit computer so he would have computers to sell to everyone, not just people that could assemble a computer kit. The Apple I as delivered was still a kit computer as it did not have a power supply, case, or keyboard as delivered to the Byte Shop.
315) 26.240257, Stan Frankel - Wikipedia, the free encyclopedia.txt#1, term: computer, content:After losing his security clearance (and thus his job) during the red scare of the early 1950s, Frankel became an independent computer consultant. He was responsible for designing the CONAC computer for the Continental Oil Company during 19541957 and the LGP-30 single-user desk computer in 1956, which was licensed from a computer he designed at Caltech called MINAC. The LGP-30 was moderately successful, selling over 500 units. He served as a consultant to Packard Bell Computer on the design of the PB-250. His last computing project was the SCM Marchant Cogito 240SR electronic calculator introduced in 1965.
316) 26.240257, Tom Kilburn - Wikipedia, the free encyclopedia.txt#13, term: computer, content:Kilburn received the IEEE Computer Society W. Wallace McDowell Award in 1971 "for his achievement in designing and building some of the first  as well as some of the most powerful  computers in the world",[20] the British Computer Society IT Award in 1973,[21] the Royal Medal of the Royal Society, in 1978,[22] the IEEE Computer Society Computer Pioneer Award in 1982,[23] the Eckert-Mauchly Award in 1983,[24] and the Mountbatten Medal. 1997.[25] A building at the University of Manchester, which houses the School of Computer Science, is named "The Kilburn Building" in his honour.[26] His nomination for the Royal Society read:
317) 26.240257, Transistor computer - Wikipedia, the free encyclopedia.txt#7, term: computer, content:The Philco Transac models S-1000 scientific computer and S-2000 electronic data processing computer, were the first commercially produced large-scale all transistor computers, which were introduced in 1957. The Philco computer name "Transac" stands for Transistor-Automatic-Computer. Both of these Philco computer models, used the surface-barrier transistor in its circuitry designs, which was the world's first high-frequency transistor that was suitable for high-speed computers.[12][13][14] The surface-barrier transistor was developed by Philco in 1953.[15]
318) 24.638367, Computer science - Wikipedia, the free encyclopedia.txt#36, term: computer, content:Since computer science is a relatively new field, it is not as widely taught in schools and universities as other academic subjects. For example, in 2014, Code.org estimated that only 10 percent of high schools in the United States offered computer science education.[52] A 2010 report by Association for Computing Machinery (ACM) and Computer Science Teachers Association (CSTA) revealed that only 14 out of 50 states have adopted significant education standards for high school computer science.[53] However, computer science education is growing. Some countries, such as Israel, New Zealand and South Korea, have already included computer science in their respective national secondary education curriculum.[54][55] Several countries are following suit.[56][57]
319) 24.638367, Computing - Wikipedia, the free encyclopedia.txt#15, term: computer, content:Computer software or just "software", is a collection of computer programs and related data that provides the instructions for telling a computer what to do and how to do it. Software refers to one or more computer programs and data held in the storage of the computer for some purposes. In other words, software is a set of programs, procedures, algorithms and its documentation concerned with the operation of a data processing system. Program software performs the function of the program it implements, either by directly providing instructions to the computer hardware or by serving as input to another piece of software. The term was coined to contrast with the old term hardware (meaning physical devices). In contrast to hardware, software is intangible.[5] Software is also sometimes used in a more narrow sense, meaning application software only.
320) 24.638367, Computing - Wikipedia, the free encyclopedia.txt#30, term: computer, content:A programmer, computer programmer, or coder is a person who writes computer software. The term computer programmer can refer to a specialist in one area of computer programming or to a generalist who writes code for many kinds of software. One who practices or professes a formal approach to programming may also be known as a programmer analyst. A programmer's primary computer language (C, C++, Java, Lisp, Python, etc.) is often prefixed to the above titles, and those who work in a web environment often prefix their titles with web. The term programmer can be used to refer to a software developer, software engineer, computer scientist, or software analyst. However, members of these professions typically[citation needed] possess other software engineering skills, beyond programming.
321) 24.638367, Human–computer interaction - Wikipedia, the free encyclopedia.txt#1, term: computer, content:As a field of research, human-computer interaction is situated at the intersection of computer science, behavioral sciences, design, media studies, and several other fields of study. The term was popularized by Stuart K. Card and Allen Newell of Carnegie Mellon University and Thomas P. Moran of IBM Research in their seminal 1983 book, The Psychology of Human-Computer Interaction, although the authors first used the term in 1980[1] and the first known use was in 1975.[2] The term connotes that, unlike other tools with only limited uses (such as a hammer, useful for driving nails, but not much else), a computer has many uses and this takes place as an open-ended dialog between the user and the computer. The notion of dialog likens human-computer interaction to human-to-human interaction, an analogy the discussion of which is crucial to theoretical considerations in the field.[3][4]
322) 23.708277, 86-DOS - Wikipedia, the free encyclopedia.txt#0, term: computer, content:86-DOS was an operating system developed and marketed by Seattle Computer Products (SCP) for its Intel 8086-based computer kit. Initially known as QDOS (Quick and Dirty Operating System) the name was changed to 86-DOS once SCP started licensing the operating system in 1980.
323) 23.708277, Abacus - Wikipedia, the free encyclopedia.txt#37, term: computer, content:The binary abacus is used to explain how computers manipulate numbers.[52] The abacus shows how numbers, letters, and signs can be stored in a binary system on a computer, or via ASCII. The device consists of a series of beads on parallel wires arranged in three separate rows. The beads represent a switch on the computer in either an 'on' or 'off' position.
324) 23.708277, Analog computer - Wikipedia, the free encyclopedia.txt#19, term: computer, content:Computer Engineering Associates was spun out of Caltech in 1950 to provide commercial services using the "Direct Analogy Electric Analog Computer" ("the largest and most impressive general-purpose analyzer facility for the solution of field problems") developed there by Gilbert D. McCann, Charles H. Wilts, and Bart Locanthi.[15][16]
325) 23.708277, Analog computer - Wikipedia, the free encyclopedia.txt#31, term: computer, content:In the 1960s, the major manufacturer was Electronic Associates of Princeton, New Jersey, with its 231R Analog Computer (vacuum tubes, 20 integrators) and subsequently its 8800 Analog Computer (solid state operational amplifiers, 64 integrators). Its challenger was Applied Dynamics of Ann Arbor, Michigan.
326) 23.708277, Analog computer - Wikipedia, the free encyclopedia.txt#42, term: computer, content:Online, there is a remarkably clear illustrated reference (OP 1140) that describes[19] the fire control computer mechanisms. For adding and subtracting, precision miter-gear differentials were in common use in some computers; the Ford Instrument Mark I Fire Control Computer contained about 160 of them.
327) 23.708277, Association for Computing Machinery - Wikipedia, the free encyclopedia.txt#4, term: computer, content:ACM also sponsors other computer science related events such as the worldwide ACM International Collegiate Programming Contest (ICPC), and has sponsored some other events such as the chess match between Garry Kasparov and the IBM Deep Blue computer.
328) 23.708277, BASIC - Wikipedia, the free encyclopedia.txt#5, term: computer, content:Time-sharing allowed multiple remote interactive users to share use of the computer, interacting with the computer from terminals with keyboards and teletype printers, and later display screens, in much the same way as desktop computers or personal computers would be used later.
329) 23.708277, British Computer Society - Wikipedia, the free encyclopedia.txt#6, term: computer, content:The forerunner of BCS was the "London Computer Group" (LCG), founded in 1956. BCS was formed a year later from the merger of the LCG and an unincorporated association of scientists into an unincorporated club. In October 1957, BCS was incorporated, by Articles of Association, as "The British Computer Society Ltd": the first President of BCS was Sir Maurice Wilkes (19132010), FRS.
330) 23.708277, Bus (computing) - Wikipedia, the free encyclopedia.txt#0, term: computer, content:In computer architecture, a bus or buss[1] is a communication system that transfers data between components inside a computer, or between computers. This expression covers all related hardware components (wire, optical fiber, etc.) and software, including communication protocols.[2]
331) 23.708277, Bus (computing) - Wikipedia, the free encyclopedia.txt#14, term: computer, content:Early computer buses were bundles of wire that attached computer memory and peripherals. Anecdotally termed the "digit trunk",[4] they were named after electrical power buses, or busbars. Almost always, there was one bus for memory, and one or more separate buses for peripherals. These were accessed by separate instructions, with completely different timings and protocols.
332) 23.708277, Central processing unit - Wikipedia, the free encyclopedia.txt#3, term: computer, content:Computers such as the ENIAC had to be physically rewired to perform different tasks, which caused these machines to be called "fixed-program computers".[4] Since the term "CPU" is generally defined as a device for software (computer program) execution, the earliest devices that could rightly be called CPUs came with the advent of the stored-program computer.
333) 23.708277, Charles Babbage - Wikipedia, the free encyclopedia.txt#1, term: computer, content:Considered by some to be a "father of the computer",[2] Babbage, along with Lovelace, is credited with inventing the first mechanical computer that eventually led to more complex designs. His varied work in other fields has led him to be described as "pre-eminent" among the many polymaths of his century.[1]
334) 23.708277, Clock rate - Wikipedia, the free encyclopedia.txt#4, term: computer, content:The first electromechanical general purpose computer, the Z3 operated at a frequency of about 510Hz. The first electronic general purpose computer, the ENIAC, used a 100 kHz clock in its cycling unit. As each instruction took 20 cycles, it had an instruction rate of 5 kHz.
335) 23.708277, COBOL - Wikipedia, the free encyclopedia.txt#24, term: computer, content:Meanwhile, RCA and Sperry Rand worked on creating COBOL compilers. The first COBOL program ran on 17 August on an RCA 501.[59] On December 6 and 7, the same COBOL program (albeit with minor changes) ran on an RCA computer and a Remington-Rand Univac computer, demonstrating that compatibility could be achieved.[60]
336) 23.708277, Compiler - Wikipedia, the free encyclopedia.txt#0, term: computer, content:A compiler is a computer program (or a set of programs) that transforms source code written in a programming language (the source language) into another computer language (the target language), with the latter often having a binary form known as object code.[1] The most common reason for converting source code is to create an executable program.
337) 23.708277, Computer - Simple English Wikipedia, the free encyclopedia.txt#33, term: computer, content:A computer is now almost always an electronic device. It usually contains materials that will become toxic waste when disposed of. When a new computer is bought in some places, laws require that the cost of its waste management must also be paid for. This is called product stewardship.
338) 23.708277, Computer - Wikipedia, the free encyclopedia.txt#16, term: computer, content:The first modern analog computer was a tide-predicting machine, invented by Sir William Thomson in 1872. The differential analyser, a mechanical analog computer designed to solve differential equations by integration using wheel-and-disc mechanisms, was conceptualized in 1876 by James Thomson, the brother of the more famous Lord Kelvin.[13]
339) 23.708277, Computer - Wikipedia, the free encyclopedia.txt#19, term: computer, content:By 1938 the United States Navy had developed an electromechanical analog computer small enough to use aboard a submarine. This was the Torpedo Data Computer, which used trigonometry to solve the problem of firing a torpedo at a moving target. During World War II similar devices were developed in other countries as well.
340) 23.708277, Computer - Wikipedia, the free encyclopedia.txt#54, term: computer, content:Admiral Grace Hopper, an American computer scientist and developer of the first compiler, is credited for having first used the term "bugs" in computing after a dead moth was found shorting a relay in the Harvard Mark II computer in September 1947.[59]
341) 23.708277, Computer - Wikipedia, the free encyclopedia.txt#61, term: computer, content:The sequence of operations that the control unit goes through to process an instruction is in itself like a short computer program, and indeed, in some more complex CPU designs, there is another yet smaller computer called a microsequencer, which runs a microcode program that causes all of these events to happen.
342) 23.708277, Computer - Wikipedia, the free encyclopedia.txt#75, term: computer, content:While a computer may be viewed as running one gigantic program stored in its main memory, in some systems it is necessary to give the appearance of running several programs simultaneously. This is achieved by multitasking i.e. having the computer switch rapidly between running each program in turn.[67]
343) 23.708277, Computer - Wikipedia, the free encyclopedia.txt#82, term: computer, content:In the 1970s, computer engineers at research institutions throughout the United States began to link their computers together using telecommunications technology. The effort was funded by ARPA (now DARPA), and the computer network that resulted was called the ARPANET.[71] The technologies that made the Arpanet possible spread and evolved.
344) 23.708277, Computer animation - Wikipedia, the free encyclopedia.txt#13, term: computer, content:In contrast, a newer method called motion capture makes use of live action footage.[15] When computer animation is driven by motion capture, a real performer acts out the scene as if they were the character to be animated. His/her motion is recorded to a computer using video cameras and markers and that performance is then applied to the animated character.
345) 23.708277, Computer animation - Wikipedia, the free encyclopedia.txt#21, term: computer, content:Realism in computer animation can mean making each frame look photorealistic, in the sense that the scene is rendered to resemble a photograph or make the characters' animation believable and lifelike.[26] Computer animation can also be realistic with or without the photorealistic rendering.[27]
346) 23.708277, Computer cluster - Wikipedia, the free encyclopedia.txt#0, term: computer, content:A computer cluster consists of a set of loosely or tightly connected computers that work together so that, in many respects, they can be viewed as a single system. Unlike grid computers, computer clusters have each node set to perform the same task, controlled and scheduled by software.[1][bettersourceneeded]
347) 23.708277, Computer engineering - Wikipedia, the free encyclopedia.txt#14, term: computer, content:In this specialty, computer engineers focus on developing visual sensing technology to sense an environment, representation of an environment, and manipulation of the environment. The gathered three-dimensional information is then implemented to perform a variety of tasks. These include, improved human modeling, image communication, and human-computer interfaces, as well as devices such as special-purpose cameras with versatile vision sensors.[9]
348) 23.708277, Computer graphics - Wikipedia, the free encyclopedia.txt#12, term: computer, content:It was not long before major corporations started taking an interest in computer graphics. TRW, Lockheed-Georgia, General Electric and Sperry Rand are among the many companies that were getting started in computer graphics by the mid-1960s. IBM was quick to respond to this interest by releasing the IBM 2250 graphics terminal, the first commercially available graphics computer. Ralph Baer, a supervising engineer at Sanders Associates, came up with a home video game in 1966 that was later licensed to Magnavox and called the Odyssey. While very simplistic, and requiring fairly inexpensive electronic parts, it allowed the player to move points of light around on a screen. It was the first consumer computer graphics product. David C. Evans was director of engineering at Bendix Corporation's computer division from 1953 to 1962, after which he worked for the next five years as a visiting professor at Berkeley. There he continued his interest in computers and how they interfaced with people. In 1966, the University of Utah recruited Evans to form a computer science program, and computer graphics quickly became his primary interest. This new department would become the world's primary research center for computer graphics.
349) 23.708277, Computer graphics - Wikipedia, the free encyclopedia.txt#59, term: computer, content:As an academic discipline, computer graphics studies the manipulation of visual and geometric information using computational techniques. It focuses on the mathematical and computational foundations of image generation and processing rather than purely aesthetic issues. Computer graphics is often differentiated from the field of visualization, although the two fields have many similarities.
350) 23.708277, Computer hardware - Wikipedia, the free encyclopedia.txt#8, term: computer, content:The motherboard is the main component of a computer. It is a large rectangular board with integrated circuitry that connects the other parts of the computer including the CPU, the RAM, the disk drives (CD, DVD, hard disk, or any others) as well as any peripherals connected via the ports or the expansion slots.
351) 23.708277, Computer hardware - Wikipedia, the free encyclopedia.txt#10, term: computer, content:An expansion card in computing is a printed circuit board that can be inserted into an expansion slot of a computer motherboard or backplane to add functionality to a computer system via the expansion bus. Expansions cards can be used to obtain or expand on features not offered by the motherboard.
352) 23.708277, Computer keyboard - Wikipedia, the free encyclopedia.txt#4, term: computer, content:While typewriters are the definitive ancestor of all key-based text entry devices, the computer keyboard as a device for electromechanical data entry and communication derives largely from the utility of two devices: teleprinters (or teletypes) and keypunches. It was through such devices that modern computer keyboards inherited their layouts.
353) 23.708277, Computer memory - Wikipedia, the free encyclopedia.txt#18, term: computer, content:This approach has its pitfalls. If the location specified is incorrect, this will cause the computer to write the data to some other part of the program. The results of an error like this are unpredictable. In some cases, the incorrect data might overwrite memory used by the operating system. Computer crackers can take advantage of this to create viruses and malware.
354) 23.708277, Computer music - Wikipedia, the free encyclopedia.txt#11, term: computer, content:We can distinguish two groups of computer-generated music: music in which a computer generated the score, which could be performed by humans, and music which is both composed and performed by computers. There is a large genre of music that is organized, synthesized, and created on computers.[citation needed]
355) 23.708277, Computer network - Wikipedia, the free encyclopedia.txt#19, term: computer, content:A network interface controller (NIC) is computer hardware that provides a computer with the ability to access the transmission media, and has the ability to process low-level network information. For example, the NIC may have a connector for accepting a cable, or an aerial for wireless transmission and reception, and the associated circuitry.
356) 23.708277, Computer program - Wikipedia, the free encyclopedia.txt#13, term: computer, content:Computer programming is the process of writing or editing source code. Editing source code involves testing, analyzing, refining, and sometimes coordinating with other programmers on a jointly developed program. A person who practices this skill is referred to as a computer programmer, software developer, and sometimes coder.
357) 23.708277, Computer program - Wikipedia, the free encyclopedia.txt#37, term: computer, content:Independent of the host computer, a hardware device might have embedded firmware to control its operation. Firmware is used when the computer program is rarely or never expected to change, or when the program must not be lost when the power is off.[24]
358) 23.708277, Computer science - Wikipedia, the free encyclopedia.txt#13, term: computer, content:The academic, political, and funding aspects of computer science tend to depend on whether a department formed with a mathematical emphasis or with an engineering emphasis. Computer science departments with a mathematics emphasis and with a numerical orientation consider alignment with computational science. Both types of departments tend to make efforts to bridge the field educationally if not across all research.
359) 23.708277, Computer science - Wikipedia, the free encyclopedia.txt#21, term: computer, content:Programming language theory is a branch of computer science that deals with the design, implementation, analysis, characterization, and classification of programming languages and their individual features. It falls within the discipline of computer science, both depending on and affecting mathematics, software engineering, and linguistics. It is an active research area, with numerous dedicated academic journals.
360) 23.708277, Computer security - Wikipedia, the free encyclopedia.txt#77, term: computer, content:In the criminal division of the United States Department of Justice operates a section called the Computer Crime and Intellectual Property Section. The CCIPS is in charge of investigating computer crime and intellectual property crime and is specialized in the search and seizure of digital evidence in computers and networks.[107]
361) 23.708277, Computer simulation - Wikipedia, the free encyclopedia.txt#3, term: computer, content:A computer model is the algorithms and equations used to capture the behavior of the system being modeled. By contrast, computer simulation is the actual running of the program that contains these equations or algorithms. Simulation, therefore, is the process of running a model. Thus one would not "build a simulation"; instead, one would "build a model", and then either "run the model" or equivalently "run a simulation".
362) 23.708277, Computer-aided design - Wikipedia, the free encyclopedia.txt#1, term: computer, content:Its use in designing electronic systems is known as electronic design automation, or EDA. In mechanical design it is known as mechanical design automation (MDA) or computer-aided drafting (CAD), which includes the process of creating a technical drawing with the use of computer software.[4]
363) 23.708277, Computing - Wikipedia, the free encyclopedia.txt#19, term: computer, content:System software, or systems software, is computer software designed to operate and control the computer hardware and to provide a platform for running application software. System software includes operating systems, utility software, device drivers, window systems, and firmware. Frequently development tools such as compilers, linkers, and debuggers are classified as system software.
364) 23.708277, Cray - Wikipedia, the free encyclopedia.txt#18, term: computer, content:SGI set up a separate Cray Research Business Unit in August 1999 in preparation for detachment. On March 2, 2000, the unit was sold to Tera Computer Company. Tera Computer Company was then renamed Cray Inc. when the deal closed on April 4.
365) 23.708277, Desktop publishing - Wikipedia, the free encyclopedia.txt#11, term: computer, content:There are two types of pages in desktop publishing, electronic pages and virtual paper pages to be printed on physical paper pages. All computerized documents are technically electronic, which are limited in size only by computer memory or computer data storage space.
366) 23.708277, Digital Equipment Corporation - Wikipedia, the free encyclopedia.txt#0, term: computer, content:Digital Equipment Corporation, also known as DEC[1] and using the trademark Digital, was a major American company in the computer industry from the 1960s to the 1990s. It was a leading vendor of computer systems, including computers, software, and peripherals, and its PDP and successor VAX products were the most successful of all minicomputers in terms of sales.
367) 23.708277, Digital Equipment Corporation - Wikipedia, the free encyclopedia.txt#44, term: computer, content:The evolution of the PDP-11 followed earlier systems, eventually including a single-user deskside personal computer form, the MicroPDP-11. In total, around 600,000 PDP-11s of all models were sold. and a wide variety of third-party peripheral vendors had also entered the computer product ecosystem.
368) 23.708277, E6B - Wikipedia, the free encyclopedia.txt#5, term: computer, content:Many airspeed indicator (ASI) instruments have a movable ring built into the face of the instrument that is essentially a subset of the flight computer. Just like on the flight computer, the ring is aligned with the air temperature and the pressure altitude, allowing the true airspeed (TAS) to be read at the needle.
369) 23.708277, Graphical user interface - Wikipedia, the free encyclopedia.txt#8, term: computer, content:A series of elements conforming a visual language have evolved to represent information stored in computers. This makes it easier for people with few computer skills to work with and use computer software. The most common combination of such elements in GUIs is the windows, icons, menus, pointer (WIMP) paradigm, especially in personal computers.
370) 23.708277, Hang (computing) - Wikipedia, the free encyclopedia.txt#7, term: computer, content:Hardware can cause a computer to hang, either because it is intermittent or because it is mismatched with other hardware in the computer[2] (this can occur when one makes an upgrade). Hardware can also become defective over time due to dirt or heat damage.
371) 23.708277, History of computing hardware - Wikipedia, the free encyclopedia.txt#57, term: computer, content:The theoretical basis for the stored-program computer had been laid by Alan Turing in his 1936 paper. In 1945 Turing joined the National Physical Laboratory and began work on developing an electronic stored-program digital computer. His 1945 report Proposed Electronic Calculator was the first specification for such a device.
372) 23.708277, Home computer - Wikipedia, the free encyclopedia.txt#5, term: computer, content:As early as 1965, some experimental projects such as Jim Sutherland's ECHO IV(hr) explored the possible utility of a computer in the home.[12][13] In 1969, the Honeywell Kitchen Computer was marketed as a luxury gift item, and would have inaugurated the era of home computing, but none were sold.[14]
373) 23.708277, IBM 701 - Wikipedia, the free encyclopedia.txt#0, term: computer, content:The IBM 701, known as the Defense Calculator while in development, was announced to the public on April 29, 1952, and was IBMs first commercial scientific computer.[1] Its business computer siblings were the IBM 702 and IBM 650. It was based on the IAS machine.[2]
374) 23.708277, IBM PC compatible - Wikipedia, the free encyclopedia.txt#1, term: computer, content:Early IBM PC compatibles used the same computer bus as the original PC and AT models. The IBM AT compatible bus was later named the Industry Standard Architecture bus by manufacturers of compatible computers. The term "IBM PC compatible" is now a historical description only, since IBM has ended its personal computer sales.
375) 23.708277, Image scanner - Wikipedia, the free encyclopedia.txt#35, term: computer, content:Scans must virtually always be transferred from the scanner to a computer or information storage system for further processing or storage. There are two basic issues: (1) how the scanner is physically connected to the computer and (2) how the application retrieves the information from the scanner.
376) 23.708277, Imperative programming - Wikipedia, the free encyclopedia.txt#0, term: computer, content:In computer science, imperative programming is a programming paradigm that uses statements that change a program's state. In much the same way that the imperative mood in natural languages expresses commands, an imperative program consists of commands for the computer to perform. Imperative programming focuses on describing how a program operates.
377) 23.708277, Information system - Wikipedia, the free encyclopedia.txt#1, term: computer, content:A computer information system is a system composed of people and computers that processes or interprets information.[1][2][3][4] The term is also sometimes used in more restricted senses to refer to only the software used to run a computerized database or to refer to only a computer system.
378) 23.708277, Installation (computer programs) - Wikipedia, the free encyclopedia.txt#2, term: computer, content:Some computer programs can be executed by simply copying them into a folder stored on a computer and executing them. Other programs are supplied in a form unsuitable for immediate execution and therefore need an installation procedure. Once installed, the program can be executed again and again, without the need to reinstall before each execution.
379) 23.708277, Installation (computer programs) - Wikipedia, the free encyclopedia.txt#15, term: computer, content:An installation program or installer is a computer program that installs files, such as applications, drivers, or other software, onto a computer. Some installers are specifically made to install the files they contain; other installers are general-purpose and work by reading the contents of the software package to be installed.
380) 23.708277, Instruction set - Wikipedia, the free encyclopedia.txt#6, term: computer, content:Theoretically important types are the minimal instruction set computer and the one instruction set computer, but these are not implemented in commercial processors. Another variation is the very long instruction word (VLIW) where the processor receives many instructions encoded and retrieved in one instruction word.
381) 23.708277, Intel - Wikipedia, the free encyclopedia.txt#106, term: computer, content:Intel has become one of the world's most recognizable computer brands following its long-running Intel Inside campaign. The idea for "Intel Inside" came out of a meeting between Intel and one of the major computer resellers, MicroAge.[218]
382) 23.708277, Interactive fiction - Wikipedia, the free encyclopedia.txt#23, term: computer, content:In June 1977, Marc Blank, Bruce K. Daniels, Tim Anderson, and Dave Lebling began writing the mainframe version of Zork (also known as Dungeon), at the MIT Laboratory for Computer Science. The game was programmed in a computer language called MDL, a variant of LISP.
383) 23.708277, Internet - Wikipedia, the free encyclopedia.txt#7, term: computer, content:Historically, as early as 1849, the word internetted was used uncapitalized as an adjective, meaning Interconnected or interwoven.[11] The designers of early computer networks used internet both as a noun and as a verb in shorthand form of internetwork or internetworking, meaning interconnecting computer networks.[12]
384) 23.708277, Interpreter (computing) - Wikipedia, the free encyclopedia.txt#0, term: computer, content:In computer science, an interpreter is a computer program that directly executes, i.e. performs, instructions written in a programming or scripting language, without previously compiling them into a machine language program. An interpreter generally uses one of the following strategies for program execution:
385) 23.708277, John Mauchly - Wikipedia, the free encyclopedia.txt#0, term: computer, content:John William Mauchly (August 30, 1907 January 8, 1980) was an American physicist who, along with J. Presper Eckert, designed ENIAC, the first general purpose electronic digital computer, as well as EDVAC, BINAC and UNIVAC I, the first commercial computer made in the United States.
386) 23.708277, Kermit (protocol) - Wikipedia, the free encyclopedia.txt#0, term: computer, content:Kermit is a computer file transfer/management protocol and a set of communications software tools primarily used in the early years of personal computing in the 1980s; it provides a consistent approach to file transfer, terminal emulation, script programming, and character set conversion across many different computer hardware and OS platforms.
387) 23.708277, Library (computing) - Wikipedia, the free encyclopedia.txt#0, term: computer, content:In computer science, a library is a collection of non-volatile resources used by computer programs, often to develop software. These may include configuration data, documentation, help data, message templates, pre-written code and subroutines, classes, values or type specifications. In IBM's OS/360 and its successors they are referred to as partitioned data sets.
388) 23.708277, Linux - Wikipedia, the free encyclopedia.txt#84, term: computer, content:The Ubuntu derivatives Edubuntu and The Linux Schools Project, as well as the Debian derivative Skolelinux, provide education-oriented software packages. They also include tools for administering and building school computer labs and computer-based classrooms, such as the Linux Terminal Server Project (LTSP).
389) 23.708277, Machine code - Wikipedia, the free encyclopedia.txt#14, term: computer, content:Using a microcode layer to implement an emulator enables the computer to present the architecture of an entirely different computer. The System/360 line used this to allow porting programs from earlier IBM machines to the new family of computers, e.g. an IBM 1401/1440/1460 emulator on the IBM S/360 model 40.
390) 23.708277, Matrix (mathematics) - Wikipedia, the free encyclopedia.txt#71, term: computer, content:Although most computer languages are not designed with commands or libraries for matrices, as early as the 1970s, some engineering desktop computers such as the HP 9830 had ROM cartridges to add BASIC commands for matrices. Some computer languages such as APL were designed to manipulate matrices, and various mathematical programs can be used to aid computing with matrices.[48]
391) 23.708277, Microcode - Wikipedia, the free encyclopedia.txt#35, term: computer, content:A few computers were built using "writable microcode". In this design, rather than storing the microcode in ROM or hard-wired logic, the microcode is stored in a RAM called a Writable Control Store or WCS. Such a computer is sometimes called a Writable Instruction Set Computer or WISC.[19]
392) 23.708277, Microcomputer - Wikipedia, the free encyclopedia.txt#17, term: computer, content:In 1979, the launch of the VisiCalc spreadsheet (initially for the Apple II) first turned the microcomputer from a hobby for computer enthusiasts into a business tool. After the 1981 release by IBM of its IBM PC, the term personal computer became generally used for microcomputers compatible with the IBM PC architecture (PC compatible).
393) 23.708277, MIPS instruction set - Wikipedia, the free encyclopedia.txt#60, term: computer, content:Through the 1990s, the MIPS architecture was widely adopted by the embedded market, including for use in computer networking, telecommunications, video arcade games, video game consoles, computer printers, digital set-top boxes, digital televisions, DSL and cable modems, and personal digital assistants.
394) 23.708277, Object Pascal - Wikipedia, the free encyclopedia.txt#1, term: computer, content:Object Pascal is an extension of the Pascal language that was developed at Apple Computer by a team led by Larry Tesler in consultation with Niklaus Wirth, the inventor of Pascal. It is descended from an earlier object-oriented version of Pascal called Clascal, which was available on the Lisa computer.
395) 23.708277, Operating system - Wikipedia, the free encyclopedia.txt#71, term: computer, content:Multitasking refers to the running of multiple independent computer programs on the same computer; giving the appearance that it is performing the tasks at the same time. Since most computers can do at most one or two things at one time, this is generally done via time-sharing, which means that each program uses a share of the computer's time to execute.
396) 23.708277, Package manager - Wikipedia, the free encyclopedia.txt#2, term: computer, content:A software package is an archive file containing a computer program as well as necessary metadata for its deployment. The computer program can be in source code that has to be compiled and built first.[2] Package metadata include package description, package version, and dependencies (other packages that need to be installed beforehand).
397) 23.708277, Personal computer - Wikipedia, the free encyclopedia.txt#4, term: computer, content:The Programma 101 was the first commercial "desktop personal computer", produced by the Italian company Olivetti and invented by the Italian engineer Pier Giorgio Perotto, inventor of the magnetic card system. The project started in 1962. It was launched at the 1964 New York World's Fair, and volume production began in 1965, the computer retailing for $3,200.[3][unreliable source?]
398) 23.708277, Personal computer - Wikipedia, the free encyclopedia.txt#54, term: computer, content:Computer hardware is a comprehensive term for all physical parts of a computer, as distinguished from the data it contains or operates on, and the software that provides instructions for the hardware to accomplish tasks. The boundary between hardware and software might be slightly blurry, with the existence of firmware that is software "built into" the hardware.
399) 23.708277, Personal computer - Wikipedia, the free encyclopedia.txt#102, term: computer, content:Toxic chemicals found in some computer hardware include lead, mercury, cadmium, chromium, plastic (PVC), and barium. Overall, a computer is about 17% lead, copper, zinc, mercury, and cadmium; 23% is plastic, 14% is aluminum, and 20% is iron.
400) 23.708277, Personal digital assistant - Wikipedia, the free encyclopedia.txt#19, term: computer, content:Some users find that data input is quicker on their computer than on their PDA, since text input via a touchscreen or small-scale keyboard is slower than a full-size keyboard. Transferring data to a PDA via the computer is therefore a lot quicker than having to manually input all data on the handheld device.[citation needed]
401) 23.708277, Portable computer - Wikipedia, the free encyclopedia.txt#0, term: computer, content:A portable computer is a computer that is designed to be moved from one place to another and includes a display and keyboard. Portable computers, by their nature, are generally microcomputers.[1] Portable computers, because of their size, are also commonly known as 'Lunchbox' or 'Luggable' computers. They can also be called a 'Portable Workstation' or 'Portable PC'.
402) 23.708277, Presentation program - Wikipedia, the free encyclopedia.txt#4, term: computer, content:The first commercial computer software specifically intended for creating WYSIWYG presentations was developed at Hewlett Packard in 1979 and called BRUNO and later HP-Draw. The first software displaying a presentation on a personal computer screen was VCN ExecuVision, developed in 1982. This program allowed users to choose from a library of images to accompany the text of their presentation.
403) 23.708277, Productivity software - Wikipedia, the free encyclopedia.txt#1, term: computer, content:Productivity software traditionally run directly on a computer. For example, Commodore Plus/4 model of computer contained in ROM four applications of productivity software. Productivity software is one of the reasons people use personal computers. Productivity software help the professional or common user to enhance and complete their tasks.
404) 23.708277, Programming language - Wikipedia, the free encyclopedia.txt#23, term: computer, content:Fourth-generation programming languages (4GL) are a computer programming languages which aim to provide a higher level of abstraction of the internal computer hardware details than 3GLs. Fifth generation programming languages (5GL) are programming languages based on solving problems using constraints given to the program, rather than using an algorithm written by a programmer.
405) 23.708277, Quantum computing - Wikipedia, the free encyclopedia.txt#60, term: computer, content:The capacity of a quantum computer to accelerate classical algorithms has rigid limitsupper bounds of quantum computation's complexity. The overwhelming part of classical calculations cannot be accelerated on a quantum computer.[86] A similar fact takes place for particular computational tasks, like the search problem, for which Grover's algorithm is optimal.[87]
406) 23.708277, Rendering (computer graphics) - Wikipedia, the free encyclopedia.txt#1, term: computer, content:Rendering is one of the major sub-topics of 3D computer graphics, and in practice is always connected to the others. In the graphics pipeline, it is the last major step, giving the final appearance to the models and animation. With the increasing sophistication of computer graphics since the 1970s, it has become a more distinct subject.
407) 23.708277, Royal Dutch Shell - Wikipedia, the free encyclopedia.txt#11, term: computer, content:Around 1952, Shell was the first company to purchase and use a computer in the Netherlands.[23] The computer, a Ferranti Mark 1*, was assembled and used at the Shell laboratory in Amsterdam. In 1970 Shell acquired the mining company Billiton, which it subsequently sold in 1994 and now forms part of BHP Billiton.[24]
408) 23.708277, Server (computing) - Wikipedia, the free encyclopedia.txt#10, term: computer, content:The purpose of a server is to share data as well as to share resources and distribute work. A server computer can serve its own computer programs as well; depending on the scenario, this could be part of a quid pro quo transaction, or simply a technical possibility. The following table shows several scenarios in which a server is used.
409) 23.708277, Software engineering - Wikipedia, the free encyclopedia.txt#32, term: computer, content:Dijkstra who developed computer languages in the last century refuted the concepts of "software engineering" which was prevalent thirty years ago in the 1980s, arguing that those terms were poor analogies for what he called the "radical novelty" of computer science:
410) 23.708277, Sound card - Wikipedia, the free encyclopedia.txt#39, term: computer, content:Several Japanese computer platforms, including the PC-88, PC-98, MSX, and FM-7, featured built-in FM synthesis sound from Yamaha by the mid-1980s. By 1989, the FM Towns computer platform featured built-in PCM sample-based sound and supported the CD-ROM format.[11]
411) 23.708277, Spreadsheet - Wikipedia, the free encyclopedia.txt#58, term: computer, content:Some spreadsheet implementations in Excel allow a cell references to another spreadsheet (not the current open and active file) on the same computer or a local network. It may also refer to a cell in another open and active spreadsheet on the same computer or network that is defined as shareable. These references contain the complete filename, such as:
412) 23.708277, Supercomputer - Wikipedia, the free encyclopedia.txt#0, term: computer, content:A supercomputer is a computer with a high-level computational capacity compared to a general-purpose computer. Performance of a supercomputer is measured in floating-point operations per second (FLOPS) instead of million instructions per second (MIPS). As of 2015, there are supercomputers which can perform up to quadrillions of FLOPS.[2]
413) 23.708277, Supercomputer - Wikipedia, the free encyclopedia.txt#19, term: computer, content:Heat management is a major issue in complex electronic devices, and affects powerful computer systems in various ways.[51] The thermal design power and CPU power dissipation issues in supercomputing surpass those of traditional computer cooling technologies. The supercomputing awards for green computing reflect this issue.[52][53][54]
414) 23.708277, Teleprinter - Wikipedia, the free encyclopedia.txt#42, term: computer, content:Paper tape was sometimes used to prepare input for the computer session off line and to capture computer output. The popular Teletype Model 33 used 7-bit ASCII code (with an eighth parity bit) instead of Baudot. The common modem communications settings, Start/Stop Bits and Parity, stem from the Teletype era.
415) 23.708277, Tuncer ?ren - Wikipedia, the free encyclopedia.txt#0, term: computer, content:Tuncer ren (born c. 1935 in stanbul) is Turkish Canadian systems engineer, professor emeritus of Computer Science at the School of Electrical Engineering and Computer Science (EECS) of the University of Ottawa, Canada, and Director, The McLeod Modeling and Simulation Network (M&SNet) of the SCS. He is known for his contributions to the methodology of modelling and simulation.[1]
416) 23.708277, User interface - Wikipedia, the free encyclopedia.txt#7, term: computer, content:Tools used for incorporating human factors in the interface design are developed based on knowledge of computer science, such as computer graphics, operating systems, programming languages. Nowadays, we use the expression graphical user interface for humanmachine interface on computers, as nearly all of them are now using graphics.
417) 23.708277, Video game console - Wikipedia, the free encyclopedia.txt#10, term: computer, content:The Epoch Game Pocket Computer was released in Japan in 1984. The Game Pocket Computer featured an LCD screen with 75 X 64 resolution, and could produce graphics at about the same level as early Atari 2600 games. The system sold poorly, and as a result only 5 games were made for it.
418) 23.708277, Wearable computer - Wikipedia, the free encyclopedia.txt#8, term: computer, content:Due to the varied definitions of "wearable" and "computer", the first wearable computer could be as early as the first abacus on a necklace, a 16th-century abacus ring, the first wristwatch made by Breguet for the Queen of Naples in 1810, or the covert timing devices hidden in shoes to cheat at roulette by Thorp and Shannon in the 1960s and 1970s.[7]
419) 23.708277, Wearable computer - Wikipedia, the free encyclopedia.txt#14, term: computer, content:The first wearable timepiece was made by watchmaker Breguet for the Queen of Naples in 1810. It was a small ladies' pocket watch on a bracelet chain.[13] A wristwatch is a "wearable computer" in the sense that it can be worn and that it also computes time. But it is not a general-purpose computer in the sense of the modern word.
420) 23.708277, Wearable computer - Wikipedia, the free encyclopedia.txt#31, term: computer, content:Dr. Bruce H Thomas and Dr. Wayne Piekarski developed the Tinmith wearable computer system to support augmented reality. This work was first published internationally in 2000 at the ISWC conference. The work was carried out at the Wearable Computer Lab in the University of South Australia.
421) 23.708277, Z1 (computer) - Wikipedia, the free encyclopedia.txt#1, term: computer, content:The Z1 was the first freely programmable computer in the world which used Boolean logic and binary floating point numbers, however it was unreliable in operation.[1][2] It was completed in 1938 and financed completely from private funds. This computer was destroyed in the bombardment of Berlin in December 1943, during World War II, together with all construction plans.
422) 23.708277, Z22 (computer) - Wikipedia, the free encyclopedia.txt#1, term: computer, content:By the end of 1958 the ZMMD-group had built a working ALGOL 58 compiler for the Z22 computer. ZMMD was an abbreviation for Zrich (where Rutishauser worked), Mnchen (workplace of Bauer and Samelson), Mainz (location of the Z22 computer), Darmstadt (workplace of Bottenbruch).
423) 23.708277, Z3 (computer) - Wikipedia, the free encyclopedia.txt#0, term: computer, content:The Z3 was an electromechanical computer designed by Konrad Zuse. It was the world's first working programmable, fully automatic digital computer.[1] The Z3 was built with 2,100 relays, implementing a 22-bit word length that operated at a clock frequency of about 510Hz.[2] Program code[3] and constant data were stored on punched film.
424) 23.469997, 3D computer graphics software - Wikipedia, the free encyclopedia.txt#1, term: computer, content:3D modeling software is a class of 3D computer graphics software used to produce 3D models. Individual programs of this class are called modeling applications or modelers.
425) 23.469997, Algorithm - Wikipedia, the free encyclopedia.txt#36, term: computer, content:(Quasi-)formal description: Written in prose but much closer to the high-level language of a computer program, the following is the more formal coding of the algorithm in pseudocode or pidgin code:
426) 23.469997, Analog computer - Wikipedia, the free encyclopedia.txt#6, term: computer, content:The south-pointing chariot can be considered the earliest analog computer. It was a mechanical-geared wheeled vehicle used for to discern the southern cardinal direction.
427) 23.469997, Antivirus software - Wikipedia, the free encyclopedia.txt#20, term: computer, content:In 1991, the European Institute for Computer Antivirus Research (EICAR) was founded to further antivirus research and improve development of antivirus software.[48][49]
428) 23.469997, Antivirus software - Wikipedia, the free encyclopedia.txt#76, term: computer, content:Cloud antivirus is a technology that uses lightweight agent software on the protected computer, while offloading the majority of data analysis to the provider's infrastructure.[146]
429) 23.469997, Application software - Wikipedia, the free encyclopedia.txt#1, term: computer, content:Applications may be bundled with the computer and its system software or published separately, and may be coded as proprietary, open-source or university projects.[2]
430) 23.469997, ARM architecture - Wikipedia, the free encyclopedia.txt#27, term: computer, content:In 2005, ARM Holdings took part in the development of Manchester University's computer SpiNNaker, which used ARM cores to simulate the human brain.[51]
431) 23.469997, ARPANET - Wikipedia, the free encyclopedia.txt#4, term: computer, content:The earliest ideas for a computer network intended to allow general communications among computer users were formulated by computer scientist J. C. R. Licklider of Bolt, Beranek and Newman (BBN), in April 1963, in memoranda discussing the concept of the "Intergalactic Computer Network". Those ideas encompassed many of the features of the contemporary Internet. In October 1963, Licklider was appointed head of the Behavioral Sciences and Command and Control programs at the Defense Department's Advanced Research Projects Agency (ARPA). He convinced Ivan Sutherland and Bob Taylor that this network concept was very important and merited development, although Licklider left ARPA before any contracts were assigned for development.[8]
432) 23.469997, Artificial intelligence - Wikipedia, the free encyclopedia.txt#20, term: computer, content:Machine learning is the study of computer algorithms that improve automatically through experience[63][64] and has been central to AI research since the field's inception.[65]
433) 23.469997, Association for Computing Machinery - Wikipedia, the free encyclopedia.txt#1, term: computer, content:The ACM and the IEEE Computer Society are the umbrella organizations for US academic and scholarly interests in computing. Unlike the IEEE, the ACM is solely dedicated to computing.
434) 23.469997, Atanasoff–Berry computer - Wikipedia, the free encyclopedia.txt#19, term: computer, content:Eckert and Mauchly did not themselves first invent the automatic electronic digital computer, but instead derived that subject matter from one Dr. John Vincent Atanasoff.
435) 23.469997, Bit - Wikipedia, the free encyclopedia.txt#21, term: computer, content:Certain bitwise computer processor instructions (such as bit set) operate at the level of manipulating bits rather than manipulating data interpreted as an aggregate of bits.
436) 23.469997, Bus (computing) - Wikipedia, the free encyclopedia.txt#29, term: computer, content:Buses such as Wishbone have been developed by the open source hardware movement in an attempt to further remove legal and patent constraints from computer design.
437) 23.469997, Byte - Wikipedia, the free encyclopedia.txt#12, term: computer, content:Despite standardization efforts, ambiguity still exists in the meanings of the SI (or metric) prefixes used with the unit byte, especially concerning the prefixes kilo (k or K), mega (M), and giga (G). Computer memory has a binary architecture in which multiples are expressed in powers of 2. In some fields of the software and computer hardware industries the SI-prefixed quantities of byte and bits are used with a meaning of binary multiples of powers, while producers of computer storage devices practice adherence to SI multiples. For example, a computer disk drive capacity of 100gigabytes is specified when the disk contains 100 billion bytes (93gibibytes) of storage space.
438) 23.469997, Charles Babbage - Wikipedia, the free encyclopedia.txt#61, term: computer, content:John Tucker, Professor of Computer Science at Swansea University, argues that it was the Welsh mathematician Robert Recorde who first laid down the foundations of these concepts.[141]
439) 23.469997, Colossus computer - Wikipedia, the free encyclopedia.txt#30, term: computer, content:Britain had such vitality that it could immediately after the war embark on so many well-conceived and well-executed projects in the computer field.[52]
440) 23.469997, Colossus computer - Wikipedia, the free encyclopedia.txt#32, term: computer, content:the COLOSSUS project was an important source of this vitality, one that has been largely unappreciated, as has the significance of its places in the chronology of the invention of the digital computer.[53]
441) 23.469997, Command-line interface - Wikipedia, the free encyclopedia.txt#4, term: computer, content:Command-line interfaces are often preferred by more advanced computer users, as they often provide a more concise and powerful means to control a program or operating system.
442) 23.469997, Command-line interface - Wikipedia, the free encyclopedia.txt#88, term: computer, content:While most computer users now use a GUI almost exclusively, more advanced users have access to powerful command-line environments:
443) 23.469997, Computer - Simple English Wikipedia, the free encyclopedia.txt#17, term: computer, content:Because of machines like this, new ways of talking to these machines were invented, and new types of machines were invented, and eventually the computer as we know it was born.
444) 23.469997, Computer - Wikipedia, the free encyclopedia.txt#31, term: computer, content:The Manchester Small-Scale Experimental Machine, nicknamed Baby, was the world's first stored-program computer. It was built at the Victoria University of Manchester by Frederic C. Williams, Tom Kilburn and Geoff Tootill, and ran its first program on 21June 1948.[36] It was designed as a testbed for the Williams tube the first random-access digital storage device.[37] Although the computer was considered "small and primitive" by the standards of its time, it was the first working machine to contain all of the elements essential to a modern electronic computer.[38] As soon as the SSEM had demonstrated the feasibility of its design, a project was initiated at the university to develop it into a more usable computer, the Manchester Mark 1.
445) 23.469997, Computer - Wikipedia, the free encyclopedia.txt#92, term: computer, content:The term hardware covers all of those parts of a computer that are tangible objects. Circuits, displays, power supplies, cables, keyboards, printers and mice are all hardware.
446) 23.469997, Computer animation - Wikipedia, the free encyclopedia.txt#29, term: computer, content:Computer-assisted animation is usually classed as two-dimensional (2D) animation. Creators drawings either hand drawn (pencil to paper) or interactively drawn(drawn on the computer) using different assisting appliances and are positioned into specific software packages. Within the software package the creator will place drawings into different key frames which fundamentally create an outline of the most important movements. The computer will then fill in all the " in-between frames", commonly known as Tweening. Computer-assisted animation is basically using new technologies to cut down the time scale that traditional animation could take, but still having the elements of traditional drawings of characters or objects.[38]
447) 23.469997, Computer architecture - Wikipedia, the free encyclopedia.txt#1, term: computer, content:The first documented computer architecture was in the correspondence between Charles Babbage and Ada Lovelace, describing the analytical engine. Two other early and important examples were:
448) 23.469997, Computer architecture - Wikipedia, the free encyclopedia.txt#3, term: computer, content:Subsequently, Brooks, a Stretch designer, started Chapter 2 of a book (Planning a Computer System: Project Stretch, ed. W. Buchholz, 1962) by writing,
449) 23.469997, Computer architecture - Wikipedia, the free encyclopedia.txt#4, term: computer, content:Computer architecture, like other architecture, is the art of determining the needs of the user of a structure and then designing to meet those needs as effectively as possible within economic and technological constraints.
450) 23.469997, Computer music - Wikipedia, the free encyclopedia.txt#20, term: computer, content:The world's first digital computer music was generated in Australia by programmer Geoff Hill on the CSIRAC computer which was designed and built by Trevor Pearcey and Maston Beard, although it was only used to play standard tunes of the day. Subsequently, one of the first composers to write music with a computer was Iannis Xenakis. He wrote programs in the FORTRAN language that generated numeric data that he transcribed into scores to be played by traditional musical instruments. An example is ST/48 of 1962. Although Xenakis could well have composed this music by hand, the intensity of the calculations needed to transform probabilistic mathematics into musical notation was best left to the number-crunching power of the computer.[citation needed]
451) 23.469997, Computer network - Wikipedia, the free encyclopedia.txt#2, term: computer, content:Computer networks differ in the transmission medium used to carry their signals, the communications protocols to organize network traffic, the network's size, topology and organizational intent.
452) 23.469997, Computer network - Wikipedia, the free encyclopedia.txt#83, term: computer, content:Network services are applications hosted by servers on a computer network, to provide some functionality for members or users of the network, or to help the network itself to operate.
453) 23.469997, Computer program - Wikipedia, the free encyclopedia.txt#14, term: computer, content:The sometimes lengthy process of computer programming is usually referred to as software development. The term software engineering is becoming popular as the process is seen as an engineering discipline.
454) 23.469997, Computer program - Wikipedia, the free encyclopedia.txt#28, term: computer, content:Multiple lines of the same computer program may be simultaneously executed using threads. Multithreading processors are optimized to execute multiple threads efficiently.
455) 23.469997, Computer simulation - Wikipedia, the free encyclopedia.txt#0, term: computer, content:A computer simulation is a simulation, run on a single computer, or a network of computers, to reproduce behavior of a system. The simulation uses an abstract model (a computer model, or a computational model) to simulate the system. Computer simulations have become a useful part of mathematical modeling of many natural systems in physics (computational physics), astrophysics, climatology, chemistry and biology, human systems in economics, psychology, social science, and engineering. Simulation of a system is represented as the running of the system's model. It can be used to explore and gain new insights into new technology and to estimate the performance of systems too complex for analytical solutions.[1]
456) 23.469997, Computer simulation - Wikipedia, the free encyclopedia.txt#4, term: computer, content:Computer simulation developed hand-in-hand with the rapid growth of the computer, following its first large-scale deployment during the Manhattan Project in World War II to model the process of nuclear detonation. It was a simulation of 12 hard spheres using a Monte Carlo algorithm. Computer simulation is often used as an adjunct to, or substitute for, modeling systems for which simple closed form analytic solutions are not possible. There are many types of computer simulations; their common feature is the attempt to generate a sample of representative scenarios for a model in which a complete enumeration of all possible states of the model would be prohibitive or impossible.
457) 23.469997, Computer simulation - Wikipedia, the free encyclopedia.txt#15, term: computer, content:Other applications of CGI computer simulations are being developed to graphically display large amounts of data, in motion, as changes occur during a simulation run.
458) 23.469997, Computer simulation - Wikipedia, the free encyclopedia.txt#17, term: computer, content:Notable, and sometimes controversial, computer simulations used in science include: Donella Meadows' World3 used in the Limits to Growth, James Lovelock's Daisyworld and Thomas Ray's Tierra.
459) 23.469997, Computer simulation - Wikipedia, the free encyclopedia.txt#20, term: computer, content:The reliability and the trust people put in computer simulations depends on the validity of the simulation model, therefore verification and validation are of crucial importance in the development of computer simulations. Another important aspect of computer simulations is that of reproducibility of the results, meaning that a simulation model should not provide a different answer for each execution. Although this might seem obvious, this is a special point of attention in stochastic simulations, where random numbers should actually be semi-random numbers. An exception to reproducibility are human-in-the-loop simulations such as flight simulations and computer games. Here a human is part of the simulation and thus influences the outcome in a way that is hard, if not impossible, to reproduce exactly.
460) 23.469997, Computer-aided design - Wikipedia, the free encyclopedia.txt#10, term: computer, content:Computer-aided design is one of the many tools used by engineers and designers and is used in many ways depending on the profession of the user and the type of software in question.
461) 23.469997, Computing - Wikipedia, the free encyclopedia.txt#39, term: computer, content:A system administrator, IT systems administrator, systems administrator, or sysadmin is a person employed to maintain and operate a computer system and/or network. The duties of a system administrator are wide-ranging, and vary widely from one organization to another. Sysadmins are usually charged with installing, supporting and maintaining servers or other computer systems, and planning for and responding to service outages and other problems. Other duties may include scripting or light programming, project management for systems-related projects, supervising or training computer operators, and being the consultant for computer problems beyond the knowledge of technical support staff.
462) 23.469997, CSIRAC - Wikipedia, the free encyclopedia.txt#5, term: computer, content:In 1950 or 1951, CSIRAC was used to play music, the first known use of a digital computer for the purpose. The music was never recorded, but it has been accurately reconstructed.[4]
463) 23.469997, DARPA - Wikipedia, the free encyclopedia.txt#4, term: computer, content:DARPA-funded projects have provided significant technologies that influenced many non-military fields, such as computer networking and graphical user interfaces in information technology.
464) 23.469997, Database - Wikipedia, the free encyclopedia.txt#8, term: computer, content:Since DBMSs comprise a significant economical market, computer and storage vendors often take into account DBMS requirements in their own development plans.[8]
465) 23.469997, Desktop computer - Wikipedia, the free encyclopedia.txt#13, term: computer, content:An all-in-one desktop computer integrates the system's internal components into the same case as the display, thus occupying a smaller footprint than desktops that incorporate a tower.[20]
466) 23.469997, Digital Equipment Corporation - Wikipedia, the free encyclopedia.txt#105, term: computer, content:DEC's Western Research Lab created the Itsy Pocket Computer. This was developed into the Compaq iPaq line of PDAs, which replaced the Compaq Aero PDA.
467) 23.469997, Drum memory - Wikipedia, the free encyclopedia.txt#0, term: computer, content:Drum memory was a magnetic data storage device invented by Gustav Tauschek in 1932 in Austria.[1] It was widely used in the 1950s and into the 1960s as computer memory.
468) 23.469997, Drum memory - Wikipedia, the free encyclopedia.txt#7, term: computer, content:The first mass-produced computer, the IBM 650, had about 8.5 kilobytes of drum memory (later doubled to about 17 kilobytes in the Model 4).
469) 23.469997, Embedded system - Wikipedia, the free encyclopedia.txt#5, term: computer, content:One of the very first recognizably modern embedded systems was the Apollo Guidance Computer, developed by Charles Stark Draper at the MIT Instrumentation Laboratory. At the project's inception, the Apollo guidance computer was considered the riskiest item in the Apollo project as it employed the then newly developed monolithic integrated circuits to reduce the size and weight. An early mass-produced embedded system was the Autonetics D-17 guidance computer for the Minuteman missile, released in 1961. When the Minuteman II went into production in 1966, the D-17 was replaced with a new computer that was the first high-volume use of integrated circuits.
470) 23.469997, ENIAC - Wikipedia, the free encyclopedia.txt#30, term: computer, content:Mechanical and electrical computing machines have been around since the 19th century, but the 1930s and 1940s are considered the beginning of the modern computer era.
471) 23.469997, Exclusive or - Wikipedia, the free encyclopedia.txt#37, term: computer, content:In computer graphics, XOR-based drawing methods are often used to manage such items as bounding boxes and cursors on systems without alpha channels or overlay planes.
472) 23.469997, Floating point - Wikipedia, the free encyclopedia.txt#5, term: computer, content:The speed of floating-point operations, commonly measured in terms of FLOPS, is an important characteristic of a computer system, especially for applications that involve intensive mathematical calculations.
473) 23.469997, Grace Hopper - Wikipedia, the free encyclopedia.txt#24, term: computer, content:Building 1482 aboard Naval Air Station North Island, housing the Naval Computer and Telecommunication Station San Diego, is named the Grace Hopper Building.
474) 23.469997, Graphical user interface - Wikipedia, the free encyclopedia.txt#14, term: computer, content:There are also actions performed by programs that affect the GUI. For example, there are components like inotify or D-Bus to facilitate communication between computer programs.
475) 23.469997, History of computing hardware - Wikipedia, the free encyclopedia.txt#40, term: computer, content:In 1944, the Harvard Mark I was constructed at IBM's Endicott laboratories;[57] it was a similar general purpose electro-mechanical computer to the Z3, but was not quite Turing-complete.
476) 23.469997, Home computer - Wikipedia, the free encyclopedia.txt#53, term: computer, content:Three microcomputers were the prototypes for what would later become the home computer market segment; but when introduced they sold as much to hobbyists and small businesses as to the home.
477) 23.469997, HTML - Wikipedia, the free encyclopedia.txt#49, term: computer, content:HTML documents can be delivered by the same means as any other computer file. However, they are most often delivered either by HTTP from a web server or by email.
478) 23.469997, Human computer - Wikipedia, the free encyclopedia.txt#2, term: computer, content:Since the end of the 20th century, the term "human computer" has also been applied to individuals with prodigious powers of mental arithmetic, also known as mental calculators.
479) 23.469997, Human–computer interaction - Wikipedia, the free encyclopedia.txt#5, term: computer, content:Leading academic research centers include CMU's Human-Computer Interaction Institute, GVU Center at Georgia Tech, and the University of Maryland HumanComputer Interaction Lab.
480) 23.469997, IBM System i - Wikipedia, the free encyclopedia.txt#0, term: computer, content:The IBM System i is IBM's previous generation of midrange computer systems for IBM i users, and was subsequently replaced by the IBM Power Systems in April 2008.
481) 23.469997, Institute of Electrical and Electronics Engineers - Wikipedia, the free encyclopedia.txt#12, term: computer, content:IEEE produces over 30% of the world's literature in the electrical and electronics engineering and computer science fields, publishing well over 100 peer-reviewed journals.[8]
482) 23.469997, Intel - Wikipedia, the free encyclopedia.txt#70, term: computer, content:The Classmate PC is the company's first low-cost netbook computer.[160] In 2014, the company released an updated version of the Classmate PC.[161]
483) 23.469997, Intel 8080 - Wikipedia, the free encyclopedia.txt#25, term: computer, content:The 8080 also changed how computers were created. When the 8080 was introduced, computer systems were usually created by computer manufacturers such as Digital Equipment Corporation, Hewlett Packard, or IBM. A manufacturer would produce the entire computer, including processor, terminals, and system software such as compilers and operating system. The 8080 was actually designed for just about any application except a complete computer system. Hewlett Packard developed the HP 2640 series of smart terminals around the 8080. The HP 2647 was a terminal which ran BASIC on the 8080. Microsoft would market as its founding product the first popular programming language for the 8080, and would later acquire DOS for the IBM-PC.
484) 23.469997, Interpreter (computing) - Wikipedia, the free encyclopedia.txt#33, term: computer, content:The book Structure and Interpretation of Computer Programs presents examples of meta-circular interpretation for Scheme and its dialects. Other examples of languages with a self-interpreter are Forth and Pascal.
485) 23.469997, J. Presper Eckert - Wikipedia, the free encyclopedia.txt#6, term: computer, content:In the following months, Eckert and Mauchly started up the Electronic Control Company which built the Binary Automatic Computer (BINAC). One of the major advances of this machine, which was used from August 1950, was that data was stored on magnetic tape. The Electronic Control Company soon became the EckertMauchly Computer Corporation and it received an order from the National Bureau of Standards to build the Universal Automatic Computer (UNIVAC). He was awarded the Howard N. Potts Medal in 1949. In 1950, EckertMauchly Computer Corporation ran into financial troubles and was acquired by Remington Rand Corporation. The UNIVAC I was finished on December 21, 1950.
486) 23.469997, J. Presper Eckert - Wikipedia, the free encyclopedia.txt#7, term: computer, content:In 1968, "For pioneering and continuing contributions in creating, developing, and improving the high-speed electronic digital computer", he was awarded the National Medal of Science.[3]
487) 23.469997, Logic gate - Wikipedia, the free encyclopedia.txt#23, term: computer, content:These logic circuits are known as computer memory. They vary in performance, based on factors of speed, complexity, and reliability of storage, and many different types of designs are used based on the application.
488) 23.469997, Microphone - Wikipedia, the free encyclopedia.txt#79, term: computer, content:Typically, an array is made up of omnidirectional microphones distributed about the perimeter of a space, linked to a computer that records and interprets the results into a coherent form.
489) 23.469997, MIPS instruction set - Wikipedia, the free encyclopedia.txt#2, term: computer, content:Computer architecture courses in universities and technical schools often study the MIPS architecture.[6] The architecture greatly influenced later RISC architectures such as Alpha.
490) 23.469997, MIPS instruction set - Wikipedia, the free encyclopedia.txt#87, term: computer, content:Many other pseudoinstructions and floating-point instructions present in MIPS R2000 are given in Appendix B.10 of Computer Organization and Design, Fourth Edition by Patterson and Hennessy.
491) 23.469997, Multimedia - Wikipedia, the free encyclopedia.txt#5, term: computer, content:Multimedia games and simulations may be used in a physical environment with special effects, with multiple users in an online network, or locally with an offline computer, game system, or simulator.
492) 23.469997, Multiprocessing - Wikipedia, the free encyclopedia.txt#19, term: computer, content:SIMD multiprocessing finds wide use in certain domains such as computer simulation, but is of little use in general-purpose desktop and business computing environments.[citation needed]
493) 23.469997, Operating system - Wikipedia, the free encyclopedia.txt#84, term: computer, content:A device driver is a specific type of computer software developed to allow interaction with hardware devices. Typically this constitutes an interface for communicating with the device, through the specific computer bus or communications subsystem that the hardware is connected to, providing commands to and/or receiving data from the device, and on the other end, the requisite interfaces to the operating system and software applications. It is a specialized hardware-dependent computer program which is also operating system specific that enables another program, typically an operating system or applications software package or computer program running under the operating system kernel, to interact transparently with a hardware device, and usually provides the requisite interrupt handling necessary for any necessary asynchronous time-dependent hardware interfacing needs.
494) 23.469997, OS X - Wikipedia, the free encyclopedia.txt#76, term: computer, content:The Apple Certified Help Desk Specialist (ACHDS) is a computer certification that verifies a person's understanding of the core functionality and key services of Mac OS X.[166]
495) 23.469997, Personal computer - Wikipedia, the free encyclopedia.txt#31, term: computer, content:At their introduction in 1981, the US $1,795 price of the Osborne 1 and its competitor Kaypro was considered an attractive price point; these systems had text-only displays and only floppy disks for storage. By 1982, Michael Dell observed that a personal computer system selling at retail for about $3,000 US was made of components that cost the dealer about $600; typical gross margin on a computer unit was around $1,000.[45] The total value of personal computer purchases in the US in 1983 was about $4 billion, comparable to total sales of pet food. By late 1998, the average selling price of personal computer systems in the United States had dropped below $1,000.[46]
496) 23.469997, Personal computer - Wikipedia, the free encyclopedia.txt#90, term: computer, content:Software applications are common for word processing, Internet browsing, Internet faxing, e-mail and other digital messaging, multimedia playback, playing of computer game, and computer programming. The user of a modern personal computer may have significant knowledge of the operating environment and application programs, but is not necessarily interested in programming nor even able to write programs for the computer. Therefore, most software written primarily for personal computers tends to be designed with simplicity of use, or "user-friendliness" in mind. However, the software industry continuously provide a wide range of new products for use in personal computers, targeted at both the expert and the non-expert user.
497) 23.469997, Personal digital assistant - Wikipedia, the free encyclopedia.txt#20, term: computer, content:Most PDAs come with the ability to synchronize to a computer. This is done through synchronization software provided with the handheld, or sometime with the computer's operating system. Examples of synchronization software include:
498) 23.469997, Personal digital assistant - Wikipedia, the free encyclopedia.txt#24, term: computer, content:Some PDAs can synchronize some or all of their data using their wireless networking capabilities, rather than having to be directly connected to a personal computer via a cable.
499) 23.469997, Portable computer - Wikipedia, the free encyclopedia.txt#2, term: computer, content:In 1973 the IBM Palo Alto Scientific Center developed a portable computer prototype called SCAMP (Special Computer APL Machine Portable) based on the IBM PALM processor with a Philips compact cassette drive, small CRT and full function keyboard. SCAMP emulated an IBM 1130 minicomputer in order to run APL\1130.[2] In 1973 APL was generally available only on mainframe computers, and most desktop sized microcomputers such as the Wang 2200 or HP 9800 offered only BASIC. Because SCAMP was the first to emulate APL\1130 performance on a portable, single user computer, PC Magazine in 1983 designated SCAMP a "revolutionary concept" and "the world's first personal computer".[3][4]
500) 23.469997, Portable computer - Wikipedia, the free encyclopedia.txt#7, term: computer, content:The first mass-produced microprocessor-based portable computer released in 1981 was the Osborne 1, developed by Osborne, which owed much to the NoteTaker's design.
501) 23.469997, Portable computer - Wikipedia, the free encyclopedia.txt#9, term: computer, content:In January 1983, the first IBM PC compatible portable computer (and indeed the first 100% IBM PC compatible, or "clone," of any kind) was the Compaq Portable.
502) 23.469997, QNX - Wikipedia, the free encyclopedia.txt#15, term: computer, content:In September 2010, the company announced a tablet computer, the BlackBerry PlayBook, and a new operating system BlackBerry Tablet OS based on QNX to run on the tablet.[13]
503) 23.469997, Quantum computing - Wikipedia, the free encyclopedia.txt#31, term: computer, content:For physically implementing a quantum computer, many different candidates are being pursued, among them (distinguished by the physical system used to realize the qubits):
504) 23.469997, Register machine - Wikipedia, the free encyclopedia.txt#0, term: computer, content:In mathematical logic and theoretical computer science a register machine is a generic class of abstract machines used in a manner similar to a Turing machine. All the models are Turing equivalent.
505) 23.469997, Server (computing) - Wikipedia, the free encyclopedia.txt#5, term: computer, content:SERVER n. A kind of DAEMON which performs a service for the requester, which often runs on a computer other than the one on which the server runs.
506) 23.469997, Software - Wikipedia, the free encyclopedia.txt#25, term: computer, content:Computer software has special economic characteristics that make its design, creation, and distribution different from most other economic goods.[specify][6][7]
507) 23.469997, Software engineering - Wikipedia, the free encyclopedia.txt#25, term: computer, content:Software engineering is a direct sub-field of engineering and has an overlap with computer science and management science[citation needed]. It is also considered a part of overall systems engineering.
508) 23.469997, Stan Frankel - Wikipedia, the free encyclopedia.txt#7, term: computer, content:Frankel, S P, The Logical Design of a Simple General Purpose Computer, IRE Transactions on Electronic Computers, March 1957, p 5-14.
509) 23.469997, Stan Frankel - Wikipedia, the free encyclopedia.txt#8, term: computer, content:Frankel, S P, On the Minimum Logical Complexity Required for a General Purpose Computer, IRE Transactions on Electronic Computers, December 1958, p 282-284.
510) 23.469997, Tablet computer - Wikipedia, the free encyclopedia.txt#12, term: computer, content:Intel announced a StrongARM[30] processor-based touchscreen tablet computer in 1999, under the name WebPAD. It was later re-branded as the "Intel Web Tablet".[31]
511) 23.469997, Telecommunications engineering - Wikipedia, the free encyclopedia.txt#30, term: computer, content:A telecom equipment engineer is an electronics engineer that designs equipment such as routers, switches, multiplexers, and other specialized computer/electronics equipment designed to be used in the telecommunication network infrastructure.
512) 23.469997, Trackball - Wikipedia, the free encyclopedia.txt#7, term: computer, content:A similar trackball device at the German Bundesanstalt fr Flugsicherung(de) was constructed by a team around Rainer Mallebrein of Telefunken Konstanz as part of the development for the Telefunken computer infrastructure around the main frame TR 440(de), process computer TR86 and video terminal SIG100-86,[6] which began in 1965.[4] This trackball was called Rollkugel (German for "rolling ball"). Somewhat later, the idea of "reversing" this device led to the introduction of the first computer ball mouse (still named Rollkugel, model RKS 100-86), which was offered as an alternative input device to light pens and trackballs for Telefunken's computer systems since 1968.[4][7]
513) 23.469997, USB - Wikipedia, the free encyclopedia.txt#44, term: computer, content:The connectors the USB committee specifies support a number of USB's underlying goals, and reflect lessons learned from the many connectors the computer industry has used.
514) 23.469997, Video game console - Wikipedia, the free encyclopedia.txt#8, term: computer, content:RCA and Atari soon released their own cartridge-based consoles, the RCA Studio II and the Atari 2600 (originally branded as the Atari Video Computer System), respectively.
515) 23.469997, Wearable computer - Wikipedia, the free encyclopedia.txt#10, term: computer, content:Steve Mann, a professor at the University of Toronto, was hailed as the father of the wearable computer and the ISSCC's first virtual panelist, by moderator Woodward Yang of Harvard University (Cambridge Mass.).
516) 23.469997, Z3 (computer) - Wikipedia, the free encyclopedia.txt#12, term: computer, content:Zuse's coworker Helmut Schreyer built an electronic digital experimental model of a computer using 100 vacuum tubes[23] in 1942, but it was lost at the end of the war.
517) 23.229273, 3D computer graphics - Wikipedia, the free encyclopedia.txt#3, term: computer, content:William Fetter was credited with coining the term computer graphics in 1961[1][2] to describe his work at Boeing. One of the first displays of computer animation was Futureworld (1976), which included an animation of a human face and a hand that had originally appeared in the 1971 experimental short A Computer Animated Hand, created by University of Utah students Edwin Catmull and Fred Parke.[3]
518) 23.229273, Algorithm - Wikipedia, the free encyclopedia.txt#23, term: computer, content:In computer systems, an algorithm is basically an instance of logic written in software by software developers to be effective for the intended "target" computer(s) to produce output from given input (perhaps null). An optimal algorithm, even running in old hardware, would produce faster results than a non optimal (higher time complexity) algorithm for the same purpose, running in more efficient hardware; that is why the algorithms, like computer hardware, are considered technology.
519) 23.229273, Analog computer - Wikipedia, the free encyclopedia.txt#0, term: computer, content:An analog computer is a form of computer that uses the continuously changeable aspects of physical phenomena such as electrical, mechanical, or hydraulic quantities to model the problem being solved. In contrast, digital computers represent varying quantities symbolically, as their numerical values change. As an analog computer does not use discrete values, but rather continuous values, processes cannot be reliably repeated with exact equivalence, as they can with Turing machines. Unlike digital signal processing, analog computers do not suffer from the quantization noise, but are limited by analog noise.
520) 23.229273, Analog computer - Wikipedia, the free encyclopedia.txt#29, term: computer, content:The accuracy of an analog computer is limited by its computing elements as well as quality of the internal power and electrical interconnections. The precision of the analog computer readout was limited chiefly by the precision of the readout equipment used, generally three or four significant figures. The precision of a digital computer is limited by the word size; arbitrary-precision arithmetic, while relatively slow, provides any practical degree of precision that might be needed.
521) 23.229273, Antivirus software - Wikipedia, the free encyclopedia.txt#2, term: computer, content:Although the roots of the computer virus date back as early as 1949, when the Hungarian scientist John von Neumann published the "Theory of self-reproducing automata",[4] the first known computer virus appeared in 1971 and was dubbed the "Creeper virus".[5] This computer virus infected Digital Equipment Corporation's (DEC) PDP-10 mainframe computers running the TENEX operating system.[6][7]
522) 23.229273, Antivirus software - Wikipedia, the free encyclopedia.txt#6, term: computer, content:The first IBM PC compatible "in the wild" computer virus, and one of the first real widespread infections, was "Brain" in 1986. From then, the number of viruses has grown exponentially.[18][19] Most of the computer viruses written in the early and mid-1980s were limited to self-reproduction and had no specific damage routine built into the code. That changed when more and more programmers became acquainted with computer virus programming and created viruses that manipulated or even destroyed data on infected computers.[20]
523) 23.229273, Antivirus software - Wikipedia, the free encyclopedia.txt#47, term: computer, content:Real-time protection, on-access scanning, background guard, resident shield, autoprotect, and other synonyms refer to the automatic protection provided by most antivirus, anti-spyware, and other anti-malware programs. This monitors computer systems for suspicious activity such as computer viruses, spyware, adware, and other malicious objects in 'real-time', in other words while data loaded into the computer's active memory: when inserting a CD, opening an email, or browsing the web, or when a file already on the computer is opened or executed.[96]
524) 23.229273, Application software - Wikipedia, the free encyclopedia.txt#2, term: computer, content:In information technology, an application is a computer program designed to help people perform an activity. An application thus differs from an operating system (which runs a computer), a utility (which performs maintenance or general-purpose chores), and a programming tool (with which computer programs are created)[original research?]. Depending on the activity for which it was designed, an application can manipulate text, numbers, graphics, or a combination of these elements. Some application packages focus on a single task, such as word processing; others, called integrated software include several applications.[3]
525) 23.229273, Association for Computing Machinery - Wikipedia, the free encyclopedia.txt#14, term: computer, content:ACM's primary historical competitor has been the IEEE Computer Society, which is the largest subgroup of the Institute of Electrical and Electronics Engineers. The IEEE focuses more on hardware and standardization issues than theoretical computer science, but there is considerable overlap with ACM's agenda. They occasionally cooperate on projects like developing computing curricula.[10] Some of the major awards in Computer science are given jointly by ACM and the IEEECS.[11]
526) 23.229273, Billiard-ball computer - Wikipedia, the free encyclopedia.txt#0, term: computer, content:A billiard-ball computer, also known as a conservative logic circuit, is an idealized model of a reversible mechanical computer based on Newtonian dynamics, proposed in 1982 by Edward Fredkin and Tommaso Toffoli.[1] Instead of using electronic signals like a conventional computer, it relies on the motion of spherical billiard balls in a friction-free environment made of buffers against which the balls bounce perfectly. It was devised to investigate the relation between computation and reversible processes in physics.
527) 23.229273, BIOS - Wikipedia, the free encyclopedia.txt#2, term: computer, content:Most BIOS implementations are specifically designed to work with a particular computer or motherboard model, by interfacing with various devices that make up the complementary system chipset. Originally, BIOS firmware was stored in a ROM chip on the PC motherboard; in modern computer systems, the BIOS contents are stored on flash memory so it can be rewritten without removing the chip from the motherboard. This allows easy updates to the BIOS firmware so new features can be added or bugs can be fixed, but it also creates a possibility for the computer to become infected with BIOS rootkits.
528) 23.229273, Booting - Wikipedia, the free encyclopedia.txt#33, term: computer, content:When a computer is turned off, its softwareincluding operating systems, application code, and dataremains stored on non-volatile memory. When the computer is powered on, it typically does not have an operating system or its loader in random access memory (RAM). The computer first executes a relatively small program stored in read-only memory (ROM) along with a small amount of needed data, to access the nonvolatile device or devices from which the operating system programs and data can be loaded into RAM.
529) 23.229273, Command-line interface - Wikipedia, the free encyclopedia.txt#15, term: computer, content:The command-line interface evolved from a form of dialog once conducted by humans over teleprinter (TTY) machines, in which human operators remotely exchanged information, usually one line of text at a time. Early computer systems often used teleprinter machines as the means of interaction with a human operator. The computer became one end of the human-to-human teleprinter model. So instead of a human communicating with another human over a teleprinter, a human communicated with a computer.
530) 23.229273, Computer - Simple English Wikipedia, the free encyclopedia.txt#0, term: computer, content:A computer is a machine (mostly electronic) that is able to take information (input), do some work on or make changes to the information (process), to make new information (output). Computers have existed for much of human history. Examples of early computers are the astrolabe and the abacus. There are four main processing steps in a computer, and they are: inputting, outputting, storage and processing. These four steps help the computer to work.
531) 23.229273, Computer - Simple English Wikipedia, the free encyclopedia.txt#6, term: computer, content:Computer programs are designed or written by computer programmers. A few programmers write programs in the computer's own language called machine code. Most programs are written using a programming language like C++, Java, and Python. These programming languages are more like the language you talk and write with every day. A program called a compiler translates the user's instructions into binary code (machine code) that the computer will understand and do what is needed.
532) 23.229273, Computer - Wikipedia, the free encyclopedia.txt#40, term: computer, content:In practical terms, a computer program may be just a few instructions or extend to many millions of instructions, as do the programs for word processors and web browsers for example. A typical modern computer can execute billions of instructions per second (gigaflops) and rarely makes a mistake over many years of operation. Large computer programs consisting of several million instructions may take teams of programmers years to write, and due to the complexity of the task almost certainly contain errors.
533) 23.229273, Computer - Wikipedia, the free encyclopedia.txt#87, term: computer, content:The ability to store and execute lists of instructions called programs makes computers extremely versatile, distinguishing them from calculators. The ChurchTuring thesis is a mathematical statement of this versatility: any computer with a minimum capability (being Turing-complete) is, in principle, capable of performing the same tasks that any other computer can perform. Therefore, any type of computer (netbook, supercomputer, cellular automaton, etc.) is able to perform the same computational tasks, given enough time and storage capacity.
534) 23.229273, Computer - Wikipedia, the free encyclopedia.txt#89, term: computer, content:Historically, computers evolved from mechanical computers and eventually from vacuum tubes to transistors. However, conceptually computational systems as flexible as a personal computer can be built out of almost anything. For example, a computer can be made out of billiard balls (billiard ball computer); an often quoted example.[citation needed] More realistically, modern computers are made out of transistors made of photolithographed semiconductors.
535) 23.229273, Computer animation - Wikipedia, the free encyclopedia.txt#7, term: computer, content:An early step in the history of computer animation was the sequel to the 1973 film Westworld, a science-fiction film about a society in which robots live and work among humans.[4] The sequel, Futureworld (1976), used the 3D wire-frame imagery, which featured a computer-animated hand and face both created by University of Utah graduates Edwin Catmull and Fred Parke. This imagery originally appeared in their student film A Computer Animated Hand, which they completed in 1971.[5][6]
536) 23.229273, Computer animation - Wikipedia, the free encyclopedia.txt#8, term: computer, content:Developments in CGI technologies are reported each year at SIGGRAPH, an annual conference on computer graphics and interactive techniques that is attended by thousands of computer professionals each year.[7] Developers of computer games and 3D video cards strive to achieve the same visual quality on personal computers in real-time as is possible for CGI films and animation. With the rapid advancement of real-time rendering quality, artists began to use game engines to render non-interactive movies, which led to the art form Machinima.
537) 23.229273, Computer architecture - Wikipedia, the free encyclopedia.txt#16, term: computer, content:Computer organization also helps plan the selection of a processor for a particular project. Multimedia projects may need very rapid data access, while virtual machines may need fast interrupts. Sometimes certain tasks need additional components as well. For example, a computer capable of running a virtual machine needs virtual memory hardware so that the memory of different virtual computers can be kept separated. Computer organization and features also affect power consumption and processor cost.
538) 23.229273, Computer engineering - Wikipedia, the free encyclopedia.txt#20, term: computer, content:According to the BLS, Job Outlook employment for computer hardware engineers, 2014-24 is 3% ("Slower than average" in their own words when compared to other occupations)"[14] and is down from 7% for 2012 to 2022 BLS estimate[15] and is further down from 9% in the BLS 2010 to 2020 estimate." Today, computer hardware is somehow equal to Electronic and Computer Engineering (ECE) and has divided to many subcategories, the most significant of them is Embedded system design.[8]
539) 23.229273, Computer graphics - Wikipedia, the free encyclopedia.txt#7, term: computer, content:Electronics pioneer Hewlett-Packard went public in 1957 after incorporating the decade prior, and established strong ties with Stanford University through its founders, who were alumni. This began the decades-long transformation of the southern San Francisco Bay Area into the world's leading computer technology hub - now known as Silicon Valley. The field of computer graphics developed with the emergence of computer graphics hardware.
540) 23.229273, Computer graphics - Wikipedia, the free encyclopedia.txt#44, term: computer, content:Despite these differences, 3D computer graphics rely on similar algorithms as 2D computer graphics do in the frame and raster graphics (like in 2D) in the final rendered display. In computer graphics software, the distinction between 2D and 3D is occasionally blurred; 2D applications may use 3D techniques to achieve effects such as lighting, and primarily 3D may use 2D rendering techniques.
541) 23.229273, Computer hardware - Wikipedia, the free encyclopedia.txt#11, term: computer, content:Computer data storage, often called storage or memory, refers to computer components and recording media that retain digital data. Data storage is a core function and fundamental component of computers. The price of solid-state drives (SSD), which store data on flash memory, has dropped a lot in recent years, making them a better choice than ever to add to a computer to make booting up and accessing files faster.[8]
542) 23.229273, Computer keyboard - Wikipedia, the free encyclopedia.txt#57, term: computer, content:One test for whether the computer has crashed is pressing the caps lock key. The keyboard sends the key code to the keyboard driver running in the main computer; if the main computer is operating, it commands the light to turn on. All the other indicator lights work in a similar way. The keyboard driver also tracks the Shift, alt and control state of the keyboard.
543) 23.229273, Computer memory - Wikipedia, the free encyclopedia.txt#12, term: computer, content:Non-volatile memory is computer memory that can retain the stored information even when not powered. Examples of non-volatile memory include read-only memory (see ROM), flash memory, most types of magnetic computer storage devices (e.g. hard disk drives, floppy disks and magnetic tape), optical discs, and early computer storage methods such as paper tape and punched cards.
544) 23.229273, Computer monitor - Wikipedia, the free encyclopedia.txt#0, term: computer, content:A computer monitor or a computer display is an electronic visual display for computers. A monitor usually comprises the display device, circuitry, casing, and power supply. The display device in modern monitors is typically a thin film transistor liquid crystal display (TFT-LCD) or a flat panel LED display, while older monitors used a cathode ray tubes (CRT). It can be connected to the computer via VGA, DVI, HDMI, DisplayPort, Thunderbolt, LVDS (Low-voltage differential signaling) or other proprietary connectors and signals.
545) 23.229273, Computer monitor - Wikipedia, the free encyclopedia.txt#1, term: computer, content:Originally, computer monitors were used for data processing while television receivers were used for entertainment. From the 1980s onwards, computers (and their monitors) have been used for both data processing and entertainment, while televisions have implemented some computer functionality. The common aspect ratio of televisions, and computer monitors, has changed from 4:3 to 16:10, to 16:9.
546) 23.229273, Computer mouse - Wikipedia, the free encyclopedia.txt#12, term: computer, content:The Xerox Alto was one of the first computers designed for individual use in 1973, and is regarded as the grandfather of computers that utilize the mouse.[21] Inspired by PARC's Alto, the Lilith, a computer which had been developed by a team around Niklaus Wirth at ETH Zrich between 1978 and 1980, provided a mouse as well. The third marketed version of an integrated mouse shipped as a part of a computer and intended for personal computer navigation came with the Xerox 8010 Star Information System in 1981.
547) 23.229273, Computer music - Wikipedia, the free encyclopedia.txt#0, term: computer, content:Computer music is the application of computing technology in music composition, to help human composers create new music or to have computers independently create music, such as with algorithmic composition programs. It includes the theory and application of new and existing computer software technologies and basic aspects of music, such as sound synthesis, digital signal processing, sound design, sonic diffusion, acoustics, and psychoacoustics. The field of computer music can trace its roots back to the origins of electronic music, and the very first experiments and innovations with electronic instruments at the turn of the 20th century.
548) 23.229273, Computer program - Wikipedia, the free encyclopedia.txt#22, term: computer, content:The main disadvantage of interpreters is computer programs run slower than when compiled. Interpreting code is slower because the interpreter must decode each statement and then perform it. However, software development may be faster using an interpreter because testing is immediate when the compiling step is omitted. Another disadvantage of interpreters is an interpreter must be present on the executing computer. By contrast, compiled computer programs need no compiler present during execution.
549) 23.229273, Computer program - Wikipedia, the free encyclopedia.txt#26, term: computer, content:Typically, computer programs are stored in non-volatile memory until requested either directly or indirectly to be executed by the computer user. Upon such a request, the program is loaded into random-access memory, by a computer program called an operating system, where it can be accessed directly by the central processor. The central processor then executes ("runs") the program, instruction by instruction, until termination. A program in execution is called a process.[20] Termination is either by normal self-termination or by error software or hardware error.
550) 23.229273, Computer program - Wikipedia, the free encyclopedia.txt#29, term: computer, content:A computer program in execution is normally treated as being different from the data the program operates on. However, in some cases, this distinction is blurred when a computer program modifies itself. The modified computer program is subsequently executed as part of the same program. Self-modifying code is possible for programs written in machine code, assembly language, Lisp, C, COBOL, PL/1, and Prolog.
551) 23.229273, Computer program - Wikipedia, the free encyclopedia.txt#36, term: computer, content:A stored-program computer requires an initial computer program stored in its read-only memory to boot. The boot process is to identify and initialize all aspects of the system, from processor registers to device controllers to memory contents.[25] Following the initialization process, this initial computer program loads the operating system and sets the program counter to begin normal operations.
552) 23.229273, Computer science - Wikipedia, the free encyclopedia.txt#0, term: computer, content:Computer science is the scientific and practical approach to computation and its applications. It is the systematic study of the feasibility, structure, expression, and mechanization of the methodical procedures (or algorithms) that underlie the acquisition, representation, processing, storage, communication of, and access to information. An alternate, more succinct definition of computer science is the study of automating algorithmic processes that scale. A computer scientist specializes in the theory of computation and the design of computational systems.[1]
553) 23.229273, Computer science - Wikipedia, the free encyclopedia.txt#6, term: computer, content:Time has seen significant improvements in the usability and effectiveness of computing technology.[17] Modern society has seen a significant shift in the users of computer technology, from usage only by experts and professionals, to a near-ubiquitous user base. Initially, computers were quite costly, and some degree of human aid was needed for efficient usein part from professional computer operators. As computer adoption became more widespread and affordable, less human assistance was needed for common usage.
554) 23.229273, Computer science - Wikipedia, the free encyclopedia.txt#12, term: computer, content:The relationship between computer science and software engineering is a contentious issue, which is further muddied by disputes over what the term "software engineering" means, and how computer science is defined.[36] David Parnas, taking a cue from the relationship between other engineering and science disciplines, has claimed that the principal focus of computer science is studying the properties of computation in general, while the principal focus of software engineering is the design of specific computations to achieve practical goals, making the two separate but complementary disciplines.[37]
555) 23.229273, Computer science - Wikipedia, the free encyclopedia.txt#28, term: computer, content:Computer security is a branch of computer technology, whose objective includes protection of information from unauthorized access, disruption, or modification while maintaining the accessibility and usability of the system for its intended users. Cryptography is the practice and study of hiding (encryption) and therefore deciphering (decryption) information. Modern cryptography is largely related to computer science, for many encryption and decryption algorithms are based on their computational complexity.
556) 23.229273, Computer-aided design - Wikipedia, the free encyclopedia.txt#0, term: computer, content:Computer-aided drafting (CAD) is the use of computer systems to aid in the creation, modification, analysis, or optimization of a design.[1] CAD software is used to increase the productivity of the designer, improve the quality of design, improve communications through documentation, and to create a database for manufacturing.[2] CAD output is often in the form of electronic files for print, machining, or other manufacturing operations. The term CADD, (for Computer Aided Design and Drafting) is also used.[3]
557) 23.229273, Computing - Wikipedia, the free encyclopedia.txt#0, term: computer, content:Computing is any goal-oriented activity requiring, benefiting from, or creating a mathematical sequence of steps known as an algorithm  e.g. through computers. Computing includes designing, developing and building hardware and software systems; processing, structuring, and managing various kinds of information; doing scientific research on and with computers; making computer systems behave intelligently; and creating and using communications and entertainment media. The field of computing includes computer engineering, software engineering, computer science, information systems, and information technology.
558) 23.229273, Computing - Wikipedia, the free encyclopedia.txt#13, term: computer, content:A computer is a machine that manipulates data according to a set of instructions called a computer program. The program has an executable form that the computer can use directly to execute the instructions. The same program in its human-readable source code form, enables a programmer to study and develop a sequence of steps known as an algorithm. Because the instructions can be carried out in different types of computers, a single set of source instructions converts to machine instructions according to the central processing unit type.
559) 23.229273, Computing - Wikipedia, the free encyclopedia.txt#38, term: computer, content:Information technology (IT) is the application of computers and telecommunications equipment to store, retrieve, transmit and manipulate data,[37] often in the context of a business or other enterprise.[38] The term is commonly used as a synonym for computers and computer networks, but it also encompasses other information distribution technologies such as television and telephones. Several industries are associated with information technology, such as computer hardware, software, electronics, semiconductors, internet, telecom equipment, e-commerce and computer services.[39][40]
560) 23.229273, Cray - Wikipedia, the free encyclopedia.txt#3, term: computer, content:The company's predecessor, Cray Research, Inc. (CRI), was founded in 1972 by computer designer Seymour Cray.[7] Seymour Cray went on to form the spin-off Cray Computer Corporation (CCC), in 1989, which went bankrupt in 1995, while Cray Research was bought by SGI the next year. Cray Inc. was formed in 2000 when Tera Computer Company purchased the Cray Research Inc. business from SGI and adopted the name of its acquisition.[8]
561) 23.229273, Data (computing) - Wikipedia, the free encyclopedia.txt#8, term: computer, content:Fundamentally, computers follow a sequence of instructions they are given in the form of data. A set of instructions to perform a given task (or tasks) is called a "program". In the nominal case, the program, as executed by the computer, will consist of binary machine code. The elements of storage manipulated by the program, but not actually executed by the CPU, are also data. The Marvellous twist is that program instructions; and data that the program manipulates, are both stored in exactly the same way. Therefore, it is possible for computer programs to operate on other computer programs, by manipulating their programmatic data.
562) 23.229273, Desktop computer - Wikipedia, the free encyclopedia.txt#3, term: computer, content:The very first "programmable calculator/computer" was marketed in the second half of the 1960s, starting with the Italian machinery Programma 101 (1965) computer at typewriter size.[1] More desktop models were introduced in 1971, leading to a model programmable in BASIC in 1972.[2] This one used a smaller version of a minicomputer design based on read-only memory (ROM) and had small one-line LED alphanumeric displays. They could draw computer graphics with a plotter.
563) 23.229273, Digital camera - Wikipedia, the free encyclopedia.txt#51, term: computer, content:A line-scan camera traditionally has a single row of pixel sensors, instead of a matrix of them. The lines are continuously fed to a computer that joins them to each other and makes an image. This is most commonly done by connecting the camera output to a frame grabber which resides in a PCI slot of an industrial computer. The frame grabber acts to buffer the image and sometimes provide some processing before delivering to the computer software for processing.
564) 23.229273, ENIAC - Wikipedia, the free encyclopedia.txt#35, term: computer, content:For a variety of reasons (including Mauchly's June 1941 examination of the AtanasoffBerry Computer, prototyped in 1939 by John Atanasoff and Clifford Berry), U.S. Patent 3,120,606 for ENIAC, applied for in 1947 and granted in 1964, was voided by the 1973 decision of the landmark federal court case Honeywell v. Sperry Rand, putting the invention of the electronic digital computer in the public domain and providing legal recognition to Atanasoff as the inventor of the first electronic digital computer.
565) 23.229273, Ferranti Mark 1 - Wikipedia, the free encyclopedia.txt#0, term: computer, content:The Ferranti Mark 1, also known as the Manchester Electronic Computer in its sales literature,[1] and thus sometimes called the Manchester Ferranti, was the world's first commercially available general-purpose electronic computer.[2] It was "the tidied up and commercialised version of the Manchester computer".[3] The first machine was delivered to the University of Manchester in February 1951, ahead of the UNIVAC I, which was turned over to the United States Census Bureau on 31 March 1951 (but not delivered until late December the following year[4]).
566) 23.229273, Firmware - Wikipedia, the free encyclopedia.txt#10, term: computer, content:Since 1996 most automobiles have employed an on-board computer and various sensors to detect mechanical problems. As of 2010[update], modern vehicles also employ computer-controlled ABS systems and computer-operated transmission control units (TCUs). The driver can also get in-dash information while driving in this manner, such as real-time fuel economy and tire pressure readings. Local dealers can update most vehicle firmware.
567) 23.229273, Grace Hopper - Wikipedia, the free encyclopedia.txt#0, term: computer, content:Grace Brewster Murray Hopper (December 9, 1906 January 1, 1992), ne Grace Brewster Murray, was an American computer scientist and United States Navy Rear Admiral.[1] She was one of the first programmers of the Harvard Mark I computer in 1944,[2] invented the first compiler for a computer programming language,[3][4][5][6][7] and was one of those who popularized the idea of machine-independent programming languages which led to the development of COBOL, one of the first high-level programming languages.
568) 23.229273, Grace Hopper - Wikipedia, the free encyclopedia.txt#22, term: computer, content:Brewster Academy, a school located in Wolfeboro, New Hampshire, United States, dedicated their computer lab to her in 1985, calling it the Grace Murray Hopper Center for Computer Learning.[24] The academy bestows a Grace Murray Hopper Prize to a graduate who excelled in the field of computer systems.[42] Hopper had spent her childhood summers at a family home in Wolfeboro.
569) 23.229273, Graphics tablet - Wikipedia, the free encyclopedia.txt#26, term: computer, content:Finally, tablets are gaining popularity as a replacement for the computer mouse as a pointing device.[when?] They can feel more intuitive to some users than a mouse, as the position of a pen on a tablet typically corresponds to the location of the pointer on the GUI shown on the computer screen. Those artists using a pen for graphics work will as a matter of convenience use a tablet and pen for standard computer operations rather than put down the pen and find a mouse.
570) 23.229273, History of computing hardware - Wikipedia, the free encyclopedia.txt#74, term: computer, content:In 1951, British scientist Maurice Wilkes developed the concept of microprogramming from the realisation that the Central Processing Unit of a computer could be controlled by a miniature, highly specialised computer program in high-speed ROM. Microprogramming allows the base instruction set to be defined or extended by built-in programs (now called firmware or microcode).[98] This concept greatly simplified CPU development. He first described this at the University of Manchester Computer Inaugural Conference in 1951, then published in expanded form in IEEE Spectrum in 1955.[citation needed]
571) 23.229273, History of computing hardware - Wikipedia, the free encyclopedia.txt#89, term: computer, content:The early 1960s saw the advent of supercomputing. The Atlas Computer was a joint development between the University of Manchester, Ferranti, and Plessey, and was first installed at Manchester University and officially commissioned in 1962 as one of the world's first supercomputers - considered to be the most powerful computer in the world at that time.[119] It was said that whenever Atlas went offline half of the United Kingdom's computer capacity was lost.[120] It was a second-generation machine, using discrete germanium transistors. Atlas also pioneered the Atlas Supervisor, "considered by many to be the first recognisable modern operating system".[121]
572) 23.229273, Home computer - Wikipedia, the free encyclopedia.txt#4, term: computer, content:The line between 'business' and 'home' computer market segments blurred or vanished completely once IBM PC compatibles became commonly used in the home, since now both categories of computers typically use the same processor architectures, peripherals, operating systems, and applications. Often the only difference may be the sales outlet through which they are purchased. Another change from the home computer era is that the once-common endeavour of writing one's own software programs has almost vanished from home computer use.[10][11]
573) 23.229273, Home computer - Wikipedia, the free encyclopedia.txt#12, term: computer, content:Some game consoles offered "programming packs" consisting of a version of BASIC in a ROM cartridge. Atari's BASIC Programming for the Atari 2600 was one of these. For the ColecoVision console, Coleco even announced an expansion module which would convert it into a full-fledged computer system. This never materialised, but a standalone computer, the Coleco Adam was eventually released.[19] The Magnavox Odyssey game console had a built-in keyboard to support its C7420 Home Computer Module.
574) 23.229273, Home computer - Wikipedia, the free encyclopedia.txt#37, term: computer, content:Gutman wrote that when the first computer boom ended in 1984, "Suddenly, everybody was saying that the home computer was a fad, just another hula hoop".[62] Robert Lydon, publisher of Personal Computing, stated in 1985 that the home market "never really existed. It was a fad. Just about everyone who was going to buy a computer for their home has done it", and predicted that Apple would cease to exist within two years.[63]
575) 23.229273, Human–computer interaction - Wikipedia, the free encyclopedia.txt#6, term: computer, content:Humancomputer interaction studies the ways in which humans make, or don't make, use of computational artifacts, systems and infrastructures. In doing so, much of the research in the field seeks to `improve' human-computer interaction by improving the `usability' of computer interfaces.[9] How `usability' is to be precisely understood, how it relates to other social and cultural values and when it is, and when it may not be a desirable property of computer interfaces is increasingly debated.[10][11]
576) 23.229273, Hybrid computer - Wikipedia, the free encyclopedia.txt#3, term: computer, content:Hybrid computers can be used to obtain a very good but relatively imprecise 'seed' value, using an analog computer front-end, which is then fed into a digital computer iterative process to achieve the final desired degree of precision. With a three or four digit, highly accurate numerical seed, the total digital computation time to reach the desired precision is dramatically reduced, since many fewer iterations are required. One of the main technical problems to be overcome in hybrid computers is minimizing digital-computer noise in analog computing elements and grounding systems.
577) 23.229273, IBM SSEC - Wikipedia, the free encyclopedia.txt#0, term: computer, content:The IBM Selective Sequence Electronic Calculator (SSEC) was an electromechanical computer built by IBM. Its design was started in late 1944, and it operated from January 1948 to 1952. It had many of the features of a stored-program computer and was the first operational machine able to treat its instructions as data, but it was not fully electronic.[1] Although the SSEC proved useful for several high-profile applications it soon became obsolete. As the last large electromechanical computer ever built, its greatest success was the publicity it provided for IBM.
578) 23.229273, J. Lyons and Co. - Wikipedia, the free encyclopedia.txt#13, term: computer, content:In 1964, Lyons sold their half-stake; and English Electric merged the company with Marconi's computer interests to form English Electric LEO Marconi Computers. A continuing problem in the British computer industry was both lack of investment capital and competition with the much larger U.S. computer companies, such as IBM. English Electric LEO Marconi Computers merged with other companies to form International Computers Limited, (ICL), which was bought by Fujitsu in 1990.
579) 23.229273, JSTOR - Wikipedia, the free encyclopedia.txt#12, term: computer, content:The following month, federal authorities charged Swartz with several "data theft"-related crimes, including wire fraud, computer fraud, unlawfully obtaining information from a protected computer, and recklessly damaging a protected computer.[21][22] Prosecutors in the case claimed that Swartz acted with the intention of making the papers available on P2P file-sharing sites.[20][23]
580) 23.229273, Konrad Zuse - Wikipedia, the free encyclopedia.txt#0, term: computer, content:Konrad Zuse (German: [knat tsuz]; 22 June 1910  18 December 1995) was a German civil engineer, inventor and computer pioneer. His greatest achievement was the world's first programmable computer; the functional program-controlled Turing-complete Z3 became operational in May 1941. Thanks to this machine and its predecessors, Zuse has often been regarded as the inventor of the modern computer.[2][3][4][5][6][7]
581) 23.229273, Konrad Zuse - Wikipedia, the free encyclopedia.txt#1, term: computer, content:Zuse was also noted for the S2 computing machine, considered the first process control computer. He founded one of the earliest computer businesses in 1941, producing the Z4, which became the world's first commercial computer. From 1943[8] to 1945[9] he designed the first high-level programming language, Plankalkl.[10] In 1969, Zuse suggested the concept of a computation-based universe in his book Rechnender Raum (Calculating Space).
582) 23.229273, LINC - Wikipedia, the free encyclopedia.txt#0, term: computer, content:The LINC (Laboratory INstrument Computer) is a 12-bit, 2048-word computer. The LINC is considered the first minicomputer and a forerunner to the personal computer.[1] Originally named the "Linc", suggesting the project's origins at MIT's Lincoln Laboratory, it was renamed LINC after the project moved from the Lincoln Laboratory.[2] The LINC was designed by Wesley A. Clark and Charles Molnar.
583) 23.229273, Linux - Wikipedia, the free encyclopedia.txt#39, term: computer, content:Another business model is to give away the software in order to sell hardware. This used to be the norm in the computer industry, with operating systems such as CP/M, Apple DOS and versions of Mac OS prior to 7.6 freely copyable (but not modifiable). As computer hardware standardized throughout the 1980s, it became more difficult for hardware manufacturers to profit from this tactic, as the OS would run on any manufacturer's computer that shared the same architecture.
584) 23.229273, Microprocessor - Wikipedia, the free encyclopedia.txt#38, term: computer, content:A computer-on-a-chip combines the microprocessor core (CPU), memory, and I/O (input/output) lines onto one chip. The computer-on-a-chip patent, called the "microcomputer patent" at the time, U.S. Patent 4,074,351, was awarded to Gary Boone and Michael J. Cochran of TI. Aside from this patent, the standard meaning of microcomputer is a computer using one or more microprocessors as its CPU(s), while the concept defined in the patent is more akin to a microcontroller.
585) 23.229273, Motorola 6800 - Wikipedia, the free encyclopedia.txt#2, term: computer, content:In addition to the ICs, Motorola also provided a complete assembly language development system. The customer could use the software on a remote timeshare computer or on an in-house minicomputer system. The Motorola EXORciser was a desktop computer built with the M6800 ICs that could be used for prototyping and debugging new designs. An expansive documentation package included datasheets on all ICs, two assembly language programming manuals, and a 700-page application manual that showed how to design a point-of-sale computer terminal.[7]
586) 23.229273, Non-volatile memory - Wikipedia, the free encyclopedia.txt#0, term: computer, content:Non-volatile memory, nonvolatile memory, NVM or non-volatile storage is a type of computer memory that can retrieve stored information even after having been power cycled (turned off and back on). Examples of non-volatile memory include read-only memory, flash memory, ferroelectric RAM (F-RAM), most types of magnetic computer storage devices (e.g. hard disk drives, floppy disks, and magnetic tape), optical discs, and early computer storage methods such as paper tape and punched cards.
587) 23.229273, Optical computing - Wikipedia, the free encyclopedia.txt#6, term: computer, content:A significant challenge to optical computing is that computation is a nonlinear process in which multiple signals must interact. Light, which is an electromagnetic wave, can only interact with another electromagnetic wave in the presence of electrons in a material,[8] and the strength of this interaction is much weaker for electromagnetic waves, such as light, than for the electronic signals in a conventional computer. This may result in the processing elements for an optical computer requiring more power and larger dimensions than those for a conventional electronic computer using transistors.[citation needed]
588) 23.229273, Personal computer - Wikipedia, the free encyclopedia.txt#34, term: computer, content:A workstation is a high-end personal computer designed for technical, mathematical, or scientific applications. Intended primarily to be used by one person at a time, they are commonly connected to a local area network and run multi-user operating systems. Workstations are used for tasks such as computer-aided design, drafting and modeling, computation-intensive scientific and engineering calculations, image processing, architectural modeling, and computer graphics for animation and motion picture visual effects.[54]
589) 23.229273, Personal computer - Wikipedia, the free encyclopedia.txt#59, term: computer, content:A computer case is an enclosure that contains the main components of a computer. They are usually constructed from steel or aluminum combined with plastic, although other materials such as wood have been used. Cases are available in different sizes and shapes; the size and shape of a computer case is usually determined by the configuration of the motherboard that it is designed to accommodate, since this is the largest and most central component of most computers.
590) 23.229273, Personal computer - Wikipedia, the free encyclopedia.txt#71, term: computer, content:A visual display unit, computer monitor or just display, is a piece of electrical equipment, usually separate from the computer case, which displays visual images without producing a permanent computer record. A display device is usually either a CRT or some form of flat panel such as a TFT LCD. Multi-monitor setups are also quite common.
591) 23.229273, Personal computer - Wikipedia, the free encyclopedia.txt#72, term: computer, content:The display unit houses an electronic circuitry that generates its picture from signals received from the computer. Within the computer, either integral to the motherboard or plugged into it as an expansion card, there is pre-processing circuitry to convert the microprocessor's output data to a format compatible with the display unit's circuitry. The images from computer monitors originally contained only text, but as graphical user interfaces emerged and became common, they began to display more images and multimedia content.
592) 23.229273, Programmer - Wikipedia, the free encyclopedia.txt#5, term: computer, content:Computer programmers write, test, debug, and maintain the detailed instructions, called computer programs, that computers must follow to perform their functions. Programmers also conceive, design, and test logical structures for solving problems by computer. Many technical innovations in programming advanced computing technologies and sophisticated new languages and programming tools have redefined the role of a programmer and elevated much of the programming work done today. Job titles and descriptions may vary, depending on the organization.
593) 23.229273, Quantum computing - Wikipedia, the free encyclopedia.txt#56, term: computer, content:In December 2015 NASA publicly displayed the world's first fully operational $15-million quantum computer made by the Canadian company D-Wave at the Quantum Artificial Intelligence Laboratory at its Ames Research Center in California's Moffett Field. The device was purchased in 2013 via a partnership with Google and Universities Space Research Association. Despite using quantum effects the algorithm run on the quantum computer does not outperform Selbys algorithm run on a classical computer.[82]
594) 23.229273, Read-only memory - Wikipedia, the free encyclopedia.txt#9, term: computer, content:Every stored-program computer may use a form of non-volatile storage (that is, storage that retains its data when power is removed) to store the initial program that runs when the computer is powered on or otherwise begins execution (a process known as bootstrapping, often abbreviated to "booting" or "booting up"). Likewise, every non-trivial computer needs some form of mutable memory to record changes in its state as it executes.
595) 23.229273, Software - Wikipedia, the free encyclopedia.txt#13, term: computer, content:Computer software has to be "loaded" into the computer's storage (such as the hard drive or memory). Once the software has loaded, the computer is able to execute the software. This involves passing instructions from the application software, through the system software, to the hardware which ultimately receives the instruction as machine code. Each instruction causes the computer to carry out an operationmoving data, carrying out a computation, or altering the control flow of instructions.
596) 23.229273, Sound card - Wikipedia, the free encyclopedia.txt#1, term: computer, content:A sound card (also known as an audio card) is an internal computer expansion card that facilitates economical input and output of audio signals to and from a computer under control of computer programs. The term sound card is also applied to external audio interfaces that use software to generate sound, as opposed to using hardware inside the PC. Typical uses of sound cards include providing the audio component for multimedia applications such as music composition, editing video or audio, presentation, education and entertainment (games) and video projection.
597) 23.229273, Spreadsheet - Wikipedia, the free encyclopedia.txt#27, term: computer, content:Bricklin has spoken of watching his university professor create a table of calculation results on a blackboard. When the professor found an error, he had to tediously erase and rewrite a number of sequential entries in the table, triggering Bricklin to think that he could replicate the process on a computer, using the blackboard as the model to view results of underlying formulas. His idea became VisiCalc, the first application that turned the personal computer from a hobby for computer enthusiasts into a business tool.
598) 23.229273, Tom Kilburn - Wikipedia, the free encyclopedia.txt#0, term: computer, content:Tom Kilburn CBE, FRS (11 August 1921  17 January 2001) was an English mathematician and computer scientist. Over the course of a productive 30-year career, he was involved in the development of five computers of great historical significance. With Freddie Williams he worked on the WilliamsKilburn Tube and the world's first stored-program computer, the Small-Scale Experimental Machine (SSEM), while working at the Victoria University of Manchester. His work propelled Manchester and Britain into the forefront of the emerging field of computer science.
599) 23.229273, Tom Kilburn - Wikipedia, the free encyclopedia.txt#12, term: computer, content:Over the years, Kilburn received numerous awards and accolades. He was elected a Fellow of the Royal Society (FRS) in 1965,[15] a Distinguished Fellow of the British Computer Society in 1974[16] and a fellow of the Computer History Museum "for his contributions to early computer design including random access digital storage, virtual memory and multiprogramming" in 2000.[17] He was created a Commander of the Most Excellent Order of the British Empire (CBE) in 1973,[18] and was awarded an honorary doctorate of science from the University of Bath in 1979.[19]
600) 23.229273, Transistor computer - Wikipedia, the free encyclopedia.txt#4, term: computer, content:The first fully transistorized computer was either the Harwell CADET which first operated in February 1955, although the price paid for this was that it only operated at the slow speed of 58kHz,[citation needed] or the prototype IBM 604 transistor calculator, described in the next section. The Burroughs Corporation claimed the SM-65 Atlas ICBM / THOR ABLE guidance computer (MOD 1) that it delivered to the US Air Force at the Cape Canaveral missile range in June 1957 was "the world's first operational transistorized computer".[citation needed]
601) 23.229273, UNIVAC I - Wikipedia, the free encyclopedia.txt#0, term: computer, content:The UNIVAC I (UNIVersal Automatic Computer I) was the first commercial computer produced in the United States.[1] It was designed principally by J. Presper Eckert and John Mauchly, the inventors of the ENIAC. Design work was started by their company, EckertMauchly Computer Corporation, and was completed after the company had been acquired by Remington Rand (which later became part of Sperry, now Unisys). In the years before successor models of the UNIVAC I appeared, the machine was simply known as "the UNIVAC".[2]
602) 23.229273, Windows 2000 - Wikipedia, the free encyclopedia.txt#52, term: computer, content:The Sysprep method is started on a standardized reference computer  though the hardware need not be similar  and it copies the required installation files from the reference computer to the target computers. The hard drive does not need to be in the target computer and may be swapped out to it at any time, with the hardware configured later. The Winnt.exe program must also be passed a /unattend switch that points to a valid answer file and a /s file that points to one or more valid installation sources.
603) 22.49165, Computer - Simple English Wikipedia, the free encyclopedia.txt#31, term: computer, content:An operating system tells the computer how to understand what jobs it has to do, how to do these jobs, and how to tell people the results. It tells the electronics inside the computer, or "hardware", how to work to get the results it needs. This lets most computers have the same operating system, or list of orders to tell it how to talk to the user, while each computer can have its own computer programs or list of jobs to do what its user needs. Having different programs and operating systems makes it easy to learn how to use computers for new things. When a user needs to use a computer for something different, the user can learn how to use a new program. Some operating systems can have simple command lines or a fully user-frendly GUI
604) 22.49165, Computer graphics - Wikipedia, the free encyclopedia.txt#4, term: computer, content:Many powerful tools have been developed to visualize data. Computer generated imagery can be categorized into several different types: two dimensional (2D), three dimensional (3D), and animated graphics. As technology has improved, 3D computer graphics have become more common, but 2D computer graphics are still widely used. Computer graphics has emerged as a sub-field of computer science which studies methods for digitally synthesizing and manipulating visual content. Over the past decade, other specialized fields have been developed like information visualization, and scientific visualization more concerned with "the visualization of three dimensional phenomena (architectural, meteorological, medical, biological, etc.), where the emphasis is on realistic renderings of volumes, surfaces, illumination sources, and so forth, perhaps with a dynamic (time) component".[3]
605) 22.49165, Computer network - Wikipedia, the free encyclopedia.txt#6, term: computer, content:A computer network facilitates interpersonal communications allowing users to communicate efficiently and easily via various means: email, instant messaging, chat rooms, telephone, video telephone calls, and video conferencing. Providing access to information on shared storage devices is an important feature of many networks. A network allows sharing of files, data, and other types of information giving authorized users the ability to access information stored on other computers on the network. A network allows sharing of network and computing resources. Users may access and use resources provided by devices on the network, such as printing a document on a shared network printer. Distributed computing uses computing resources across a network to accomplish tasks. A computer network may be used by computer crackers to deploy computer viruses or computer worms on devices connected to the network, or to prevent these devices from accessing the network via a denial of service attack.
606) 22.49165, Computing - Wikipedia, the free encyclopedia.txt#29, term: computer, content:Computer programming in general is the process of writing, testing, debugging, and maintaining the source code and documentation of computer programs. This source code is written in a programming language, which is an artificial language often more restrictive or demanding than natural languages, but easily translated by the computer. The purpose of programming is to invoke the desired behavior (customization) from the machine. The process of writing high quality source code requires knowledge of both the application's domain and the computer science domain. The highest-quality software is thus developed by a team of various domain experts, each person a specialist in some area of development. But the term programmer may apply to a range of program quality, from hacker to open source contributor to professional. And a single programmer could do most or all of the computer programming needed to generate the proof of concept to launch a new "killer" application.
607) 22.49165, Imperative programming - Wikipedia, the free encyclopedia.txt#5, term: computer, content:The hardware implementation of almost all computers is imperative;[note 1] nearly all computer hardware is designed to execute machine code, which is native to the computer, written in the imperative style. From this low-level perspective, the program state is defined by the contents of memory, and the statements are instructions in the native machine language of the computer. Higher-level imperative languages use variables and more complex statements, but still follow the same paradigm. Recipes and process checklists, while not computer programs, are also familiar concepts that are similar in style to imperative programming; each step is an instruction, and the physical world holds the state. Since the basic ideas of imperative programming are both conceptually familiar and directly embodied in the hardware, most computer languages are in the imperative style.
608) 22.49165, Wearable computer - Wikipedia, the free encyclopedia.txt#46, term: computer, content:In 1998, Seiko marketed the Ruputer, a computer in a (fairly large) wristwatch, to mediocre returns. In 2001, IBM developed and publicly displayed two prototypes for a wristwatch computer running Linux. The last message about them dates to 2004, saying the device would cost about $250, but it is still under development. In 2002, Fossil, Inc. announced the Fossil Wrist PDA, which ran the Palm OS. Its release date was set for summer of 2003, but was delayed several times and was finally made available on January 5, 2005. Timex Datalink is another example of a practical wearable computer. Hitachi launched a wearable computer called Poma in 2002. Eurotech offers the ZYPAD, a wrist wearable touch screen computer with GPS, Wi-Fi and Bluetooth connectivity and which can run a number of custom applications.[45] In 2013, a wearable computing device on the wrist to control body temperature was developed at MIT.[46]
609) 22.177063, Central processing unit - Wikipedia, the free encyclopedia.txt#4, term: computer, content:The idea of a stored-program computer was already present in the design of J. Presper Eckert and John William Mauchly's ENIAC, but was initially omitted so that it could be finished sooner.[5] On June30, 1945, before ENIAC was made, mathematician John von Neumann distributed the paper entitled First Draft of a Report on the EDVAC. It was the outline of a stored-program computer that would eventually be completed in August 1949.[6] EDVAC was designed to perform a certain number of instructions (or operations) of various types. Significantly, the programs written for EDVAC were to be stored in high-speed computer memory rather than specified by the physical wiring of the computer.[7] This overcame a severe limitation of ENIAC, which was the considerable time and effort required to reconfigure the computer to perform a new task. With von Neumann's design, the program that EDVAC ran could be changed simply by changing the contents of the memory. EDVAC, however, was not the first stored-program computer; the Manchester Small-Scale Experimental Machine, a small prototype stored-program computer, ran its first program on 21 June 1948[8] and the Manchester Mark 1 ran its first program during the night of 1617 June 1949.[9]
610) 22.177063, Computer - Simple English Wikipedia, the free encyclopedia.txt#36, term: computer, content:A computer has several main parts. When comparing a computer to a human body, the CPU is like a brain. It does most of the 'thinking' and tells the rest of the computer how to work. The CPU is on the Motherboard, which is like the skeleton. It provides the basis for where the other parts go, and carries the nerves that connect them to each other and the CPU. The motherboard is connected to a power supply, which provides electricity to the entire computer. The various drives (CD drive, floppy drive, and on many newer computers, USB flash drive) act like eyes, ears, and fingers, and allow the computer to read different types of storage, in the same way that a human can read different types of books. The hard drive is like a human's memory, and keeps track of all the data stored on the computer. Most computers have a sound card or another method of making sound, which is like vocal cords, or a voice box. Connected to the sound card are speakers, which are like a mouth, and are where the sound comes out. Computers might also have a graphics card, which helps the computer to create visual effects, such as 3D environments, or more realistic colors, and more powerful graphics cards can make more realistic or more advanced images, in the same way a well trained artist can.
611) 22.177063, Computer animation - Wikipedia, the free encyclopedia.txt#17, term: computer, content:Computer animation can be created with a computer and an animation software. Some impressive animation can be achieved even with basic programs; however, the rendering can take a lot of time on an ordinary home computer.[19] Professional animators of movies, television and video games games could make photorealistic animation with high detail. This level of quality for movie animation would take hundreds of years to create on a home computer. Instead, many powerful workstation computers are used. Graphics workstation computers use two to four processors, and they are a lot more powerful than an actual home computer and are specialized for rendering. A large number of workstations (known as a "render farm") are networked together to effectively act as a giant computer. The result is a computer-animated movie that can be completed in about one to five years (however, this process is not composed solely of rendering). A workstation typically costs $2,000-16,000 with the more expensive stations being able to render much faster due to the more technologically-advanced hardware that they contain. Professionals also use digital movie cameras, motion/performance capture, bluescreens, film editing software, props, and other tools used for movie animation.
612) 22.177063, Computer engineering - Wikipedia, the free encyclopedia.txt#19, term: computer, content:According to the U.S. Bureau of Labor Statistics (BLS), "computer applications software engineers and computer systems software engineers are projected to be among the faster than average growing occupations" from 2014-24 is 17%.[10] This is down from the 2012 to 2022 BLS estimate of 22% for software developers.[7][11] And, further down from the 30% 2010 to 2020 BLS estimate.[12] In addition, growing concerns over cyber security add up to put computer software engineering high above the average rate of increase for all fields. However, some of the work will be outsourced in foreign countries. Due to this, job growth will not be as fast as during the last decade, as jobs that would have gone to computer software engineers in the United States would instead go to computer software engineers in countries such as India.[13] In addition the BLS Job Outlook for Computer Programmers, 2014-24 has an -8% (a decline in their words)[13] for those who program computers (i.e. embedded systems) who are not computer application developers.
613) 22.177063, Programmer - Wikipedia, the free encyclopedia.txt#0, term: computer, content:A programmer, computer programmer, developer, dev, coder, or software engineer a person who writes computer software. The term computer programmer can refer to a specialist in one area of computer programming or to a generalist who writes code for many kinds of software. One who practices or professes a formal approach to programming may also be known as a programmer analyst. A programmer's primary computer language (Assembly, COBOL, C, C++, C#, Java, Lisp, Python, etc.) is often prefixed to these titles, and those who work in a Web environment often prefix their titles with Web. The term programmer can be used to refer to a software developer, Web developer, mobile applications developer, embedded firmware developer, software engineer, computer scientist, or software analyst. However, members of these professions possess other software engineering skills, beyond programming; for this reason, the term programmer, or code monkey, is sometimes considered an insulting or derogatory oversimplification of these other professions.[1] This has sparked much debate amongst developers, analysts, computer scientists, programmers, and outsiders who continue to be puzzled at the subtle differences in the definitions of these occupations.[2][3][4][5][6]
614) 21.20533, Home computer - Wikipedia, the free encyclopedia.txt#35, term: computer, content:In 1977, referring to computers used in home automation at the dawn of the home computer era, Digital Equipment Corporation CEO Ken Olsen is quoted as saying "There is no reason for any individual to have a computer in his home."[43] Despite Olsen's warning, in the late 1970s and early 1980s, from about 1977 to 1983, it was widely predicted[44] that computers would soon revolutionize many aspects of home and family life as they had business practices in the previous decades.[45] Mothers would keep their recipe catalog in "kitchen computer" databases and turn to a medical database for help with child care, fathers would use the family's computer to manage family finances and track automobile maintenance. Children would use online encyclopedias[46] for school work and would be avid video gamers. The computer would even be tasked with babysitting younger children.[47] Home automation would bring about the intelligent home of the 1980s. Using Videotex, NAPLPS or some sort of vaguely conceptualized computer technology, television would gain interactivity. It would be possible to do the week's grocery shopping through the television.[48] The "personalized newspaper" (to be displayed on the television screen) was another commonly predicted application.[49] Morning coffee would be brewed automatically under computer control.[50][51] The same household computer would control the home's lighting and temperature. Robots would take the garbage out, and be programmed to perform new tasks via the home computer. Electronics were expensive, so it was generally assumed that each home would have only one computer for the entire family to use.[52] Home control would be performed in a multitasking time-sharing arrangement, with interfaces to the various devices it was expected to control.
615) 20.531971, Computer music - Wikipedia, the free encyclopedia.txt#18, term: computer, content:Another 'cybernetic' approach to computer composition uses specialized hardware to detect external stimuli which are then mapped by the computer to realize the performance. Examples of this style of computer music can be found in the middle-80's work of David Rokeby (Very Nervous System) where audience/performer motions are 'translated' to MIDI segments. Computer controlled music is also found in the performance pieces by the Canadian composer Udo Kasemets such as the Marce(ntennia)l Circus C(ag)elebrating Duchamp (1987), a realization of the Marcel Duchamp process piece Erratum Musical using an electric model train to collect a hopper-car of stones to be deposited on a drum wired to an Analog:Digital converter, mapping the stone impacts to a score display (performed in Toronto by pianist Gordon Monahan during the 1987 Duchamp Centennial), or his installations and performance works (e.g. Spectrascapes) based on his Geo(sono)scope (1986) 15x4-channel computer-controlled audio mixer. In these latter works, the computer generates sound-scapes from tape-loop sound samples, live shortwave or sine-wave generators.[citation needed]
616) 20.531971, Computer-aided manufacturing - Wikipedia, the free encyclopedia.txt#0, term: computer, content:Computer-aided manufacturing (CAM) is the use of software to control machine tools and related ones in the manufacturing of workpieces.[1][2][3][4][5] This is not the only definition for CAM, but it is the most common;[1] CAM may also refer to the use of a computer to assist in all operations of a manufacturing plant, including planning, management, transportation and storage.[6][7] Its primary purpose is to create a faster production process and components and tooling with more precise dimensions and material consistency, which in some cases, uses only the required amount of raw material (thus minimizing waste), while simultaneously reducing energy consumption.[citation needed] CAM is now a system used in schools and lower educational purposes. CAM is a subsequent computer-aided process after computer-aided design (CAD) and sometimes computer-aided engineering (CAE), as the model generated in CAD and verified in CAE can be input into CAM software, which then controls the machine tool. CAM is used in many schools alongside Computer Aided Design (CAD) to create objects.
617) 20.531971, Stored-program computer - Wikipedia, the free encyclopedia.txt#6, term: computer, content:The University of Manchester's Small-Scale Experimental Machine (SSEM)[15] is generally recognized as world's first electronic computer that ran a stored programan event that occurred on 21 June 1948.[16][17] However the SSEM was not regarded as a full-fledged computer, but more a proof of concept predecessor to the Manchester Mark 1 computer, which was first put to research work in April 1949. On 6 May 1949 the EDSAC in Cambridge ran its first program, making it arguably "the first complete and fully operational regular electronic digital stored-program computer".[18] It is sometimes claimed that the IBM SSEC, operational in January 1948, was the first stored-program computer;[19] this claim is controversial, not least because of the hierarchical memory system of the SSEC, and because some aspects of its operations, like access to relays or tape drives, were determined by plugging.[20] The first stored-program computer to be built in continental Europe was the MESM, completed in the Soviet Union in 1951.[21]
618) 20.325613, Alan Turing - Wikipedia, the free encyclopedia.txt#44, term: computer, content:In 1948 Turing, working with his former undergraduate colleague, D. G. Champernowne, began writing a chess program for a computer that did not yet exist. By 1950, the programme was completed and dubbed the Turochamp.[93] In 1952, he tried to implement it on a Ferranti Mark 1, but lacking enough power, the computer was unable to execute the programme. Instead, Turing played a game in which he simulated the computer, taking about half an hour per move. The game was recorded.[94] The program lost to Turing's colleague Alick Glennie, although it is said that it won a game against Champernowne's wife.
619) 20.325613, Artificial intelligence - Wikipedia, the free encyclopedia.txt#68, term: computer, content:A derivative of the Turing test is the Completely Automated Public Turing test to tell Computers and Humans Apart (CAPTCHA). As the name implies, this helps to determine that a user is an actual person and not a computer posing as a human. In contrast to the standard Turing test, CAPTCHA administered by a machine and targeted to a human as opposed to being administered by a human and targeted to a machine. A computer asks a user to complete a simple test then generates a grade for that test. Computers are unable to solve the problem, so correct solutions are deemed to be the result of a person taking the test. A common type of CAPTCHA is the test that requires the typing of distorted letters, numbers or symbols that appear in an image undecipherable by a computer.[198]
620) 20.325613, BIOS - Wikipedia, the free encyclopedia.txt#27, term: computer, content:Later computers would display a message like "No bootable disk found"; some would prompt for a disk to be inserted and a key to be pressed, and when a key was pressed they would restart the boot process. A modern BIOS may display nothing or may automatically enter the BIOS configuration utility when the boot process fails. Unlike earlier BIOSes, modern versions are often written with the assumption that if the computer cannot be booted from a hard disk, the user will not have software that they want to boot from removable media instead. (Lately, typically it will only be a specialist computer technician who does that, only to get the computer back into a condition where it can be booted from the hard disk.)
621) 20.325613, Booting - Wikipedia, the free encyclopedia.txt#3, term: computer, content:Boot is short for bootstrap[1][2] or bootstrap load and derives from the phrase to pull oneself up by one's bootstraps.[3] The usage calls attention to the requirement that, if most software is loaded onto a computer by other software already running on the computer, some mechanism must exist to load the initial software onto the computer.[4] Early computers used a variety of ad-hoc methods to get a small program into memory to solve this problem. The invention of read-only memory (ROM) of various types solved this paradox by allowing computers to be shipped with a start up program that could not be erased. Growth in the capacity of ROM has allowed ever more elaborate start up procedures to be implemented.
622) 20.325613, Computer - Wikipedia, the free encyclopedia.txt#32, term: computer, content:The Mark 1 in turn quickly became the prototype for the Ferranti Mark 1, the world's first commercially available general-purpose computer.[39] Built by Ferranti, it was delivered to the University of Manchester in February 1951. At least seven of these later machines were delivered between 1953 and 1957, one of them to Shell labs in Amsterdam.[40] In October 1947, the directors of British catering company J. Lyons & Company decided to take an active role in promoting the commercial development of computers. The LEO I computer became operational in April 1951[41] and ran the world's first regular routine office computer job.
623) 20.325613, Computer - Wikipedia, the free encyclopedia.txt#47, term: computer, content:While it is possible to write computer programs as long lists of numbers (machine language) and while this technique was used with many early computers,[55] it is extremely tedious and potentially error-prone to do so in practice, especially for complicated programs. Instead, each basic instruction can be given a short name that is indicative of its function and easy to remember a mnemonic such as ADD, SUB, MULT or JUMP. These mnemonics are collectively known as a computer's assembly language. Converting programs written in assembly language into something the computer can actually understand (machine language) is usually done by a computer program called an assembler.
624) 20.325613, Computer data storage - Wikipedia, the free encyclopedia.txt#17, term: computer, content:As the RAM types used for primary storage are volatile (uninitialized at start up), a computer containing only such storage would not have a source to read instructions from, in order to start the computer. Hence, non-volatile primary storage containing a small startup program (BIOS) is used to bootstrap the computer, that is, to read a larger program from non-volatile secondary storage to RAM and start to execute it. A non-volatile technology used for this purpose is called ROM, for read-only memory (the terminology may be somewhat confusing as most ROM types are also capable of random access).
625) 20.325613, Computer hardware - Wikipedia, the free encyclopedia.txt#21, term: computer, content:When using computer hardware, an upgrade means adding new hardware to a computer that improves its performance, adds capacity or new features. For example, a user could perform a hardware upgrade to replace the hard drive with a SSD to get a boost in performance or increase the amount of files that may be stored. Also, the user could increase the RAM so the computer may run more smoothly. The user could add a USB 3.0 expansion card in order to fully use USB 3.0 devices, or could upgrade the GPU for extra rendering power. Performing such hardware upgrades may be necessary for older computers to meet a programs' system requirements.
626) 20.325613, Computer keyboard - Wikipedia, the free encyclopedia.txt#56, term: computer, content:The first computer keyboards were for mainframe computer data terminals and used discrete electronic parts. The first keyboard microprocessor was introduced in 1972 by General Instruments, but keyboards have been using the single-chip 8048 microcontroller variant since it became available in 1978. The keyboard switch matrix is wired to its inputs, it converts the keystrokes to key codes, and, for a detached keyboard, sends the codes down a serial cable (the keyboard cord) to the main processor on the computer motherboard. This serial keyboard cable communication is only bi-directional to the extent that the computer's electronics controls the illumination of the caps lock, num lock and scroll lock lights.
627) 20.325613, Computer music - Wikipedia, the free encyclopedia.txt#2, term: computer, content:Much of the work on computer music has drawn on the relationship between music theory and mathematics, a relationship which has been noted since the Ancient Greeks described the "harmony of the spheres". The world's first computer to play music was CSIRAC, which was designed and built by Trevor Pearcey and Maston Beard in the 1950s. Mathematician Geoff Hill programmed the CSIRAC to play popular musical melodies from the very early 1950s. In 1951 it publicly played the "Colonel Bogey March"[1] of which no known recordings exist. However, CSIRAC played standard repertoire and was not used to extend musical thinking or composition practice which is current computer-music practice.
628) 20.325613, Computer music - Wikipedia, the free encyclopedia.txt#8, term: computer, content:Advances in computing power and software for manipulation of digital media have dramatically affected the way computer music is generated and performed. Current-generation micro-computers are powerful enough to perform very sophisticated audio synthesis using a wide variety of algorithms and approaches. Computer music systems and approaches are now ubiquitous, and so firmly embedded in the process of creating music that we hardly give them a second thought: computer-based synthesizers, digital mixers, and effects units have become so commonplace that use of digital rather than analog technology to create and record music is the norm, rather than the exception.[citation needed]
629) 20.325613, Computing - Wikipedia, the free encyclopedia.txt#36, term: computer, content:Its subfields can be divided into practical techniques for its implementation and application in computer systems and purely theoretical areas. Some, such as computational complexity theory, which studies fundamental properties of computational problems, are highly abstract, while others, such as computer graphics, emphasize real-world applications. Still others focus on the challenges in implementing computations. For example, programming language theory studies approaches to description of computations, while the study of computer programming itself investigates various aspects of the use of programming languages and complex systems, and humancomputer interaction focuses on the challenges in making computers and computations useful, usable, and universally accessible to humans.
630) 20.325613, Data (computing) - Wikipedia, the free encyclopedia.txt#2, term: computer, content:Digital data is data that is represented using the binary number system of ones (1) and zeros (0). As opposed to analog representation. In modern (post 1960) computer systems, all data is digital. Data within a computer, in most cases, moves as parallel data. Data moving to or from a computer, in most cases, moves as serial data. See Parallel communication and Serial communication. Data sourced from an analog device, such as a temperature sensor, must pass through an "analog to digital converter" or "ADC" (see Analog-to-digital converter) to convert the analog data to digital data.
631) 20.325613, Desktop computer - Wikipedia, the free encyclopedia.txt#0, term: computer, content:A desktop computer is a personal computer designed for regular use at a single location on or near a desk or table due to its size and power requirements. The most common configuration has a case that houses the power supply, motherboard (a printed circuit board with a microprocessor as the central processing unit, memory, bus, and other electronic component's), disk storage (usually one or more hard disk drives, optical disc drives, and in early models floppy disk drives); a keyboard and mouse for input; and computer monitor and printer for output. The case may be oriented horizontally and placed atop a desk or vertically and placed underneath or beside a desk.
632) 20.325613, Digital electronics - Wikipedia, the free encyclopedia.txt#78, term: computer, content:The cost of a logic gate is crucial, primarily because very many gates are needed to build a computer or other advanced digital system and because the more gates can be used, the more capable and/or fast the machine can be. Since the majority of a digital computer is simply an interconnected network of logic gates, the overall cost of building a computer correlates strongly with the price per logic gate. In the 1930s, the earliest digital logic systems were constructed from telephone relays because these were inexpensive and relatively reliable. After that, engineers always used the cheapest available electronic switches that could still fulfill the requirements.
633) 20.325613, DNA computing - Wikipedia, the free encyclopedia.txt#12, term: computer, content:By default, the computer is supposed to play first in the central square. The human player has then as a starter eight different types of DNA strands assigned to each of eight boxes that may be played. To indicate that box nr. i is being ticked, the human player pours into all bins the strands corresponding to input #i. These strands bind to certain DNA enzymes present in the bins, resulting in one of these two bins in the deformation of the DNA enzymes which binds to the substrate and cuts it. The corresponding bin becomes fluorescent, indicating which box is being played by the DNA computer. The various DNA enzymes are divided into various bins in such a way to ensure the victory of the DNA computer against the human player.
634) 20.325613, Electrical engineering - Wikipedia, the free encyclopedia.txt#19, term: computer, content:In 1941, Konrad Zuse presented the Z3, the world's first fully functional and programmable computer using electromechanical parts. In 1943, Tommy Flowers designed and built the Colossus, the world's first fully functional, electronic, digital and programmable computer.[21] In 1946, the ENIAC (Electronic Numerical Integrator and Computer) of John Presper Eckert and John Mauchly followed, beginning the computing era. The arithmetic performance of these machines allowed engineers to develop completely new technologies and achieve new objectives, including the Apollo program which culminated in landing astronauts on the Moon.[22]
635) 20.325613, Electromechanics - Wikipedia, the free encyclopedia.txt#5, term: computer, content:Electric typewriters developed, up to the 1980s, as "power-assisted typewriters". They contained a single electrical component, the motor. Where the keystroke had previously moved a typebar directly, now it engaged mechanical linkages that directed mechanical power from the motor into the typebar. This was also true of the later IBM Selectric. At Bell Labs, in the 1940s, the Bell Model V computer was developed. It was an electromechanical relay-based device; cycles took seconds. In 1968 electromechanical systems were still under serious consideration for an aircraft flight control computer, until a device based on large scale integration electronics was adopted in the Central Air Data Computer.
636) 20.325613, Ferranti Mark 1 - Wikipedia, the free encyclopedia.txt#8, term: computer, content:Included in the Ferranti Mark 1's instruction set was a hoot command, which enabled the machine to give auditory feedback to its operators. The sound generated could be altered in pitch, a feature which was exploited when the Mark 1 made the earliest known recording of computer-generated music, playing a medley which included "God Save the King", "Baa Baa Black Sheep", and "In the Mood".[7] The recording was made by the BBC towards the end of 1951, with the programming being done by Christopher Strachey, a mathematics teacher at Harrow and a friend of Alan Turing. It was not, however, the first computer to have played music; CSIRAC, Australia's first digital computer, achieved that with a rendition of "Colonel Bogey".[8]
637) 20.325613, Ferranti Mark 1 - Wikipedia, the free encyclopedia.txt#9, term: computer, content:In November 1951, Dr. Dietrich Prinz wrote one of the oldest computer games, a chess-playing program for the Manchester Ferranti Mark 1 computer. The limitation of the Mark 1 computer did not allow for a whole game of chess to be programmed. Prinz could only program mate-in-two chess problems. The program examined every possible move for White and Black (thousands of possible moves) until a solution was found, which took 1520 minutes on average. The programs restrictions were: no castling, no double pawn move, no en passant capture, no pawn promotion, and no distinction between checkmate and stalemate.[citation needed]
638) 20.325613, Grace Hopper - Wikipedia, the free encyclopedia.txt#12, term: computer, content:In the 1970s, Hopper advocated for the Defense Department to replace large, centralized systems with networks of small, distributed computers. Any user on any computer node could access common databases located on the network.[20]:119 She developed the implementation of standards for testing computer systems and components, most significantly for early programming languages such as FORTRAN and COBOL. The Navy tests for conformance to these standards led to significant convergence among the programming language dialects of the major computer vendors. In the 1980s, these tests (and their official administration) were assumed by the National Bureau of Standards (NBS), known today as the National Institute of Standards and Technology (NIST).
639) 20.325613, History of computing hardware - Wikipedia, the free encyclopedia.txt#71, term: computer, content:In October 1947, the directors of J. Lyons & Company, a British catering company famous for its teashops but with strong interests in new office management techniques, decided to take an active role in promoting the commercial development of computers. The LEO I computer became operational in April 1951[92] and ran the world's first regular routine office computer job. On 17 November 1951, the J. Lyons company began weekly operation of a bakery valuations job on the LEO (Lyons Electronic Office). This was the first business application to go live on a stored program computer.[93]
640) 20.325613, Home computer - Wikipedia, the free encyclopedia.txt#3, term: computer, content:By contrast, advertisements in the specialty computer press often simply listed specifications.[5][6] If no packaged software was available for a particular application, the home computer user could program oneprovided they had invested the requisite hours to learn computer programming, as well as the idiosyncrasies of their system.[7][8] Since most systems shipped with the BASIC programming language included on the system ROM, it was easy for users to get started creating their own simple applications. Many users found programming to be a fun and rewarding experience, and an excellent introduction to the world of digital technology.[9]
641) 20.325613, Human–computer interaction - Wikipedia, the free encyclopedia.txt#13, term: computer, content:Three areas of study have substantial overlap with HCI even as the focus of inquiry shifts. In the study of personal information management (PIM), human interactions with the computer are placed in a larger informational context  people may work with many forms of information, some computer-based, many not (e.g., whiteboards, notebooks, sticky notes, refrigerator magnets) in order to understand and effect desired changes in their world. In computer-supported cooperative work (CSCW), emphasis is placed on the use of computing systems in support of the collaborative work of a group of people. The principles of human interaction management (HIM) extend the scope of CSCW to an organizational level and can be implemented without use of computers.
642) 20.325613, Human–computer interaction - Wikipedia, the free encyclopedia.txt#40, term: computer, content:Traditionally, as explained in a journal article discussing user modeling and user-adapted interaction, computer use was modeled as a human-computer dyad in which the two were connected by a narrow explicit communication channel, such as text-based terminals. Much work has been done to make the interaction between a computing system and a human. However, as stated in the introduction, there is much room for mishaps and failure. Because of this, human-computer interaction shifted focus beyond the interface (to respond to observations as articulated by D. Engelbart: "If ease of use was the only valid criterion, people would stick to tricycles and never try bicycles."[23]
643) 20.325613, IBM PC compatible - Wikipedia, the free encyclopedia.txt#15, term: computer, content:IBM believed that some companies such as Eagle, Corona, and Handwell infringed on its copyright, and after Apple Computer, Inc. v. Franklin Computer Corp. successfully forced the clone makers to stop using the BIOS. The Phoenix BIOS in 1984, however, and similar products such as AMI BIOS, permitted computer makers to legally build essentially 100%-compatible clones without having to reverse-engineer the PC BIOS themselves.[20][21][22] A September 1985 InfoWorld chart listed seven compatibles with 256 KB RAM, two disk drives, and monochrome monitors for $1,495 to $2,320, while the equivalent IBM PC cost $2,820.[23]
644) 20.325613, International Federation for Information Processing - Wikipedia, the free encyclopedia.txt#32, term: computer, content:IFIP TC 13 on Human-Computer Interaction was founded in 1989. It aims to encourage empirical research (using valid and reliable methodology, with studies of the methods themselves where necessary); to promote the use of knowledge and methods from the human sciences in both design and evaluation of computer systems; to promote better understanding of the relation between formal design methods and system usability and acceptability; to develop guidelines, models and methods by which designers may be able to provide better human-oriented computer systems; and to co-operate with other groups, inside and outside IFIP, so as to promote user-orientation and "humani-zation" in system design.
645) 20.325613, Konrad Zuse - Wikipedia, the free encyclopedia.txt#23, term: computer, content:Zuse founded another company, Zuse KG in Haunetal-Neukirchen in 1949; in 1957 the companys head office moved to Bad Hersfeld. The Z4 was finished and delivered to the ETH Zurich, Switzerland in September 1950. At that time, it was the only working computer in continental Europe, and the second computer in the world to be sold, only beaten by the BINAC, which never worked properly after it was delivered. Other computers, all numbered with a leading Z, up to Z43,[35] were built by Zuse and his company. Notable are the Z11, which was sold to the optics industry and to universities, and the Z22, the first computer with a memory based on magnetic storage.[36]
646) 20.325613, Media player (software) - Wikipedia, the free encyclopedia.txt#4, term: computer, content:A home theater PC or media center computer is a convergence device that combines some or all the capabilities of a personal computer with a software application that supports video, photo, audio playback, and sometimes video recording functionality. Although computers with some of these capabilities were available from the late 1980s, the "Home Theater PC" term first appeared in mainstream press in 1996. Since 2007, other types of consumer electronics, including gaming systems and dedicated media devices have crossed over to manage video and music content. The term "media center" also refers to specialized computer programs designed to run on standard personal computers.[6]
647) 20.325613, Motorola 6800 - Wikipedia, the free encyclopedia.txt#14, term: computer, content:Mike Wiles was a design engineer in Jeff LaVell's group and made numerous customer visits with Tom Bennett during 6800 product definition phase. He is listed as an inventor on eighteen 6800 patents but is best known for a computer program, MIKBUG.[23] This was a monitor for a 6800 computer system that allowed the user to examine the contents of RAM and to save or load programs to tape. This 512 byte program occupied half of an MCM6830 ROM.[24] This ROM was used in the Motorola MEK6800 design evaluation kit and early hobby computer kits.[25] Wiles stayed with Motorola, moved to Austin and helped design the MC6801 microcontroller that was released in 1978.[26]
648) 20.325613, Motorola 6800 - Wikipedia, the free encyclopedia.txt#45, term: computer, content:Sphere was a small startup company and had difficulties delivering all of the products they announced. They filed for a Chapter 11 bankruptcy in April 1977.[85] The Altair 680B was popular but MITS focused most of the resources on their Altair 8800 computer system and they exited the hobby market in 1978. The Southwest Technical Products computer was the most successful 6800 based personal computer.[86][87] Other companies, for instance, Smoke signal Broadcasting (California), Gimix (Chicago), Midwest Scientific (Olathe, Kansas), and Helix Systems (Hazelwood, Missouri), started producing SWTPC 6800 compatible boards and complete systems. The 8080 systems were far more popular than the 6800 ones.[88]
649) 20.325613, Operating system - Wikipedia, the free encyclopedia.txt#88, term: computer, content:Client/server networking allows a program on a computer, called a client, to connect via a network to another computer, called a server. Servers offer (or host) various services to other network computers and users. These services are usually provided through ports or numbered access points beyond the server's network address. Each port number is usually associated with a maximum of one running program, which is responsible for handling requests to that port. A daemon, being a user program, can in turn access the local hardware resources of that computer by passing requests to the operating system kernel.
650) 20.325613, PDP-8 - Wikipedia, the free encyclopedia.txt#0, term: computer, content:The 12-bit PDP-8, produced by Digital Equipment Corporation (DEC), is the first successful commercial minicomputer. DEC introduced it on March 22, 1965 priced at 18,500 US$,[1] equivalent to 138,900 US$ today,[2] and eventually sold more than 50,000 systems, the most of any computer up to that time.[3][4] It was the first widely sold computer in the DEC PDP series of computers (the PDP-5 was not originally intended to be a general-purpose computer).[5] The chief engineer who designed the initial version of the PDP-8 was Edson de Castro, who later founded Data General.[6]
651) 20.325613, Personal computer - Wikipedia, the free encyclopedia.txt#1, term: computer, content:Software applications for most personal computers include, but are not limited to, word processing, spreadsheets, databases, web browsers and e-mail clients, digital media playback, games and many personal productivity and special-purpose software applications. Modern personal computers often have connections to the Internet, allowing access to the World Wide Web and a wide range of other resources. Personal computers may be connected to a local area network (LAN), either by a cable or a wireless connection. A personal computer may be a laptop computer or a desktop computer running an operating system such as Windows, Linux (and the various operating systems based on it), or Macintosh OS.
652) 20.325613, Personal computer - Wikipedia, the free encyclopedia.txt#44, term: computer, content:A desktop replacement computer (DTR) is a personal computer that provides the full capabilities of a desktop computer while remaining mobile. Such computers are often actually larger, bulkier laptops. Because of their increased size, this class of computers usually includes more powerful components and a larger display than generally found in smaller portable computers, and can have a relatively limited battery capacity or none at all in some cases. Some use a limited range of desktop components to provide better performance at the expense of battery life. Desktop replacement computers are sometimes called desknotes, as a portmanteau of words "desktop" and "notebook," though the term is also applied to desktop replacement computers in general.[56]
653) 20.325613, Server (computing) - Wikipedia, the free encyclopedia.txt#7, term: computer, content:The server is part of the clientserver model; in this model, a server serves data for clients. The nature of communication between a client and server is request and response. This is in contrast with peer-to-peer model in which the relationship is on-demand reciprocation. In principle, any computerized process that can be used or called by another process (particularly remotely, particularly to share a resource) is a server, and the calling process or processes is a client. Thus any general purpose computer connected to a network can host servers. For example, if files on a device are shared by some process, that process is a file server. Similarly, web server software can run on any capable computer, and so a laptop or a personal computer can host a web server.
654) 20.325613, Software bug - Wikipedia, the free encyclopedia.txt#2, term: computer, content:The results of bugs may be extremely serious. Bugs in the code controlling the Therac-25 radiation therapy machine were directly responsible for some patient deaths in the 1980s. In 1996, the European Space Agency's US$1billion prototype Ariane 5 rocket had to be destroyed less than a minute after launch due to a bug in the on-board guidance computer program. In June 1994, a Royal Air Force Chinook helicopter crashed into the Mull of Kintyre, killing 29. This was initially dismissed as pilot error, but an investigation by Computer Weekly uncovered sufficient evidence to convince a House of Lords inquiry that it may have been caused by a software bug in the aircraft's engine control computer.[1]
655) 20.325613, USB - Wikipedia, the free encyclopedia.txt#104, term: computer, content:Sleep-and-charge USB ports can be used to charge electronic devices even when the computer is switched off. Normally, when a computer is powered off the USB ports are powered down, preventing phones and other devices from charging. Sleep-and-charge USB ports remain powered even when the computer is off. On laptops, charging devices from the USB port when it is not being powered from AC drains the laptop battery faster; most laptops have a facility to stop charging if their own battery charge level gets too low.[116]
656) 20.325613, Vannevar Bush - Wikipedia, the free encyclopedia.txt#19, term: computer, content:In September 1940, Norbert Wiener approached Bush with a proposal to build a digital computer. Bush declined to provide NDRC funding for it on the grounds that he did not believe that it could be completed before the end of the war. The supporters of digital computers were disappointed at the decision, which they attributed to a preference for outmoded analog technology. In June 1943, the Army provided $500,000 to build the computer, which became ENIAC, the first general-purpose electronic computer. Having delayed its funding, Bush's prediction proved correct as ENIAC was not completed until December 1945, after the war had ended.[40] His critics saw his attitude as a failure of vision.[41]
657) 20.325613, Video game - Wikipedia, the free encyclopedia.txt#54, term: computer, content:Sales of different types of games vary widely between countries due to local preferences. Japanese consumers tend to purchase much more console games than computer games, with a strong preference for games catering to local tastes.[citation needed] Another key difference is that, despite the decline of arcades in the West, arcade games remain the largest sector of the Japanese gaming industry.[citation needed] In South Korea, computer games are generally preferred over console games, especially MMORPG games and real-time strategy games. Computer games are also popular in China.[85]
658) 20.117142, 16-bit - Wikipedia, the free encyclopedia.txt#4, term: computer, content:16-bit processors have been almost entirely supplanted in the personal computer industry, and are used less than 32-bit (or 8-bit) CPUs in embedded applications.
659) 20.117142, 64-bit computing - Wikipedia, the free encyclopedia.txt#16, term: computer, content:A common misconception is that 64-bit architectures are no better than 32-bit architectures unless the computer has more than 4GB of random access memory.[30] This is not entirely true:
660) 20.117142, Algorithm - Wikipedia, the free encyclopedia.txt#0, term: computer, content:In mathematics and computer science, an algorithm (i/lrm/ AL-g-ri-dhm) is a self-contained step-by-step set of operations to be performed. Algorithms perform calculation, data processing, and/or automated reasoning tasks.
661) 20.117142, Algorithm - Wikipedia, the free encyclopedia.txt#22, term: computer, content:Most algorithms are intended to be implemented as computer programs. However, algorithms are also implemented by other means, such as in a biological neural network (for example, the human brain implementing arithmetic or an insect looking for food), in an electrical circuit, or in a mechanical device.
662) 20.117142, Analog computer - Wikipedia, the free encyclopedia.txt#2, term: computer, content:The advent of digital computing and its success made analog computers largely obsolete in 1950s and 1960s, though they remain in use in some specific applications, like the flight computer in aircraft, and for teaching control systems in universities.
663) 20.117142, Analog computer - Wikipedia, the free encyclopedia.txt#15, term: computer, content:By 1912 Arthur Pollen had developed an electrically driven mechanical analog computer for fire-control systems, based on the differential analyser. It was used by the Imperial Russian Navy in World War I.[citation needed]
664) 20.117142, Analog computer - Wikipedia, the free encyclopedia.txt#68, term: computer, content:When compensated for temperature, the forward voltage drop of a transistor's base-emitter junction can provide a usably accurate logarithmic or exponential function. Op amps scale the output voltage so that it is usable with the rest of the computer.
665) 20.117142, Analog computer - Wikipedia, the free encyclopedia.txt#72, term: computer, content:Key mechanical components might include rotating shafts for carrying data within the computer, miter gear differentials, disc/ball/roller integrators, cams (2-D and 3-D), mechanical resolvers and multipliers, and torque servos.
666) 20.117142, Antivirus software - Wikipedia, the free encyclopedia.txt#0, term: computer, content:Antivirus or anti-virus software (often abbreviated as AV), sometimes known as anti-malware software, is computer software used to prevent, detect and remove malicious software.[1]
667) 20.117142, Antivirus software - Wikipedia, the free encyclopedia.txt#8, term: computer, content:There are competing claims for the innovator of the first antivirus product. Possibly, the first publicly documented removal of an "in the wild" computer virus (i.e. the "Vienna virus") was performed by Bernd Fix in 1987.[22][23]
668) 20.117142, ARPANET - Wikipedia, the free encyclopedia.txt#8, term: computer, content:The first-generation IMPs were built by BBN Technologies using a rugged computer version of the Honeywell DDP-516 computer configured with 24KB of expandable magnetic-core memory, and a 16-channel Direct Multiplex Control (DMC) direct memory access unit.[23] The DMC established custom interfaces with each of the host computers and modems. In addition to the front-panel lamps, the DDP-516 computer also features a special set of 24 indicator lamps showing the status of the IMP communication channels. Each IMP could support up to four local hosts, and could communicate with up to six remote IMPs via leased lines. The network connected one computer in Utah with three in California. Later, the Department of Defense allowed the universities to join the network for sharing hardware and software resources.
669) 20.117142, Artificial intelligence - Wikipedia, the free encyclopedia.txt#38, term: computer, content:In the course of 50 years of research, AI has developed a large number of tools to solve the most difficult problems in computer science. A few of the most general of these methods are discussed below.
670) 20.117142, Artificial intelligence - Wikipedia, the free encyclopedia.txt#55, term: computer, content:Deep learning in artificial neural networks with many layers has transformed many important subfields of artificial intelligence, including computer vision, speech recognition, natural language processing and others.[161][162][163]
671) 20.117142, Artificial intelligence - Wikipedia, the free encyclopedia.txt#103, term: computer, content:Other influential fictional intelligences include HAL, the computer in charge of the spaceship in 2001: A Space Odyssey, released as both a film and a book in 1968 and written by Arthur C. Clarke.
672) 20.117142, Assembly language - Wikipedia, the free encyclopedia.txt#60, term: computer, content:For any given personal computer, mainframe, embedded system, and game console, both past and present, at least one  possibly dozens  of assemblers have been written. For some examples, see the list of assemblers.
673) 20.117142, Association for Computing Machinery - Wikipedia, the free encyclopedia.txt#5, term: computer, content:ACM publishes over 50 journals[4] including the prestigious[5] Journal of the ACM, and two general magazines for computer professionals, Communications of the ACM (also known as Communications or CACM) and Queue. Other publications of the ACM include:
674) 20.117142, Association for Computing Machinery - Wikipedia, the free encyclopedia.txt#8, term: computer, content:In 1997, ACM Press published Wizards and Their Wonders: Portraits in Computing (ISBN 0897919602), written by Christopher Morgan, with new photographs by Louis Fabian Bachrach. The book is a collection of historic and current portrait photographs of figures from the computer industry.
675) 20.117142, Association for Computing Machinery - Wikipedia, the free encyclopedia.txt#17, term: computer, content:The ACM Fellows Program was established by Council of the Association for Computing Machinery in 1993 "to recognize and honor outstanding ACM members for their achievements in computer science and information technology and for their significant contributions to the mission of the ACM."
676) 20.117142, Association for Computing Machinery - Wikipedia, the free encyclopedia.txt#32, term: computer, content:The ACM-W gives an annual Athena Lecturer Award to honor outstanding women researchers who have made fundamental contributions to computer science. This program began in 2006. Speakers are nominated by SIG officers.[28]
677) 20.117142, Atanasoff–Berry computer - Wikipedia, the free encyclopedia.txt#0, term: computer, content:The AtanasoffBerry computer (ABC) was the first automatic electronic digital computer, an early electronic digital computing device that has remained somewhat obscure. The ABC's priority is debated among historians of computer technology, because it was not programmable, nor Turing-complete.[1] Many credit[weaselwords] John Mauchly and J. Presper Eckert, creators of the ENIAC,[2] which came into use in July 1946, with the title. Others cite[weaselwords] the programmable British Colossus computer which was demonstrated to be working on December 8, 1943. Some historians argue[weaselwords] that the credit undisputedly belongs to Iowa State mathematics and physics professor John Vincent Atanasoff for his work with the ABC, with the help of graduate student Clifford Berry.[3]
678) 20.117142, Atanasoff–Berry computer - Wikipedia, the free encyclopedia.txt#15, term: computer, content:George W. Snedecor, the head of Iowa State's Statistics Department, was very likely the first user of an electronic digital computer to solve real world mathematics problems. He submitted many of these problems to Atanasoff.[8]
679) 20.117142, Atanasoff–Berry computer - Wikipedia, the free encyclopedia.txt#18, term: computer, content:The case was legally resolved on October 19, 1973 when U.S. District Judge Earl R. Larson held the ENIAC patent invalid, ruling that the ENIAC derived many basic ideas from the AtanasoffBerry Computer.
680) 20.117142, BASIC - Wikipedia, the free encyclopedia.txt#28, term: computer, content:An application for the Nintendo 3DS and Nintendo DSi called Petit Computer allows for programming in a slightly modified version of BASIC with DS button support. A 3DS sequel was released in Japan in November 2014.
681) 20.117142, BASIC - Wikipedia, the free encyclopedia.txt#43, term: computer, content:New BASIC programmers on a home computer might start with a simple program, perhaps using the language's PRINT statement to display a message on the screen; a well-known and often-replicated example is Kernighan and Ritchie's Hello world program:
682) 20.117142, Billiard-ball computer - Wikipedia, the free encyclopedia.txt#3, term: computer, content:Logic gates based on billiard-ball computer designs have also been made to operate using live soldier crabs of the species Mictyris guinotae in place of the billiard balls.[4][5][6]
683) 20.117142, Bit - Wikipedia, the free encyclopedia.txt#22, term: computer, content:In the 1980s, when bitmapped computer displays became popular, some computers provided specialized bit block transfer ("bitblt" or "blit") instructions to set or copy the bits that corresponded to a given rectangular area on the screen.
684) 20.117142, Boolean algebra - Wikipedia, the free encyclopedia.txt#97, term: computer, content:Algebra being a fundamental tool in any area amenable to mathematical treatment, these considerations combine to make the algebra of two values of fundamental importance to computer hardware, mathematical logic, and set theory.
685) 20.117142, Booting - Wikipedia, the free encyclopedia.txt#4, term: computer, content:There are many different methods available to load a short initial program into a computer. These methods reach from simple, physical input to removable media that can hold more complex programs.
686) 20.117142, Brian Randell - Wikipedia, the free encyclopedia.txt#6, term: computer, content:In the 1960s Randell was "involved in the original "NATO Software Engineering conferences" in 1968 on Software engineering, at the time he was working at IBM in the very secret Project Y and then ACS super-computer projects.
687) 20.117142, C++ - Wikipedia, the free encyclopedia.txt#34, term: computer, content:In addition, templates are a compile time mechanism in C++ that is Turing-complete, meaning that any computation expressible by a computer program can be computed, in some form, by a template metaprogram prior to runtime.
688) 20.117142, Calculator - Wikipedia, the free encyclopedia.txt#26, term: computer, content:The Casio Computer Company, in Japan, released the Model 14-A calculator in 1957, which was the world's first all-electric (relatively) "compact" calculator. It did not use electronic logic but was based on relay technology, and was built into a desk.
689) 20.117142, COBOL - Wikipedia, the free encyclopedia.txt#114, term: computer, content:The COBOL community has always been isolated from the computer science community. No academic computer scientists participated in the design of COBOL: all of those on the committee came from commerce or government. Computer scientists at the time were more interested in fields like numerical analysis, physics and system programming than the commercial file-processing problems which COBOL development tackled.[156] Jean Sammet attributed COBOL's unpopularity to an initial "snob reaction" due to its inelegance, the lack of influential computer scientists participating in the design process and a disdain for business data processing.[157] The COBOL specification used a unique "notation", or metalanguage, to define its syntax rather than the new BackusNaur form because few committee members had heard of it. This resulted in "severe" criticism.[158][159][160][58]
690) 20.117142, Colossus computer - Wikipedia, the free encyclopedia.txt#26, term: computer, content:A Colossus computer was thus not a fully general Turing complete machine. However, Professor Benjamin Wells of the Departments of Computer Science and Mathematics, University of San Francisco, has shown[51] that a Universal Turing Machine could have been run on the set of ten Colossus computers. This means that Colossus satisfies the definition of Turing completeness. Most of the other computing machines of this era were also not Turing complete (e.g. the AtanasoffBerry Computer, the Bell Labs relay machines (by George Stibitz et al.), or the first designs of Konrad Zuse).[citation needed] The notion of a computer as a general purpose machinethat is, as more than a calculator devoted to solving difficult but specific problemsdid not become prominent until after World War II.
691) 20.117142, Colossus computer - Wikipedia, the free encyclopedia.txt#41, term: computer, content:There was a fictional computer named Colossus in the 1970 movie Colossus: The Forbin Project. This was sheer coincidence as it pre-dates the public release of information about Colossus, or even its name.
692) 20.117142, Command-line interface - Wikipedia, the free encyclopedia.txt#82, term: computer, content:From the 1960s onwards, user interaction with computers was primarily by means of command-line interfaces, initially on machines like the Teletype Model 33 ASR, but then on early CRT-based computer terminals such as the VT52.
693) 20.117142, Compiler - Wikipedia, the free encyclopedia.txt#7, term: computer, content:In many application domains the idea of using a higher level language quickly caught on. Because of the expanding functionality supported by newer programming languages and the increasing complexity of computer architectures, compilers have become more complex.
694) 20.117142, Computer - Wikipedia, the free encyclopedia.txt#13, term: computer, content:Charles Babbage, an English mechanical engineer and polymath, originated the concept of a programmable computer. Considered the "father of the computer",[14] he conceptualized and invented the first mechanical computer in the early 19th century. After working on his revolutionary difference engine, designed to aid in navigational calculations, in 1833 he realized that a much more general design, an Analytical Engine, was possible. The input of programs and data was to be provided to the machine via punched cards, a method being used at the time to direct mechanical looms such as the Jacquard loom. For output, the machine would have a printer, a curve plotter and a bell. The machine would also be able to punch numbers onto cards to be read in later. The Engine incorporated an arithmetic logic unit, control flow in the form of conditional branching and loops, and integrated memory, making it the first design for a general-purpose computer that could be described in modern terms as Turing-complete.[15][16]
695) 20.117142, Computer - Wikipedia, the free encyclopedia.txt#30, term: computer, content:Early computing machines had fixed programs. Changing its function required the re-wiring and re-structuring of the machine.[27] With the proposal of the stored-program computer this changed. A stored-program computer includes by design an instruction set and can store in memory a set of instructions (a program) that details the computation. The theoretical basis for the stored-program computer was laid by Alan Turing in his 1936 paper. In 1945 Turing joined the National Physical Laboratory and began work on developing an electronic stored-program digital computer. His 1945 report Proposed Electronic Calculator was the first specification for such a device. John von Neumann at the University of Pennsylvania also circulated his First Draft of a Report on the EDVAC in 1945.[17]
696) 20.117142, Computer - Wikipedia, the free encyclopedia.txt#45, term: computer, content:Once told to run this program, the computer will perform the repetitive addition task without further human intervention. It will almost never make a mistake and a modern PC can complete the task in a fraction of a second.
697) 20.117142, Computer - Wikipedia, the free encyclopedia.txt#55, term: computer, content:A general purpose computer has four main components: the arithmetic logic unit (ALU), the control unit, the memory, and the input and output devices (collectively termed I/O). These parts are interconnected by buses, often made of groups of wires.
698) 20.117142, Computer - Wikipedia, the free encyclopedia.txt#83, term: computer, content:In time, the network spread beyond academic and military institutions and became known as the Internet. The emergence of networking involved a redefinition of the nature and boundaries of the computer. Computer operating systems and applications were modified to include the ability to define and access the resources of other computers on the network, such as peripheral devices, stored information, and the like, as extensions of the resources of an individual computer. Initially these facilities were available primarily to people working in high-tech environments, but in the 1990s the spread of applications like e-mail and the World Wide Web, combined with the development of cheap, fast networking technologies like Ethernet and ADSL saw computer networking become almost ubiquitous. In fact, the number of computers that are networked is growing phenomenally. A very large proportion of personal computers regularly connect to the Internet to communicate and receive information. "Wireless" networking, often utilizing mobile phone networks, has meant networking is becoming increasingly ubiquitous even in mobile computing environments.
699) 20.117142, Computer - Wikipedia, the free encyclopedia.txt#93, term: computer, content:Software refers to parts of the computer which do not have a material form, such as programs, data, protocols, etc. When software is stored in hardware that cannot easily be modified (such as BIOS ROM in an IBM PC compatible), it is sometimes called "firmware".
700) 20.117142, Computer - Wikipedia, the free encyclopedia.txt#95, term: computer, content:Firmware is the technology which has the combination of both hardware and software such as BIOS chip inside a computer. This chip (hardware) is located on the motherboard and has the BIOS set up (software) stored in it.
701) 20.117142, Computer animation - Wikipedia, the free encyclopedia.txt#10, term: computer, content:Computer animation helped to create blockbuster films, Toy Story 3 (2010), Avatar (2009), Shrek 2 (2004), Cars 2 (2011), Life of Pi (2012), Frozen (2013), and Inside Out (2015).
702) 20.117142, Computer architecture - Wikipedia, the free encyclopedia.txt#11, term: computer, content:Besides instructions, the ISA defines items in the computer that are available to a programe.g. data types, registers, addressing modes, and memory. Instructions locate these available items with register indexes (or names) and memory addressing modes.
703) 20.117142, Computer cluster - Wikipedia, the free encyclopedia.txt#2, term: computer, content:They are usually deployed to improve performance and availability over that of a single computer, while typically being much more cost-effective than single computers of comparable speed or availability.[4]
704) 20.117142, Computer cluster - Wikipedia, the free encyclopedia.txt#25, term: computer, content:However, the use of a clustered file system is essential in modern computer clusters.[citation needed] Examples include the IBM General Parallel File System, Microsoft's Cluster Shared Volumes or the Oracle Cluster File System.
705) 20.117142, Computer cluster - Wikipedia, the free encyclopedia.txt#40, term: computer, content:Microsoft Windows computer cluster Server 2003 based on the Windows Server platform provides pieces for High Performance Computing like the Job Scheduler, MSMPI library and management tools.
706) 20.117142, Computer graphics - Wikipedia, the free encyclopedia.txt#10, term: computer, content:E. E. Zajac, a scientist at Bell Telephone Laboratory (BTL), created a film called "Simulation of a two-giro gravity attitude control system" in 1963.[7] In this computer-generated film, Zajac showed how the attitude of a satellite could be altered as it orbits the Earth. He created the animation on an IBM 7090 mainframe computer. Also at BTL, Ken Knowlton, Frank Sinden and Michael Noll started working in the computer graphics field. Sinden created a film called Force, Mass and Motion illustrating Newton's laws of motion in operation. Around the same time, other scientists were creating computer graphics to illustrate their research. At Lawrence Radiation Laboratory, Nelson Max created the films Flow of a Viscous Fluid and Propagation of Shock Waves in a Solid Form. Boeing Aircraft created a film called Vibration of an Aircraft.
707) 20.117142, Computer graphics - Wikipedia, the free encyclopedia.txt#20, term: computer, content:Japan's Osaka University developed the LINKS-1 Computer Graphics System, a supercomputer that used up to 257 Zilog Z8001 microprocessors, in 1982, for the purpose of rendering realistic 3D computer graphics. According to the Information Processing Society of Japan: "The core of 3D image rendering is calculating the luminance of each pixel making up a rendered surface from the given viewpoint, light source, and object position. The LINKS-1 system was developed to realize an image rendering methodology in which each pixel could be parallel processed independently using ray tracing. By developing a new software methodology specifically for high-speed image rendering, LINKS-1 was able to rapidly render highly realistic images. It was used to create the world's first 3D planetarium-like video of the entire heavens that was made completely with computer graphics. The video was presented at the Fujitsu pavilion at the 1985 International Exposition in Tsukuba."[8] The LINKS-1 was the world's most powerful computer, as of 1984.[9]
708) 20.117142, Computer graphics - Wikipedia, the free encyclopedia.txt#43, term: computer, content:3D graphics compared to 2D graphics are graphics that use a three-dimensional representation of geometric data. For the purpose of performance this is stored in the computer. This includes images that may be for later display or for real-time viewing.
709) 20.117142, Computer graphics - Wikipedia, the free encyclopedia.txt#48, term: computer, content:To create the illusion of movement, an image is displayed on the computer screen then quickly replaced by a new image that is similar to the previous image, but shifted slightly. This technique is identical to the illusion of movement in television and motion pictures.
710) 20.117142, Computer hardware - Wikipedia, the free encyclopedia.txt#7, term: computer, content:A power supply unit (PSU) converts alternating current (AC) electric power to low-voltage DC power for the internal components of the computer. Laptops are capable of running from a built-in battery, normally for a period of hours.[7]
711) 20.117142, Computer keyboard - Wikipedia, the free encyclopedia.txt#3, term: computer, content:In normal usage, the keyboard is used as a text entry interface to type text and numbers into a word processor, text editor or other programs. In a modern computer, the interpretation of key presses is generally left to the software. A computer keyboard distinguishes each physical key from every other and reports all key presses to the controlling software. Keyboards are also used for computer gaming, either with regular keyboards or by using keyboards with special gaming features, which can expedite frequently used keystroke combinations. A keyboard is also used to give commands to the operating system of a computer, such as Windows' Control-Alt-Delete combination, which brings up a task window or shuts down the machine. A command-line interface is a type of user interface operated entirely through a keyboard, or another device doing the job of one.
712) 20.117142, Computer memory - Wikipedia, the free encyclopedia.txt#15, term: computer, content:Nearly everything a computer programmer does requires him or her to consider how to manage memory. Even storing a number in memory requires the programmer to specify how the memory should store it.
713) 20.117142, Computer memory - Wikipedia, the free encyclopedia.txt#22, term: computer, content:Protected memory is a system where each program is given an area of memory to use and is not permitted to go outside that range. Use of protected memory greatly enhances both the reliability and security of a computer system.
714) 20.117142, Computer monitor - Wikipedia, the free encyclopedia.txt#4, term: computer, content:Multiple technologies have been used for computer monitors. Until the 21st century most used cathode ray tubes but they have largely been superseded by LCD monitors.
715) 20.117142, Computer monitor - Wikipedia, the free encyclopedia.txt#18, term: computer, content:In 2010 the computer industry started to move over from 16:10 to 16:9 because 16:9 was chosen to be the standard high-definition television display size, and because they were cheaper to manufacture.
716) 20.117142, Computer monitor - Wikipedia, the free encyclopedia.txt#42, term: computer, content:According to an NSA document leaked to Der Spiegel, the NSA sometimes swaps the monitor cables on targeted computers with a bugged monitor cable in order to allow the NSA to remotely see what's displayed on the targeted computer monitor.[15]
717) 20.117142, Computer mouse - Wikipedia, the free encyclopedia.txt#0, term: computer, content:A computer mouse is a pointing device (hand control) that detects two-dimensional motion relative to a surface. This motion is typically translated into the motion of a pointer on a display, which allows a smooth control of the graphical user interface.
718) 20.117142, Computer mouse - Wikipedia, the free encyclopedia.txt#45, term: computer, content:While the electrical interface and the format of the data transmitted by commonly available mice is currently standardized on USB, in the past it varied between different manufacturers. A bus mouse used a dedicated interface card for connection to an IBM PC or compatible computer.
719) 20.117142, Computer mouse - Wikipedia, the free encyclopedia.txt#77, term: computer, content:The Mac OS Desk Accessory Puzzle in 1984 was the first game designed specifically for a mouse.[74] The device often functions as an interface for PC-based computer games and sometimes for video game consoles.
720) 20.117142, Computer multitasking - Wikipedia, the free encyclopedia.txt#25, term: computer, content:Processes that are entirely independent are not much trouble to program in a multitasking environment. Most of the complexity in multitasking systems comes from the need to share computer resources between tasks and to synchronize the operation of co-operating tasks.[citation needed]
721) 20.117142, Computer network - Wikipedia, the free encyclopedia.txt#34, term: computer, content:Overlay networks have been around since the invention of networking when computer systems were connected over telephone lines using modems, before any data network existed.
722) 20.117142, Computer network - Wikipedia, the free encyclopedia.txt#66, term: computer, content:An enterprise private network is a network that a single organization builds to interconnect its office locations (e.g., production sites, head offices, remote offices, shops) so they can share computer resources.
723) 20.117142, Computer programming - Wikipedia, the free encyclopedia.txt#13, term: computer, content:In computer programming, readability refers to the ease with which a human reader can comprehend the purpose, control flow, and operation of source code. It affects the aspects of quality above, including portability, usability and most importantly maintainability.
724) 20.117142, Computer programming - Wikipedia, the free encyclopedia.txt#28, term: computer, content:Many computer languages provide a mechanism to call functions provided by shared libraries. Provided the functions in a library follow the appropriate run time conventions (e.g., method of passing arguments), then these functions may be written in any other language.
725) 20.117142, Computer science - Wikipedia, the free encyclopedia.txt#26, term: computer, content:Computer performance analysis is the study of work flowing through computers with the general goals of improving throughput, controlling response time, using resources efficiently, eliminating bottlenecks, and predicting performance under anticipated peak loads.[47]
726) 20.117142, Computer security - Wikipedia, the free encyclopedia.txt#0, term: computer, content:Computer security, also known as cybersecurity or IT security, is the protection of information systems from theft or damage to the hardware, the software, and to the information on them, as well as from disruption or misdirection of the services they provide.[1]
727) 20.117142, Computer security - Wikipedia, the free encyclopedia.txt#2, term: computer, content:The field is of growing importance due to the increasing reliance on computer systems in most societies[4] and the growth of "smart" devices, including smartphones, televisions and tiny devices as part of the Internet of Things  and of the Internet and wireless network such as Bluetooth and Wi-Fi.
728) 20.117142, Computer security - Wikipedia, the free encyclopedia.txt#17, term: computer, content:Computer security is critical in almost any industry which uses computers. Currently, most electronic devices such as computer, laptops and cellphones come with built in firewall security software, but despite this, computers are not 100 percent accurate and dependable to protect our data (Smith, Grabosky & Urbas, 2004.) There are many different ways of hacking into computer, it can be done through network system, clicking into unknown links, connecting unfamiliar Wi-Fi, download software and files from unsafe sites, power consumption, electromagnetic radiation waves and many more. However, computer can be protected through well build software and hardware. Such as having strong internal interactions of properties of the software complexity can prevent software crash and security failure.[10]
729) 20.117142, Computer security - Wikipedia, the free encyclopedia.txt#36, term: computer, content:In computer security a countermeasure is an action, device, procedure, or technique that reduces a threat, a vulnerability, or an attack by eliminating or preventing it, by minimizing the harm it can cause, or by discovering and reporting it so that corrective action can be taken.[52][53][54]
730) 20.117142, Computer security - Wikipedia, the free encyclopedia.txt#38, term: computer, content:A state of computer "security" is the conceptual ideal, attained by the use of the three processes: threat prevention, detection, and response. These processes are based on various policies and system components, which include the following:
731) 20.117142, Computer security - Wikipedia, the free encyclopedia.txt#41, term: computer, content:While formal verification of the correctness of computer systems is possible,[56][57] it is not yet common. Operating systems formally verified include seL4,[58] and SYSGO's PikeOS[59][60]  but these make up a very small percentage of the market.
732) 20.117142, Computer security - Wikipedia, the free encyclopedia.txt#71, term: computer, content:The 1986 18 U.S.C.1030, more commonly known as the Computer Fraud and Abuse Act is the key legislation. It prohibits unauthorized access or damage of "protected computers" as defined in 18 U.S.C.1030(e)(2).
733) 20.117142, Computer simulation - Wikipedia, the free encyclopedia.txt#14, term: computer, content:Similarly, CGI computer simulations of CAT scans can simulate how a tumor might shrink or change during an extended period of medical treatment, presenting the passage of time as a spinning view of the visible human head, as the tumor changes.
734) 20.117142, Computer speaker - Wikipedia, the free encyclopedia.txt#2, term: computer, content:More sophisticated computer speakers can have a subwoofer unit, to enhance bass output. The larger subwoofer enclosure usually contains the amplifiers for the subwoofer and the left and right speakers.
735) 20.117142, Computer speaker - Wikipedia, the free encyclopedia.txt#3, term: computer, content:Some computer displays have rather basic speakers built-in. Laptop computers have built-in integrated speakers, usually small and of restricted sound quality to conserve space.
736) 20.117142, Control system - Wikipedia, the free encyclopedia.txt#41, term: computer, content:Logic systems and feedback controllers are usually implemented with programmable logic controllers which are devices available from electrical supply houses. They include a little computer and a simplified system for programming. Most often they are programmed with personal computers.
737) 20.117142, Data (computing) - Wikipedia, the free encyclopedia.txt#3, term: computer, content:Data representing quantities, characters, or symbols on which operations are performed by a computer, stored and recorded on magnetic, optical, or mechanical recording media, and transmitted in the form of digital electrical signals.[2]
738) 20.117142, Difference engine - Wikipedia, the free encyclopedia.txt#1, term: computer, content:The historical difficulty in producing error-free tables by teams of mathematicians and human "computers" spurred Charles Babbage's desire to build a mechanism to automate the process. It is considered to be the world's first computer[by whom?].
739) 20.117142, Differential analyser - Wikipedia, the free encyclopedia.txt#0, term: computer, content:The differential analyser is a mechanical analogue computer designed to solve differential equations by integration, using wheel-and-disc mechanisms to perform the integration.[1] It was one of the first advanced computing devices to be used operationally.[2]
740) 20.117142, Digital electronics - Wikipedia, the free encyclopedia.txt#58, term: computer, content:To save costly engineering effort, much of the effort of designing large logic machines has been automated. The computer programs are called "electronic design automation tools" or just "EDA."
741) 20.117142, Digital electronics - Wikipedia, the free encyclopedia.txt#63, term: computer, content:Often, real logic systems are designed as a series of sub-projects, which are combined using a "tool flow." The tool flow is usually a "script," a simplified computer language that can invoke the software design tools in the right order.
742) 20.117142, Digital electronics - Wikipedia, the free encyclopedia.txt#66, term: computer, content:Parts of tool flows are "debugged" by verifying the outputs of simulated logic against expected inputs. The test tools take computer files with sets of inputs and outputs, and highlight discrepancies between the simulated behavior and the expected behavior.
743) 20.117142, Digital Equipment Corporation - Wikipedia, the free encyclopedia.txt#9, term: computer, content:In 1957 when the pair and Ken's brother Stan went looking for capital, they found that the American business community was hostile to investing in computer companies. Many smaller computer companies had come and gone in the 1950s, wiped out when new technical developments rendered their platforms obsolete, and even large companies like RCA and General Electric were failing to make a profit in the market. The only serious expression of interest came from Georges Doriot and his American Research and Development Corporation (AR&D). Worried that a new computer company would find it difficult to arrange further financing, Doriot suggested the fledgling company change its business plan to focus less on computers, and even change their name from "Digital Computer Corporation".[7]
744) 20.117142, Digital Equipment Corporation - Wikipedia, the free encyclopedia.txt#17, term: computer, content:With the company established and a successful product on the market, DEC turned its attention to the computer market once again as part of its planned "Phase II".[8] In August 1959, Ben Gurley started design of the company's first computer, the PDP-1. In keeping with Doriot's instructions, the name was an initialism for "Programmable Data Processor", leaving off the term "computer". As Gurley put it, "We aren't building computers, we're building 'Programmable Data Processors'." The prototype was first shown publicly at the Joint Computer Conference in Boston in December 1959.[15] The first PDP-1 was delivered to Bolt, Beranek and Newman in November 1960,[16] and formally accepted the next April.[17] The PDP-1 sold in basic form for $120,000, or about $900,000 in 2011 US dollars.[18] By the time production ended in 1969, 53 PDP-1s had been delivered.[13][19]
745) 20.117142, Digital Equipment Corporation - Wikipedia, the free encyclopedia.txt#97, term: computer, content:The VT100 computer terminal became the industry standard, implementing a useful subset of the ANSI X3.64 standard, and even today terminal emulators such as HyperTerminal, PuTTY and Xterm still emulate a VT100 (or its more capable successor, the VT220).
746) 20.117142, Digital photography - Wikipedia, the free encyclopedia.txt#7, term: computer, content:The first commercially available digital camera was the 1990 Dycam Model1; it also sold as the Logitech Fotoman. It used a CCD image sensor, stored pictures digitally, and connected directly to a computer for downloading images.[6][7][8]
747) 20.117142, DirectX - Wikipedia, the free encyclopedia.txt#31, term: computer, content:At SIGGRAPH 2014, Intel released a demo showing a computer generated asteroid field, in which DirectX 12 was claimed to be 50%-70% more efficient than DirectX 11 in rendering speed and CPU power consumption.[45][46]
748) 20.117142, DNA computing - Wikipedia, the free encyclopedia.txt#8, term: computer, content:In 1994 Leonard Adleman presented the first prototype of a DNA-Computer. The TT-100 was a test tube filled with 100 microliters of a DNA-solution. He managed to solve for example an instance of the directed Hamiltonian path problem.[12]
749) 20.117142, DOS - Wikipedia, the free encyclopedia.txt#14, term: computer, content:Available DOS systems in 2012 are FreeDOS, DR-DOS, ROM-DOS, PTS-DOS, RxDOS and REAL/32. Some computer manufacturers, including Dell and HP, sell computers with FreeDOS as the OEM operating system.[20][21]
750) 20.117142, Drum memory - Wikipedia, the free encyclopedia.txt#1, term: computer, content:For many early computers, drum memory formed the main working memory of the computer. It was so common that these computers were often referred to as drum machines.[2] Some drum memories were also used as secondary storage.[3]
751) 20.117142, Drum memory - Wikipedia, the free encyclopedia.txt#6, term: computer, content:One of the earliest functioning computers to employ drum memory was the AtanasoffBerry computer. However, it employed capacitance rather than magnetism to store information. The outer surface of the drum was lined with electrical contacts leading to capacitors contained within.
752) 20.117142, Educational entertainment - Wikipedia, the free encyclopedia.txt#1, term: computer, content:Modern forms of edutainment include television productions, film, museum exhibits, and computer software which use entertainment to attract and maintain an audience, while incorporating deliberate educational content or messages.
753) 20.117142, EDVAC - Wikipedia, the free encyclopedia.txt#5, term: computer, content:A contract to build the new computer was signed in April 1946 with an initial budget of US$100,000. The contract named the device the Electronic Discrete Variable Automatic Calculator. The final cost of EDVAC, however, was similar to the ENIAC's, at just under $500,000.
754) 20.117142, Electronic delay storage automatic calculator - Wikipedia, the free encyclopedia.txt#18, term: computer, content:In the mid-1960s, a successor to the EDSAC 2 was planned, but the move was instead made to the Titan, a prototype Atlas 2 developed from the Atlas Computer of the University of Manchester, Ferranti, and Plessey.
755) 20.117142, Email client - Wikipedia, the free encyclopedia.txt#0, term: computer, content:In Internet, an email client, email reader or more formally mail user agent (MUA) is a computer program in the category of groupware environments used to access and manage a user's email.
756) 20.117142, Embedded system - Wikipedia, the free encyclopedia.txt#10, term: computer, content:Telecommunications systems employ numerous embedded systems from telephone switches for the network to cell phones at the end user. Computer networking uses dedicated routers and network bridges to route data.
757) 20.117142, Embedded system - Wikipedia, the free encyclopedia.txt#19, term: computer, content:The program instructions written for embedded systems are referred to as firmware, and are stored in read-only memory or Flash memory chips. They run with limited computer hardware resources: little memory, small or non-existent keyboard or screen.
758) 20.117142, Enigma machine - Wikipedia, the free encyclopedia.txt#81, term: computer, content:The effort to break the Enigma was not disclosed until the 1970s. Since then, interest in the Enigma machine has grown. Enigmas are on public display in museums around the world, and several are in the hands of private collectors and computer history enthusiasts.[39]
759) 20.117142, Exclusive or - Wikipedia, the free encyclopedia.txt#28, term: computer, content:On some computer architectures, it is more efficient to store a zero in a register by XOR-ing the register with itself (bits XOR-ed with themselves are always zero) instead of loading and storing the value zero.
760) 20.117142, Execution (computing) - Wikipedia, the free encyclopedia.txt#1, term: computer, content:Programs for a computer may execute in a batch process without human interaction, or a user may type commands in an interactive session of an interpreter. In this case the "commands" are simply programs, whose execution is chained together.
761) 20.117142, Fairchild Semiconductor - Wikipedia, the free encyclopedia.txt#31, term: computer, content:Fairchild research developed the Clipper architecture, a 32-bit RISC-like computer architecture, in the 1980s, resulting in the shipping of the C100 chip in 1986. The technology was later sold to Intergraph, its main customer.
762) 20.117142, Ferranti - Wikipedia, the free encyclopedia.txt#1, term: computer, content:The firm was known for work in the area of power grid systems and defence electronics. In addition, in 1951 Ferranti began selling the first commercially available computer, the Ferranti Mark 1.
763) 20.117142, Ferranti Pegasus - Wikipedia, the free encyclopedia.txt#1, term: computer, content:The Pegasus 1 was first delivered in 1956 and the Pegasus 2 was delivered in 1959. Ferranti sold twenty-six copies of the Pegasus 1 and fourteen copies of the Pegasus 2, making it Ferranti's most popular valve computer.[3]
764) 20.117142, File format - Wikipedia, the free encyclopedia.txt#0, term: computer, content:A file format is a standard way that information is encoded for storage in a computer file. It specifies how bits are used to encode information in a digital storage medium. File formats may be either proprietary or free and may be either unpublished or open.
765) 20.117142, Firmware - Wikipedia, the free encyclopedia.txt#7, term: computer, content:Most computer peripherals are themselves special-purpose computers. Devices such as printers, scanners, cameras and USB flash drives have internally stored firmware; some devices may also permit field upgrading of their firmware.
766) 20.117142, Floating point - Wikipedia, the free encyclopedia.txt#28, term: computer, content:In 1989, mathematician and computer scientist William Kahan was honored with the Turing Award for being the primary architect behind this proposal; he was aided by his student (Jerome Coonen) and a visiting professor (Harold Stone).[7]
767) 20.117142, Floating point - Wikipedia, the free encyclopedia.txt#37, term: computer, content:The standard provides for many closely related formats, differing in only a few details. Five of these formats are called basic formats and others are termed extended formats; three of these are especially widely used in computer hardware and languages:
768) 20.117142, FLOPS - Wikipedia, the free encyclopedia.txt#1, term: computer, content:Although the final S stands for "second", singular "flop" is often used, either as a back formation or an abbreviation for "floating-point operation"; e.g. a flop count is a count of these operations carried out by a given algorithm or computer program.
769) 20.117142, FLOPS - Wikipedia, the free encyclopedia.txt#9, term: computer, content:In June 2007 Top500.org reported the fastest computer in the world to be the IBM Blue Gene/L supercomputer, measuring a peak of 596teraFLOPS.[6] The Cray XT4 hit second place with 101.7teraFLOPS.
770) 20.117142, FLOPS - Wikipedia, the free encyclopedia.txt#23, term: computer, content:On June 10, 2013, China's Tianhe-2 was ranked the world's fastest with 33.86 petaFLOPS.[30]. Holding this position for three years, another Chinese computer, TaihuLight, took the top place with 93 petaFLOPS, [31]
771) 20.117142, Graphics tablet - Wikipedia, the free encyclopedia.txt#27, term: computer, content:Graphics tablets are available in various sizes and price ranges; A6-sized tablets being relatively inexpensive and A3-sized tablets far more expensive. Modern tablets usually connect to the computer via a USB or HDMI interface.
772) 20.117142, Harvard Mark I - Wikipedia, the free encyclopedia.txt#0, term: computer, content:The IBM Automatic Sequence Controlled Calculator (ASCC), called Mark I by Harvard Universitys staff,[1] was a general purpose electro-mechanical computer that was used in the war effort during the last part of World War II.
773) 20.117142, History of computing hardware - Wikipedia, the free encyclopedia.txt#1, term: computer, content:Before the 20th century, most calculations were done by humans. Early mechanical tools to help humans with digital calculations were called "calculating machines", by proprietary names, or even as they are now, calculators. The machine operator was called the computer.
774) 20.117142, History of computing hardware - Wikipedia, the free encyclopedia.txt#20, term: computer, content:The Engine incorporated an arithmetic logic unit, control flow in the form of conditional branching and loops, and integrated memory, making it the first design for a general-purpose computer that could be described in modern terms as Turing-complete.[33][34]
775) 20.117142, History of computing hardware - Wikipedia, the free encyclopedia.txt#24, term: computer, content:Following Babbage, although unaware of his earlier work, was Percy Ludgate, an accountant from Dublin, Ireland. He independently designed a programmable mechanical computer, which he described in a work that was published in 1909.[39]
776) 20.117142, History of computing hardware - Wikipedia, the free encyclopedia.txt#69, term: computer, content:It was finally delivered to the U.S. Army's Ballistics Research Laboratory at the Aberdeen Proving Ground in August 1949, but due to a number of problems, the computer only began operation in 1951, and then only on a limited basis.
777) 20.117142, History of computing hardware - Wikipedia, the free encyclopedia.txt#85, term: computer, content:A second generation computer, the IBM 1401, captured about one third of the world market. IBM installed more than ten thousand 1401s between 1960 and 1964.
778) 20.117142, Home computer - Wikipedia, the free encyclopedia.txt#8, term: computer, content:To save the cost of a dedicated monitor, the home computer would often connect through an RF modulator to the family TV set, which served as both video display and sound system.[16]
779) 20.117142, Human computer - Wikipedia, the free encyclopedia.txt#6, term: computer, content:The Indian mathematician Radhanath Sikdar was employed as a "computer" for the Great Trigonometrical Survey of India in 1840. It was he who first identified and calculated the height of the world's highest mountain, later called Mount Everest.[citation needed]
780) 20.117142, Institute of Electrical and Electronics Engineers - Wikipedia, the free encyclopedia.txt#22, term: computer, content:Most IEEE members are electrical and electronics engineers, but the organization's wide scope of interests has attracted people in other disciplines as well (e.g., computer science, software engineering, mechanical engineering, civil engineering, biology, physics, and mathematics).
781) 20.117142, Intel - Wikipedia, the free encyclopedia.txt#108, term: computer, content:As is often the case with computer lore, other tidbits have been combined to explain how things evolved. "Intel Inside" has not escaped that tendency and there are other "explanations" that had been floating around.
782) 20.117142, Intel - Wikipedia, the free encyclopedia.txt#121, term: computer, content:On November 4, 2009, New York's attorney general filed an antitrust lawsuit against Intel Corp, claiming the company used "illegal threats and collusion" to dominate the market for computer microprocessors.
783) 20.117142, Intel 4004 - Wikipedia, the free encyclopedia.txt#5, term: computer, content:The Intel 4004 was designed by physically cutting sheets of Rubylith into thin strips to lay out the circuits to be printed, a process made obsolete by current computer graphic design capabilities.[14]
784) 20.117142, Intel 80486 - Wikipedia, the free encyclopedia.txt#26, term: computer, content:Although the 486 was eventually overtaken by the Pentium for personal computer applications, Intel had continued production for use in embedded systems. In May 2006 Intel announced that production of the 80486 would stop at the end of September 2007.[14]
785) 20.117142, Interactive fiction - Wikipedia, the free encyclopedia.txt#3, term: computer, content:Text adventures are one of the oldest types of computer games and form a subset of the adventure genre. The player uses text input to control the game, and the game state is relayed to the player via text output.
786) 20.117142, International Federation for Information Processing - Wikipedia, the free encyclopedia.txt#3, term: computer, content:The name was changed to IFIP in 1961. The organisation's original contribution was to define the ALGOL 60 programming language, in one of the first examples of truly international collaboration in computer science, leaving a durable mark on the entire field.
787) 20.117142, International Federation for Information Processing - Wikipedia, the free encyclopedia.txt#27, term: computer, content:IFIP TC10 was founded in 1976 and revised in 1987. It aims to promote State-of-the-Art concepts, methodologies and tools in the life cycle of computer systems and to coordinate the exchange of information around these practices.
788) 20.117142, Internet protocol suite - Wikipedia, the free encyclopedia.txt#10, term: computer, content:A computer called a router is provided with an interface to each network. It forwards packets back and forth between them.[10] Originally a router was called gateway, but the term was changed to avoid confusion with other types of gateways.
789) 20.117142, Konrad Zuse - Wikipedia, the free encyclopedia.txt#22, term: computer, content:During World War 2, Zuse founded one of the earliest computer companies: the Zuse-Ingenieurbro Hopferau. Capital was raised in 1946 through ETH Zurich and an IBM option on Zuse's patents.
790) 20.117142, Logical conjunction - Wikipedia, the free encyclopedia.txt#18, term: computer, content:In computer networking, bit masks are used to derive the network address of a subnet within an existing network from a given IP address, by ANDing the IP address and the subnet mask.
791) 20.117142, Machine - Wikipedia, the free encyclopedia.txt#18, term: computer, content:Modern computers are electronic ones. They use electric charge, current or magnetization to store and manipulate information. Computer architecture deals with detailed design of computers. There are also simplified models of computers, like State machine and Turing machine.
792) 20.117142, Machine - Wikipedia, the free encyclopedia.txt#22, term: computer, content:Modern machines include sensors, actuators and computer controllers. The shape, texture and color of covers provide a styling and operational interface between the mechanical components of a machine and its users.
793) 20.117142, Machine code - Wikipedia, the free encyclopedia.txt#6, term: computer, content:A computer program is a sequence of instructions that are executed by a CPU. While simple processors execute instructions one after another, superscalar processors are capable of executing several instructions at once.
794) 20.117142, Magnetic-core memory - Wikipedia, the free encyclopedia.txt#0, term: computer, content:Magnetic-core memory was the predominant form of random-access computer memory for 20 years between about 1955 and 1975. Such memory is often just called core memory, or, informally, core.
795) 20.117142, Magnetic-core memory - Wikipedia, the free encyclopedia.txt#3, term: computer, content:Although core memory is obsolete, any computer memory is still occasionally called "core"; in particular, a file recording the contents of memory after a system error is usually called a core dump.
796) 20.117142, Manchester Mark 1 - Wikipedia, the free encyclopedia.txt#5, term: computer, content:The practical construction of a von Neumann computer depended on the availability of a suitable memory device. The University of Manchester's Small-Scale Experimental Machine (SSEM), the world's first stored-program computer, had successfully demonstrated the practicality of the stored-program approach and of the Williams tube, an early form of computer memory based on a standard cathode ray tube (CRT), by running its first program in June 1948.[6] Early electronic computers were generally programmed by being rewired, or via plugs and patch panels; there was no separate program stored in memory, as in a modern computer. It could take several days to reprogram ENIAC, for instance.[7] Stored-program computers were also being developed by other researchers, notably the National Physical Laboratory's Pilot ACE, Cambridge University's EDSAC, and the US Army's EDVAC.[8] The SSEM and the Mark 1 differed primarily in their use of Williams tubes as memory devices, instead of mercury delay lines.[9]
797) 20.117142, Massively multiplayer online game - Wikipedia, the free encyclopedia.txt#5, term: computer, content:As video game developers applied MMOG ideas to other computer and video game genres, new acronyms started to develop, such as MMORTS. MMOG emerged as a generic term to cover this growing class of games.
798) 20.117142, Media player (software) - Wikipedia, the free encyclopedia.txt#0, term: computer, content:A media player is a computer program for playing multimedia files. Media players display standard media control icons known from physical devices such as tape recorders and CD players, such as Play (), Pause (), and Stop () buttons.
799) 20.117142, Microcomputer - Wikipedia, the free encyclopedia.txt#5, term: computer, content:In common usage, "microcomputer" has been largely supplanted by the term "personal computer" or "PC," which specifies a computer that has been designed to be used by one individual at a time, a term first coined in 1959.[9] IBM first promoted the term "personal computer" to differentiate themselves from other microcomputers, often called "home computers", and also IBM's own mainframes and minicomputers[citation needed] . However, following its release, the IBM PC itself was widely imitated, as well as the term[citation needed]. The component parts were commonly available to producers and the BIOS was reverse engineered through cleanroom design techniques. IBM PC compatible "clones" became commonplace, and the terms "personal computer," and especially "PC" stuck with the general public.
800) 20.117142, Microcomputer - Wikipedia, the free encyclopedia.txt#18, term: computer, content:In 2012, the Raspberry Pi credit-card-sized single-board computer was launched, directly inspired by Acorn's BBC Micro of 1981, and including support for BBC BASIC.[20]
801) 20.117142, Microprocessor - Wikipedia, the free encyclopedia.txt#27, term: computer, content:Three projects delivered a microprocessor at about the same time: Garrett AiResearch's Central Air Data Computer (CADC), Texas Instruments (TI) TMS 1000 (1971 September), and Intel's 4004 (1971 November).
802) 20.117142, MIPS instruction set - Wikipedia, the free encyclopedia.txt#96, term: computer, content:More advanced free emulators are available from the GXemul (formerly known as the mips64emul project) and QEMU projects. These emulate the various MIPS III and IV microprocessors in addition to entire computer systems which use them.
803) 20.117142, Motorola 6800 - Wikipedia, the free encyclopedia.txt#12, term: computer, content:Bill Lattin joined Motorola in 1969 and his group provided the computer simulation tools for characterizing the new MOS circuits in the 6800. Lattin and Frank Jenkins had both attended UC Berkeley and studied computer circuit simulators under Donald Pederson, the designer of the SPICE circuit simulator.[18] Motorola's simulator, MTIME, was an advanced version of the TIME circuit simulator that Jenkins had developed at Berkeley. The group published a technical paper, "MOS-device modeling for computer implementation" in 1973 describing a "5-V single-supply n-channel technology" operating at 1MHz. They could simulate a 50 MOSFET circuit on an IBM 370/165 mainframe computer.[19] In November 1975, Lattin joined Intel to work on their next generation microprocessor.[20]
804) 20.117142, Motorola 6800 - Wikipedia, the free encyclopedia.txt#51, term: computer, content:A clone of the 6800 processor was used in the Bulgarian computer Pyldin-601.[citation needed] About 35000 of these computers were produced from 1988 to 1992.[citation needed] They were used mainly for educational and industrial purposes.
805) 20.117142, Multi-core processor - Wikipedia, the free encyclopedia.txt#13, term: computer, content:Since computer manufacturers have long implemented symmetric multiprocessing (SMP) designs using discrete CPUs, the issues regarding implementing multi-core processor architecture and supporting it with software are well known.
806) 20.117142, Multimedia - Wikipedia, the free encyclopedia.txt#28, term: computer, content:Software engineers may use multimedia in Computer Simulations for anything from entertainment to training such as military or industrial training. Multimedia for software interfaces are often done as a collaboration between creative professionals and software engineers.
807) 20.117142, Multiprocessing - Wikipedia, the free encyclopedia.txt#14, term: computer, content:In a single-instruction stream, single-data stream computer one processor sequentially processes instructions, each instruction processes one data item. One example is the "von Neumann" architecture with RISC.
808) 20.117142, Multiprocessing - Wikipedia, the free encyclopedia.txt#15, term: computer, content:In a single-instruction stream, multiple data stream computer one processor handles a stream of instructions, each one of which can perform calculations in parallel on multiple data locations.
809) 20.117142, Negation - Wikipedia, the free encyclopedia.txt#24, term: computer, content:Inverting the condition and reversing the outcomes produces code that is logically equivalent to the original code, i.e. will have identical results for any input (note that depending on the compiler used, the actual instructions performed by the computer may differ).
810) 20.117142, Netbook - Wikipedia, the free encyclopedia.txt#0, term: computer, content:Netbook is a generic name given to a category of small, lightweight, legacy-free, and inexpensive computers that were introduced in 2007. Netbooks compete in the same market segment as tablet computers and Chromebooks (a variation on the portable network computer).
811) 20.117142, Non-English-based programming languages - Wikipedia, the free encyclopedia.txt#0, term: computer, content:Non-English-based programming languages are computer programming languages that, unlike better-known programming languages, do not use keywords taken from, or inspired by, the English vocabulary.
812) 20.117142, Operating system - Wikipedia, the free encyclopedia.txt#26, term: computer, content:The enormous investment in software for these systems made since the 1960s caused most of the original computer manufacturers to continue to develop compatible operating systems along with the hardware. Notable supported mainframe operating systems include:
813) 20.117142, Operating system - Wikipedia, the free encyclopedia.txt#32, term: computer, content:Unix-like systems run on a wide variety of computer architectures. They are used heavily for servers in business, as well as workstations in academic and engineering environments. Free UNIX variants, such as Linux and BSD, are popular in these areas.
814) 20.117142, Operating system - Wikipedia, the free encyclopedia.txt#90, term: computer, content:A computer being secure depends on a number of technologies working properly. A modern operating system provides access to a number of resources, which are available to software running on the system, and to external devices like networks via the kernel.
815) 20.117142, Pascal (programming language) - Wikipedia, the free encyclopedia.txt#17, term: computer, content:A compiler based on the Pascal-P4 compiler, which created native binaries, was released for the IBM System/370 mainframe computer by the Australian Atomic Energy Commission; it was called the "AAEC Pascal Compiler" after the abbreviation of the name of the Commission.[8]
816) 20.117142, PDP-11 - Wikipedia, the free encyclopedia.txt#23, term: computer, content:By the late 1990s, not only DEC but most of the New England computer industry which had been built around minicomputers similar to the PDP-11 collapsed in the face of microcomputer-based workstations and servers.
817) 20.117142, PDP-8 - Wikipedia, the free encyclopedia.txt#9, term: computer, content:As design advances reduced the costs of logic and memory, the programmer's time became more important. Subsequent computer designs emphasized ease of programming, typically using a larger and more intuitive instruction set.[clarification needed]
818) 20.117142, Pentium FDIV bug - Wikipedia, the free encyclopedia.txt#9, term: computer, content:A 1995 article in Science describes the value of number theory problems in discovering computer bugs and gives the mathematical background and history of Brun's constant, the problem Nicely was working on when he discovered the bug.[7]
819) 20.117142, Personal computer - Wikipedia, the free encyclopedia.txt#9, term: computer, content:By the early 1970s, people in academic or research institutions had the opportunity for single-person use of a computer system in interactive mode for extended durations, although these systems would still have been too expensive to be owned by a single person.
820) 20.117142, Personal computer - Wikipedia, the free encyclopedia.txt#21, term: computer, content:In the 2010s, several companies such as Hewlett-Packard and Sony sold off their PC and laptop divisions. As a result, the personal computer was declared dead several times during this time.[15]
821) 20.117142, Personal digital assistant - Wikipedia, the free encyclopedia.txt#18, term: computer, content:Synchronization also prevents the loss of information stored on the device if it is lost, stolen, or destroyed. When the PDA is repaired or replaced, it can be "re-synced" with the computer, restoring the user's data.
822) 20.117142, Portable computer - Wikipedia, the free encyclopedia.txt#5, term: computer, content:Xerox NoteTaker, developed in 1976 at Xerox PARC, was a precursor to later portable computers from Osborne Computer Corporation and Compaq, though it remained a prototype and did not enter production.
823) 20.117142, Portable Network Graphics - Wikipedia, the free encyclopedia.txt#6, term: computer, content:The original PNG specification was authored by an ad-hoc group of computer graphics experts and enthusiasts. Discussions and decisions about the format were done exclusively via email. The original authors listed on RFC 2083 are:[8]
824) 20.117142, Post Office Research Station - Wikipedia, the free encyclopedia.txt#1, term: computer, content:In 1943 the world's first programmable electronic computer, Colossus Mark 1 was built by Tommy Flowers and his team, followed in 1944 and 1945 by nine Colossus Mark 2s. These were used at Bletchley Park in Cryptanalysis of the Lorenz cipher.
825) 20.117142, PowerPC - Wikipedia, the free encyclopedia.txt#39, term: computer, content:In 2003, BAE SYSTEMS Platform Solutions delivered the Vehicle-Management Computer for the F-35 fighter jet. This platform consists of dual PowerPCs made by Freescale in a triple redundant setup.[10]
826) 20.117142, Printer (computing) - Wikipedia, the free encyclopedia.txt#15, term: computer, content:Inkjet printers operate by propelling variably sized droplets of liquid ink onto almost any sized page. They are the most common type of computer printer used by consumers.
827) 20.117142, QNX - Wikipedia, the free encyclopedia.txt#5, term: computer, content:The BlackBerry PlayBook tablet computer designed by BlackBerry uses a version of QNX as the primary operating system. Devices from BlackBerry running the BlackBerry 10 operating system are also based on QNX.
828) 20.117142, Quantum computing - Wikipedia, the free encyclopedia.txt#20, term: computer, content:For problems with all four properties, the time for a quantum computer to solve this will be proportional to the square root of the number of inputs. It can be used to attack symmetric ciphers such as Triple DES and AES by attempting to guess the secret key.[20]
829) 20.117142, Quantum computing - Wikipedia, the free encyclopedia.txt#28, term: computer, content:A very different approach to the stability-decoherence problem is to create a topological quantum computer with anyons, quasi-particles used as threads and relying on braid theory to form stable logic gates.[26][27]
830) 20.117142, Quantum computing - Wikipedia, the free encyclopedia.txt#49, term: computer, content:In February 2013, a new technique, boson sampling, was reported by two groups using photons in an optical lattice that is not a universal quantum computer but may be good enough for practical problems. Science Feb 15, 2013
831) 20.117142, Random-access machine - Wikipedia, the free encyclopedia.txt#1, term: computer, content:The RAM's equivalent of the universal Turing machine with its program in the registers as well as its data is called the random-access stored-program machine or RASP. It is an example of the so-called von Neumann architecture and is closest to the common notion of computer.
832) 20.117142, Random-access machine - Wikipedia, the free encyclopedia.txt#25, term: computer, content:In the context of a more computer-like model using his RPT (repeat) instruction Minsky (1967) tantalizes us with a solution to the problem (cf p.214, p.259) but offers no firm resolution. He asserts:
833) 20.117142, Relay - Wikipedia, the free encyclopedia.txt#21, term: computer, content:In computer memories, latching relays and other relays were replaced by delay line memory, which in turn was replaced by a series of ever-faster and ever-smaller memory technologies.
834) 20.117142, Sabre (computer system) - Wikipedia, the free encyclopedia.txt#3, term: computer, content:Sabre Holdings' history starts with SABRE (Semi-automated Business Research Environment), a computer reservation system which was developed to automate the way American Airlines booked reservations.
835) 20.117142, SCSI - Wikipedia, the free encyclopedia.txt#16, term: computer, content:iSCSI (Internet Small Computer System Interface) usually uses Ethernet connectors and cables as its physical transport, but can run over any physical transport capable of transporting IP.
836) 20.117142, Semi-Automatic Ground Environment - Wikipedia, the free encyclopedia.txt#22, term: computer, content:develop a digital computer that could receive vast quantities of data from multiple radars and perform real-time processing to produce targeting information for intercepting aircraft and missiles
837) 20.117142, Serious game - Wikipedia, the free encyclopedia.txt#5, term: computer, content:Mike Zyda provided an update and a logical approach to the term in his 2005 article in IEEE Computer entitled, "From Visual Simulation to Virtual Reality to Games". Zyda's definition begins with "game" and proceeds from there:
838) 20.117142, Shor's algorithm - Wikipedia, the free encyclopedia.txt#0, term: computer, content:Shor's algorithm, named after mathematician Peter Shor, is a quantum algorithm (an algorithm that runs on a quantum computer) for integer factorization formulated in 1994. Informally it solves the following problem: given an integer N, find its prime factors.
839) 20.117142, Shor's algorithm - Wikipedia, the free encyclopedia.txt#2, term: computer, content:If a quantum computer with a sufficient number of qubits could operate without succumbing to noise and other quantum decoherence phenomena, Shor's algorithm could be used to break public-key cryptography schemes such as the widely used RSA scheme. RSA is based on the assumption that factoring large numbers is computationally intractable. So far as is known, this assumption is valid for classical (non-quantum) computers; no classical algorithm is known that can factor in polynomial time. However, Shor's algorithm shows that factoring is efficient on an ideal quantum computer, so it may be feasible to defeat RSA by constructing a large quantum computer. It was also a powerful motivator for the design and construction of quantum computers and for the study of new quantum computer algorithms. It has also facilitated research on new cryptosystems that are secure from quantum computers, collectively called post-quantum cryptography.
840) 20.117142, Shor's algorithm - Wikipedia, the free encyclopedia.txt#20, term: computer, content:Shor's period-finding algorithm relies heavily on the ability of a quantum computer to be in many states simultaneously. Physicists call this behavior a "superposition" of states. To compute the period of a function f, we evaluate the function at all points simultaneously.
841) 20.117142, Slide rule - Wikipedia, the free encyclopedia.txt#25, term: computer, content:This method is similar to the addition/subtraction technique used for high-speed electronic circuits with the logarithmic number system in specialized computer applications like the Gravity Pipe (GRAPE) supercomputer and hidden Markov models.
842) 20.117142, Sound card - Wikipedia, the free encyclopedia.txt#51, term: computer, content:A USB audio interface may also describe a device allowing a computer which has a sound-card, yet lacks a standard audio socket, to be connected to an external device which requires such a socket, via its USB socket.
843) 20.117142, Spreadsheet - Wikipedia, the free encyclopedia.txt#78, term: computer, content:Just as the early programming languages were designed to generate spreadsheet printouts, programming techniques themselves have evolved to process tables (also known as spreadsheets or matrices) of data more efficiently in the computer itself.
844) 20.117142, Stack machine - Wikipedia, the free encyclopedia.txt#54, term: computer, content:(These should not be confused with hybrid computers that combine both digital and analogue features, e.g. an otherwise digital computer that accesses analogue multiplication or differential equation solving by memory mapping and conversion to and from an analogue device's inputs and outputs.)
845) 20.117142, Stored-program computer - Wikipedia, the free encyclopedia.txt#0, term: computer, content:A stored-program computer is one that stores program instructions in electronic memory.[1] Often the definition is extended with the requirement that the treatment of programs and data in memory be interchangeable or uniform.[2][3][4]
846) 20.117142, Stored-program computer - Wikipedia, the free encyclopedia.txt#3, term: computer, content:The stored-program computer idea can be traced back to the 1936 theoretical concept of a universal Turing machine.[11] Von Neumann was aware of this paper, and he impressed it on his collaborators as well.[12]
847) 20.117142, Suanpan - Wikipedia, the free encyclopedia.txt#18, term: computer, content:In mainland China, formerly accountants and financial personnel had to pass certain graded examinations in bead arithmetic before they were qualified. Starting from about 2002 or 2004, this requirement has been entirely replaced by computer accounting.
848) 20.117142, Subroutine - Wikipedia, the free encyclopedia.txt#22, term: computer, content:To remove the need for self-modifying code, computer designers eventually provided an indirect jump instruction, whose operand, instead of being the return address itself, was the location of a variable or processor register containing the return address.
849) 20.117142, Telephone exchange - Wikipedia, the free encyclopedia.txt#72, term: computer, content:A time-division subswitch reads a complete cycle of time slots into a memory, and then writes it out in a different order, also under control of a cyclic computer memory. This causes some delay in the signal.
850) 20.117142, Teleprinter - Wikipedia, the free encyclopedia.txt#45, term: computer, content:Although printing news, messages, and other text at a distance is still universal, the dedicated teleprinter tied to a pair of leased copper wires was made functionally obsolete by the fax, personal computer, inkjet printer, broadband, and the Internet.
851) 20.117142, Texas Instruments - Wikipedia, the free encyclopedia.txt#29, term: computer, content:The 7400 series of transistor-transistor logic (TTL) chips, developed by Texas Instruments in the 1960s, popularized the use of integrated circuits in computer logic. The military grade version of this was the 5400 series.[citation needed]
852) 20.117142, Text editor - Wikipedia, the free encyclopedia.txt#3, term: computer, content:Word processors were developed to allow formatting of text for presentation on a printed page, while text produced by text editors is generally used for other purposes, such as input data for a computer program.
853) 20.117142, Theory of computation - Wikipedia, the free encyclopedia.txt#2, term: computer, content:The theory of computation can be considered the creation of models of all kinds in the field of computer science. Therefore, mathematics and logic are used. In the last century it became an independent academic discipline and was separated from mathematics.
854) 20.117142, Tommy Flowers - Wikipedia, the free encyclopedia.txt#2, term: computer, content:From 1935 onward, he explored the use of electronics for telephone exchanges. By 1939, he was convinced that an all-electronic system was possible. This background in switching electronics would prove crucial for his computer design in World War II.
855) 20.117142, TOP500 - Wikipedia, the free encyclopedia.txt#15, term: computer, content:IBM Roadrunner[23] is no longer on the list (or any other using the Cell coprocessor, or PowerXCell as in the Roadrunner supercomputer), but it is an example of a computer that would easily be included, if it had not been decommissioned, as it is faster than the one ranked 500th.[24]
856) 20.117142, Torpedo Data Computer - Wikipedia, the free encyclopedia.txt#24, term: computer, content:The equations implemented in the angle solver can be found in the Torpedo Data Computer manual.[39] The Submarine Torpedo Fire Control Manual[40] discusses the calculations in a general sense and a greatly abbreviated form of that discussion is presented here.
857) 20.117142, Torpedo Data Computer - Wikipedia, the free encyclopedia.txt#34, term: computer, content:There is fairly complete documentation available for a Japanese torpedo fire control computer that goes through the details of correcting for the ballistic and parallax factors. While the TDC may not have used exactly the same approach, it was likely very similar.
858) 20.117142, Touchscreen - Wikipedia, the free encyclopedia.txt#12, term: computer, content:In 1987, Casio launched the Casio PB-1000 pocket computer with a touchscreen consisting of a 4x4 matrix, resulting in 16 touch areas in its small LCD graphic screen.
859) 20.117142, Transistor computer - Wikipedia, the free encyclopedia.txt#1, term: computer, content:The University of Manchester's experimental Transistor Computer was first operational in November 1953 and it is widely believed to be the first transistor computer to come into operation anywhere in the world. There were two versions of the Transistor Computer, the prototype, operational in 1953, and the full-size version, commissioned in April 1955. The 1953 machine had 92 point-contact transistors and 550 diodes, manufactured by STC. It had a 48-bit machine word.[1] The 1955 machine had a total of 200 point-contact transistors and 1300 point diodes,[1] which resulted in a power consumption of 150 watts. There were considerable reliability problems with the early batches of transistors and the average error free run in 1955 was only 1.5 hours. The Computer also used a small number of tubes in its clock generator, so it was not the first fully transistorized machine.[2]
860) 20.117142, Transistor computer - Wikipedia, the free encyclopedia.txt#5, term: computer, content:In Japan the ETL Mark III began operation in July 1956; the Canadian DRTE Computer in 1957, while in Austria, the Mailfterl was completed in May 1958,[5] being the first transistorized computers in Asia and mainland Europe.
861) 20.117142, Turing completeness - Wikipedia, the free encyclopedia.txt#14, term: computer, content:The computational systems (algebras, calculi) that are discussed as Turing complete systems are those intended for studying theoretical computer science. They are intended to be as simple as possible, so that it would be easier to understand the limits of computation. Here are a few:
862) 20.117142, Turing completeness - Wikipedia, the free encyclopedia.txt#18, term: computer, content:The untyped lambda calculus is Turing complete, but many typed lambda calculi, including System F, are not. The value of typed systems is based in their ability to represent most typical computer programs while detecting more errors.
863) 20.117142, University of Manchester - Wikipedia, the free encyclopedia.txt#34, term: computer, content:The Faculty of Engineering and Physical Sciences comprises the schools of Chemical Engineering and Analytical Science;Chemistry; Computer Science; Earth, Atmospheric and Environmental Science; Physics and Astronomy; Electrical and Electronic Engineering; Materials; Mathematics; and Mechanical, Aerospace and Civil Engineering.
864) 20.117142, Unix - Wikipedia, the free encyclopedia.txt#0, term: computer, content:Unix (trademarked as UNIX) is a family of multitasking, multiuser computer operating systems that derive from the original AT&T Unix, developed in the 1970s at the Bell Labs research center by Ken Thompson, Dennis Ritchie, and others.[3]
865) 20.117142, USB - Wikipedia, the free encyclopedia.txt#38, term: computer, content:Another use for USB mass storage devices is the portable execution of software applications (such as web browsers and VoIP clients) with no need to install them on the host computer.[50][51]
866) 20.117142, User interface - Wikipedia, the free encyclopedia.txt#5, term: computer, content:In complex systems, the humanmachine interface is typically computerized. The term humancomputer interface refers to this kind of system. In the context of computing the term typically extends as well to the software dedicated to control the physical elements used for human-computer interaction.
867) 20.117142, Vector graphics editor - Wikipedia, the free encyclopedia.txt#4, term: computer, content:Special vector editors are used for computer-assisted drafting. These are not suitable for artistic or decorative graphics, but are rich in tools and object libraries used to ensure precision and standards compliance of drawings and blueprints.
868) 20.117142, Vector graphics editor - Wikipedia, the free encyclopedia.txt#5, term: computer, content:Finally, 3D computer graphics software such as Maya, Blender or 3D Studio Max can also be thought of as an extension of the traditional 2D vector editors, as they share some common concepts and tools.
869) 20.117142, Video game industry - Wikipedia, the free encyclopedia.txt#26, term: computer, content:In the computer games industry, it is easier to create a startup, resulting in many successful companies. The console games industry is a more closed one, and a game developer must have up to three licenses from the console manufacturer:
870) 20.117142, Von Neumann architecture - Wikipedia, the free encyclopedia.txt#25, term: computer, content:The date information in the following chronology is difficult to put into proper order. Some dates are for first running a test program, some dates are the first time the computer was demonstrated or completed, and some dates are for the first delivery or installation.
871) 20.117142, Wearable computer - Wikipedia, the free encyclopedia.txt#19, term: computer, content:Thorp refers to himself as the inventor of the first "wearable computer"[15] In other variations, the system was a concealed cigarette-pack sized analog computer designed to predict the motion of roulette wheels. A data-taker would use microswitches hidden in his shoes to indicate the speed of the roulette wheel, and the computer would indicate an octant of the roulette wheel to bet on by sending musical tones via radio to a miniature speaker hidden in a collaborator's ear canal. The system was successfully tested in Las Vegas in June 1961, but hardware issues with the speaker wires prevented it from being used beyond test runs.[16] This was not a wearable computer, because it could not be re-purposed during use; rather it was an example of task-specific hardware. This work was kept secret until it was first mentioned in Thorp's book Beat the Dealer (revised ed.) in 1966[16] and later published in detail in 1969.[17]
872) 20.117142, Webcam - Wikipedia, the free encyclopedia.txt#4, term: computer, content:The most popular use of webcams is the establishment of video links, permitting computers to act as bs or videoconference stations. Other popular uses include security surveillance, computer vision, video broadcasting, and for recording social videos.
873) 20.117142, Webcam - Wikipedia, the free encyclopedia.txt#10, term: computer, content:Webcam features and performance can vary by program, computer operating system, and also by the computer's processor capabilities. Video calling support has also been added to several popular instant messaging programs.
874) 20.117142, Williams tube - Wikipedia, the free encyclopedia.txt#3, term: computer, content:The process of creating the charge well is used as the write operation in a computer memory, storing a single binary digit, or bit. A collection of dots or spaces, often one horizontal row on the display, represents a computer word. There is a relationship between the size and spacing of the dots and their lifetime, as well as the ability to reject crosstalk with adjacent dots. This produces an upper limit on the memory density, and each Williams tube could typically store about 10242560 bits of data. Because the electron beam is essentially inertia free and can be moved anywhere on the display, the computer can access any location, making it a random access memory. Typically the computer would load the address as an X and Y pair into the driver circuity and then trigger a time base generator that would sweep the selected locations, reading from or writing to the internal registers, normally implemented as flip-flops.
875) 20.117142, WIMP (computing) - Wikipedia, the free encyclopedia.txt#5, term: computer, content:WIMP-style user interfaces place visually impaired users at a disadvantage, especially when alternative text-based interfaces are not made available, and researchers are exploring other alternatives that make modern computer systems more accessible.[10]
876) 20.117142, Windows 95 - Wikipedia, the free encyclopedia.txt#34, term: computer, content:The release included a number of "Fun Stuff" items on the CD, including music videos of Edie Brickell's "Good Times"[34] and Weezer's "Buddy Holly", a trailer for the 1995 film Rob Roy, and the computer game Hover!.[35]
877) 20.117142, Windows XP - Wikipedia, the free encyclopedia.txt#16, term: computer, content:In an effort to prevent copyright infringement of XP, it also introduced Windows Product Activation, which requires that each Windows license be "activated" and tied to a unique ID generated using information from the computer hardware.
878) 20.117142, Word processor - Wikipedia, the free encyclopedia.txt#3, term: computer, content:Word processing was one of the earliest applications for the personal computer in office productivity and was the most popular application on home and personal computers until the World Wide Web rose to prominence in the mid-1990s.
879) 20.117142, World Wide Web - Wikipedia, the free encyclopedia.txt#24, term: computer, content:The computer receiving the HTTP request delivers it to web server software listening for requests on port 80. If the web server can fulfill the request it sends an HTTP response back to the browser indicating success:
880) 20.117142, Z1 (computer) - Wikipedia, the free encyclopedia.txt#0, term: computer, content:The Z1 was a mechanical computer designed by Konrad Zuse from 1935 to 1936 and built by him from 1936 to 1938. It was a binary electrically driven mechanical calculator with limited programmability, reading instructions from punched celluloid film.
881) 20.117142, Z3 (computer) - Wikipedia, the free encyclopedia.txt#13, term: computer, content:The Tommy Flowers-built Colossus (1943)[24] and the AtanasoffBerry Computer (1942) used thermionic valves (vacuum tubes) and binary representation of numbers. Programming was by means of re-plugging patch panels and setting switches.
882) 20.117142, Z3 (computer) - Wikipedia, the free encyclopedia.txt#14, term: computer, content:The ENIAC computer, completed after the war, used vacuum tubes to implement switches and used decimal representation for numbers. Until 1948 programming was, as with Colossus, by patch leads and switches.
883) 20.117142, Zilog Z80 - Wikipedia, the free encyclopedia.txt#3, term: computer, content:In the early 1980s, the Z80 was the most commonly used CPU of all time, and, along with the MOS Technology 6502 family, dominated the home computer market from the late 1970s to the mid 1980s.[4][5]
884) 18.966621, 3D computer graphics - Wikipedia, the free encyclopedia.txt#11, term: computer, content:Not all computer graphics that appear 3D are based on a wireframe model. 2D computer graphics with 3D photorealistic effects are often achieved without wireframe modeling and are sometimes indistinguishable in the final form. Some graphic art software includes filters that can be applied to 2D vector graphics or 2D raster graphics on transparent layers. Visual artists may also copy or visualize 3D effects and manually render photorealistic effects without the use of filters.
885) 18.966621, 86-DOS - Wikipedia, the free encyclopedia.txt#18, term: computer, content:In 1984 Seattle Computer Products released an OEM version of MS-DOS 2.0 for the SCP S-100 computer with SCP-500 Disk Master Floppy controller. It added support for 5.25" DD/1S (180 KB) and DD/2S (360 KB) FAT12 formats and supported the older formats as well, although possibly with some of the parameters modified compared to MS-DOS 1.25.[25]
886) 18.966621, Alan Turing - Wikipedia, the free encyclopedia.txt#60, term: computer, content:A plaque at the statue's feet reads 'Father of computer science, mathematician, logician, wartime codebreaker, victim of prejudice'. There is also a Bertrand Russell quotation: "Mathematics, rightly viewed, possesses not only truth, but supreme beauty a beauty cold and austere, like that of sculpture." The sculptor buried his own old Amstrad computer under the plinth as a tribute to "the godfather of all modern computers".[127]
887) 18.966621, Algorithm - Wikipedia, the free encyclopedia.txt#10, term: computer, content:Algorithms are essential to the way computers process data. Many computer programs contain algorithms that detail the specific instructions a computer should perform (in a specific order) to carry out a specified task, such as calculating employees' paychecks or printing students' report cards. Thus, an algorithm can be considered to be any sequence of operations that can be simulated by a Turing-complete system. Authors who assert this thesis include Minsky (1967), Savage (1987) and Gurevich (2000):
888) 18.966621, Algorithm - Wikipedia, the free encyclopedia.txt#30, term: computer, content:Simulation of an algorithm: computer (computor) language: Knuth advises the reader that "the best way to learn an algorithm is to try it . . . immediately take pen and paper and work through an example".[35] But what about a simulation or execution of the real thing? The programmer must translate the algorithm into a language that the simulator/computer/computor can effectively execute. Stone gives an example of this: when computing the roots of a quadratic equation the computor must know how to take a square root. If they don't then for the algorithm to be effective it must provide a set of rules for extracting a square root.[36]
889) 18.966621, Analog computer - Wikipedia, the free encyclopedia.txt#3, term: computer, content:Setting up an analog computer required scale factors to be chosen, along with initial conditionsthat is, starting values. Another essential was creating the required network of interconnections between computing elements. Sometimes it was necessary to re-think the structure of the problem so that the computer would function satisfactorily. No variables could be allowed to exceed the computer's limits, and differentiation was to be avoided, typically by rearranging the "network" of interconnects, using integrators in a different sense.
890) 18.966621, Analog computer - Wikipedia, the free encyclopedia.txt#36, term: computer, content:The largest manufacturer of hybrid computers was Electronics Associates. Their hybrid computer model 8900 was made of a digital computer and one or more analog consoles. These systems were mainly dedicated to large projects such as the Apollo program and Space Shuttle at NASA, or Ariane in Europe, especially during the integration step where at the beginning everything is simulated, and progressively real components replace their simulated part.[citation needed]
891) 18.966621, Antivirus software - Wikipedia, the free encyclopedia.txt#79, term: computer, content:Some antivirus vendors maintain websites with free online scanning capability of the entire computer, critical areas only, local disks, folders or files. Periodic online scanning is a good idea for those that run antivirus applications on their computers because those applications are frequently slow to catch threats. One of the first things that malicious software does in an attack is disable any existing antivirus software and sometimes the only way to know of an attack is by turning to an online resource that is not installed on the infected computer.[150]
892) 18.966621, Application software - Wikipedia, the free encyclopedia.txt#0, term: computer, content:An application program (app or application for short) is a computer program designed to perform a group of coordinated functions, tasks, or activities for the benefit of the user. Examples of an application include a word processor, a spreadsheet, an accounting application, a web browser, a media player, an aeronautical flight simulator, a console game or a photo editor. The collective noun application software refers to all applications collectively.[1] This contrasts with system software, which is mainly involved with running the computer.
893) 18.966621, Artificial intelligence - Wikipedia, the free encyclopedia.txt#86, term: computer, content:Searle's strong AI hypothesis states that "The appropriately programmed computer with the right inputs and outputs would thereby have a mind in exactly the same sense human beings have minds."[227] John Searle counters this assertion with his Chinese room argument, which asks us to look inside the computer and try to find where the "mind" might be.[228]
894) 18.966621, Assembly language - Wikipedia, the free encyclopedia.txt#0, term: computer, content:An assembly (or assembler) language,[1] often abbreviated asm, is a low-level programming language for a computer, or other programmable device, in which there is a very strong (generally one-to-one) correspondence between the language and the architecture's machine code instructions. Each assembly language is specific to a particular computer architecture. In contrast, most high-level programming languages are generally portable across multiple architectures but require interpreting or compiling. Assembly language may also be called symbolic machine code.[2]
895) 18.966621, Assembly language - Wikipedia, the free encyclopedia.txt#49, term: computer, content:Assembly languages, and the use of the word assembly, date to the introduction of the stored-program computer. The Electronic Delay Storage Automatic Calculator (EDSAC) had an assembler called initial orders featuring one-letter mnemonics in 1949.[19] SOAP (Symbolic Optimal Assembly Program) was an assembly language for the IBM 650 computer written by Stan Poley in 1955.[20]
896) 18.966621, Atanasoff–Berry computer - Wikipedia, the free encyclopedia.txt#21, term: computer, content:In 1997, a team of researchers led by John Gustafson from Ames Laboratory (located on the Iowa State campus) finished building a working replica of the AtanasoffBerry Computer at a cost of $350,000 (equivalent to $0.52 million in 2015). The replica ABC is now on permanent display in the first floor lobby of the Durham Center for Computation and Communication at Iowa State University. As of May 2012, it is on loan to the Computer History Museum in Mountain View, California for a major exhibition.
897) 18.966621, BASIC - Wikipedia, the free encyclopedia.txt#4, term: computer, content:Before the mid-1960s, the only computers were mainframes. Users submitted jobs on punched cards or similar media to computer operators. The computer stored these, then used a batch processing system to run this queue of jobs one after another - allowing very high levels of utilization of these expensive machines. As the performance of computing hardware rose through the 1960s, multi-processing was developed. This allowed a mix of batch jobs to be run together, but the real revolution was the development of time-sharing.
898) 18.966621, BIOS - Wikipedia, the free encyclopedia.txt#53, term: computer, content:Early BIOS versions did not have passwords or boot-device selection options. The BIOS was hard-coded to boot from the first floppy drive, or, if that failed, the first hard disk. Access control in early AT-class machines was by a physical keylock switch (which was not hard to defeat if the computer case could be opened). Anyone who could switch on the computer could boot it.[citation needed]
899) 18.966621, Booting - Wikipedia, the free encyclopedia.txt#0, term: computer, content:In computing, booting (or booting up) is the initialization of a computerized system. The system can be a computer or a computer appliance. The booting process can be "hard", after electrical power to the CPU is switched from off to on (in order to diagnose particular hardware errors), or "soft", when those power-on self-tests (POST) can be avoided. Soft booting can be initiated by hardware such as a button press, or by software command. Booting is complete when the normal, operative, runtime environment is attained.
900) 18.966621, Booting - Wikipedia, the free encyclopedia.txt#27, term: computer, content:Apple Inc.'s first computer, the Apple 1 introduced in 1976, featured PROM chips that eliminated the need for a front panel for the boot process (as it was the case with the Altair 8800) in a commercial computer. According to Apple's ad announcing it "No More Switches, No More Lights ... the firmware in PROMS enables you to enter, display and debug programs (all in hex) from the keyboard."[23]
901) 18.966621, Booting - Wikipedia, the free encyclopedia.txt#31, term: computer, content:The IBM Personal Computer included ROM-based firmware called the BIOS; one of the functions of that firmware was to perform a power-on self test when the machine was powered up, and then to read software from a boot device and execute it. Firmware compatible with the BIOS on the IBM Personal Computer is used in IBM PC compatible computers. The Extensible Firmware Interface was developed by Intel, originally for Itanium-based machines, and later also used as an alternative to the BIOS in x86-based machines, including Apple Macs using Intel processors.
902) 18.966621, Bus (computing) - Wikipedia, the free encyclopedia.txt#1, term: computer, content:Early computer buses were parallel electrical wires with multiple connections, but the term is now used for any physical arrangement that provides the same logical function as a parallel electrical bus. Modern computer buses can use both parallel and bit serial connections, and can be wired in either a multidrop (electrical parallel) or daisy chain topology, or connected by switched hubs, as in the case of USB.
903) 18.966621, Bus (computing) - Wikipedia, the free encyclopedia.txt#8, term: computer, content:The internal bus, also known as internal data bus, memory bus, system bus or Front-Side-Bus, connects all the internal components of a computer, such as CPU and memory, to the motherboard. Internal data buses are also referred to as a local bus, because they are intended to connect to local devices. This bus is typically rather quick and is independent of the rest of the computer operations.
904) 18.966621, Calculator - Wikipedia, the free encyclopedia.txt#17, term: computer, content:The fundamental difference between a calculator and computer is that a computer can be programmed in a way that allows the program to take different branches according to intermediate results, while calculators are pre-designed with specific functions (such as addition, multiplication, and logarithms) built in. The distinction is not clear-cut: some devices classed as programmable calculators have programming functionality, sometimes with support for programming languages (such as RPL or TI-BASIC).
905) 18.966621, Calculator - Wikipedia, the free encyclopedia.txt#32, term: computer, content:The Monroe Epic programmable calculator came on the market in 1967. A large, printing, desk-top unit, with an attached floor-standing logic tower, it could be programmed to perform many computer-like functions. However, the only branch instruction was an implied unconditional branch (GOTO) at the end of the operation stack, returning the program to its starting instruction. Thus, it was not possible to include any conditional branch (IF-THEN-ELSE) logic. During this era, the absence of the conditional branch was sometimes used to distinguish a programmable calculator from a computer.
906) 18.966621, COBOL - Wikipedia, the free encyclopedia.txt#3, term: computer, content:Academic computer scientists were generally uninterested in business applications when COBOL was created and were not involved in its design; it was (effectively) designed from the ground up as a computer language for businessmen, with an emphasis on inputs and outputs, whose only data types were numbers and strings of text.[13] COBOL has been criticized throughout its life, however, for its verbosity, design process and poor support for structured programming, which resulted in monolithic and incomprehensible programs.
907) 18.966621, Combinational logic - Wikipedia, the free encyclopedia.txt#1, term: computer, content:Combinational logic is used in computer circuits to perform Boolean algebra on input signals and on stored data. Practical computer circuits normally contain a mixture of combinational and sequential logic. For example, the part of an arithmetic logic unit, or ALU, that does mathematical calculations is constructed using combinational logic. Other circuits used in computers, such as half adders, full adders, half subtractors, full subtractors, multiplexers, demultiplexers, encoders and decoders are also made by using combinational logic.
908) 18.966621, Command-line interface - Wikipedia, the free encyclopedia.txt#1, term: computer, content:The CLI was the primary means of interaction with most computer systems until the introduction of the video display terminal in the mid-1960s, and continued to be used throughout the 1970s and 1980s on OpenVMS, Unix systems and personal computer systems including MS-DOS, CP/M and Apple DOS. The interface is usually implemented with a command line shell, which is a program that accepts commands as text input and converts commands to appropriate operating system functions.
909) 18.966621, Command-line interface - Wikipedia, the free encyclopedia.txt#13, term: computer, content:Some computer programs support both a CLI and a GUI. In some cases, a GUI is simply a wrapper around a separate CLI executable file. In other cases, a program may provide a CLI as an optional alternative to its GUI. CLIs and GUIs often support different functionality. For example, all features of MATLAB, a numerical analysis computer program, are available via the CLI, whereas the MATLAB GUI exposes only a subset of features.
910) 18.966621, Communications protocol - Wikipedia, the free encyclopedia.txt#26, term: computer, content:In literature there are numerous references to the analogies between computer communication and programming. By analogy we could say that the aforementioned 'xfer-mechanism' is comparable to a cpu; a 'xfer-mechanism' performs communications and a cpu performs computations and the 'framework' introduces something that allows the protocols to be designed independent of one another by providing separate execution environments for them. Furthermore, it is repeatedly stated that protocols are to computer communication what programming languages are to computation.[35][36]
911) 18.966621, Computer - Simple English Wikipedia, the free encyclopedia.txt#26, term: computer, content:A "desktop computer" is a small machine that has a screen (which is not part of the computer). Most people keep them on top of a desk, which is why they are called "desktop computers." "Laptop computers" are computers small enough to fit on your lap. This makes them easy to carry around. Both laptops and desktops are called personal computers, because one person at a time uses them for things like playing music, surfing the web, or playing video games.
912) 18.966621, Computer - Wikipedia, the free encyclopedia.txt#2, term: computer, content:The first known use of the word "computer" was in 1613 in a book called The Yong Mans Gleanings by English writer Richard Braithwait: "I haue read the truest computer of Times, and the best Arithmetician that euer breathed, and he reduceth thy dayes into a short number." It referred to a person who carried out calculations, or computations. The word continued with the same meaning until the middle of the 20th century. From the end of the 19th century the word began to take on its more familiar meaning, a machine that carries out computations.[1]
913) 18.966621, Computer - Wikipedia, the free encyclopedia.txt#25, term: computer, content:The US-built ENIAC[32] (Electronic Numerical Integrator and Computer) was the first electronic programmable computer built in the US. Although the ENIAC was similar to the Colossus it was much faster and more flexible. Like the Colossus, a "program" on the ENIAC was defined by the states of its patch cables and switches, a far cry from the stored program electronic machines that came later. Once a program was written, it had to be mechanically set into the machine with manual resetting of plugs and switches.
914) 18.966621, Computer - Wikipedia, the free encyclopedia.txt#43, term: computer, content:Program execution might be likened to reading a book. While a person will normally read each word and line in sequence, they may at times jump back to an earlier place in the text or skip sections that are not of interest. Similarly, a computer may sometimes go back and repeat the instructions in some section of the program over and over again until some internal condition is met. This is called the flow of control within the program and it is what allows the computer to perform tasks repeatedly without human intervention.
915) 18.966621, Computer animation - Wikipedia, the free encyclopedia.txt#2, term: computer, content:For 3D animations, objects (models) are built on the computer monitor (modeled) and 3D figures are rigged with a virtual skeleton. For 2D figure animations, separate objects (illustrations) and separate transparent layers are used with or without that virtual skeleton. Then the limbs, eyes, mouth, clothes, etc. of the figure are moved by the animator on key frames. The differences in appearance between key frames are automatically calculated by the computer in a process known as tweening or morphing. Finally, the animation is rendered.
916) 18.966621, Computer animation - Wikipedia, the free encyclopedia.txt#18, term: computer, content:The realistic modeling of human facial features is both one of the most challenging and sought after elements in computer-generated imagery. Computer facial animation is a highly complex field where models typically include a very large number of animation variables.[20] Historically speaking, the first SIGGRAPH tutorials on State of the art in Facial Animation in 1989 and 1990 proved to be a turning point in the field by bringing together and consolidating multiple research elements and sparked interest among a number of researchers.[21]
917) 18.966621, Computer architecture - Wikipedia, the free encyclopedia.txt#19, term: computer, content:The exact form of a computer system depends on the constraints and goals. Computer architectures usually trade off standards, power versus performance, cost, memory capacity, latency (latency is the amount of time that it takes for information from one node to travel to the source) and throughput. Sometimes other considerations, such as features, size, weight, reliability, and expandability are also factors.
918) 18.966621, Computer cluster - Wikipedia, the free encyclopedia.txt#1, term: computer, content:The components of a cluster are usually connected to each other through fast local area networks ("LAN"), with each node (computer used as a server) running its own instance of an operating system. In most circumstances, all of the nodes use the same hardware[2] and the same operating system, although in some setups (i.e. using Open Source Cluster Application Resources (OSCAR)), different operating systems can be used on each computer, and/or different hardware.[3]
919) 18.966621, Computer cluster - Wikipedia, the free encyclopedia.txt#14, term: computer, content:Computer clusters may be configured for different purposes ranging from general purpose business needs such as web-service support, to computation-intensive scientific calculations. In either case, the cluster may use a high-availability approach. Note that the attributes described below are not exclusive and a "computer cluster" may also use a high-availability approach, etc.
920) 18.966621, Computer cluster - Wikipedia, the free encyclopedia.txt#19, term: computer, content:One of the issues in designing a cluster is how tightly coupled the individual nodes may be. For instance, a single computer job may require frequent communication among nodes: this implies that the cluster shares a dedicated network, is densely located, and probably has homogeneous nodes. The other extreme is where a computer job uses one or few nodes, and needs little or no inter-node communication, approaching grid computing.
921) 18.966621, Computer data storage - Wikipedia, the free encyclopedia.txt#30, term: computer, content:Off-line storage is a computer data storage on a medium or a device that is not under the control of a processing unit.[6] The medium is recorded, usually in a secondary or tertiary storage device, and then physically removed or disconnected. It must be inserted or connected by a human operator before a computer can access it again. Unlike tertiary storage, it cannot be accessed without human interaction.
922) 18.966621, Computer engineering - Wikipedia, the free encyclopedia.txt#1, term: computer, content:Usual tasks involving computer engineers include writing software and firmware for embedded microcontrollers, designing VLSI chips, designing analog sensors, designing mixed signal circuit boards, and designing operating systems. Computer engineers are also suited for robotics research, which relies heavily on using digital systems to control and monitor electrical systems like motors, communications, and sensors.
923) 18.966621, Computer engineering - Wikipedia, the free encyclopedia.txt#2, term: computer, content:In many institutions, computer engineering students are allowed to choose areas of in-depth study in their junior and senior year, because the full breadth of knowledge used in the design and application of computers is beyond the scope of an undergraduate degree. Other institutions may require engineering students to complete one year of General Engineering before declaring computer engineering as their primary focus.[3][4][5]
924) 18.966621, Computer graphics - Wikipedia, the free encyclopedia.txt#3, term: computer, content:Computer graphics is widespread today. Computer imagery is found on television, in newspapers, for example in weather reports, or for example in all kinds of medical investigation and surgical procedures. A well-constructed graph can present complex statistics in a form that is easier to understand and interpret. In the media "such graphs are used to illustrate papers, reports, thesis", and other presentation material.[2]
925) 18.966621, Computer graphics - Wikipedia, the free encyclopedia.txt#38, term: computer, content:2D computer graphics are mainly used in applications that were originally developed upon traditional printing and drawing technologies such as typography. In those applications, the two-dimensional image is not just a representation of a real-world object, but an independent artifact with added semantic value; two-dimensional models are therefore preferred, because they give more direct control of the image than 3D computer graphics, whose approach is more akin to photography than to typography.
926) 18.966621, Computer graphics - Wikipedia, the free encyclopedia.txt#45, term: computer, content:3D computer graphics are the same as 3D models. The model is contained within the graphical data file, apart from the rendering. However, there are differences that include the 3D model is the representation of any 3D object. Until visually displayed a model is not graphic. Due to printing, 3D models are not only confined to virtual space. 3D rendering is how a model can be displayed. Also can be used in non-graphical computer simulations and calculations.
927) 18.966621, Computer keyboard - Wikipedia, the free encyclopedia.txt#7, term: computer, content:The keyboard on the teleprinter played a strong role in point-to-point and point-to-multipoint communication for most of the 20th century, while the keyboard on the keypunch device played a strong role in data entry and storage for just as long. The development of the earliest computers incorporated electric typewriter keyboards: the development of the ENIAC computer incorporated a keypunch device as both the input and paper-based output device, while the BINAC computer also made use of an electromechanically controlled typewriter for both data entry onto magnetic tape (instead of paper) and data output.
928) 18.966621, Computer keyboard - Wikipedia, the free encyclopedia.txt#8, term: computer, content:From the 1940s until the late 1960s, typewriters were the main means of data entry and output for computing, becoming integrated into what were known as computer terminals. Because of the limitations of terminals based upon printed text in comparison to the growth in data storage, processing and transmission, a general move toward video-based computer terminals was effected by the 1970s, starting with the Datapoint 3300 in 1967.
929) 18.966621, Computer keyboard - Wikipedia, the free encyclopedia.txt#9, term: computer, content:The keyboard remained the primary, most integrated computer peripheral well into the era of personal computing until the introduction of the mouse as a consumer device in 1984. By this time, text-only user interfaces with sparse graphics gave way to comparatively graphics-rich icons on screen. However, keyboards remain central to human-computer interaction to the present, even as mobile personal computing devices such as smartphones and tablets adapt the keyboard as an optional virtual, touchscreen-based means of data entry.
930) 18.966621, Computer keyboard - Wikipedia, the free encyclopedia.txt#59, term: computer, content:When pressing a keyboard key, the key contacts may "bounce" against each other for several milliseconds before they settle into firm contact. When released, they bounce some more until they revert to the uncontacted state. If the computer were watching for each pulse, it would see many keystrokes for what the user thought was just one. To resolve this problem, the processor in a keyboard (or computer) "debounces" the keystrokes, by aggregating them across time to produce one "confirmed" keystroke.
931) 18.966621, Computer mouse - Wikipedia, the free encyclopedia.txt#56, term: computer, content:Some systems allow two or more mice to be used at once as input devices. 16-bit era home computers such as the Amiga used this to allow computer games with two players interacting on the same computer (Lemmings and The Settlers for example). The same idea is sometimes used in collaborative software, e.g. to simulate a whiteboard that multiple users can draw on without passing a single mouse around.
932) 18.966621, Computer mouse - Wikipedia, the free encyclopedia.txt#74, term: computer, content:Around 1981 Xerox included mice with its Xerox Star, based on the mouse used in the 1970s on the Alto computer at Xerox PARC. Sun Microsystems, Symbolics, Lisp Machines Inc., and Tektronix also shipped workstations with mice, starting in about 1981. Later, inspired by the Star, Apple Computer released the Apple Lisa, which also used a mouse. However, none of these products achieved large-scale success. Only with the release of the Apple Macintosh in 1984 did the mouse see widespread use.[70]
933) 18.966621, Computer multitasking - Wikipedia, the free encyclopedia.txt#5, term: computer, content:The first computer using a multiprogramming system was the British Leo III owned by J. Lyons and Co. During batch processing, several different programs were loaded in the computer memory, and the first one began to run. When the first program reached an instruction waiting for a peripheral, the context of this program was stored away, and the second program in memory was given a chance to run. The process continued until all programs finished running.[citation needed]
934) 18.966621, Computer music - Wikipedia, the free encyclopedia.txt#3, term: computer, content:The oldest known recordings of computer generated music were played by the Ferranti Mark 1 computer, a commercial version of the Baby Machine from the University of Manchester in the autumn of 1951. The music program was written by Christopher Strachey. During a session recorded by the BBC, the machine managed to work its way through "Baa Baa Black Sheep", "God Save the King" and part of "In the Mood".[2]
935) 18.966621, Computer music - Wikipedia, the free encyclopedia.txt#4, term: computer, content:Two further major 1950s developments were the origins of digital sound synthesis by computer, and of algorithmic composition programs beyond rote playback. Max Mathews at Bell Laboratories developed the influential MUSIC I program and its descendents, further popularising computer music through a 1963 article in Science.[3] Amongst other pioneers, the musical chemists Lejaren Hiller and Leonard Isaacson worked on a series of algorithmic composition experiments from 1956-9, manifested in the 1957 premiere of the Illiac Suite for string quartet.[4]
936) 18.966621, Computer music - Wikipedia, the free encyclopedia.txt#21, term: computer, content:Computers have also been used in an attempt to imitate the music of great composers of the past, such as Mozart. A present exponent of this technique is David Cope. He wrote computer programs that analyse works of other composers to produce new works in a similar style. He has used this program to great effect with composers such as Bach and Mozart (his program Experiments in Musical Intelligence is famous for creating "Mozart's 42nd Symphony"), and also within his own pieces, combining his own creations with that of the computer.[citation needed]
937) 18.966621, Computer music - Wikipedia, the free encyclopedia.txt#22, term: computer, content:Melomics, a research project from the University of Mlaga, Spain, developed a computer composition cluster named Iamus, which composes complex, multi-instrument pieces for editing and performance. Since its inception, Iamus has composed a full album in 2012, appropriately named Iamus, which New Scientist described as "The first major work composed by a computer and performed by a full orchestra."[12] The group has also developed an API for developers to utilize the technology, and makes its music available on its website.
938) 18.966621, Computer network - Wikipedia, the free encyclopedia.txt#7, term: computer, content:Computer communication links that do not support packets, such as traditional point-to-point telecommunication links, simply transmit data as a bit stream. However, most information in computer networks is carried in packets. A network packet is a formatted unit of data (a list of bits or bytes, usually a few tens of bytes to a few kilobytes long) carried by a packet-switched network.
939) 18.966621, Computer network - Wikipedia, the free encyclopedia.txt#53, term: computer, content:A personal area network (PAN) is a computer network used for communication among computer and different information technological devices close to one person. Some examples of devices that are used in a PAN are personal computers, printers, fax machines, telephones, PDAs, scanners, and even video game consoles. A PAN may include wired and wireless devices. The reach of a PAN typically extends to 10 meters.[17] A wired PAN is usually constructed with USB and FireWire connections while technologies such as Bluetooth and infrared communication typically form a wireless PAN.
940) 18.966621, Computer network - Wikipedia, the free encyclopedia.txt#94, term: computer, content:Network security consists of provisions and policies adopted by the network administrator to prevent and monitor unauthorized access, misuse, modification, or denial of the computer network and its network-accessible resources.[30] Network security is the authorization of access to data in a network, which is controlled by the network administrator. Users are assigned an ID and password that allows them access to information and programs within their authority. Network security is used on a variety of computer networks, both public and private, to secure daily transactions and communications among businesses, government agencies and individuals.
941) 18.966621, Computer program - Wikipedia, the free encyclopedia.txt#9, term: computer, content:The Manchester Small-Scale Experimental Machine (June 1948) was a stored-program computer.[12] Programming transitioned away from moving cables and setting dials; instead, a computer program was stored in memory as numbers. Only three bits of memory were available to store each instruction, so it was limited to eight instructions. 32 switches were available for programming.
942) 18.966621, Computer program - Wikipedia, the free encyclopedia.txt#10, term: computer, content:Computers manufactured until the 1970s had front-panel switches for programming. The computer program was written on paper for reference. An instruction was represented by a configuration of on/off settings. After setting the configuration, an execute button was pressed. This process was then repeated. Computer programs also were manually input via paper tape or punched cards. After the medium was loaded, the starting address was set via switches and the execute button pressed.[13]
943) 18.966621, Computer program - Wikipedia, the free encyclopedia.txt#27, term: computer, content:Many operating systems support multitasking which enables many computer programs to appear to run simultaneously on one computer. Operating systems may run multiple programs through process scheduling a software mechanism to switch the CPU among processes often so users can interact with each program while it runs.[21] Within hardware, modern day multiprocessor computers or computers with multicore processors may run multiple programs.[22]
944) 18.966621, Computer science - Wikipedia, the free encyclopedia.txt#11, term: computer, content:Computer science is considered by some to have a much closer relationship with mathematics than many scientific disciplines, with some observers saying that computing is a mathematical science.[10] Early computer science was strongly influenced by the work of mathematicians such as Kurt Gdel and Alan Turing, and there continues to be a useful interchange of ideas between the two fields in areas such as mathematical logic, category theory, domain theory, and algebra.[14]
945) 18.966621, Computer science - Wikipedia, the free encyclopedia.txt#33, term: computer, content:Software engineering is the study of designing, implementing, and modifying software in order to ensure it is of high quality, affordable, maintainable, and fast to build. It is a systematic approach to software design, involving the application of engineering practices to software. Software engineering deals with the organizing and analyzing of softwareit doesn't just deal with the creation or manufacture of new software, but its internal maintenance and arrangement. Both computer applications software engineers and computer systems software engineers are projected to be among the fastest growing occupations from 2008 to 2018.
946) 18.966621, Computer science - Wikipedia, the free encyclopedia.txt#35, term: computer, content:Conferences are important events for computer science research. During these conferences, researchers from the public and private sectors present their recent work and meet. Unlike in most other academic fields, in computer science, the prestige of conference papers is greater than that of journal publications.[49][50] One proposed explanation for this is the quick development of this relatively new field requires rapid review and distribution of results, a task better handled by conferences than by journals.[51]
947) 18.966621, Computer security - Wikipedia, the free encyclopedia.txt#44, term: computer, content:Social engineering and direct computer access (physical) attacks can only be prevented by non-computer means, which can be difficult to enforce, relative to the sensitivity of the information. Training is often involved to help mitigate this risk,[61][62] but even in a highly disciplined environments (e.g. military organizations), social engineering attacks can still be difficult to foresee and prevent.
948) 18.966621, Computer security - Wikipedia, the free encyclopedia.txt#50, term: computer, content:While hardware may be a source of insecurity, such as with microchip vulnerabilities maliciously introduced during the manufacturing process,[65][66] hardware-based or assisted computer security also offers an alternative to software-only computer security. Using devices and methods such as dongles, trusted platform modules, intrusion-aware cases, drive locks, disabling USB ports, and mobile-enabled access may be considered more secure due to the physical access (or sophisticated backdoor access) required in order to be compromised. Each of these is covered in more detail below.
949) 18.966621, Computer simulation - Wikipedia, the free encyclopedia.txt#22, term: computer, content:Computer graphics can be used to display the results of a computer simulation. Animations can be used to experience a simulation in real-time, e.g., in training simulations. In some cases animations may also be useful in faster than real-time or even slower than real-time modes. For example, faster than real-time animations can be useful in visualizing the buildup of queues in the simulation of humans evacuating a building. Furthermore, simulation results are often aggregated into static images using various ways of scientific visualization.
950) 18.966621, Computing - Wikipedia, the free encyclopedia.txt#16, term: computer, content:Application software, also known as an "application" or an "app", is a computer software designed to help the user to perform specific tasks. Examples include enterprise software, accounting software, office suites, graphics software and media players. Many application programs deal principally with documents. Apps may be bundled with the computer and its system software, or may be published separately. Some users are satisfied with the bundled apps and need never install one.
951) 18.966621, Control unit - Wikipedia, the free encyclopedia.txt#1, term: computer, content:It directs the operation of the other units by providing timing and control signals.[citation needed] Most computer resources are managed by the CU.[citation needed] It directs the flow of data between the CPU and the other devices. John von Neumann included the control unit as part of the von Neumann architecture.[2] In modern computer designs, the control unit is typically an internal part of the CPU with its overall role and operation unchanged since its introduction.[citation needed]
952) 18.966621, Conventional PCI - Wikipedia, the free encyclopedia.txt#0, term: computer, content:Conventional PCI, often shortened to PCI, is a local computer bus for attaching hardware devices in a computer. PCI is the initialism for Peripheral Component Interconnect[2] and is part of the PCI Local Bus standard. The PCI bus supports the functions found on a processor bus but in a standardized format that is independent of any particular processor's native bus. Devices connected to the PCI bus appear to a bus master to be connected directly to its own bus and are assigned addresses in the processor's address space.[3][pageneeded] It is a parallel bus, synchronous to a single bus clock.
953) 18.966621, Crash (computing) - Wikipedia, the free encyclopedia.txt#0, term: computer, content:A crash (or system crash) in computing is when a computer program (such as a software application or an operating system) stops functioning properly. Often it will exit the affected program after encountering this type of error. The program responsible may appear to freeze until a crash reporting service reports the crash and potentially any details relating to it. If the program is a critical part of the operating system, the entire computer may crash, often resulting in a kernel panic or fatal system error, or in rare cases, an unstable network.
954) 18.966621, Cray - Wikipedia, the free encyclopedia.txt#27, term: computer, content:By 2009, the largest computer system Cray had delivered was the XT5 system at National Center for Computational Sciences at Oak Ridge National Laboratories.[19] This system, with over 224,000 processing cores, was dubbed Jaguar and was the fastest computer in the world as measured by the LINPACK benchmark[20] at the speed of 1.75 petaflops[21] until being surpassed by the Tianhe-1A in October 2010. It was the first system to exceed a sustained performance of 1 petaflops on a 64-bit scientific application.
955) 18.966621, Data (computing) - Wikipedia, the free encyclopedia.txt#4, term: computer, content:A program is a set of data that consists of a series of coded software instructions to control the operation of a computer or other machine.[3] Physical computer memory elements consist of an address and a byte/word of data storage. Digital data are often stored in relational databases, like tables or SQL databases, and can generally be represented as abstract key/value pairs.
956) 18.966621, Data - Wikipedia, the free encyclopedia.txt#8, term: computer, content:Mechanical computing devices are classified according to the means by which they represent data. An analog computer represents a datum as a voltage, distance, position, or other physical quantity. A digital computer represents a piece of data as a sequence of symbols drawn from a fixed alphabet. The most common digital computers use a binary alphabet, that is, an alphabet of two characters, typically denoted "0" and "1". More familiar representations, such as numbers or letters, are then constructed from the binary alphabet.
957) 18.966621, Data - Wikipedia, the free encyclopedia.txt#9, term: computer, content:Some special forms of data are distinguished. A computer program is a collection of data, which can be interpreted as instructions. Most computer languages make a distinction between programs and the other data on which programs operate, but in some languages, notably Lisp and similar languages, programs are essentially indistinguishable from other data. It is also useful to distinguish metadata, that is, a description of other data. A similar yet earlier term for metadata is "ancillary data." The prototypical example of metadata is the library catalog, which is a description of the contents of books.
958) 18.966621, Desktop publishing - Wikipedia, the free encyclopedia.txt#1, term: computer, content:Desktop publishing combines a personal computer and WYSIWYG page layout software to create publication documents on a computer for either large scale publishing or small scale local multifunction peripheral output and distribution. Desktop publishing methods provide more control over design, layout, and typography than word processing. However, word processing software has evolved to include some, though by no means all, capabilities previously available only with professional printing or desktop publishing.
959) 18.966621, Digital Equipment Corporation - Wikipedia, the free encyclopedia.txt#57, term: computer, content:Although their microcomputer efforts were eventually considered failures, the PDP-11 and VAX lines continued to sell in record numbers. Better yet, DEC was competing very well against the market leader, IBM, taking an estimated $2billion away from them in the mid-80s. In 1986, Digital's profits rose 38% when the rest of the computer industry experienced a downturn, and by 1987 the company was threatening IBM's number one position in the computer industry.[7]
960) 18.966621, DNA computing - Wikipedia, the free encyclopedia.txt#7, term: computer, content:The slow processing speed of a DNA-computer (the response time is measured in minutes, hours or days, rather than milliseconds) is compensated by its potential to make a high amount of multiple parallel computations. This allows the system to take a similar amount of time for a complex calculation as for a simple one. This is achieved by the fact that millions or billions of molecules interact with each other simultaneously. However, it is much harder to analyze the answers given by a DNA-Computer than by a digital one.
961) 18.966621, DNA computing - Wikipedia, the free encyclopedia.txt#13, term: computer, content:DNA computing is a form of parallel computing in that it takes advantage of the many different molecules of DNA to try many different possibilities at once.[14] For certain specialized problems, DNA computers are faster and smaller than any other computer built so far. Furthermore, particular mathematical computations have been demonstrated to work on a DNA computer. As an example, DNA molecules have been utilized to tackle the assignment problem.[15]
962) 18.966621, E6B - Wikipedia, the free encyclopedia.txt#25, term: computer, content:The designation "E-6B" was officially marked on the device only for a couple of years. By 1943 the Army and Navy changed the marking to their joint standard, the AN-C-74 (Army/Navy Computer 74). A year or so later it was changed to AN-5835, and then to AN-5834 (1948). The USAF called later updates the MB-4 (1953) and the CPU-26 (1958), but navigators and most instruction manuals continued using the original E-6B name. Many just called it the "Dalton Dead Reckoning Computer", one of its original markings.
963) 18.966621, EDVAC - Wikipedia, the free encyclopedia.txt#10, term: computer, content:EDVAC was delivered to the Ballistics Research Laboratory in August 1949. After a number of problems had been discovered and solved, the computer began operation in 1951 although only on a limited basis. Its completion was delayed because of a dispute over patent rights between Eckert and Mauchly and the University of Pennsylvania, resulting in Eckert and Mauchly's resignation and departure to form the EckertMauchly Computer Corporation and taking most of the senior engineers with them.
964) 18.966621, Electronic delay storage automatic calculator - Wikipedia, the free encyclopedia.txt#0, term: computer, content:Electronic delay storage automatic calculator (EDSAC) was an early British computer.[1] Inspired by John von Neumann's seminal First Draft of a Report on the EDVAC, the machine was constructed by Maurice Wilkes and his team at the University of Cambridge Mathematical Laboratory in England. EDSAC was the second electronic digital stored-program computer to go into regular service.[2]
965) 18.966621, Embedded operating system - Wikipedia, the free encyclopedia.txt#0, term: computer, content:An embedded operating system is an operating system for embedded computer systems. These operating systems are designed to be compact, efficient at resource usage, and reliable, forsaking many functions that non-embedded computer operating systems provide, and which may not be used by the specialized applications they run. They are frequently also referred to as real-time operating systems, and the term RTOS is often used as a synonym for embedded operating system.
966) 18.966621, Execution (computing) - Wikipedia, the free encyclopedia.txt#3, term: computer, content:The context in which execution takes place is crucial. Very few programs execute on a bare machine. Programs usually contain implicit and explicit assumptions about resources available at the time of execution. Most programs execute with the support of an operating system and run-time libraries specific to the source language that provide crucial services not supplied directly by the computer itself. This supportive environment, for instance, usually decouples a program from direct manipulation of the computer peripherals, providing more general, abstract services instead.
967) 18.966621, Exploit (computer security) - Wikipedia, the free encyclopedia.txt#0, term: computer, content:An exploit (from the English verb to exploit, meaning "using something to ones own advantage") is a piece of software, a chunk of data, or a sequence of commands that takes advantage of a bug or vulnerability in order to cause unintended or unanticipated behavior to occur on computer software, hardware, or something electronic (usually computerized). Such behavior frequently includes things like gaining control of a computer system, allowing privilege escalation, or a denial-of-service attack.
968) 18.966621, Federico Faggin - Wikipedia, the free encyclopedia.txt#18, term: computer, content:The Communication CoSystem (1984). The Cosystem was conceived by F. Faggin and designed and produced by Cygnet Technologies, Inc., the second startup company of Faggin. Attached to a personal computer and to a standard phone line, the CoSystem could automatically handle all the personal voice and data communications of the user, including electronic mail, data-base access, computer screen transfers during a voice communication, call record keeping, etc. The patent covering the CoSystem[27] is highly cited in the personal communication field.
969) 18.966621, Ferranti Mercury - Wikipedia, the free encyclopedia.txt#0, term: computer, content:The Mercury was an early commercial computer from the mid 1950s built by Ferranti. It was the successor to the Ferranti Mark 1, adding a floating point unit for improved performance, and increased reliability by replacing the Williams tube memory with core memory and using more solid state components. The computer had roughly 2000 vacuum tubes (mostly type CV2179/A2134 pentodes, EL81 pentodes and CV2493/ECC88 double triodes) and 2000 germanium diodes. Nineteen Mercuries were sold before Ferranti moved on to newer designs.
970) 18.966621, Finite-state machine - Wikipedia, the free encyclopedia.txt#16, term: computer, content:In addition to their use in modeling reactive systems presented here, finite state automata are significant in many different areas, including electrical engineering, linguistics, computer science, philosophy, biology, mathematics, and logic. Finite state machines are a class of automata studied in automata theory and the theory of computation. In computer science, finite state machines are widely used in modeling of application behavior, design of hardware digital systems, software engineering, compilers, network protocols, and the study of computation and languages.
971) 18.966621, Floppy disk - Wikipedia, the free encyclopedia.txt#3, term: computer, content:While floppy disk drives still have some limited uses, especially with legacy industrial computer equipment, they have been superseded by data storage methods with much greater capacity, such as USB flash sticks, flash storage cards, portable external hard disk drives, optical discs, and storage available through computer networks.
972) 18.966621, FLOPS - Wikipedia, the free encyclopedia.txt#6, term: computer, content:In June 2006 a new computer was announced by Japanese research institute RIKEN, the MDGRAPE-3. The computer's performance tops out at one petaFLOPS, almost two times faster than the Blue Gene/L, but MDGRAPE-3 is not a general purpose computer, which is why it does not appear in the Top500.org list. It has special-purpose pipelines for simulating molecular dynamics.
973) 18.966621, Fortran - Wikipedia, the free encyclopedia.txt#8, term: computer, content:By 1960, versions of FORTRAN were available for the IBM 709, 650, 1620, and 7090 computers. Significantly, the increasing popularity of FORTRAN spurred competing computer manufacturers to provide FORTRAN compilers for their machines, so that by 1963 over 40 FORTRAN compilers existed. For these reasons, FORTRAN is considered to be the first widely used programming language supported across a variety of computer architectures.
974) 18.966621, Graphical user interface - Wikipedia, the free encyclopedia.txt#0, term: computer, content:In computer science, a graphical user interface (GUI), is a type of user interface that allows users to interact with electronic devices through graphical icons and visual indicators such as secondary notation, instead of text-based user interfaces, typed command labels or text navigation. GUIs were introduced in reaction to the perceived steep learning curve of command-line interfaces (CLIs),[1][2][3] which require commands to be typed on a computer keyboard.
975) 18.966621, Graphics tablet - Wikipedia, the free encyclopedia.txt#8, term: computer, content:The first home computer graphics tablet was the KoalaPad. Though originally designed for the Apple II, the Koala eventually broadened its applicability to practically all home computers with graphics support, examples of which include the TRS-80 Color Computer, Commodore 64, and Atari 8-bit family. Competing tablets were eventually produced; the tablets produced by Atari were generally considered to be of high quality.
976) 18.966621, Graphics tablet - Wikipedia, the free encyclopedia.txt#21, term: computer, content:Graphics tablets, because of their stylus-based interface and ability to detect some or all of pressure, tilt, and other attributes of the stylus and its interaction with the tablet, are widely considered to offer a very natural way to create computer graphics, especially two-dimensional computer graphics. Indeed, many graphics packages can make use of the pressure (and, sometimes, stylus tilt or rotation) information generated by a tablet, by modifying the brush size, shape, opacity, color, or other attributes based on data received from the graphics tablet.
977) 18.966621, Graphics tablet - Wikipedia, the free encyclopedia.txt#22, term: computer, content:In East Asia, graphics tablets, known as "pen tablets", are widely used in conjunction with input-method editor software (IMEs) to write Chinese, Japanese, Korean characters (CJK). The technology is popular and inexpensive and offers a method for interacting with the computer in a more natural way than typing on the keyboard, with the pen tablet supplanting the role of the computer mouse. Uptake of handwriting recognition among users who use alphabetic scripts has been slower.
978) 18.966621, Hans Meuer - Wikipedia, the free encyclopedia.txt#1, term: computer, content:Hans Meuer[2] served as specialist, project leader, group and department chief during his 11 years at the Jlich Research Centre, Germany. For the following 33 years, he was director of the computer center and professor for computer science at the University of Mannheim, Germany. Since 1998 - 2013, he was the managing director of Prometeus GmbH, a company that runs a series of conferences[3] in fields closely associated with high performance computing.
979) 18.966621, Harvard architecture - Wikipedia, the free encyclopedia.txt#0, term: computer, content:The Harvard architecture is a computer architecture with physically separate storage and signal pathways for instructions and data. The term originated from the Harvard Mark I relay-based computer, which stored instructions on punched tape (24 bits wide) and data in electro-mechanical counters. These early machines had data storage entirely contained within the central processing unit, and provided no access to the instruction storage as data. Programs needed to be loaded by an operator; the processor could not initialize itself.
980) 18.966621, Harwell CADET - Wikipedia, the free encyclopedia.txt#1, term: computer, content:The electronics division of the Atomic Energy Research Establishment at Harwell, UK built the Harwell Dekatron Computer in 1951,[1] which was an automatic calculator where the decimal arithmetic and memory were electronic, although other functions were performed by relays. By 1953, it was evident that this did not meet AERE's computing needs, and AERE director Sir John Cockcroft encouraged them to design and build a computer using transistors throughout.
981) 18.966621, High-level programming language - Wikipedia, the free encyclopedia.txt#0, term: computer, content:In computer science, a high-level programming language is a programming language with strong abstraction from the details of the computer. In comparison to low-level programming languages, it may use natural language elements, be easier to use, or may automate (or even hide entirely) significant areas of computing systems (e.g. memory management), making the process of developing a program simpler and more understandable relative to a lower-level language. The amount of abstraction provided defines how "high-level" a programming language is.[1]
982) 18.966621, History of computing hardware - Wikipedia, the free encyclopedia.txt#53, term: computer, content:The US-built ENIAC (Electronic Numerical Integrator and Computer) was the first electronic programmable computer built in the US. Although the ENIAC was similar to the Colossus it was much faster and more flexible. It was unambiguously a Turing-complete device and could compute any problem that would fit into its memory. Like the Colossus, a "program" on the ENIAC was defined by the states of its patch cables and switches, a far cry from the stored program electronic machines that came later. Once a program was written, it had to be mechanically set into the machine with manual resetting of plugs and switches.
983) 18.966621, History of computing hardware - Wikipedia, the free encyclopedia.txt#66, term: computer, content:The other contender for being the first recognizably modern digital stored-program computer[84] was the EDSAC,[85] designed and constructed by Maurice Wilkes and his team at the University of Cambridge Mathematical Laboratory in England at the University of Cambridge in 1949. The machine was inspired by John von Neumann's seminal First Draft of a Report on the EDVAC and was one of the first usefully operational electronic digital stored-program computer.[86]
984) 18.966621, History of computing hardware - Wikipedia, the free encyclopedia.txt#72, term: computer, content:In June 1951, the UNIVAC I (Universal Automatic Computer) was delivered to the U.S. Census Bureau. Remington Rand eventually sold 46 machines at more than US$1 million each ($9.12million as of 2016).[94] UNIVAC was the first "mass produced" computer. It used 5,200 vacuum tubes and consumed 125kW of power. Its primary storage was serial-access mercury delay lines capable of storing 1,000 words of 11 decimal digits plus sign (72-bit words).
985) 18.966621, History of computing hardware - Wikipedia, the free encyclopedia.txt#95, term: computer, content:While the earliest microprocessor ICs literally contained only the processor, i.e. the central processing unit, of a computer, their progressive development naturally led to chips containing most or all of the internal electronic parts of a computer. The integrated circuit in the image on the right, for example, an Intel 8742, is an 8-bit microcontroller that includes a CPU running at 12MHz, 128 bytes of RAM, 2048 bytes of EPROM, and I/O in the same chip.
986) 18.966621, Home computer - Wikipedia, the free encyclopedia.txt#2, term: computer, content:Advertisements in the popular press for early home computers were rife with possibilities for their practical use in the home, from cataloging recipes to personal finance to home automation,[2][3][4] but these were seldom realized in practice. For example, using a typical 1980s home computer as a home automation appliance would require the computer to be kept powered on at all times and dedicated to this task. Personal finance and database use required tedious data entry.
987) 18.966621, Home computer - Wikipedia, the free encyclopedia.txt#11, term: computer, content:Still, home computers competed in the same market as the consoles. A home computer was often seen as simply as a higher end purchase than a console, adding abilities to what would still be mainly a gaming device. A common marketing tactic was to show a computer system and console playing games side by side, then emphasising the computer's greater ability by showing it running user-created programs, education software, word processing, spreadsheet and other applications while the game console showed a blank screen or continued playing the same repetitive game.
988) 18.966621, Home computer - Wikipedia, the free encyclopedia.txt#22, term: computer, content:This "peripherals sold separately" approach is another defining characteristic of the home computer era. A first time computer buyer who brought a base C-64 system home and hooked it up to their TV would find they needed to buy a disk drive (the Commodore 1541 was the only fully compatible model) or Datasette before they could make use of it as anything but a game machine or TV Typewriter.
989) 18.966621, Honeywell, Inc. v. Sperry Rand Corp. - Wikipedia, the free encyclopedia.txt#0, term: computer, content:Honeywell, Inc. v. Sperry Rand Corp., et al. 180 USPQ 673 (D. Minn. 1973) (Case 4-67 Civil 138, 180 USPO 670) was a landmark U.S. federal court case that in October 1973 invalidated the 1964 patent for the ENIAC, the world's first general-purpose electronic digital computer, thus putting the invention of the electronic digital computer into the public domain.
990) 18.966621, Honeywell, Inc. v. Sperry Rand Corp. - Wikipedia, the free encyclopedia.txt#14, term: computer, content:Following the ruling, some writers perceived recognition of Atanasoff for his title as "father of the computer" was slow in coming, and wrote books of their own. These included Pulitzer Prize-winning Iowan reporter Clark R. Mollenhoff and wife-and-husband team Alice Burks and Arthur Burks. (Arthur had been on the ENIAC's engineering staff and had requested to be added as a co-inventor following the issuance of the ENIAC patent; Alice Burks had been a computer at the Moore School.)
991) 18.966621, HP 2100 - Wikipedia, the free encyclopedia.txt#4, term: computer, content:The HP 2116As software, with a FORTRAN compiler, assembler, linker, loader, operating system, and I/O drivers were ready at the same time as the hardware. This was quite unusual, at a time when most computer vendors would roll out the hardware first with little software. The 1967 issue of the Hewlett-Packard Journal called the HP 2116A "an unusual new instrumentation computer".
992) 18.966621, Human computer - Wikipedia, the free encyclopedia.txt#0, term: computer, content:The term "computer", in use from the early 17th century (the first known written reference dates from 1613),[1] meant "one who computes": a person performing mathematical calculations, before electronic computers became commercially available. "The human computer is supposed to be following fixed rules; he has no authority to deviate from them in any detail." (Turing, 1950) Teams of people were frequently used to undertake long and often tedious calculations; the work was divided so that this could be done in parallel.
993) 18.966621, Human–computer interaction - Wikipedia, the free encyclopedia.txt#8, term: computer, content:Visions of what researchers in the field seek to achieve vary. When pursuing a cognitivist perspective, researchers of HCI may seek to align computer interfaces with the mental model that humans have of their activities. When pursuing a post-cognitivist perspective, researchers of HCI may, e.g., seek to align computer interfaces with existing social practices or existing sociocultural values.
994) 18.966621, IBM PC compatible - Wikipedia, the free encyclopedia.txt#7, term: computer, content:Like IBM, Microsoft's intention was that application writers would write to the application programming interfaces in MS-DOS or the firmware BIOS, and that this would form what would now be termed a hardware abstraction layer. Each computer would have its own Original Equipment Manufacturer (OEM) version of MS-DOS, customized to its hardware. Any software written for MS-DOS would operate on any MS-DOS computer, despite variations in hardware design.
995) 18.966621, IBM PC DOS - Wikipedia, the free encyclopedia.txt#0, term: computer, content:IBM PC DOS (Acronym for International Business Machines Corporation Personal Computer Disk Operating System) was an operating system for the IBM Personal Computer, manufactured and sold by IBM from the 1980s to the 2000s. Before version 6.1, PC DOS was an IBM-branded version of MS-DOS. For versions 6.1 and later, development diverged and PC DOS became an independent product.
996) 18.966621, Image scanner - Wikipedia, the free encyclopedia.txt#22, term: computer, content:Image scanners are usually used in conjunction with a computer which controls the scanner and stores scans. Small portable scanners, either roller-fed or "glide-over" hand-operated, operated by batteries and with storage capability, are available for use away from a computer; stored scans can be transferred later. Many can scan both small documents such as business cards and till receipts, and letter-sized documents.
997) 18.966621, Information technology - Wikipedia, the free encyclopedia.txt#6, term: computer, content:The development of transistors in the late 1940s at Bell Laboratories allowed a new generation of computers to be designed with greatly reduced power consumption. The first commercially available stored-program computer, the Ferranti Mark I, contained 4050 valves and had a power consumption of 25 kilowatts. By comparison the first transistorised computer, developed at the University of Manchester and operational by November 1953, consumed only 150 watts in its final version.[14]
998) 18.966621, Information technology - Wikipedia, the free encyclopedia.txt#18, term: computer, content:In an academic context, the Association for Computing Machinery defines IT as "undergraduate degree programs that prepare students to meet the computer technology needs of business, government, healthcare, schools, and other kinds of organizations.... IT specialists assume responsibility for selecting hardware and software products appropriate for an organization, integrating those products with organizational needs and infrastructure, and installing, customizing, and maintaining those applications for the organizations computer users."[39]
999) 18.966621, Installation (computer programs) - Wikipedia, the free encyclopedia.txt#0, term: computer, content:Installation (or setup) of a computer program (including device drivers and plugins), is the act of making the program ready for execution. Because the process varies for each program and each computer, programs (including operating systems) often come with an installer, a specialized program responsible for doing whatever is needed for their installation. Installation may be part of a larger software deployment process.
1000) 18.966621, Installation (computer programs) - Wikipedia, the free encyclopedia.txt#11, term: computer, content:Installation performed without using a computer monitor connected. In attended forms of headless installation, another machine connects to the target machine (for instance, via a local area network) and takes over the display output. Since a headless installation does not need a user at the location of the target computer, unattended headless installers may be used to install a program on multiple machines at the same time.
1001) 18.966621, Installation (computer programs) - Wikipedia, the free encyclopedia.txt#12, term: computer, content:An installation process that runs on a preset time or when a predefined condition transpires, as opposed to an installation process that starts explicitly on a user's command. For instance, a system administrator willing to install a later version of a computer program that is being used can schedule that installation to occur when that program is not running. An operating system may automatically install a device driver for a device that the user connects. (See plug and play.) Malware may also be installed automatically. For example, the infamous Conficker was installed when the user plugged an infected device to their computer.
1002) 18.966621, Instruction set - Wikipedia, the free encyclopedia.txt#5, term: computer, content:A complex instruction set computer (CISC) has many specialized instructions, some of which may only be rarely used in practical programs. A reduced instruction set computer (RISC) simplifies the processor by efficiently implementing only the instructions that are frequently used in programs, while the less common operations are implemented as subroutines, having their resulting additional processor execution time offset by infrequent use.[2]
1003) 18.966621, Intel - Wikipedia, the free encyclopedia.txt#129, term: computer, content:In November 2009, following a two-year investigation, New York Attorney General Andrew Cuomo sued Intel, accusing them of bribery and coercion, claiming that Intel bribed computer makers to buy more of their chips than those of their rivals, and threatened to withdraw these payments if the computer makers were perceived as working too closely with its competitors. Intel has denied these claims.[273]
1004) 18.966621, Intel 80386 - Wikipedia, the free encyclopedia.txt#2, term: computer, content:The 80386 was introduced in October 1985, while manufacturing of the chips in significant quantities commenced in June 1986.[6][7] Mainboards for 80386-based computer systems were cumbersome and expensive at first, but manufacturing was rationalized upon the 80386's mainstream adoption. The first personal computer to make use of the 80386 was designed and manufactured by Compaq[8] and marked the first time a fundamental component in the IBM PC compatible de facto-standard was updated by a company other than IBM.
1005) 18.966621, Intel 8080 - Wikipedia, the free encyclopedia.txt#2, term: computer, content:Although earlier microprocessors were used for calculators, cash registers, computer terminals, industrial robots,[3] and other applications, the 8080 became one of the first really widespread microprocessors. This was partly due to its simplifying NMOS implementation (making it TTL compatible), but also to its enhanced instruction set (over the 8008[4]) and its subsequent role as the original target CPU for CP/M,[5] the first de facto standard personal computer operating system.[6]
1006) 18.966621, Interactive fiction - Wikipedia, the free encyclopedia.txt#59, term: computer, content:Until recently playing an IF title meant not only downloading a game file but also downloading an interpreter in order to read game files. This has the advantage that a story file is able to run on any number of different types of computer and as the story file is isolated from the host computer it is not possible to alter or damage any of the hosts other files. However, whilst this relativity easy method of acquisition, the disadvantage is that the story file can only be used with an interpreter.
1007) 18.966621, Internet - Wikipedia, the free encyclopedia.txt#97, term: computer, content:Internet resources, hardware, and software components are the target of malicious attempts to gain unauthorized control to cause interruptions or access private information. Such attempts include computer viruses which copy with the help of humans, computer worms which copy themselves automatically, denial of service attacks, ransomware, botnets, and spyware that reports on the activity and typing of users. Usually, these activities constitute cybercrime. Defense theorists have also speculated about the possibilities of cyber warfare using similar methods on a large scale.[citation needed]
1008) 18.966621, Internet protocol suite - Wikipedia, the free encyclopedia.txt#0, term: computer, content:The Internet protocol suite is the computer networking model and set of communications protocols used on the Internet and similar computer networks. It is commonly known as TCP/IP, because its most important protocols, the Transmission Control Protocol (TCP) and the Internet Protocol (IP) were the first networking protocols defined during its development. It is occasionally known as the Department of Defense (DoD) model, because the development of the networking model was funded by DARPA, an agency of the United States Department of Defense.
1009) 18.966621, Internet protocol suite - Wikipedia, the free encyclopedia.txt#14, term: computer, content:In March 1982, the US Department of Defense declared TCP/IP as the standard for all military computer networking.[13] In 1985, the Internet Advisory Board (later renamed the Internet Architecture Board) held a three-day workshop on TCP/IP for the computer industry, attended by 250 vendor representatives, promoting the protocol and leading to its increasing commercial use.
1010) 18.966621, John Mauchly - Wikipedia, the free encyclopedia.txt#12, term: computer, content:The ENIAC design was frozen in 1944 to allow construction. Eckert and Mauchly were already aware of the limitations of the machine and began plans on a second computer, to be called EDVAC. By January 1945 they had procured a contract to build this stored-program computer. Eckert had proposed a mercury delay line memory to store both program and data. Later that year, mathematician John von Neumann learned of the project and joined in some of the engineering discussions. He produced what was understood to be an internal document describing the EDVAC.
1011) 18.966621, John Mauchly - Wikipedia, the free encyclopedia.txt#15, term: computer, content:In March 1946, just after the ENIAC was announced, the Moore School decided to change their patent policy, in order to gain commercial rights to any future and past computer development there. Eckert and Mauchly decided this was unacceptable; they resigned. However they had already been contracted to do one more thing at the Moore School: to give a series of talks on computer design.
1012) 18.966621, John von Neumann - Wikipedia, the free encyclopedia.txt#1, term: computer, content:He was a pioneer of the application of operator theory to quantum mechanics, in the development of functional analysis, and a key figure in the development of game theory and the concepts of cellular automata, the universal constructor and the digital computer. He published 150 papers in his life: 60 in pure mathematics, 20 in physics, and 60 in applied mathematics. His last work, an unfinished manuscript written while in the hospital, was later published in book form as The Computer and the Brain.
1013) 18.966621, Konrad Zuse - Wikipedia, the free encyclopedia.txt#29, term: computer, content:The 100th anniversary of the birth of this computer pioneer was celebrated by exhibitions, lectures and workshops to remember his life and work and to bring attention to the importance of his invention to the digital age.[40][41] The movie Tron: Legacy, which revolves around a world inside a computer system, features a character named Zuse, presumably in honour of Konrad Zuse.[citation needed] German posts DP AG issued a commemorative stamp at this occasion, June 6, 2010: a Zuse portrait, composed solely by the binary code numbers 1 and 0 in fine print.
1014) 18.966621, Library (computing) - Wikipedia, the free encyclopedia.txt#34, term: computer, content:At the same time many developers worked on the idea of multi-tier programs, in which a "display" running on a desktop computer would use the services of a mainframe or minicomputer for data storage or processing. For instance, a program on a GUI-based computer would send messages to a minicomputer to return small samples of a huge dataset for display. Remote procedure calls already handled these tasks, but there was no standard RPC system.
1015) 18.966621, Lisp (programming language) - Wikipedia, the free encyclopedia.txt#1, term: computer, content:Lisp was originally created as a practical mathematical notation for computer programs, influenced by the notation of Alonzo Church's lambda calculus. It quickly became the favored programming language for artificial intelligence (AI) research. As one of the earliest programming languages, Lisp pioneered many ideas in computer science, including tree data structures, automatic storage management, dynamic typing, conditionals, higher-order functions, recursion, and the self-hosting compiler.[4]
1016) 18.966621, List of BSD operating systems - Wikipedia, the free encyclopedia.txt#2, term: computer, content:NetBSD is a freely redistributable, open source version of the Unix-derivative Berkeley Software Distribution (BSD) computer operating system. It was the second open source BSD descendant to be formally released, after 386BSD, and continues to be actively developed. Noted for its portability and quality of design and implementation, it is often used in embedded systems and as a starting point for the porting of other operating systems to new computer architectures.
1017) 18.966621, Mac OS - Wikipedia, the free encyclopedia.txt#41, term: computer, content:Announced at The 1996 World Wide Developers Conference (WWDC), MkLinux is an open source computer operating system started by the OSF Research Institute and Apple Computer in February 1996 to port Linux to the PowerPC platform, and thus Macintosh computers. In the summer of 1998, the community-led MkLinux Developers Association took over development of the operating system. MkLinux is short for "Microkernel Linux," which refers to the project's adaptation of the Linux kernel to run as a server hosted atop the Mach microkernel. MkLinux is based on version 3.0 of Mach.
1018) 18.966621, Machine code - Wikipedia, the free encyclopedia.txt#13, term: computer, content:In some computer architectures, the machine code is implemented by a more fundamental underlying layer of programs called microprograms, providing a common machine language interface across a line or family of different models of computer with widely different underlying dataflows. This is done to facilitate porting of machine language programs between different models. An example of this use is the IBM System/360 family of computers and their successors. With dataflow path widths of 8 bits to 64 bits and beyond, they nevertheless present a common architecture at the machine language level across the entire line.
1019) 18.966621, Machine code - Wikipedia, the free encyclopedia.txt#16, term: computer, content:The Harvard architecture is a computer architecture with physically separate storage and signal pathways for the code (instructions) and data. Today, most processors implement such separate signal pathways for performance reasons but actually implement a Modified Harvard architecture,[citation needed] so they can support tasks like loading an executable program from disk storage as data and then executing it. Harvard architecture is contrasted to the Von Neumann architecture, where data and code are stored in the same memory which is read by the processor allowing the computer to execute commands.
1020) 18.966621, Manchester Small-Scale Experimental Machine - Wikipedia, the free encyclopedia.txt#24, term: computer, content:Williams and Kilburn reported on the SSEM in a letter to the Journal Nature, published in September 1948.[31] The machine's successful demonstration quickly led to the construction of a more practical computer, the Manchester Mark 1, work on which began in August 1948. The first version was operational by April 1949,[30] and it in turn led directly to the development of the Ferranti Mark 1, the world's first commercially available general-purpose computer.[3]
1021) 18.966621, Max Newman - Wikipedia, the free encyclopedia.txt#0, term: computer, content:Maxwell Herman Alexander "Max" Newman, FRS[5] (7 February 1897  22 February 1984) was a British mathematician and codebreaker. His work in World War II led to the construction of Colossus,[6] the world's first operational, programmable electronic computer, and he established the Royal Society Computing Machine Laboratory at the University of Manchester which produced the world's first working, modern stored program electronic computer in 1948, the Manchester Small-Scale Experimental Machine.[7][8][9][10][11]
1022) 18.966621, Max Newman - Wikipedia, the free encyclopedia.txt#23, term: computer, content:After the Automatic Computing Engine suffered delays and set backs, Turing accepted Newman's offer and joined the Computer Machine Laboratory in May 1948 as Deputy Director (there being no Director). Turing joined Kilburn and Williams to work on Baby's successor, the Manchester Mark I. Collaboration between the University and Ferranti later produced the Ferranti Mark I, the first mass-produced computer to go on sale.[18]
1023) 18.966621, Microprocessor - Wikipedia, the free encyclopedia.txt#41, term: computer, content:A low overall cost, small packaging, simple computer bus requirements, and sometimes the integration of extra circuitry (e.g. the Z80's built-in memory refresh circuitry) allowed the home computer "revolution" to accelerate sharply in the early 1980s. This delivered such inexpensive machines as the Sinclair ZX-81, which sold for US$99 (equivalent to $257.68 in 2015). A variation of the 6502, the MOS Technology 6510 was used in the Commodore 64 and yet another variant, the 8502, powered the Commodore 128.
1024) 18.966621, Minicomputer - Wikipedia, the free encyclopedia.txt#13, term: computer, content:A variety of companies emerged that built turnkey systems around minicomputers with specialized software and, in many cases, custom peripherals that addressed specialized problems such as computer aided design, computer aided manufacturing, process control, manufacturing resource planning, and so on. Many if not most minicomputers were sold through these original equipment manufacturers and value-added resellers.
1025) 18.966621, Non-uniform memory access - Wikipedia, the free encyclopedia.txt#1, term: computer, content:NUMA architectures logically follow in scaling from symmetric multiprocessing (SMP) architectures. They were developed commercially during the 1990s by Burroughs (later Unisys), Convex Computer (later Hewlett-Packard), Honeywell Information Systems Italy (HISI) (later Groupe Bull), Silicon Graphics (later Silicon Graphics International), Sequent Computer Systems (later IBM), Data General (later EMC), and Digital (later Compaq, now HP). Techniques developed by these companies later featured in a variety of Unix-like operating systems, and to an extent in Windows NT.
1026) 18.966621, Operating system - Wikipedia, the free encyclopedia.txt#2, term: computer, content:For hardware functions such as input and output and memory allocation, the operating system acts as an intermediary between programs and the computer hardware,[1][2] although the application code is usually executed directly by the hardware and frequently makes system calls to an OS function or is interrupted by it. Operating systems are found on many devices that contain a computer  from cellular phones and video game consoles to web servers and supercomputers.
1027) 18.966621, Operating system - Wikipedia, the free encyclopedia.txt#58, term: computer, content:When a computer first starts up, it is automatically running in supervisor mode. The first few programs to run on the computer, being the BIOS or EFI, bootloader, and the operating system have unlimited access to hardware  and this is required because, by definition, initializing a protected environment can only be done outside of one. However, when the operating system passes control to another program, it can place the CPU into protected mode.
1028) 18.966621, PDP-8 - Wikipedia, the free encyclopedia.txt#7, term: computer, content:The PDP-8 combined low cost, simplicity, expandability, and careful engineering for value. The greatest historical significance was that the PDP-8's low cost and high volume made a computer available to many new people for many new uses. Its continuing significance is as a historical example of value-engineered computer design.[opinion][clarification needed]
1029) 18.966621, Peripheral - Wikipedia, the free encyclopedia.txt#1, term: computer, content:There are three different types of peripherals: input devices, which interact with or send data from the user to the computer (mice, keyboards, etc.), output devices, which provide output to the user from the computer (monitors, printers, etc.), and input/output devices that perform both functions. Touchscreens are an example that combines different devices into a single hardware component that can be used both as an input and output device.
1030) 18.966621, Peripheral - Wikipedia, the free encyclopedia.txt#2, term: computer, content:A peripheral device is generally defined as any auxiliary device such as a computer mouse or keyboard that connects to and works with the computer in some way. Other examples of peripherals are image scanners, tape drives, microphones, loudspeakers, webcams, and digital cameras. Many modern devices, such as digital watches, smartphones and tablet computers, have interfaces that allow them to be used as a peripheral by desktop computers, although they are not host-dependent in the same way as other peripheral devices.
1031) 18.966621, Personal computer - Wikipedia, the free encyclopedia.txt#16, term: computer, content:In the same year, the NEC PC-98 was introduced, which was a very popular personal computer that sold in more than 18 million units.[13] Another famous personal computer, the revolutionary Amiga 1000, was unveiled by Commodore on July 23, 1985. The Amiga 1000 featured a multitasking, windowing operating system, color graphics with a 4096-color palette, stereo sound, Motorola 68000 CPU, 256kB RAM, and 880kB 3.5-inch disk drive, for US$1,295.[14]
1032) 18.966621, Personal computer - Wikipedia, the free encyclopedia.txt#36, term: computer, content:A gaming computer is a standard desktop computer that typically has high-performance hardware, such as a more powerful video card, processor and memory, in order to handle the requirements of demanding video games, which are often simply called "PC games".[55] A number of companies, such as Alienware, manufacture prebuilt gaming computers, and companies such as Razer and Logitech market mice, keyboards and headsets geared toward gamers.
1033) 18.966621, Personal computer - Wikipedia, the free encyclopedia.txt#64, term: computer, content:The motherboard, also referred to as system board or main board, is the primary circuit board within a personal computer, and other major system components plug directly into it or via a cable. A motherboard contains a microprocessor, the CPU supporting circuitry (mostly integrated circuits) that provide the interface between memory and input/output peripheral circuits, main memory, and facilities for initial setup of the computer immediately after power-on (often called boot firmware or, in IBM PC compatible computers, a BIOS or UEFI).
1034) 18.966621, Personal computer - Wikipedia, the free encyclopedia.txt#80, term: computer, content:Mice traditionally detected movement and communicated with the computer with an internal "mouse ball", and used optical encoders to detect rotation of the ball and tell the computer where the mouse has moved. However, these systems were subject to low durability, accuracy and required internal cleaning. Modern mice use optical technology to directly trace movement of the surface under the mouse and are much more accurate, durable and almost maintenance free. They work on a wider variety of surfaces and can even operate on walls, ceilings or other non-horizontal surfaces.
1035) 18.966621, Personal computer - Wikipedia, the free encyclopedia.txt#91, term: computer, content:An operating system (OS) manages computer resources and provides programmers with an interface used to access those resources. An operating system processes system data and user input, and responds by allocating and managing tasks and internal system resources as a service to users and programs of the system. An operating system performs basic tasks such as controlling and allocating memory, prioritizing system requests, controlling input and output devices, facilitating computer networking, and managing files.
1036) 18.966621, Personal computer - Wikipedia, the free encyclopedia.txt#104, term: computer, content:While daily end-users are not exposed to these toxic elements, the danger arises during the computer recycling process, which involves manually breaking down hardware and leads to the exposure of a measurable amount of lead or mercury. A measurable amount of lead or mercury can easily cause serious brain damage or ruin drinking water supplies. Computer recycling is best handled by the electronic waste (e-waste) industry, and kept segregated from the general community dump.
1037) 18.966621, Printer (computing) - Wikipedia, the free encyclopedia.txt#21, term: computer, content:Several different computer printers were simply computer-controllable versions of existing electric typewriters. The Friden Flexowriter and IBM Selectric-based printers were the most-common examples. The Flexowriter printed with a conventional typebar mechanism while the Selectric used IBM's well-known "golf ball" printing mechanism. In either case, the letter form then struck a ribbon which was pressed against the paper, printing one character at a time. The maximum speed of the Selectric printer (the faster of the two) was 15.5 characters per second.
1038) 18.966621, Processor design - Wikipedia, the free encyclopedia.txt#0, term: computer, content:Processor design is the design engineering task of creating a microprocessor, a component of computer hardware. It is a subfield of electronics engineering and computer engineering. The design process involves choosing an instruction set and a certain execution paradigm (e.g. VLIW or RISC) and results in a microarchitecture described in e.g. VHDL or Verilog. This description is then manufactured employing some of the various semiconductor device fabrication processes. This results in a die which is bonded onto some chip carrier. This chip carrier is then soldered onto some printed circuit board (PCB).
1039) 18.966621, Programmer - Wikipedia, the free encyclopedia.txt#16, term: computer, content:According to BBC News, 17% of computer science students could not find work in their field 6 months after graduation in 2009 which was the highest rate of the university subjects surveyed while 0% of medical students were unemployed in the same survey.[12] The UK category system does, however, class such degrees as information technology and game design as 'computer science', industries in which jobs can be extremely difficult to find, somewhat inflating the actual figure.[13]
1040) 18.966621, Quantum computing - Wikipedia, the free encyclopedia.txt#44, term: computer, content:In April 2012 a multinational team of researchers from the University of Southern California, Delft University of Technology, the Iowa State University of Science and Technology, and the University of California, Santa Barbara, constructed a two-qubit quantum computer on a doped diamond crystal that can easily be scaled up and is functional at room temperature. Two logical qubit directions of electron spin and nitrogen kernels spin were used, with microwave impulses. This computer ran Grover's algorithm generating the right answer from the first try in 95% of cases.[66]
1041) 18.966621, Serious game - Wikipedia, the free encyclopedia.txt#2, term: computer, content:The term "serious game" has been used long before the introduction of computer and electronic devices into entertainment. Clark Abt discussed the idea and used the term in his 1970 book Serious Games,[5] published by Viking Press. In that book, his references were primarily to the use of board and card games. But he gave a useful general definition which is still considered applicable in the computer age:
1042) 18.966621, Software bug - Wikipedia, the free encyclopedia.txt#15, term: computer, content:More complex bugs may arise from unintended interactions between different parts of a computer program. This frequently occurs because computer programs may be complex millions of lines long in some cases often having been programmed by many people over a great length of time, so that programmers are unable to mentally track every possible way in which parts may interact.
1043) 18.966621, Software engineering - Wikipedia, the free encyclopedia.txt#16, term: computer, content:In 2004, the U. S. Bureau of Labor Statistics counted 760,840 software engineers holding jobs in the U.S.; in the same time period there were some 1.4 million practitioners employed in the U.S. in all other engineering disciplines combined.[32] Due to its relative newness as a field of study, formal education in software engineering is often taught as part of a computer science curriculum, and many software engineers hold computer science degrees and have no engineering background whatsoever.[33]
1044) 18.966621, Software synthesizer - Wikipedia, the free encyclopedia.txt#0, term: computer, content:A software synthesizer, also known as a softsynth, is a computer program, or plug-in that generates digital audio, usually for music. Computer software that can create sounds or music is not new, but advances in processing speed are allowing softsynths to accomplish the same tasks that previously required dedicated hardware. Softsynths are usually cheaper and more portable than dedicated hardware, and easier to interface with other music software such as music sequencers.
1045) 18.966621, Sound card - Wikipedia, the free encyclopedia.txt#48, term: computer, content:USB sound "cards" are external devices that plug into the computer via USB. They are often used in studios and on stage by electronic musicians including live PA performers and DJs. DJs who use DJ software typically use sound cards integrated into DJ controllers or specialized DJ sound cards. DJ sound cards sometimes have inputs with phono preamplifiers to allow turntables to be connected to the computer to control the software's playback of music files with timecode vinyl.
1046) 18.966621, Tablet computer - Wikipedia, the free encyclopedia.txt#7, term: computer, content:Following their earlier tablet-computer products such as the Pencept PenPad[25][26] and the CIC Handwriter,[27] in September 1989, GRiD Systems released the first commercially available tablet-type portable computer, the GRiDPad.[28] The GRiDPad was also manufactured by the Samsung Corporation after acquiring GRiD System.[29] All three products were based on extended versions of the MS-DOS operating system.
1047) 18.966621, Telephone exchange - Wikipedia, the free encyclopedia.txt#76, term: computer, content:Composite switches are inherently fault-tolerant. If a subswitch fails, the controlling computer can sense it during a periodic test. The computer marks all the connections to the subswitch as "in use". This prevents new calls, and does not interrupt old calls that remain working. As calls in progress end, the subswitch becomes unused, and new calls avoid the subswitch because it's already "in use." Some time later, a technician can replace the circuit board. When the next test succeeds, the connections to the repaired subsystem are marked "not in use", and the switch returns to full operation.
1048) 18.966621, Teleprinter - Wikipedia, the free encyclopedia.txt#2, term: computer, content:Teleprinters have largely been replaced by fully electronic computer terminals which usually use a computer monitor instead of a printer (though the term "TTY" is still occasionally used to refer to them, such as in Unix systems). Teleprinters are still widely used in the aviation industry (AFTN and airline teletype system), and variations called Telecommunications Devices for the Deaf (TDDs) are used by the hearing impaired for typed communications over ordinary telephone lines.
1049) 18.966621, Texas Instruments - Wikipedia, the free encyclopedia.txt#18, term: computer, content:Because of TI's research and development of military temperature range silicon transistors and integrated circuits (ICs), TI won contracts for the first IC-based computer for the U.S. Air Force in 1961 and for ICs for the Minuteman Missile the following year. In 1968 TI developed the data systems for Mariner Program. In 1991 TI won the F-22 Radar and Computer development contract.
1050) 18.966621, Tom Kilburn - Wikipedia, the free encyclopedia.txt#9, term: computer, content:While Kilburn led one design team working on Meg, he led another with Dick Grimsdale and Douglas Webb, on a research project examining what he believed would be the next step forward in computer design: the use of transistors. The 48-bit machine they completed in November 1953 was the world's first transistor computer, with 550 diodes and 92 transistors, and was manufactured by STC. An improved version completed in April 1955 had 1,300 diodes and 200 transistors, and was sold by Metropolitan-Vickers as the Metrovick 950.[2]
1051) 18.966621, Tom Kilburn - Wikipedia, the free encyclopedia.txt#14, term: computer, content:Professor of Computer Engineering in the University of Manchester. He was a pioneer in the engineering realisation of the general purpose electronic digital computer and has made major contributions to the rapid rate of development that has occurred in this field over the past 15 years. His contributions cover the whole range from overall system design to the invention of high speed circuits to meet particular needs. His latest machine, 'Atlas' may well be the most advanced machine currently under construction anywhere in the World.[22]
1052) 18.966621, TOP500 - Wikipedia, the free encyclopedia.txt#9, term: computer, content:Since November 2015, no computer on the list runs Windows. In November 2014, Windows Azure[11] cloud computer was no longer on the list of fastest supercomputers (its best rank was 165 in 2012), leaving the Shanghai Supercomputer Center's Magic Cube as the only Windows-based supercomputer on the list, until Magic Cube also dropped off the list. Magic Cube was ranked 436 in its last appearance on the list released in June 2015.[12]
1053) 18.966621, Torpedo Data Computer - Wikipedia, the free encyclopedia.txt#0, term: computer, content:The Torpedo Data Computer (TDC) was an early electromechanical analog computer used for torpedo fire-control on American submarines during World War II. Britain, Germany, and Japan also developed automated torpedo fire control equipment, but none were as advanced as the US Navy's TDC,[1] as it was able to automatically track the target rather than simply offering an instantaneous firing solution. This unique capability of the TDC set the standard for submarine torpedo fire control during World War II.[2][3]
1054) 18.966621, Touchscreen - Wikipedia, the free encyclopedia.txt#11, term: computer, content:In 1986, the first graphical point of sale software was demonstrated on the 16-bit Atari 520ST color computer. It featured a color touchscreen widget-driven interface.[18] The ViewTouch[19] point of sale software was first shown by its developer, Gene Mosher, at Fall Comdex, 1986, in Las Vegas, Nevada to visitors at the Atari Computer demonstration area and was the first commercially available POS system with a widget-driven color graphic touchscreen interface.[20]
1055) 18.966621, Transistor computer - Wikipedia, the free encyclopedia.txt#0, term: computer, content:A transistor computer is a computer which uses discrete transistors instead of vacuum tubes. The "first generation" of electronic computers used vacuum tubes, which generated large amounts of heat, were bulky, and were unreliable. A "second generation" of computers, through the late 1950s and 1960s featured boards filled with individual transistors and magnetic memory cores (see History of computing hardware). These machines remained the mainstream design into the late 1960s, when integrated circuits started appearing and led to the "third generation" machines.
1056) 18.966621, Turing completeness - Wikipedia, the free encyclopedia.txt#12, term: computer, content:A computer with access to an infinite tape of data may be more powerful than a Turing machine: for instance, the tape might contain the solution to the halting problem, or some other Turing-undecidable problem. Such an infinite tape of data is called a Turing oracle. Even a Turing oracle with random data is not computable (with probability 1), since there are only countably many computations but uncountably many oracles. So a computer with a random Turing oracle can compute things that a Turing machine cannot.
1057) 18.966621, Turing completeness - Wikipedia, the free encyclopedia.txt#13, term: computer, content:All known laws of physics have consequences that are computable by a series of approximations on a digital computer. A hypothesis called digital physics states that this is no accident, that it is because the universe itself is computable on a universal Turing machine. This would imply that no computer more powerful than a universal Turing machine can be built physically (see ChurchTuring thesis  Philosophical implications).
1058) 18.966621, Unconventional computing - Wikipedia, the free encyclopedia.txt#1, term: computer, content:Historically, mechanical computers were used in industry before the advent of the transistor. Mechanical computers retain some interest today both in research and as analogue computers. Some mechanical computers have a theoretical or didactic relevance, such as billiard-ball computers or hydraulic ones,.[1] While some are actually simulated, others are not; no attempt is made to build a functioning computer through the mechanical collisions of billiard balls. The domino computer is another theoretically interesting mechanical computing scheme.
1059) 18.966621, Universal Turing machine - Wikipedia, the free encyclopedia.txt#4, term: computer, content:Davis makes a case that Turing's Automatic Computing Engine (ACE) computer "anticipated" the notions of microprogramming (microcode) and RISC processors (Davis 2000:188). Knuth cites Turing's work on the ACE computer as designing "hardware to facilitate subroutine linkage" (Knuth 1973:225); Davis also references this work as Turing's use of a hardware "stack" (Davis 2000:237 footnote 18).
1060) 18.966621, USB - Wikipedia, the free encyclopedia.txt#161, term: computer, content:Ethernet standards require electrical isolation between the networked device (computer, phone, etc.) and the network cable up to 1500 V AC or 2250 V DC for 60 seconds.[161] USB has no such requirement as it was designed for peripherals closely associated with a host computer, and in fact it connects the peripheral and host grounds. This gives Ethernet a significant safety advantage over USB with peripherals such as cable and DSL modems connected to external wiring that can assume hazardous voltages under certain fault conditions.[162]
1061) 18.966621, Von Neumann architecture - Wikipedia, the free encyclopedia.txt#33, term: computer, content:Aside from the von Neumann bottleneck, program modifications can be quite harmful, either by accident or design. In some simple stored-program computer designs, a malfunctioning program can damage itself, other programs, or the operating system, possibly leading to a computer crash. Memory protection and other forms of access control can usually protect against both accidental and malicious program modification.
1062) 18.966621, Wearable computer - Wikipedia, the free encyclopedia.txt#52, term: computer, content:The wearable computer was introduced to the US Army in 1989 as a small computer that was meant to assist soldiers in battle. Since then, the concept has grown to include the = Land Warrior program and proposal for future systems.[52] The most extensive military program in the wearables arena is the US Army's Land Warrior system,[53] which will eventually be merged into the Future Force Warrior system.[citation needed]
1063) 18.966621, Windows 2000 - Wikipedia, the free encyclopedia.txt#48, term: computer, content:Active Directory services could always be installed on a Windows 2000 Server, Advanced Server, or Datacenter Server computer, and cannot be installed on a Windows 2000 Professional computer. However, Windows 2000 Professional is the first client operating system able to exploit Active Directory's new features. As part of an organization's migration, Windows NT clients continued to function until all clients were upgraded to Windows 2000 Professional, at which point the Active Directory domain could be switched to native mode and maximum functionality achieved.
1064) 18.966621, Zilog Z80 - Wikipedia, the free encyclopedia.txt#43, term: computer, content:In East Germany, an unlicensed clone of the Z80, known as the U880, was manufactured. It was very popular and was used in Robotron's and VEB Mikroelektronik Mhlhausen's computer systems (such as the KC85-series) and also in many self-made computer systems. In Romania another unlicensed clone could be found, named MMN80CPU and produced by Microelectronica, used in home computers like TIM-S, HC, COBRA.
1065) 18.74304, Computer graphics - Wikipedia, the free encyclopedia.txt#13, term: computer, content:Also in 1966, Ivan Sutherland continued to innovate at MIT when he invented the first computer controlled head-mounted display (HMD). Called the Sword of Damocles because of the hardware required for support, it displayed two separate wireframe images, one for each eye. This allowed the viewer to see the computer scene in stereoscopic 3D. After receiving his Ph.D. from MIT, Sutherland became Director of Information Processing at ARPA (Advanced Research Projects Agency), and later became a professor at Harvard. In 1967 Sutherland was recruited by Evans to join the computer science program at the University of Utah. There he perfected his HMD. Twenty years later, NASA would re-discover his techniques in their virtual reality research. At Utah, Sutherland and Evans were highly sought after consultants by large companies but they were frustrated at the lack of graphics hardware available at the time so they started formulating a plan to start their own company. In 1969, the ACM initiated A Special Interest Group on Graphics (SIGGRAPH) which organizes conferences, graphics standards, and publications within the field of computer graphics. In 1973, the first annual SIGGRAPH conference was held, which has become one of the focuses of the organization. SIGGRAPH has grown in size and importance as the field of computer graphics has expanded over time.
1066) 18.74304, Computer graphics - Wikipedia, the free encyclopedia.txt#15, term: computer, content:As the UU computer graphics laboratory was attracting people from all over, John Warnock was one of those early pioneers; he would later found Adobe Systems and create a revolution in the publishing world with his PostScript page description language, and Adobe would go on later to create the industry standard photo editing software in Adobe Photoshop and the movie industry's special effects standard in Adobe After Effects. Tom Stockham led the image processing group at UU which worked closely with the computer graphics lab. Jim Clark was also there; he would later found Silicon Graphics. The first major advance in 3D computer graphics was created at UU by these early pioneers, the hidden-surface algorithm. In order to draw a representation of a 3D object on the screen, the computer must determine which surfaces are "behind" the object from the viewer's perspective, and thus should be "hidden" when the computer creates (or renders) the image. The 3D Core Graphics System (or Core) was the first graphical standard to be developed. A group of 25 experts of the ACM Special Interest Group SIGGRAPH developed this "conceptual framework". The specifications were published in 1977, and it became a foundation for many future developments in the field.
1067) 18.74304, Computer graphics - Wikipedia, the free encyclopedia.txt#19, term: computer, content:In the early 1980s, the availability of bit-slice and 16-bit microprocessors started to revolutionise high-resolution computer graphics terminals which now increasingly became intelligent, semi-standalone and standalone workstations. Graphics and application processing were increasingly migrated to the intelligence in the workstation, rather than continuing to rely on central mainframe and mini-computers. Typical of the early move to high resolution computer graphics intelligent workstations for the computer-aided engineering market were the Orca 1000, 2000 and 3000 workstations, developed by Orcatech of Ottawa, a spin-off from Bell-Northern Research, and led by David Pearson, an early workstation pioneer. The Orca 3000 was based on Motorola 68000 and AMD bit-slice processors and had Unix as its operating system. It was targeted squarely at the sophisticated end of the design engineering sector. Artists and graphic designers began to see the personal computer, particularly the Commodore Amiga and Macintosh, as a serious design tool, one that could save time and draw more accurately than other methods. The Macintosh remains a highly popular tool for computer graphics among graphic design studios and businesses. Modern computers, dating from the 1980s, often use graphical user interfaces (GUI) to present data and information with symbols, icons and pictures, rather than text. Graphics are one of the five key elements of multimedia technology.
1068) 18.74304, Computer simulation - Wikipedia, the free encyclopedia.txt#1, term: computer, content:Computer simulations vary from computer programs that run a few minutes to network-based groups of computers running for hours to ongoing simulations that run for days. The scale of events being simulated by computer simulations has far exceeded anything possible (or perhaps even imaginable) using traditional paper-and-pencil mathematical modeling. Over 10 years ago, a desert-battle simulation of one force invading another involved the modeling of 66,239 tanks, trucks and other vehicles on simulated terrain around Kuwait, using multiple supercomputers in the DoD High Performance Computer Modernization Program[2] Other examples include a 1-billion-atom model of material deformation;[3] a 2.64-million-atom model of the complex maker of protein in all organisms, a ribosome, in 2005;[4] a complete simulation of the life cycle of Mycoplasma genitalium in 2012; and the Blue Brain project at EPFL (Switzerland), begun in May 2005 to create the first computer simulation of the entire human brain, right down to the molecular level.[5]
1069) 18.74304, Home computer - Wikipedia, the free encyclopedia.txt#13, term: computer, content:Books of type-in program listings like BASIC Computer Games were available dedicated for the BASICs of most models of computer with titles along the lines of 64 Amazing BASIC Games for the Commodore 64.[20] While most of the programs in these books were short and simple games or demos, some titles such as Compute!'s SpeedScript series, contained productivity software that rivaled commercial packages. To avoid the tedious process of typing in a program listing from a book, these books would sometimes include a mail-in offer from the author to obtain the programs on disk or cassette for a few dollars. Before the Internet, and before most computer owners had a modem, books were a popular and low-cost means of software distributionone that had the advantage of incorporating its own documentation. These books also served a role in familiarizing new computer owners with the concepts of programming; some titles added suggested modifications to the program listings for the user to carry out. Modifying software to be compatible with one's system or writing a utility program to fit one's needs was a skill every advanced computer owner was expected to have.[21]
1070) 18.74304, Home computer - Wikipedia, the free encyclopedia.txt#17, term: computer, content:By 1985, price wars had driven the consumer electronics companies from the market, as they could no longer sustain development of what had become, for them, money-losing projects. By the late 1980s, many mass merchants sold video game consoles like the Nintendo Entertainment System, but no longer sold home computers.[28] Most computer companies  even those with a majority of sales to home users  avoided the term "home computer" because of its association with the image of, as Compute! wrote, "a low-powered, low-end machine primarily suited for playing games". Apple consistently avoided stating that it was a home-computer company, and described the IIc as "a serious computer for the serious home user" despite competing against IBM's PCjr home computer. John Sculley denied that his company sold home computers; rather, he said, Apple sold "computers for use in the home".[29][30][31] In 1990 the company reportedly refused to support joysticks on its low-cost Macintosh LC and IIsi computers to prevent customers from considering them as "game machines".[32]
1071) 18.74304, Home computer - Wikipedia, the free encyclopedia.txt#38, term: computer, content:A backlash set in; computer users were "geeks", "nerds" or worse, "hackers". The North American video game crash of 1983 soured many on home computer technology as users saw large investments in 'the technology of the future' turn into dead-ends when manufacturers pulled out of the market or went out of business. The computers that were bought for use in the family room were either forgotten in closets or relegated to basements and children's bedrooms to be used exclusively for games and the occasional book report. Home computers of the 1980s have been called "a technology in search of a use".[64] In 1984 Tandy executive Steve Leininger, designer of the TRS-80 Model I, admitted that "As an industry we haven't found any compelling reason to buy a computer for the home" other than for word processing.[53] A 1985 study found that, during a typical week, 40% of adult computer owners did not use their computers at all. Usage rates among children were higher, with households reporting that only 16-20% of children aged 617 did not use the computer during a typical week.[65]
1072) 18.74304, Home computer - Wikipedia, the free encyclopedia.txt#39, term: computer, content:It would take another 10 years for technology to mature, for the graphical user interface to make the computer approachable for non-technical users, and for the World Wide Web to provide a compelling reason for most people to want a computer in their homes. Separate 1998 studies found that 75% of Americans with Internet access accessed primarily from home and that not having Internet access at home inhibited Internet use.[65] Predicted aspects of the revolution were left by the wayside or modified in the face of an emerging reality. The cost of electronics dropped precipitously and today many families have a computer for each family member, or a laptop for mom's active lifestyle, a desktop for dad with the kids sharing a computer. Encyclopedias, recipe catalogs and medical databases are kept online and accessed over the World Wide Web not stored locally on floppy disks or CD-ROM. TV has yet to gain substantial interactivity; instead, the web has evolved alongside television, giving rise to the second screen concept. The HTPC and services like Netflix, Google TV or Apple TV along with internet video sites such as YouTube and Hulu may one day replace traditional broadcast and cable television.[66] Our coffee may be brewed automatically every morning, but the computer is a simple one embedded in the coffee maker, not under external control. As of 2008, robots are just beginning to make an impact in the home, with Roomba and Aibo leading the charge.
1073) 18.74304, Massachusetts Institute of Technology - Wikipedia, the free encyclopedia.txt#16, term: computer, content:MIT has kept pace with and helped to advance the digital age. In addition to developing the predecessors to modern computing and networking technologies,[69][70] students, staff, and faculty members at Project MAC, the Artificial Intelligence Laboratory, and the Tech Model Railroad Club wrote some of the earliest interactive computer video games like Spacewar! and created much of modern hacker slang and culture.[71] Several major computer-related organizations have originated at MIT since the 1980s: Richard Stallman's GNU Project and the subsequent Free Software Foundation were founded in the mid-1980s at the AI Lab; the MIT Media Lab was founded in 1985 by Nicholas Negroponte and Jerome Wiesner to promote research into novel uses of computer technology;[72] the World Wide Web Consortium standards organization was founded at the Laboratory for Computer Science in 1994 by Tim Berners-Lee;[73] the OpenCourseWare project has made course materials for over 2,000 MIT classes available online free of charge since 2002;[74] and the One Laptop per Child initiative to expand computer education and connectivity to children worldwide was launched in 2005.[75]
1074) 18.74304, Motorola 6800 - Wikipedia, the free encyclopedia.txt#44, term: computer, content:The MITS Altair 8800, the first successful personal computer, used the Intel 8080 microprocessor and was featured on the January 1975 cover of Popular Electronics.[81] The first personal computers using the Motorola 6800 were introduced in late 1975. Sphere Corporation of Bountiful, Utah ran a quarter-page advertisement in the July 1975 issue of Radio-Electronics for a $650 USD computer kit with a 6800 microprocessor, 4 kilobytes of RAM, a video board and a keyboard. This would display 16 lines of 32 characters on a TV or monitor.[82] The Sphere computer kits began shipping in November 1975.[83] Southwest Technical Products Corporation of San Antonio, Texas, officially announced their SWTPC 6800 Computer System in November 1975. Wayne Green visited SWTPC in August 1975 and described the SWTPC computer kit complete with photos of a working system in the October 1975 issue of 73. The SWTPC 6800 was based on the Motorola MEK6800 design evaluation kit chip set and used the MIKBUG ROM Software.[25] The MITS Altair 680 was on the cover of the November 1975 issue of Popular Electronics. The Altair 680 used a 6800 microprocessor and also had a front panel with toggle switches and LEDs. The initial design had to be revised and first deliveries of the Altair 680B were in April 1976.[84]
1075) 18.74304, Personal computer - Wikipedia, the free encyclopedia.txt#11, term: computer, content:In 1973 the IBM Los Gatos Scientific Center developed a portable computer prototype called SCAMP (Special Computer APL Machine Portable) based on the IBM PALM processor with a Philips compact cassette drive, small CRT and full function keyboard. SCAMP emulated an IBM 1130 minicomputer in order to run APL\1130.[5] In 1973 APL was generally available only on mainframe computers, and most desktop sized microcomputers such as the Wang 2200 or HP 9800 offered only BASIC. Because SCAMP was the first to emulate APL\1130 performance on a portable, single user computer, PC Magazine in 1983 designated SCAMP a "revolutionary concept" and "the world's first personal computer".[5][6] This seminal, single user portable computer now resides in the Smithsonian Institution, Washington, D.C.. Successful demonstrations of the 1973 SCAMP prototype led to the IBM 5100 portable microcomputer launched in 1975 with the ability to be programmed in both APL and BASIC for engineers, analysts, statisticians and other business problem-solvers. In the late 1960s such a machine would have been nearly as large as two desks and would have weighed about half a ton.[5]
1076) 18.74304, Quantum computing - Wikipedia, the free encyclopedia.txt#3, term: computer, content:A classical computer has a memory made up of bits, where each bit is represented by either a one or a zero. A quantum computer maintains a sequence of qubits. A single qubit can represent a one, a zero, or any quantum superposition of those two qubit states; a pair of qubits can be in any quantum superposition of 4 states, and three qubits in any superposition of 8 states. In general, a quantum computer with     n   {\displaystyle n}   qubits can be in an arbitrary superposition of up to      2  n     {\displaystyle 2^{n}}   different states simultaneously (this compares to a normal computer that can only be in one of these      2  n     {\displaystyle 2^{n}}   states at any one time). A quantum computer operates by setting the qubits in a controlled initial state that represents the problem at hand and by manipulating those qubits with a fixed sequence of quantum logic gates. The sequence of gates to be applied is called a quantum algorithm. The calculation ends with a measurement, collapsing the system of qubits into one of the      2  n     {\displaystyle 2^{n}}   pure states, where each qubit is zero or one, decomposing into a classical state. The outcome can therefore be at most     n   {\displaystyle n}   classical bits of information. Quantum algorithms are often non-deterministic, in that they provide the correct solution only with a certain known probability.
1077) 17.74165, Computer graphics - Wikipedia, the free encyclopedia.txt#8, term: computer, content:Further advances in computing led to greater advancements in interactive computer graphics. In 1959, the TX-2 computer was developed at MIT's Lincoln Laboratory. The TX-2 integrated a number of new man-machine interfaces. A light pen could be used to draw sketches on the computer using Ivan Sutherland's revolutionary Sketchpad software.[5] Using a light pen, Sketchpad allowed one to draw simple shapes on the computer screen, save them and even recall them later. The light pen itself had a small photoelectric cell in its tip. This cell emitted an electronic pulse whenever it was placed in front of a computer screen and the screen's electron gun fired directly at it. By simply timing the electronic pulse with the current location of the electron gun, it was easy to pinpoint exactly where the pen was on the screen at any given moment. Once that was determined, the computer could then draw a cursor at that location. Sutherland seemed to find the perfect solution for many of the graphics problems he faced. Even today, many standards of computer graphics interfaces got their start with this early Sketchpad program. One example of this is in drawing constraints. If one wants to draw a square for example, they do not have to worry about drawing four lines perfectly to form the edges of the box. One can simply specify that they want to draw a box, and then specify the location and size of the box. The software will then construct a perfect box, with the right dimensions and at the right location. Another example is that Sutherland's software modeled objects - not just a picture of objects. In other words, with a model of a car, one could change the size of the tires without affecting the rest of the car. It could stretch the body of car without deforming the tires.
1078) 17.421955, Alan Turing - Wikipedia, the free encyclopedia.txt#39, term: computer, content:In July 1942, Turing devised a technique termed Turingery (or jokingly Turingismus)[76] for use against the Lorenz cipher messages produced by the Germans' new Geheimschreiber (secret writer) machine. This was a teleprinter rotor cipher attachment codenamed Tunny at Bletchley Park. Turingery was a method of wheel-breaking, i.e., a procedure for working out the cam settings of Tunny's wheels.[77] He also introduced the Tunny team to Tommy Flowers who, under the guidance of Max Newman, went on to build the Colossus computer, the world's first programmable digital electronic computer, which replaced a simpler prior machine (the Heath Robinson), and whose superior speed allowed the statistical decryption techniques to be applied usefully to the messages.[78] Some have mistakenly said that Turing was a key figure in the design of the Colossus computer. Turingery and the statistical approach of Banburismus undoubtedly fed into the thinking about cryptanalysis of the Lorenz cipher,[79][80] but he was not directly involved in the Colossus development.[81]
1079) 17.421955, Antivirus software - Wikipedia, the free encyclopedia.txt#1, term: computer, content:Antivirus software was originally developed to detect and remove computer viruses, hence the name. However, with the proliferation of other kinds of malware, antivirus software started to provide protection from other computer threats. In particular, modern antivirus software can protect from: malicious browser helper objects (BHOs), browser hijackers, ransomware, keyloggers, backdoors, rootkits, trojan horses, worms, malicious LSPs, dialers, fraudtools, adware and spyware.[2] Some products also include protection from other computer threats, such as infected and malicious URLs, spam, scam and phishing attacks, online identity (privacy), online banking attacks, social engineering techniques, advanced persistent threat (APT) and botnet DDoS attacks.[3]
1080) 17.421955, ARM architecture - Wikipedia, the free encyclopedia.txt#4, term: computer, content:The British computer manufacturer Acorn Computers first developed the Acorn RISC Machine architecture (ARM)[13][14] in the 1980s to use in its personal computers. Its first ARM-based products were coprocessor modules for the BBC Micro series of computers. After the successful BBC Micro computer, Acorn Computers considered how to move on from the relatively simple MOS Technology 6502 processor to address business markets like the one that was soon dominated by the IBM PC, launched in 1981. The Acorn Business Computer (ABC) plan required that a number of second processors be made to work with the BBC Micro platform, but processors such as the Motorola 68000 and National Semiconductor 32016 were considered unsuitable, and the 6502 was not powerful enough for a graphics based user interface.[15]
1081) 17.421955, ARPANET - Wikipedia, the free encyclopedia.txt#16, term: computer, content:The first successful message on the ARPANET was sent by UCLA student programmer Charley Kline, at 10:30pm on 29 October 1969, from Boelter Hall 3420.[32] Kline transmitted from the university's SDS Sigma 7 Host computer to the Stanford Research Institute's SDS 940 Host computer. The message text was the word login; on an earlier attempt the l and the o letters were transmitted, but the system then crashed. Hence, the literal first message over the ARPANET was lo. About an hour later, after the programmers repaired the code that caused the crash, the SDS Sigma 7 computer effected a full login. The first permanent ARPANET link was established on 21 November 1969, between the IMP at UCLA and the IMP at the Stanford Research Institute. By 5 December 1969, the entire four-node network was established.[33]
1082) 17.421955, Booting - Wikipedia, the free encyclopedia.txt#48, term: computer, content:For example, on a PC with Windows XP installed on the hard drive, the user could set the boot order to the one given above, and then insert a Linux Live CD in order to try out Linux without having to install an operating system onto the hard drive. This is an example of dual booting, in which the user chooses which operating system to start after the computer has performed its Power-on self-test (POST). In this example of dual booting, the user chooses by inserting or removing the CD from the computer, but it is more common to choose which operating system to boot by selecting from a BIOS or UEFI boot menu, by using the computer keyboard; the boot menu is typically entered by pressing Delete or F11 keys during the POST.
1083) 17.421955, Bus (computing) - Wikipedia, the free encyclopedia.txt#2, term: computer, content:Computer systems generally consist of three main parts: the central processing unit (CPU) that processes data, memory that holds the programs and data to be processed, and I/O (input/output) devices as peripherals that communicate with the outside world. An early computer might use a hand-wired CPU of vacuum tubes, a magnetic drum for main memory, and a punch tape and printer for reading and writing data. In a modern system we might find a multi-core CPU, DDR4 SDRAM for memory, a solid-state drive for secondary storage, a graphics card and LCD as a display system, a mouse and keyboard for interaction, and a Wi-Fi connection for networking. In both examples, computer buses of one form or another move data between all of these devices.
1084) 17.421955, Central processing unit - Wikipedia, the free encyclopedia.txt#9, term: computer, content:In 1964, IBM introduced its System/360 computer architecture that was used in a series of computers capable of running the same programs with different speed and performance.[25] This was significant at a time when most electronic computers were incompatible with one another, even those made by the same manufacturer. To facilitate this improvement, IBM utilized the concept of a microprogram (often called "microcode"), which still sees widespread usage in modern CPUs.[26] The System/360 architecture was so popular that it dominated the mainframe computer market for decades and left a legacy that is still continued by similar modern computers like the IBM zSeries.[27][28] In 1965, Digital Equipment Corporation (DEC) introduced another influential computer aimed at the scientific and research markets, the PDP-8.[29]
1085) 17.421955, Colossus computer - Wikipedia, the free encyclopedia.txt#27, term: computer, content:Colossus was preceded by several computers, many of them first in some category. Zuse's Z3 was the first functional fully program-controlled computer, and was based on electromechanical relays, as were the (less advanced) Bell Labs machines of the late 1930s (George Stibitz, et al.). The AtanasoffBerry Computer was electronic and binary (digital) but not programmable. Assorted analog computers were semi-programmable; some of these much predated the 1930s (e.g., Vannevar Bush). Babbage's Analytical Engine design predated all these (in the mid-19th century), it was a decimal, programmable, entirely mechanical constructionbut was only partially designed and never built during Babbage's lifetime. Colossus was the first combining digital, (partially) programmable, and electronic. The first fully programmable digital electronic computer was the ENIAC which was completed in 1946.
1086) 17.421955, Computer - Simple English Wikipedia, the free encyclopedia.txt#30, term: computer, content:Computers store data and the instructions as numbers, because computers can do things with numbers very quickly. These data are stored as binary symbols (1s and 0s). A 1 or a 0 symbol stored by a computer is called a bit, which comes from the words binary digit. Computers can use many bits together to represent instructions and the data that these instructions use. A list of instructions is called a program and is stored on the computer's hard disk. Computers work through the program by using a central processing unit, and they use fast memory called RAM as a space to store the instructions and data while they are doing this. When the computer wants to store the results of the program for later, it uses the hard disk because things stored on a hard disk can still be remembered after the computer is turned off.
1087) 17.421955, Computer animation - Wikipedia, the free encyclopedia.txt#1, term: computer, content:Computer animation is essentially a digital successor to the stop motion techniques used in traditional animation with 3D models and frame-by-frame animation of 2D illustrations. Computer-generated animations are more controllable than other more physically based processes, constructing miniatures for effects shots or hiring extras for crowd scenes, and because it allows the creation of images that would not be feasible using any other technology. It can also allow a single graphic artist to produce such content without the use of actors, expensive set pieces, or props. To create the illusion of movement, an image is displayed on the computer monitor and repeatedly replaced by a new image that is similar to it, but advanced slightly in time (usually at a rate of 24 or 30 frames/second). This technique is identical to how the illusion of movement is achieved with television and motion pictures.
1088) 17.421955, Computer memory - Wikipedia, the free encyclopedia.txt#20, term: computer, content:This offers several advantages. Computer programmers no longer need to worry about where the memory is physically stored or whether the user's computer will have enough memory. It also allows multiple types of memory to be used. For example, some memory can be stored in physical RAM chips while other memory is stored on a hard drive. This drastically increases the amount of memory available to programs. The operating system will place actively used memory in physical RAM, which is much faster than hard disks. When the amount of RAM is not sufficient to run all the current programs, it can result in a situation where the computer spends more time moving memory from RAM to disk and back than it does accomplishing tasks; this is known as thrashing.
1089) 17.421955, Computer programming - Wikipedia, the free encyclopedia.txt#9, term: computer, content:The invention of the von Neumann architecture allowed computer programs to be stored in computer memory. Early programs had to be painstakingly crafted using the instructions (elementary operations) of the particular machine, often in binary notation. Every model of computer would likely use different instructions (machine language) to do the same task. Later, assembly languages were developed that let the programmer specify each instruction in a text format, entering abbreviations for each operation code instead of a number and specifying addresses in symbolic form (e.g., ADD X, TOTAL). Entering a program in assembly language is usually more convenient, faster, and less prone to human error than using machine language, but because an assembly language is little more than a different notation for a machine language, any two machines with different instruction sets also have different assembly languages.
1090) 17.421955, Computer science - Wikipedia, the free encyclopedia.txt#5, term: computer, content:Although many initially believed it was impossible that computers themselves could actually be a scientific field of study, in the late fifties it gradually became accepted among the greater academic population.[13][14] It is the now well-known IBM brand that formed part of the computer science revolution during this time. IBM (short for International Business Machines) released the IBM 704[15] and later the IBM 709[16] computers, which were widely used during the exploration period of such devices. "Still, working with the IBM [computer] was frustrating [] if you had misplaced as much as one letter in one instruction, the program would crash, and you would have to start the whole process over again".[13] During the late 1950s, the computer science discipline was very much in its developmental stages, and such issues were commonplace.[14]
1091) 17.421955, Computer speaker - Wikipedia, the free encyclopedia.txt#0, term: computer, content:Computer speakers, or multimedia speakers, are speakers sold for use with computers, although usually capable of other audio uses, e.g. for an MP3 player. Most such speakers have an internal amplifier, and consequently require a power source, which may be by a mains power supply often via an AC adapter, batteries, or a USB port (able to supply no more than 2.5W DC, 500mA at 5V). The signal input connector is often a 3.5mm jack plug (usually color-coded lime green per the PC 99 standard); RCA connectors are sometimes used, and a USB port may supply both signal and power (requiring additional circuitry, and only suitable for use with a computer). Battery-powered wireless Bluetooth speakers require no connections at all. Most computers have speakers of low power and quality built in; when external speakers are connected they disable the built-in speakers. Altec Lansing claims to have created the computer speaker market in 1990.[1]
1092) 17.421955, FLOPS - Wikipedia, the free encyclopedia.txt#28, term: computer, content:FLOPS measures the computing ability of a computer. An example of a floating-point operation is the calculation of mathematical equations; as such, FLOPS is a useful measure of supercomputer performance. MIPS is used to measure the integer performance of a computer. Examples of integer operation include data movement (A to B) or value testing (If A = B, then C). MIPS as a performance benchmark is adequate for the computer when it is used in database query, word processing, spreadsheets, or to run multiple virtual operating systems.[59][60] Frank H. McMahon, of the Lawrence Livermore National Laboratory, invented the terms FLOPS and MFLOPS (megaFLOPS) so that he could compare the so-called supercomputers of the day by the number of floating-point calculations they performed per second. This was much better than using the prevalent MIPS to compare computers as this statistic usually had little bearing on the arithmetic capability of the machine.
1093) 17.421955, Free software - Wikipedia, the free encyclopedia.txt#0, term: computer, content:Free software, freedom-respecting software, software libre, or libre software[1][2] is computer software that gives users the freedom to run the software for any purpose as well as to study, change, and distribute the software and any adapted versions.[3][4][5][6][7] The right to study and modify free software grants access to its source code. For computer programs that are covered by copyright law, this is achieved with a software license by which the author grants users the aforementioned freedoms. Software that is not covered by copyright law, such as software in the public domain, is free if the source code is in the public domain, or otherwise available without restrictions. Other legal and technical aspects, such as software patents and digital rights management may restrict users in exercising their rights, and thus prevent software from being free.[8] Free software may be developed collaboratively by volunteer computer programmers or by corporations; as part of a commercial, for-profit activity or not.
1094) 17.421955, Harwell CADET - Wikipedia, the free encyclopedia.txt#4, term: computer, content:Cooke-Yarborough described CADET as being "probably the second fully transistorised computer in the world to put to use", second to an unnamed IBM machine.[3] Both the Manchester University Transistor Computer and the Bell Laboratories TRADIC were demonstrated incorporating transistors before CADET was operational, although both required some thermionic valves to supply their faster clock power, so they were not fully transistorised.[6] In April 1955 IBM announced the IBM 608 transistor calculator, which they claim was "the first all solid-state computing machine commercially marketed"[7] and "the first completely transistorized computer available for commercial installation",[8] and which may have been demonstrated in October 1954, before the CADET.
1095) 17.421955, History of computing hardware - Wikipedia, the free encyclopedia.txt#19, term: computer, content:Charles Babbage, an English mechanical engineer and polymath, originated the concept of a programmable computer. Considered the "father of the computer",[32] he conceptualized and invented the first mechanical computer in the early 19th century. After working on his revolutionary difference engine, designed to aid in navigational calculations, in 1833 he realized that a much more general design, an Analytical Engine, was possible. The input of programs and data was to be provided to the machine via punched cards, a method being used at the time to direct mechanical looms such as the Jacquard loom. For output, the machine would have a printer, a curve plotter and a bell. The machine would also be able to punch numbers onto cards to be read in later. It employed ordinary base-10 fixed-point arithmetic.
1096) 17.421955, Home computer - Wikipedia, the free encyclopedia.txt#9, term: computer, content:By 1982, an estimated 621,000 home computers were in American households, at an average sales price of US$530.[17] After the success of the Radio Shack TRS-80, the Commodore PET and the Apple II in 1977, almost every manufacturer of consumer electronics rushed to introduce a home computer. Large numbers of new machines of all types began to appear during the late 1970s and early 1980s. Mattel, Coleco, Texas Instruments and Timex, none of which had any previous connection to the computer industry, all had short-lived home computer lines in the early 1980s. Some home computers were more successful  the BBC Micro, Sinclair ZX Spectrum, Atari 800XL and Commodore 64, sold many units over several years and attracted third-party software development.
1097) 17.421955, Human–computer interaction - Wikipedia, the free encyclopedia.txt#3, term: computer, content:The Association for Computing Machinery (ACM) defines human-computer interaction as "a discipline concerned with the design, evaluation and implementation of interactive computing systems for human use and with the study of major phenomena surrounding them".[5] An important facet of HCI is the securing of user satisfaction (or simply End User Computing Satisfaction). "Because humancomputer interaction studies a human and a machine in communication, it draws from supporting knowledge on both the machine and the human side. On the machine side, techniques in computer graphics, operating systems, programming languages, and development environments are relevant. On the human side, communication theory, graphic and industrial design disciplines, linguistics, social sciences, cognitive psychology, social psychology, and human factors such as computer user satisfaction are relevant. And, of course, engineering and design methods are relevant."[5] Due to the multidisciplinary nature of HCI, people with different backgrounds contribute to its success. HCI is also sometimes termed humanmachine interaction (HMI), manmachine interaction (MMI) or computerhuman interaction (CHI).
1098) 17.421955, Information technology - Wikipedia, the free encyclopedia.txt#5, term: computer, content:Electronic computers, using either relays or valves, began to appear in the early 1940s. The electromechanical Zuse Z3, completed in 1941, was the world's first programmable computer, and by modern standards one of the first machines that could be considered a complete computing machine. Colossus, developed during the Second World War to decrypt German messages was the first electronic digital computer. Although it was programmable, it was not general-purpose, being designed to perform only a single task. It also lacked the ability to store its program in memory; programming was carried out using plugs and switches to alter the internal wiring.[12] The first recognisably modern electronic digital stored-program computer was the Manchester Small-Scale Experimental Machine (SSEM), which ran its first program on 21 June 1948.[13]
1099) 17.421955, J. Lyons and Co. - Wikipedia, the free encyclopedia.txt#11, term: computer, content:The top management of Lyons, with its background in the use of mechanical adding machines, saw the necessity of new electrical computers for organising the distribution of cakes and other highly perishable goods. They, therefore, substantially financed the University of Cambridge's Electronic Delay Storage Automatic Calculator (EDSAC) which was the second electronic digital stored-program computer to go into regular service, and built their own programmable digital computers and became the first user of these in businesses, with the LEO I digital computer: the Lyons Electronic Office I, designed and built by Dr John Pinkerton under the able leadership of John Simmons. It handled the company's accounts and logistics. Lyons also included the weather forecast to ensure goods carried by their "fresh produce" delivery vans were not wasted in large quantities.[citation needed][8] Google chairman Eric Schmidt called this "the world's first office computer", built in 1951.[9]
1100) 17.421955, John von Neumann - Wikipedia, the free encyclopedia.txt#70, term: computer, content:While consulting for the Moore School of Electrical Engineering at the University of Pennsylvania on the EDVAC project, von Neumann wrote an incomplete First Draft of a Report on the EDVAC. The paper, whose premature distribution nullified the patent claims of EDVAC designers J. Presper Eckert and John Mauchly, described a computer architecture in which the data and the program are both stored in the computer's memory in the same address space.[127] This architecture is to this day the basis of modern computer design, unlike the earliest computers that were "programmed" using a separate memory device such as a paper tape or plugboard. Although the single-memory, stored program architecture is commonly called von Neumann architecture as a result of von Neumann's paper, the architecture's description was based on the work of J. Presper Eckert and John William Mauchly, inventors of the ENIAC computer at the University of Pennsylvania.[127]
1101) 17.421955, John von Neumann - Wikipedia, the free encyclopedia.txt#71, term: computer, content:John von Neumann also consulted for the ENIAC project. The electronics of the new ENIAC ran at one-sixth the speed, but this in no way degraded the ENIAC's performance, since it was still entirely I/O bound. Complicated programs could be developed and debugged in days rather than the weeks required for plugboarding the old ENIAC. Some of von Neumann's early computer programs have been preserved.[128] The next computer that von Neumann designed was the IAS machine at the Institute for Advanced Study in Princeton, New Jersey. He arranged its financing, and the components were designed and built at the RCA Research Laboratory nearby. John von Neumann recommended that the IBM 701, nicknamed the defense computer include a magnetic drum. It was a faster version of the IAS machine and formed the basis for the commercially successful IBM 704.[129][130]
1102) 17.421955, Microcomputer - Wikipedia, the free encyclopedia.txt#15, term: computer, content:The period from about 1971 to 1976 is sometimes called the first generation of microcomputers. Many companies such as DEC,[16] National Semiconductor,[17] Texas Instruments[18] offered their microcomputers for use in terminal control, peripheral device interface control and industrial machine control. There were also machines for engineering development and hobbyist personal use.[19] In 1975, the Processor Technology SOL-20 was designed, which consisted of one board which included all the parts of the computer system. The SOL-20 had built-in EPROM software which eliminated the need for rows of switches and lights. The MITS Altair just mentioned played an instrumental role in sparking significant hobbyist interest, which itself eventually led to the founding and success of many well-known personal computer hardware and software companies, such as Microsoft and Apple Computer. Although the Altair itself was only a mild commercial success, it helped spark a huge industry.
1103) 17.421955, Motorola 6809 - Wikipedia, the free encyclopedia.txt#12, term: computer, content:The 6809 was used in Commodore's dual-CPU SuperPET computer, and, in its 68A09 incarnation, in the unique vector graphics based Vectrex home video game console with built-in screen display, and was also used in the Milton Bradley Expansion (MBX) system (an arcade console for use with the Texas Instruments TI-99/4A home computer). The 6809E was featured in the TRS-80 Color Computer (CoCo), the Acorn System 2, 3 and 4 computers (as an optional alternative to their standard 6502), the Fujitsu FM-7, the Canon CX-1, the Welsh-made Dragon 32/64 home computers, and the SWTPC, Gimix, Smoke Signal Broadcasting, etc. SS-50 Bus bus systems, in addition to several of Motorola's own EXORmacs and EXORset development systems. In France, Thomson micro-informatique produced a series of micro-computers based on the 6809E (TO7, TO7/70, TO8, TO8D, TO9, TO9Plus, MO5, MO6, MO5E and MO5NR).
1104) 17.421955, Operating system - Wikipedia, the free encyclopedia.txt#87, term: computer, content:Currently most operating systems support a variety of networking protocols, hardware, and applications for using them. This means that computers running dissimilar operating systems can participate in a common network for sharing resources such as computing, files, printers, and scanners using either wired or wireless connections. Networks can essentially allow a computer's operating system to access the resources of a remote computer to support the same functions as it could if those resources were connected directly to the local computer. This includes everything from simple communication, to using networked file systems or even sharing another computer's graphics or sound hardware. Some network services allow the resources of a computer to be accessed transparently, such as SSH which allows networked users direct access to a computer's command line interface.
1105) 17.421955, Personal computer - Wikipedia, the free encyclopedia.txt#33, term: computer, content:"PC" is an initialism for "personal computer". However, it is sometimes used in a different sense, referring to a personal computer with an Intel x86-compatible processor, very often running (but not necessarily limited to) Microsoft Windows, which is a combination sometimes also called Wintel, although large portion of PCs are not shipped with preinstalled Windows operating systems. Some PCs, including the OLPC XOs, are equipped with x86 or x64 processors but not designed to run Microsoft Windows. "PC" is used in contrast with "Mac", an Apple Macintosh computer.[49][50][51][52] This sense of the word is used in the Get a Mac advertisement campaign that ran between 2006 and 2009, as well as its rival, I'm a PC campaign, that appeared in 2008. Since Apple's transition to Intel processors starting 2005, all Macintosh computers are now PCs.[53]
1106) 17.421955, Quantum computing - Wikipedia, the free encyclopedia.txt#0, term: computer, content:Quantum computing studies theoretical computation systems (quantum computers) that make direct use of quantum-mechanical phenomena, such as superposition and entanglement, to perform operations on data.[1] Quantum computers are different from binary digital electronic computers based on transistors. Whereas common digital computing requires that the data are encoded into binary digits (bits), each of which is always in one of two definite states (0 or 1), quantum computation uses quantum bits (qubits), which can be in superpositions of states. A quantum Turing machine is a theoretical model of such a computer, and is also known as the universal quantum computer. Quantum computers share theoretical similarities with non-deterministic and probabilistic computers. The field of quantum computing was initiated by the work of Paul Benioff[2] and Yuri Manin in 1980,[3] Richard Feynman in 1982,[4] and David Deutsch in 1985.[5] A quantum computer with spins as quantum bits was also formulated for use as a quantum spacetime in 1968.[6]
1107) 17.421955, Quantum computing - Wikipedia, the free encyclopedia.txt#5, term: computer, content:A quantum computer with a given number of qubits is fundamentally different from a classical computer composed of the same number of classical bits. For example, to represent the state of an n-qubit system on a classical computer would require the storage of 2n complex coefficients. Although this fact may seem to indicate that qubits can hold exponentially more information than their classical counterparts, care must be taken not to overlook the fact that the qubits are only in a probabilistic superposition of all of their states. This means that when the final state of the qubits is measured, they will only be found in one of the possible configurations they were in before measurement. Moreover, it is incorrect to think of the qubits as only being in one particular state before measurement since the fact that they were in a superposition of states before the measurement was made directly affects the possible outcomes of the computation.
1108) 17.421955, Server (computing) - Wikipedia, the free encyclopedia.txt#6, term: computer, content:Strictly speaking, the term server refers to a computer program or process (running program). Through metonymy, it refers to a device used to (or a device dedicated to) running one or several server programs. On a network, such a device is called a host. In addition to server, the words serve and service (as noun and as verb) are frequently used, though servicer and servant are not.[a] The word service (noun) may refer to either the abstract form of functionality, e.g. Web service. Alternatively, it may refer to a computer program that turns a computer into a server, e.g. Windows service. Originally used as "servers serve users" (and "users use servers"), in the sense "obey", today one often says that "servers serve data", in the same sense as "give". For instance, web servers "serve [up] web pages to users" or "service their requests".
1109) 17.421955, Sound card - Wikipedia, the free encyclopedia.txt#14, term: computer, content:In 1988 a panel of computer-game CEOs stated at the Consumer Electronics Show that the PC's limited sound capability prevented it from becoming the leading home computer, that it needed a $4979 sound card with better capability than current products, and that once such hardware was widely installed their companies would support it. Sierra On-Line, which had pioneered supporting EGA and VGA video, and 3 1/2" disks, that year promised to support AdLib, IBM Music Feature, and Roland MT-32 in its games; the cards cost $195 to $600.[4] A 1989 Computer Gaming World survey found that 18 of 25 game companies planned to support AdLib, six Roland and Covox, and seven Creative Music System/Game Blaster.[5]
1110) 17.421955, SPARC - Wikipedia, the free encyclopedia.txt#30, term: computer, content:Fujitsu's K computer ranked #1 in TOP500  June 2011 and November 2011 lists. It combines 88,128 SPARC64 VIIIfx CPUs, each with eight cores, for a total of 705,024 cores  almost twice as many as any other system in the TOP500 at that time. The K Computer was more powerful than the next five systems on the list combined, and had the highest performance-to-power ratio of any other supercomputer system.[31] It also ranked #6 in Green500  June 2011 list, with a score of 824.56 MFLOPS/W.[32] In the November 2012 release of TOP500, the K computer ranked #3, using by far the most power of the top three.[33] It ranked #85 on the corresponding Green500 release.[34] Newer HPC processors, IXfx and XIfx, were included in recent PRIMEHPC FX10 and FX100 servers.
1111) 17.421955, Theory of computation - Wikipedia, the free encyclopedia.txt#1, term: computer, content:In order to perform a rigorous study of computation, computer scientists work with a mathematical abstraction of computers called a model of computation. There are several models in use, but the most commonly examined is the Turing machine.[2] Computer scientists study the Turing machine because it is simple to formulate, can be analyzed and used to prove results, and because it represents what many consider the most powerful possible "reasonable" model of computation (see ChurchTuring thesis).[3] It might seem that the potentially infinite memory capacity is an unrealizable attribute, but any decidable problem[4] solved by a Turing machine will always require only a finite amount of memory. So in principle, any problem that can be solved (decided) by a Turing machine can be solved by a computer that has a finite amount of memory.
1112) 17.421955, Vacuum tube - Wikipedia, the free encyclopedia.txt#49, term: computer, content:To meet the reliability requirements of the 1951 US digital computer Whirlwind, "special-quality" tubes with extended life, and a long-lasting cathode in particular, were produced. The problem of short lifetime was traced to evaporation of silicon, used in the tungsten alloy to make the heater wire easier to draw. Elimination of silicon from the heater wire alloy (and more frequent replacement of the wire drawing dies) allowed production of tubes that were reliable enough for the Whirlwind project. The tubes developed for Whirlwind were later used in the giant SAGE air-defense computer system. High-purity nickel tubing and cathode coatings free of materials that can poison emission (such as silicates and aluminium) also contribute to long cathode life. The first such "computer tube" was Sylvania's 7AK7 of 1948. By the late 1950s it was routine for special-quality small-signal tubes to last for hundreds of thousands of hours, if operated conservatively. This increased reliability also made mid-cable amplifiers in submarine cables possible.
1113) 17.421955, Version control - Wikipedia, the free encyclopedia.txt#11, term: computer, content:When data that is under revision control is modified, after being retrieved by checking out, this is not in general immediately reflected in the revision control system (in the repository), but must instead be checked in or committed. A copy outside revision control is known as a "working copy". As a simple example, when editing a computer file, the data stored in memory by the editing program is the working copy, which is committed by saving. Concretely, one may print out a document, edit it by hand, and only later manually input the changes into a computer and save it. For source code control, the working copy is instead a copy of all files in a particular revision, generally stored locally on the developer's computer;[note 1] in this case saving the file only changes the working copy, and checking into the repository is a separate step.
1114) 17.421955, Video game - Wikipedia, the free encyclopedia.txt#5, term: computer, content:In 1971, Computer Space, created by Nolan Bushnell and Ted Dabney, was the first commercially sold, coin-operated video game. It used a black-and-white television for its display, and the computer system was made of 74 series TTL chips.[6] The game was featured in the 1973 science fiction film Soylent Green. Computer Space was followed in 1972 by the Magnavox Odyssey, the first home console. Modeled after a late 1960s prototype console developed by Ralph H. Baer called the "Brown Box", it also used a standard television.[2][7] These were followed by two versions of Atari's Pong; an arcade version in 1972 and a home version in 1975 that dramatically increased video game popularity.[8] The commercial success of Pong led numerous other companies to develop Pong clones and their own systems, spawning the video game industry.[9]
1115) 17.421955, Von Neumann architecture - Wikipedia, the free encyclopedia.txt#0, term: computer, content:The von Neumann architecture, also known as the von Neumann model and Princeton architecture, is a computer architecture based on that described in 1945 by the mathematician and physicist John von Neumann and others in the First Draft of a Report on the EDVAC.[1] This describes a design architecture for an electronic digital computer with parts consisting of a processing unit containing an arithmetic logic unit and processor registers, a control unit containing an instruction register and program counter, a memory to store both data and instructions, external mass storage, and input and output mechanisms.[1][2] The meaning has evolved to be any stored-program computer in which an instruction fetch and a data operation cannot occur at the same time because they share a common bus. This is referred to as the von Neumann bottleneck and often limits the performance of the system.[3]
1116) 16.764284, 3D computer graphics software - Wikipedia, the free encyclopedia.txt#7, term: computer, content:After producing video, studios then edit or composite the video using programs such as Adobe Premiere Pro or Final Cut Pro at the mid-level, or Autodesk Combustion, Digital Fusion, Shake at the high-end. Match moving software is commonly used to match live video with computer-generated video, keeping the two in sync as the camera moves.
1117) 16.764284, Algorithm - Wikipedia, the free encyclopedia.txt#4, term: computer, content:An informal definition could be "a set of rules that precisely defines a sequence of operations."[11] which would include all computer programs, including programs that do not perform numeric calculations. Generally, a program is only an algorithm if it stops eventually.[12]
1118) 16.764284, Algorithm - Wikipedia, the free encyclopedia.txt#89, term: computer, content:Turinghis model of computation is now called a Turing machinebegins, as did Post, with an analysis of a human computer that he whittles down to a simple set of basic motions and "states of mind". But he continues a step further and creates a machine as a model of computation of numbers.[92]
1119) 16.764284, Analog computer - Wikipedia, the free encyclopedia.txt#32, term: computer, content:Although the basic technology for analog computers is usually operational amplifiers (also called "continuous current amplifiers" because they have no low frequency limitation), in the 1960s an attempt was made in the French ANALAC computer to use an alternative technology: medium frequency carrier and non dissipative reversible circuits.
1120) 16.764284, Analog computer - Wikipedia, the free encyclopedia.txt#81, term: computer, content:The Simulation Council (or Simulations Council) was an association of analog computer users in USA. It is now known as The Society for Modeling and Simulation International. The Simulation Council newsletters from 1952 to 1963 are available online and show the concerns and technologies at the time, and the common use of analog computers for missilry.[25]
1121) 16.764284, Analog computer - Wikipedia, the free encyclopedia.txt#82, term: computer, content:Computer theorists often refer to idealized analog computers as real computers (because they operate on the set of real numbers). Digital computers, by contrast, must first quantize the signal into a finite number of values, and so can only work with the rational number set (or, with an approximation of irrational numbers).
1122) 16.764284, Antikythera mechanism - Wikipedia, the free encyclopedia.txt#0, term: computer, content:The Antikythera mechanism (/ntkr/ ANT-i-ki-THEER- or /ntkr/ ANT-i-KITH--r) is an ancient analog computer[1][2][3][4] and orrery used to predict astronomical positions and eclipses for calendrical and astrological purposes,[5][6][7] as well as the Olympiads, the cycles of the ancient Olympic Games.[8][9]
1123) 16.764284, Antikythera mechanism - Wikipedia, the free encyclopedia.txt#8, term: computer, content:Generally referred to as the first known analog computer,[20] the quality and complexity of the mechanism's manufacture suggests it has undiscovered predecessors made during the Hellenistic period.[21] Its construction relied upon theories of astronomy and mathematics developed by Greek astronomers, and is estimated to have been created around the late second century BC.[5]
1124) 16.764284, Antikythera wreck - Wikipedia, the free encyclopedia.txt#2, term: computer, content:The wreck yielded numerous statues, coins and other artifacts dating back to the 4th century BC, as well as the severely corroded remnants of a device many regard as the world's oldest known analog computer, the Antikythera mechanism. The ancient works of art including parts of the ship itself are now displayed at the National Archaeological Museum of Athens.
1125) 16.764284, Antivirus software - Wikipedia, the free encyclopedia.txt#38, term: computer, content:One of the few solid theoretical results in the study of computer viruses is Frederick B. Cohen's 1987 demonstration that there is no algorithm that can perfectly detect all possible viruses.[30] However, using different layers of defense, a good detection rate may be achieved.
1126) 16.764284, Antivirus software - Wikipedia, the free encyclopedia.txt#68, term: computer, content:Detecting rootkits is a major challenge for anti-virus programs. Rootkits have full administrative access to the computer and are invisible to users and hidden from the list of running processes in the task manager. Rootkits can modify the inner workings of the operating system[132] and tamper with antivirus programs.
1127) 16.764284, Application software - Wikipedia, the free encyclopedia.txt#5, term: computer, content:The word "application", once used as an adjective, is not restricted to the "of or pertaining to application software" meaning.[5] For example, concepts such as application programming interface (API), application server, application virtualization, application lifecycle management and portable application apply to all computer programs alike, not just application software.
1128) 16.764284, Arcade game - Wikipedia, the free encyclopedia.txt#8, term: computer, content:In 1971 students at Stanford University set up the Galaxy Game, a coin-operated version of the Spacewar video game. This ranks as the earliest known instance of a coin-operated video game. Later in the same year, Nolan Bushnell created the first mass-manufactured game, Computer Space, for Nutting Associates.
1129) 16.764284, Arithmetic - Wikipedia, the free encyclopedia.txt#23, term: computer, content:The product of a and b is written as a  b or ab. When a or b are expressions not written simply with digits, it is also written by simple juxtaposition:ab. In computer programming languages and software packages in which one can only use characters normally found on a keyboard, it is often written with an asterisk:a * b.
1130) 16.764284, ARPANET - Wikipedia, the free encyclopedia.txt#35, term: computer, content:NCP provided a standard set of network services that could be shared by several applications running on a single host computer. This led to the evolution of application protocols that operated, more or less, independently of the underlying network service. When the ARPANET migrated to the Internet protocols in 1983, the major application protocols migrated with it.
1131) 16.764284, Artificial intelligence - Wikipedia, the free encyclopedia.txt#13, term: computer, content:For difficult problems, most of these algorithms can require enormous computational resources  most experience a "combinatorial explosion": the amount of memory or computer time required becomes astronomical when the problem goes beyond a certain size. The search for more efficient problem-solving algorithms is a high priority for AI research.[43]
1132) 16.764284, Artificial intelligence - Wikipedia, the free encyclopedia.txt#25, term: computer, content:Machine perception[75] is the ability to use input from sensors (such as cameras, microphones, tactile sensors, sonar and others more exotic) to deduce aspects of the world. Computer vision[76] is the ability to analyze visual input. A few selected subproblems are speech recognition,[77] facial recognition and object recognition.[78]
1133) 16.764284, Artificial intelligence - Wikipedia, the free encyclopedia.txt#66, term: computer, content:For example, performance at draughts (i.e. checkers) is optimal,[196] performance at chess is super-human and nearing strong super-human (see computer chess:computers versus human) and performance at many everyday tasks (such as recognizing a face or crossing a room without bumping into something) is sub-human.
1134) 16.764284, Assembly language - Wikipedia, the free encyclopedia.txt#64, term: computer, content:Some higher level computer languages, such as C and Borland Pascal, support inline assembly where sections of assembly code, in practice usually brief, can be embedded into the high level language code. The Forth language commonly contains an assembler used in CODE words.
1135) 16.764284, Automaton - Wikipedia, the free encyclopedia.txt#4, term: computer, content:Complex mechanical devices are known to have existed in Hellenistic Greece, though the only surviving example is the Antikythera mechanism, the earliest known analog computer.[10] It is thought to have come originally from Rhodes, where there was apparently a tradition of mechanical engineering; the island was renowned for its automata; to quote Pindar's seventh Olympic Ode:
1136) 16.764284, BASIC - Wikipedia, the free encyclopedia.txt#18, term: computer, content:These languages introduced many extensions to the original home-computer BASIC, such as improved string manipulation and graphics support, access to the file system and additional data types. More important were the facilities for structured programming, including additional control structures and proper subroutines supporting local variables.
1137) 16.764284, Berkeley Software Distribution - Wikipedia, the free encyclopedia.txt#8, term: computer, content:The success of 3BSD was a major factor in the Defense Advanced Research Projects Agency's (DARPA) decision to fund Berkeley's Computer Systems Research Group (CSRG), which would develop a standard Unix platform for future DARPA research in the VLSI Project.
1138) 16.764284, Berkeley Software Distribution - Wikipedia, the free encyclopedia.txt#34, term: computer, content:In 1989, David A. Curry wrote about the differences between BSD and System V. He characterized System V as being often regarded as the "standard Unix." However, he described BSD as more popular among university and government computer centers, due to its advanced features and performance:[19]
1139) 16.764284, Binary number - Wikipedia, the free encyclopedia.txt#17, term: computer, content:The numeric value represented in each case is dependent upon the value assigned to each symbol. In a computer, the numeric values may be represented by two different voltages; on a magnetic disk, magnetic polarities may be used. A "positive", "yes", or "on" state is not necessarily equivalent to the numerical value of one; it depends on the architecture in use.
1140) 16.764284, Bletchley Park - Wikipedia, the free encyclopedia.txt#51, term: computer, content:The National Museum of Computing is housed in Block H, which is rented from the Bletchley Park Trust. Its Colossus and Tunny galleries tell an important part of allied breaking of German codes during World War II. There is a working reconstruction of a Colossus computer that was used on the high-level Lorenz cipher, codenamed Tunny by the British.[121]
1141) 16.764284, Boolean algebra - Wikipedia, the free encyclopedia.txt#96, term: computer, content:A central concept of set theory is membership. Now an organization may permit multiple degrees of membership, such as novice, associate, and full. With sets however an element is either in or out. The candidates for membership in a set work just like the wires in a digital computer: each candidate is either a member or a nonmember, just as each wire is either high or low.
1142) 16.764284, Booting - Wikipedia, the free encyclopedia.txt#44, term: computer, content:Most computers are also capable of booting over a computer network. In this scenario, the operating system is stored on the disk of a server, and certain parts of it are transferred to the client using a simple protocol such as the Trivial File Transfer Protocol (TFTP). After these parts have been transferred, the operating system takes over the control of the booting process.
1143) 16.764284, Brian Randell - Wikipedia, the free encyclopedia.txt#1, term: computer, content:Brian Randell (born 1936) is a British computer scientist, and Emeritus Professor at the School of Computing Science, Newcastle University, U.K. He specializes in research in software fault tolerance and dependability, and is a noted authority on the early prior to 1950 history of computers.
1144) 16.764284, British Computer Society - Wikipedia, the free encyclopedia.txt#0, term: computer, content:The British Computer Society is a professional body and a learned society that represents those working in Information Technology both in the United Kingdom and internationally. Established in 1957, in 2009 it rebranded as BCS  The Chartered Institute for IT, although this has not been reflected in a legal name change.[1]
1145) 16.764284, British Computer Society - Wikipedia, the free encyclopedia.txt#10, term: computer, content:On 21 September 2009, the British Computer Society went through a transformation and re-branded itself as "BCS  The Chartered Institute for IT".[1] In 2010, an Extraordinary General Meeting was called to discuss the direction of the BCS.[4][5] The debate has been covered by the computing press.[6][7][8][9]
1146) 16.764284, Bus (computing) - Wikipedia, the free encyclopedia.txt#13, term: computer, content:Over time, several groups of people worked on various computer bus standards, including the IEEE Bus Architecture Standards Committee (BASC), the IEEE "Superbus" study group, the open microprocessor initiative (OMI), the open microsystems initiative (OMI), the "Gang of Nine" that developed EISA, etc.[citation needed]
1147) 16.764284, Bus (computing) - Wikipedia, the free encyclopedia.txt#18, term: computer, content:Later computer programs began to share memory common to several CPUs. Access to this memory bus had to be prioritized, as well. The simple way to prioritize interrupts or bus access was with a daisy chain. In this case signals will naturally flow through the bus in physical or logical order, eliminating the need for complex scheduling.
1148) 16.764284, Byte - Wikipedia, the free encyclopedia.txt#2, term: computer, content:The term byte was coined by Werner Buchholz in July 1956, during the early design phase for the IBM Stretch[7][8] computer, which had addressing to the bit and variable field length (VFL) instructions with a byte size encoded in the instruction. It is a deliberate respelling of bite to avoid accidental mutation to bit.[1]
1149) 16.764284, Calculator - Wikipedia, the free encyclopedia.txt#4, term: computer, content:Computer operating systems as far back as early Unix have included interactive calculator programs such as dc and hoc, and calculator functions are included in almost all PDA-type devices (save a few dedicated address book and dictionary devices).
1150) 16.764284, Cellular automaton - Wikipedia, the free encyclopedia.txt#11, term: computer, content:In 1969, German computer pioneer Konrad Zuse published his book Calculating Space, proposing that the physical laws of the universe are discrete by nature, and that the entire universe is the output of a deterministic computation on a single cellular automaton; "Zuse's Theory" became the foundation of the field of study called digital physics.[19]
1151) 16.764284, Central processing unit - Wikipedia, the free encyclopedia.txt#19, term: computer, content:The fundamental operation of most CPUs, regardless of the physical form they take, is to execute a sequence of stored instructions that is called a program. The instructions to be executed are kept in some kind of computer memory. Nearly all CPUs follow the fetch, decode and execute steps in their operation, which are collectively known as the instruction cycle.
1152) 16.764284, Central processing unit - Wikipedia, the free encyclopedia.txt#30, term: computer, content:The control unit of the CPU contains circuitry that uses electrical signals to direct the entire computer system to carry out stored program instructions. The control unit does not execute program instructions; rather, it directs other parts of the system to do so. The control unit communicates with both the ALU and memory.
1153) 16.764284, Charles Babbage - Wikipedia, the free encyclopedia.txt#0, term: computer, content:Charles Babbage KH FRS (/bbd/; 26 December 1791 18 October 1871) was an English polymath.[1] A mathematician, philosopher, inventor and mechanical engineer, Babbage, along with Ada Lovelace, is best remembered for originating the concept of a programmable computer.
1154) 16.764284, Charles Babbage - Wikipedia, the free encyclopedia.txt#60, term: computer, content:While Babbage's machines were mechanical and unwieldy, their basic architecture was similar to a modern computer. The data and program memory were separated, operation was instruction-based, the control unit could make conditional jumps, and the machine had a separate I/O unit.[140]
1155) 16.764284, COBOL - Wikipedia, the free encyclopedia.txt#5, term: computer, content:In April 1959, representatives from academia, computer users and manufacturers met at the University of Pennsylvania to organize a formal meeting on common business languages. Representatives among others, included Grace Hopper, inventor of the English-like data processing language FLOW-MATIC, Jean Sammet and Saul Gorn.[15][16]
1156) 16.764284, COBOL - Wikipedia, the free encyclopedia.txt#17, term: computer, content:At a mid-September meeting, the committee discussed the new language's name. Suggestions included "BUSY" (Business System), "INFOSYL" (Information System Language) and "COCOSYL" (Common Computer Systems Language).[47] The name "COBOL" was suggested by Bob Bemer.[48][49]
1157) 16.764284, Colossus computer - Wikipedia, the free encyclopedia.txt#0, term: computer, content:Colossus was the name of a series of computers developed by British codebreakers in 1943-1945 to help in the cryptanalysis of the Lorenz cipher. Colossus used thermionic valves (vacuum tubes) and thyratrons to perform Boolean and counting operations. Colossus is thus regarded[1] as the world's first programmable, electronic, digital computer, although it was programmed by plugs and switches and not by a stored program.
1158) 16.764284, Colossus computer - Wikipedia, the free encyclopedia.txt#3, term: computer, content:The destruction of the Colossus hardware and blueprints, as part of the effort to maintain a project secrecy that was kept up into the 1970s, deprived most of those involved with Colossus of credit for their pioneering advancements in electronic digital computing during their lifetimes. A functioning replica of a Colossus computer was completed in 2007 and is on display at The National Museum of Computing at Bletchley Park.[7]
1159) 16.764284, Command-line interface - Wikipedia, the free encyclopedia.txt#0, term: computer, content:A command-line interface or command language interpreter (CLI), also known as command-line user interface, console user interface,[1] and character user interface (CUI), is a means of interacting with a computer program where the user (or client) issues commands to the program in the form of successive lines of text (command lines).
1160) 16.764284, Command-line interface - Wikipedia, the free encyclopedia.txt#22, term: computer, content:CLIs are often used by programmers and system administrators, in engineering and scientific environments, and by technically advanced personal computer users. CLIs are also popular among people with visual disability, since the commands and responses can be displayed using Refreshable Braille displays.
1161) 16.764284, Command-line interface - Wikipedia, the free encyclopedia.txt#77, term: computer, content:The terms command-line interpreter, command line shell, command language interpreter, or identical abbreviationCLI, are applied to computer programs designed to interpret a sequence of lines of text which may be entered by a user, read from a file or another kind of data stream. The context of interpretation is usually one of a given operating system or programming language.
1162) 16.764284, Compiler - Wikipedia, the free encyclopedia.txt#5, term: computer, content:Software for early computers was primarily written in assembly language. Although the first high level language is nearly as old as the first computer, the limited memory capacity of early computers led to substantial technical challenges when the first compilers were designed.[discuss]
1163) 16.764284, Compiler - Wikipedia, the free encyclopedia.txt#13, term: computer, content:A native or hosted compiler is one which output is intended to directly run on the same type of computer and operating system that the compiler itself runs on. The output of a cross compiler is designed to run on a different platform. Cross compilers are often used when developing software for embedded systems that are not intended to support a software development environment.
1164) 16.764284, Computability theory - Wikipedia, the free encyclopedia.txt#0, term: computer, content:Computability theory, also called recursion theory, is a branch of mathematical logic, of computer science, and of the theory of computation that originated in the 1930s with the study of computable functions and Turing degrees. The field has since grown to include the study of generalized computability and definability. In these areas, recursion theory overlaps with proof theory and effective descriptive set theory.
1165) 16.764284, Computable function - Wikipedia, the free encyclopedia.txt#8, term: computer, content:Equivalently, computable functions can be formalized as functions which can be calculated by an idealized computing agent such as a Turing machine or a register machine. Formally speaking, a partial function     f :   N   k     N    {\displaystyle f:\mathbb {N} ^{k}\rightarrow \mathbb {N} }   can be calculated if and only if there exists a computer program with the following properties:
1166) 16.764284, Computational science - Wikipedia, the free encyclopedia.txt#3, term: computer, content:Scientists and engineers develop computer programs, application software, that model systems being studied and run these programs with various sets of input parameters. In some cases, these models require massive amounts of calculations (usually floating-point) and are often executed on supercomputers or distributed computing platforms.
1167) 16.764284, Computer - Simple English Wikipedia, the free encyclopedia.txt#2, term: computer, content:A computer user can control it by a user interface. Input devices include keyboard, mouse, buttons, touch screen. Some very new computers can also be controlled with voice commands or hand gestures or even brain signals through electrodes implanted in the brain or along nerves.
1168) 16.764284, Computer - Simple English Wikipedia, the free encyclopedia.txt#22, term: computer, content:Nearly all modern computers use the stored-program architecture in some form. It has become the main concept which defines a modern computer. Most of the technologies used to build computers have changed since the 1940s, but many current computers still use the von-Neumann architecture.
1169) 16.764284, Computer - Wikipedia, the free encyclopedia.txt#29, term: computer, content:Von Neumann acknowledged that the central concept of the modern computer was due to this paper.[35] Turing machines are to this day a central object of study in theory of computation. Except for the limitations imposed by their finite memory stores, modern computers are said to be Turing-complete, which is to say, they have algorithm execution capability equivalent to a universal Turing machine.
1170) 16.764284, Computer - Wikipedia, the free encyclopedia.txt#39, term: computer, content:The defining feature of modern computers which distinguishes them from all other machines is that they can be programmed. That is to say that some type of instructions (the program) can be given to the computer, and it will process them. Modern computers based on the von Neumann architecture often have machine code in the form of an imperative programming language.
1171) 16.764284, Computer - Wikipedia, the free encyclopedia.txt#50, term: computer, content:Though considerably easier than in machine language, writing long programs in assembly language is often difficult and is also error prone. Therefore, most practical programs are written in more abstract high-level programming languages that are able to express the needs of the programmer more conveniently (and thereby help reduce programmer error). High level languages are usually "compiled" into machine language (or sometimes into assembly language and then into machine language) using another computer program called a compiler.[57] High level languages are less related to the workings of the target computer than assembly language, and more related to the language and structure of the problem(s) to be solved by the final program. It is therefore often possible to use different compilers to translate the same high level language program into the machine language of many different types of computer. This is part of the means by which software like video games may be made available for different computer architectures such as personal computers and various video game consoles.
1172) 16.764284, Computer - Wikipedia, the free encyclopedia.txt#57, term: computer, content:The control unit (often called a control system or central controller) manages the computer's various components; it reads and interprets (decodes) the program instructions, transforming them into control signals that activate other parts of the computer.[60] Control systems in advanced computers may change the order of execution of some instructions to improve performance.
1173) 16.764284, Computer - Wikipedia, the free encyclopedia.txt#97, term: computer, content:When unprocessed data is sent to the computer with the help of input devices, the data is processed and sent to output devices. The input devices may be hand-operated or automated. The act of processing is mainly regulated by the CPU. Some examples of hand-operated input devices are:
1174) 16.764284, Computer animation - Wikipedia, the free encyclopedia.txt#6, term: computer, content:Early digital computer animation was developed at Bell Telephone Laboratories in the 1960s by Edward E. Zajac, Frank W. Sinden, Kenneth C. Knowlton, and A. Michael Noll. Other digital animation was also practiced at the Lawrence Livermore National Laboratory.[3]
1175) 16.764284, Computer animation - Wikipedia, the free encyclopedia.txt#26, term: computer, content:In 2D computer animation, moving objects are often referred to as "sprites." A sprite is an image that has a location associated with it. The location of the sprite is changed slightly, between each displayed frame, to make the sprite appear to move.[37] The following pseudocode makes a sprite move from left to right:
1176) 16.764284, Computer animation - Wikipedia, the free encyclopedia.txt#31, term: computer, content:Computer-generated animation is known as 3-dimensional (3D) animation. Creators will design an object or character with an X,Y and Z axis. Unlike the traditional way of animation no pencil to paper drawings create the way computer generated animation works. The object or character created will then be taken into a software, key framing and tweening are also carried out in computer generated animation but are also a lot of techniques used that do not relate to traditional animation. Animators can break physical laws by using mathematical algorithms to cheat, mass, force and gravity rulings. Fundamentally, time scale and quality could be said to be a preferred way to produce animation as they are two major things that are enhanced by using computer generated animation. Another great aspect of CGA is the fact you can create a flock of creatures to act independently when created as a group. An animal's fur can be programmed to wave in the wind and lie flat when it rains instead of programming each strand of hair separately.[38]
1177) 16.764284, Computer architecture - Wikipedia, the free encyclopedia.txt#5, term: computer, content:Brooks went on to help develop the IBM System/360 (now called the IBM zSeries) line of computers, in which architecture became a noun defining what the user needs to know. Later, computer users came to use the term in many less-explicit ways.
1178) 16.764284, Computer architecture - Wikipedia, the free encyclopedia.txt#22, term: computer, content:Counting machine language instructions would be misleading because they can do varying amounts of work in different ISAs. The "instruction" in the standard measurements is not a count of the ISA's actual machine language instructions, but a unit of measurement, usually based on the speed of the VAX computer architecture.
1179) 16.764284, Computer architecture - Wikipedia, the free encyclopedia.txt#29, term: computer, content:Power consumption is another measurement that is important in modern computers. Power efficiency can often be traded for speed or lower cost. The typical measurement when referring to power consumption in Computer Architecture is MIPS/W (millions of instructions per second per watt).
1180) 16.764284, Computer cluster - Wikipedia, the free encyclopedia.txt#6, term: computer, content:Computer clustering relies on a centralized management approach which makes the nodes available as orchestrated shared servers. It is distinct from other approaches such as peer to peer or grid computing which also use many nodes, but with a far more distributed nature.[6]
1181) 16.764284, Computer cluster - Wikipedia, the free encyclopedia.txt#11, term: computer, content:The first production system designed as a cluster was the Burroughs B5700 in the mid-1960s. This allowed up to four computers, each with either one or two processors, to be tightly coupled to a common disk storage subsystem in order to distribute the workload. Unlike standard multiprocessor systems, each computer could be restarted without disrupting overall operation.
1182) 16.764284, Computer cluster - Wikipedia, the free encyclopedia.txt#24, term: computer, content:As the computer clusters were appearing during the 1980s, so were supercomputers. One of the elements that distinguished the three classes at that time was that the early supercomputers relied on shared memory. To date clusters do not typically use physically shared memory, while many supercomputer architectures have also abandoned it.
1183) 16.764284, Computer cluster - Wikipedia, the free encyclopedia.txt#36, term: computer, content:The development and debugging of parallel programs on a cluster requires parallel language primitives as well as suitable tools such as those discussed by the High Performance Debugging Forum (HPDF) which resulted in the HPD specifications.[22][30] Tools such as TotalView were then developed to debug parallel implementations on computer clusters which use MPI or PVM for message passing.
1184) 16.764284, Computer cluster - Wikipedia, the free encyclopedia.txt#43, term: computer, content:Although most computer clusters are permanent fixtures, attempts at flash mob computing have been made to build short-lived clusters for specific computations. However, larger scale volunteer computing systems such as BOINC-based systems have had more followers.
1185) 16.764284, Computer data storage - Wikipedia, the free encyclopedia.txt#36, term: computer, content:An uninterruptible power supply (UPS) can be used to give a computer a brief window of time to move information from primary volatile storage into non-volatile storage before the batteries are exhausted. Some systems, for example EMC Symmetrix, have integrated batteries that maintain volatile storage for several minutes.
1186) 16.764284, Computer data storage - Wikipedia, the free encyclopedia.txt#39, term: computer, content:As early as 2006, notebook and desktop computer manufacturers started using flash-based solid-state drives (SSDs) as default configuration options for the secondary storage either in addition to or instead of the more traditional HDD.[15][16][17][18][19]
1187) 16.764284, Computer engineering - Wikipedia, the free encyclopedia.txt#8, term: computer, content:Computer engineers work in coding, cryptography, and information protection to develop new methods for protecting various information, such as digital images and music, fragmentation, copyright infringement and other forms of tampering. Examples include work on wireless communications, multi-antenna systems, optical transmission, and digital watermarking.[9]
1188) 16.764284, Computer engineering - Wikipedia, the free encyclopedia.txt#16, term: computer, content:This specialty of computer engineering requires adequate knowledge of electronics and electrical systems. Engineers working in this area work on enhancing the speed, reliability, and energy efficiency of next-generation very-large-scale integrated (VLSI) circuits and microsystems. An example of this specialty is work done on reducing the power consumption of VLSI algorithms and architecture.[9]
1189) 16.764284, Computer graphics - Wikipedia, the free encyclopedia.txt#28, term: computer, content:CGI became ubiquitous in earnest during this era. Video games and CGI cinema had spread the reach of computer graphics to the mainstream by the late 1990s, and continued to do so at an accelerated pace in the 2000s. CGI was also adopted en masse for television advertisements widely in the late 1990s and 2000s, and so became familiar to a massive audience.
1190) 16.764284, Computer graphics - Wikipedia, the free encyclopedia.txt#32, term: computer, content:In scientific computing, the GPGPU technique to pass large amounts of data bidirectionally between a GPU and CPU was invented; speeding up analysis on many kinds of bioinformatics and molecular biology experiments. The technique has also been used for Bitcoin mining and has applications in computer vision.
1191) 16.764284, Computer graphics - Wikipedia, the free encyclopedia.txt#39, term: computer, content:A large form of digital art being pixel art is created through the use of raster graphics software, where images are edited on the pixel level. Graphics in most old (or relatively limited) computer and video games, graphing calculator games, and many mobile phone games are mostly pixel art.
1192) 16.764284, Computer keyboard - Wikipedia, the free encyclopedia.txt#0, term: computer, content:In computing, a computer keyboard is a typewriter-style device which uses an arrangement of buttons or keys to act as a mechanical lever or electronic switch. Following the decline of punch cards and paper tape, interaction via teleprinter-style keyboards became the main input device for computers.
1193) 16.764284, Computer keyboard - Wikipedia, the free encyclopedia.txt#47, term: computer, content:In the mid-1970s, lower-cost direct-contact key switches were introduced, but their life in switch cycles was much shorter (rated ten million cycles) because they were open to the environment. This became more acceptable, however, for use in computer terminals at the time, which began to see increasingly shorter model lifespans as they advanced.
1194) 16.764284, Computer memory - Wikipedia, the free encyclopedia.txt#4, term: computer, content:In the early 1940s, memory technology mostly permitted a capacity of a few bytes. The first electronic programmable digital computer, the ENIAC, using thousands of octal-base radio vacuum tubes, could perform simple calculations involving 20 numbers of ten decimal digits which were held in the vacuum tube accumulators.
1195) 16.764284, Computer memory - Wikipedia, the free encyclopedia.txt#14, term: computer, content:Proper management of memory is vital for a computer system to operate properly. Modern operating systems have complex systems to properly manage memory. Failure to do so can lead to bugs, slow performance, and at worst case, takeover by viruses and malicious software.
1196) 16.764284, Computer memory - Wikipedia, the free encyclopedia.txt#17, term: computer, content:In early computer systems, programs typically specified the location to write memory and what data to put there. This location was a physical location on the actual memory hardware. The slow processing of such computers did not allow for the complex memory management systems used today. Also, as most such systems were single-task, sophisticated systems were not required as much.
1197) 16.764284, Computer monitor - Wikipedia, the free encyclopedia.txt#13, term: computer, content:On two-dimensional display devices such as computer monitors the display size or viewable image size is the actual amount of screen space that is available to display a picture, video or working space, without obstruction from the case or other aspects of the unit's design. The main measurements for display devices are: width, height, total area and the diagonal.
1198) 16.764284, Computer mouse - Wikipedia, the free encyclopedia.txt#22, term: computer, content:The German company Telefunken published on their early ball mouse on October 2, 1968.[11] Telefunken's mouse was sold as optional equipment for their computer systems. Bill English, builder of Engelbart's original mouse,[26] created a ball mouse in 1972 while working for Xerox PARC.[27]
1199) 16.764284, Computer mouse - Wikipedia, the free encyclopedia.txt#75, term: computer, content:The Macintosh design,[71] commercially successful and technically influential, led many other vendors to begin producing mice or including them with their other computer products (by 1986, Atari ST, Amiga, Windows 1.0, GEOS for the Commodore 64, and the Apple IIGS).[72]
1200) 16.764284, Computer mouse - Wikipedia, the free encyclopedia.txt#81, term: computer, content:Due to their similarity to the WIMP desktop metaphor interface for which mice were originally designed, and to their own tabletop game origins, computer strategy games are most commonly played with mice. In particular, real-time strategy and MOBA games usually require the use of a mouse.
1201) 16.764284, Computer multitasking - Wikipedia, the free encyclopedia.txt#4, term: computer, content:In the early days of computing, CPU time was expensive, and peripherals were very slow. When the computer ran a program that needed access to a peripheral, the central processing unit (CPU) would have to stop executing program instructions while the peripheral processed the data. This was usually very inefficient.
1202) 16.764284, Computer music - Wikipedia, the free encyclopedia.txt#7, term: computer, content:Interesting sounds must have a fluidity and changeability that allows them to remain fresh to the ear. In computer music this subtle ingredient is bought at a high computational cost, both in terms of the number of items requiring detail in a score and in the amount of interpretive work the instruments must produce to realize this detail in sound.[11]
1203) 16.764284, Computer music - Wikipedia, the free encyclopedia.txt#28, term: computer, content:OMax is a software environment developed in IRCAM. OMax uses OpenMusic and Max. It is based on researches on stylistic modeling carried out by Gerard Assayag and Shlomo Dubnov and on researches on improvisation with the computer by G. Assayag, M. Chemillier and G. Bloch (a.k.a. the OMax Brothers) in the Ircam Music Representations group.[citation needed]
1204) 16.764284, Computer network - Wikipedia, the free encyclopedia.txt#31, term: computer, content:Network topology is the layout or organizational hierarchy of interconnected nodes of a computer network. Different network topologies can affect throughput, but reliability is often more critical. With many technologies, such as bus networks, a single failure can cause the network to fail entirely. In general the more interconnections there are, the more robust the network is; but the more expensive it is to install.
1205) 16.764284, Computer network - Wikipedia, the free encyclopedia.txt#61, term: computer, content:A backbone network is part of a computer network infrastructure that provides a path for the exchange of information between different LANs or sub-networks. A backbone can tie together diverse networks within the same building, across different buildings, or over a wide area.
1206) 16.764284, Computer network - Wikipedia, the free encyclopedia.txt#95, term: computer, content:Network surveillance is the monitoring of data being transferred over computer networks such as the Internet. The monitoring is often done surreptitiously and may be done by or at the behest of governments, by corporations, criminal organizations, or individuals. It may or may not be legal and may or may not require authorization from a court or other independent agency.
1207) 16.764284, Computer program - Wikipedia, the free encyclopedia.txt#3, term: computer, content:The earliest programmable machines preceded the invention of the digital computer. In 1801, Joseph-Marie Jacquard devised a loom that would weave a pattern by following a series of perforated cards. Patterns, including flowers and leaves, could be weaved and repeated by arranging the cards.[3]
1208) 16.764284, Computer programming - Wikipedia, the free encyclopedia.txt#15, term: computer, content:Following a consistent programming style often helps readability. However, readability is more than just programming style. Many factors, having little or nothing to do with the ability of the computer to efficiently compile and execute the code, contribute to readability.[17] Some of these factors include:
1209) 16.764284, Computer science - Wikipedia, the free encyclopedia.txt#29, term: computer, content:Computational science (or scientific computing) is the field of study concerned with constructing mathematical models and quantitative analysis techniques and using computers to analyze and solve scientific problems. In practical use, it is typically the application of computer simulation and other forms of computation to problems in various scientific disciplines.
1210) 16.764284, Computer security - Wikipedia, the free encyclopedia.txt#11, term: computer, content:Privilege escalation describes a situation where an attacker with some level of restricted access is able to, without authorization, elevate their privileges or access level. So for example a standard computer user may be able to fool the system into giving them access to restricted data; or even to "become root" and have full unrestricted access to a system.
1211) 16.764284, Computer security - Wikipedia, the free encyclopedia.txt#34, term: computer, content:As with physical security, the motivations for breaches of computer security vary between attackers. Some are thrill-seekers or vandals, others are activists or criminals looking for financial gain. State-sponsored attackers are now common and well resourced, but started with amateurs such as Markus Hess who hacked for the KGB, as recounted by Clifford Stoll, in The Cuckoo's Egg.
1212) 16.764284, Computer security - Wikipedia, the free encyclopedia.txt#60, term: computer, content:In early 2007, American apparel and home goods company TJX announced that it was the victim of an unauthorized computer systems intrusion[81] and that the hackers had accessed a system that stored data on credit card, debit card, check, and merchandise return transactions.[82]
1213) 16.764284, Computer security - Wikipedia, the free encyclopedia.txt#61, term: computer, content:The computer worm known as Stuxnet reportedly ruined almost one-fifth of Iran's nuclear centrifuges[83] by disrupting industrial programmable logic controllers (PLCs) in a targeted attack generally believed to have been launched by Israel and the United States[84][85][86][87] although neither has publicly acknowledged this.
1214) 16.764284, Computer security - Wikipedia, the free encyclopedia.txt#62, term: computer, content:In early 2013, massive breaches of computer security by the NSA were revealed, including deliberately inserting a backdoor in a NIST standard for encryption[88] and tapping the links between Google's data centres.[89] These were disclosed by NSA contractor Edward Snowden.[90]
1215) 16.764284, Computer simulation - Wikipedia, the free encyclopedia.txt#9, term: computer, content:Even small errors in the original data can accumulate into substantial error later in the simulation. While all computer analysis is subject to the "GIGO" (garbage in, garbage out) restriction, this is especially true of digital simulation. Indeed, observation of this inherent, cumulative error in digital systems was the main catalyst for the development of chaos theory.
1216) 16.764284, Computer-aided design - Wikipedia, the free encyclopedia.txt#6, term: computer, content:Starting around the mid 1970s, as computer aided design systems began to provide more capability than just an ability to reproduce manual drafting with electronic drafting, the cost benefit for companies to switch to CAD became apparent. The benefit of CAD systems over manual drafting are the capabilities one often takes for granted from computer systems today; automated generation of Bill of Material, auto layout in integrated circuits, interference checking, and many others. Eventually CAD provided the designer with the ability to perform engineering calculations. During this transition, calculations were still performed either by hand or by those individuals who could run computer programs. CAD was a revolutionary change in the engineering industry, where draftsmen, designers and engineering roles begin to merge. It did not eliminate departments, as much as it merged departments and empowered draftsman, designers and engineers. CAD is just another example of the pervasive effect computers were beginning to have on industry. Current computer-aided design software packages range from 2D vector-based drafting systems to 3D solid and surface modelers. Modern CAD packages can also frequently allow rotations in three dimensions, allowing viewing of a designed object from any desired angle, even from the inside looking out. Some CAD software is capable of dynamic mathematical modeling, in which case it may be marketed as CAD.
1217) 16.764284, Computer-aided design - Wikipedia, the free encyclopedia.txt#9, term: computer, content:CAD has become an especially important technology within the scope of computer-aided technologies, with benefits such as lower product development costs and a greatly shortened design cycle. CAD enables designers to layout and develop work on screen, print it out and save it for future editing, saving time on their drawings.
1218) 16.764284, Computer-aided design - Wikipedia, the free encyclopedia.txt#34, term: computer, content:CAD implementations have evolved dramatically since then. Initially, with 3D in the 1970s, it was typically limited to producing drawings similar to hand-drafted drawings. Advances in programming and computer hardware,[25][26] notably solid modeling in the 1980s, have allowed more versatile applications of computers in design activities.
1219) 16.764284, Computing - Wikipedia, the free encyclopedia.txt#20, term: computer, content:A computer network, often simply referred to as a network, is a collection of hardware components and computers interconnected by communication channels that allow sharing of resources and information.[6] Where at least one process in one device is able to send/receive data to/from at least one process residing in a remote device, then the two devices are said to be in a network.
1220) 16.764284, Computing - Wikipedia, the free encyclopedia.txt#25, term: computer, content:A user is an agent, either a human agent (end-user) or software agent, who uses a computer or network service. A user often has a user account and is identified by a username (also user name), screen name (also screenname), nickname (also nick), or handle, which derives from the identical Citizen's Band radio term.
1221) 16.764284, Computing - Wikipedia, the free encyclopedia.txt#37, term: computer, content:"Information systems (IS)" is the study of complementary networks of hardware and software (see information technology) that people and organizations use to collect, filter, process, create, and distribute data.[17][18][19][20][21] Computing Careers says on their website that "A majority of IS programs are located in business schools; however, they may have different names such as management information systems, computer information systems, or business information systems. All IS degrees combine business and computing topics, but the emphasis between technical and organizational issues varies among programs. For example, programs differ substantially in the amount of programming required."[22] The study bridges business and computer science using the theoretical foundations of information and computation to study various business models and related algorithmic processes within a computer science discipline.[23][24][25][26][27][28][29][30][31][32] Computer Information System(s) (CIS) is a field studying computers and algorithmic processes, including their principles, their software and hardware designs, their applications, and their impact on society[33][34][35] while IS emphasizes functionality over design.[36]
1222) 16.764284, Conditional (computer programming) - Wikipedia, the free encyclopedia.txt#0, term: computer, content:In computer science, conditional statements, conditional expressions and conditional constructs are features of a programming language, which perform different computations or actions depending on whether a programmer-specified boolean condition evaluates to true or false. Apart from the case of branch predication, this is always achieved by selectively altering the control flow based on some condition.
1223) 16.764284, Control flow - Wikipedia, the free encyclopedia.txt#0, term: computer, content:In computer science, control flow (or alternatively, flow of control) is the order in which individual statements, instructions or function calls of an imperative program are executed or evaluated. The emphasis on explicit control flow distinguishes an imperative programming language from a declarative programming language.
1224) 16.764284, Control flow - Wikipedia, the free encyclopedia.txt#15, term: computer, content:In the 1950s, computer memories were very small by current standards so subroutines were used primarily[citation needed] to reduce program size; a piece of code was written once and then used many times from various other places in the program.
1225) 16.764284, Control system - Wikipedia, the free encyclopedia.txt#30, term: computer, content:Control engineering in many applications produces control systems that are more complex than PID control. Examples of such fields include fly-by-wire aircraft control systems, chemical plants, and oil refineries. Model predictive control systems are designed using specialized computer-aided-design software and empirical mathematical models of the system to be controlled.
1226) 16.764284, Database - Wikipedia, the free encyclopedia.txt#9, term: computer, content:Databases and DBMSs can be categorized according to the database model(s) that they support (such as relational or XML), the type(s) of computer they run on (from a server cluster to a mobile phone), the query language(s) used to access the database (such as SQL or XQuery), and their internal engineering, which affects performance, scalability, resilience, and security.
1227) 16.764284, Database - Wikipedia, the free encyclopedia.txt#73, term: computer, content:Database security deals with all various aspects of protecting the database content, its owners, and its users. It ranges from protection from intentional unauthorized database uses to unintentional database accesses by unauthorized entities (e.g., a person or a computer program).
1228) 16.764284, DEC Alpha - Wikipedia, the free encyclopedia.txt#46, term: computer, content:Alpha was also implemented in the Piranha, a research prototype developed by Compaq's Corporate Research and Nonstop Hardware Development groups at the Western Research Laboratory and Systems Research Center. Piranha was a multicore design for transaction processing workloads that contained eight simple cores. It was described at the 27th Annual International Symposium on Computer Architecture in June 2000.[11]
1229) 16.764284, Desktop computer - Wikipedia, the free encyclopedia.txt#2, term: computer, content:Prior to the widespread use of microprocessors, a computer that could fit on a desk was considered remarkably small; the type of computers most commonly used were minicomputers, which were themselves desk-sized. Early computers took up the space of a whole room. Minicomputers generally fit into one or a few refrigerator-sized racks.
1230) 16.764284, Differential equation - Wikipedia, the free encyclopedia.txt#18, term: computer, content:A partial differential equation (PDE) is a differential equation that contains unknown multivariable functions and their partial derivatives. (This is in contrast to ordinary differential equations, which deal with functions of a single variable and their derivatives.) PDEs are used to formulate problems involving functions of several variables, and are either solved by hand, or used to create a relevant computer model.
1231) 16.764284, Digital electronics - Wikipedia, the free encyclopedia.txt#7, term: computer, content:Purely electronic circuit elements soon replaced their mechanical and electromechanical equivalents, at the same time that digital calculation replaced analog. The bipolar junction transistor was invented in 1947. From 1955 onwards transistors replaced vacuum tubes in computer designs, giving rise to the "second generation" of computers.
1232) 16.764284, Digital electronics - Wikipedia, the free encyclopedia.txt#9, term: computer, content:At the University of Manchester, a team under the leadership of Tom Kilburn designed and built a machine using the newly developed transistors instead of valves.[5] Their first transistorised computer and the first in the world, was operational by 1953, and a second version was completed there in April 1955.
1233) 16.764284, Digital electronics - Wikipedia, the free encyclopedia.txt#14, term: computer, content:Computer-controlled digital systems can be controlled by software, allowing new functions to be added without changing hardware. Often this can be done outside of the factory by updating the product's software. So, the product's design errors can be corrected after the product is in a customer's hands.
1234) 16.764284, Digital electronics - Wikipedia, the free encyclopedia.txt#65, term: computer, content:Writing and debugging tool flows is an established engineering specialty in companies that produce digital designs. The tool flow usually terminates in a detailed computer file or set of files that describe how to physically construct the logic. Often it consists of instructions to draw the transistors and wires on an integrated circuit or a printed circuit board.
1235) 16.764284, Digital Equipment Corporation - Wikipedia, the free encyclopedia.txt#98, term: computer, content:The X Window System, the network transparent window system used on UNIX and Linux, and also available on other operating systems, was developed at MIT jointly between Project Athena and the Laboratory for Computer Science. Digital was the primary sponsor for this project, which was a contemporary of the GNU Project but not associated with it.
1236) 16.764284, Digital photography - Wikipedia, the free encyclopedia.txt#0, term: computer, content:Digital photography is a form of photography that uses cameras containing arrays of electronic photodetectors to capture images focused by a lens, as opposed to an exposure on photographic film. The captured images are digitized and stored as a computer file ready for further digital processing, viewing, digital publishing or printing.
1237) 16.764284, Digital photography - Wikipedia, the free encyclopedia.txt#1, term: computer, content:Until the advent of such technology, photographs were made by exposing light sensitive photographic film and paper, which were processed in liquid chemical solutions to develop and stabilize the image. Digital photographs are typically created solely by and computer-based photoelectric and mechanical techniques, without wet bath chemical processing.
1238) 16.764284, Digital photography - Wikipedia, the free encyclopedia.txt#2, term: computer, content:Digital photography is one of several forms of digital imaging. Digital images are also created by non-photographic equipment such as computer tomography scanners and radio telescopes. Digital images can also be made by scanning other printed photographic images or negatives.
1239) 16.764284, Digital photography - Wikipedia, the free encyclopedia.txt#10, term: computer, content:Except for some linear array type of cameras at the highest-end and simple web cams at the lowest-end, a digital memory device (usually a memory card; floppy disks and CD-RWs are less common) is used for storing images, which may be transferred to a computer later.
1240) 16.764284, Digital photography - Wikipedia, the free encyclopedia.txt#34, term: computer, content:Other areas of progress include improved sensors, more powerful software, advanced camera processors (sometimes using more than one processor, e.g., the Canon 7d camera has 2 Digic 4 processors), enlarged gamut displays, built in GPS & WiFi, and computer controlled lighting.
1241) 16.764284, Digital signal - Wikipedia, the free encyclopedia.txt#7, term: computer, content:In computer architecture and other digital systems, a waveform that switches between two voltage levels (or less commonly, other waveforms) representing the two states of a Boolean value (0 and 1, or Low and High, or false and true) is referred to as a digital signal or logic signal or binary signal when it is interpreted in terms of only two possible digits.
1242) 16.764284, DNA computing - Wikipedia, the free encyclopedia.txt#15, term: computer, content:Aran Nayebi[18] has provided a general implementation of Strassen's matrix multiplication algorithm on a DNA computer, although there are problems with scaling. In addition, Caltech researchers have created a circuit made from 130 unique DNA strands, which is able to calculate the square root of numbers up to 15.[19]
1243) 16.764284, Drum memory - Wikipedia, the free encyclopedia.txt#2, term: computer, content:Drums were displaced as primary computer memory by magnetic core memory which was faster (no moving parts), less expensive and more dense.[citation needed] Similarly, drums were replaced by hard disk drives for secondary storage, which were also less expensive and more dense. The manufacture of drums ceased in the 1970s.
1244) 16.764284, EDVAC - Wikipedia, the free encyclopedia.txt#1, term: computer, content:ENIAC inventors John Mauchly and J. Presper Eckert proposed the EDVAC's construction in August 1944. A contract to build the new computer was signed in April 1946 with an initial budget of US$100,000. EDVAC was delivered to the Ballistics Research Laboratory in August 1949.
1245) 16.764284, EDVAC - Wikipedia, the free encyclopedia.txt#2, term: computer, content:Functionally, EDVAC was a binary serial computer with automatic addition, subtraction, multiplication, programmed division and automatic checking with an ultrasonic serial memory[1] capacity of 1,000 44-bit words. EDVAC's average addition time was 864 microseconds and its average multiplication time was 2,900 microseconds.
1246) 16.764284, EDVAC - Wikipedia, the free encyclopedia.txt#6, term: computer, content:The EDVAC was a binary serial computer with automatic addition, subtraction, multiplication, programmed division and automatic checking with an ultrasonic serial memory[1] capacity of 1,000 44-bit words (later set to 1,024 words, thus giving a memory, in modern terms, of 5.5 kilobytes).
1247) 16.764284, EDVAC - Wikipedia, the free encyclopedia.txt#9, term: computer, content:The computer had almost 6,000 vacuum tubes and 12,000 diodes, and consumed 56 kW of power. It covered 490ft (45.5 m) of floor space and weighed 17,300lb (7,850kg). The full complement of operating personnel was thirty people per eight-hour shift.
1248) 16.764284, Electrical engineering - Wikipedia, the free encyclopedia.txt#21, term: computer, content:Electrical engineering has many subdisciplines, the most common of which are listed below. Although there are electrical engineers who focus exclusively on one of these subdisciplines, many deal with a combination of them. Sometimes certain fields, such as electronic engineering and computer engineering, are considered separate disciplines in their own right.
1249) 16.764284, Electrical engineering - Wikipedia, the free encyclopedia.txt#50, term: computer, content:Fundamental to the discipline are the sciences of physics and mathematics as these help to obtain both a qualitative and quantitative description of how such systems will work. Today most engineering work involves the use of computers and it is commonplace to use computer-aided design programs when designing electrical systems. Nevertheless, the ability to sketch ideas is still invaluable for quickly communicating with others.
1250) 16.764284, Electronic literature - Wikipedia, the free encyclopedia.txt#4, term: computer, content:A gradual transition into the digital world beginning with new advancements in technology to makes things more efficient and accessible. This is comparable to the release of the printing press in the 15th century, as people did not consider it a major contributor to literature at first. In the 1960s and 1970s, the creation of the personal computer allowed people to begin expanding literature into the electronic realm.
1251) 16.764284, Email client - Wikipedia, the free encyclopedia.txt#1, term: computer, content:Client is meant to be a role. For example, a web application which provides message management, composition, and reception functions may internally act as an email client; as a whole, it is commonly referred to as webmail. Likewise, email client may be referred to a piece of computer hardware or software whose primary or most visible role is to work as an email client.
1252) 16.764284, Embedded system - Wikipedia, the free encyclopedia.txt#17, term: computer, content:Embedded systems are designed to do some specific task, rather than be a general-purpose computer for multiple tasks. Some also have real-time performance constraints that must be met, for reasons such as safety and usability; others may have low or no performance requirements, allowing the system hardware to be simplified to reduce costs.
1253) 16.764284, Embedded system - Wikipedia, the free encyclopedia.txt#20, term: computer, content:Embedded systems range from no user interface at all, in systems dedicated only to one task, to complex graphical user interfaces that resemble modern computer desktop operating systems. Simple embedded devices use buttons, LEDs, graphic or character LCDs (HD44780 LCD for example) with a simple menu system.
1254) 16.764284, Fairchild Semiconductor - Wikipedia, the free encyclopedia.txt#11, term: computer, content:By 1965, Fairchild's process improvements had brought low-cost manufacturing to the semiconductor industry making Fairchild nearly the only profitable semiconductor manufacturer in the United States. Fairchild dominated the market in DTL, op-amps and mainframe computer custom circuits.
1255) 16.764284, Fairchild Semiconductor - Wikipedia, the free encyclopedia.txt#27, term: computer, content:In 1976, the company released the first video game system to use ROM cartridges, the Fairchild Video Entertainment System (or VES) later renamed Channel F, using the F8 microprocessor. The system was successful initially, but quickly lost popularity when the Atari 2600 Video Computer System (or VCS) was released.
1256) 16.764284, Ferranti - Wikipedia, the free encyclopedia.txt#0, term: computer, content:Ferranti or Ferranti International plc was a UK electrical engineering and equipment firm that operated for over a century from 1885 until it went bankrupt in 1993 (the Belgian subsidiary lives on as Ferranti Computer Systems and is now part of the Nijkerk Holding). Known primarily for defence electronics, the company was once a constituent of the FTSE 100 Index.
1257) 16.764284, Ferranti Mercury - Wikipedia, the free encyclopedia.txt#2, term: computer, content:One team decided to produce a much smaller and more cost-effective system built entirely with transistors. It first ran in November 1953 and is believed to be the first entirely transistor-based computer. Metropolitan-Vickers later built this commercially as the Metrovick 950, delivering seven.
1258) 16.764284, Ferranti Pegasus - Wikipedia, the free encyclopedia.txt#7, term: computer, content:In 1956 the first Pegasus was used to calculate the stresses and strains in the tail plane of the Saunders-Roe SR.53[citation needed]; the results were used to check the manufacturers figures; the programmer was Anne Robson. Because of the importance of a computer it was housed in the drawing room, complete with an Adam's ceiling, of Ferranti's London office in Portland Place.
1259) 16.764284, Ferranti Pegasus - Wikipedia, the free encyclopedia.txt#8, term: computer, content:A Pegasus 1 was installed at Cyber House, Sheffield by Stafford Beer for the use of United Steel. It was the first computer installed for management cybernetics.[5] The Pegasus at Southampton University was used for analysis of ground resonance data for the Saro P.531 helicopter that eventually entered production as the Westland Scout and Westland Wasp.[6]
1260) 16.764284, Firmware - Wikipedia, the free encyclopedia.txt#5, term: computer, content:In some respects, the various firmware components are as important as the operating system in a working computer. However, unlike most modern operating systems, firmware rarely has a well-evolved automatic mechanism of updating itself to fix any functionality issues detected after shipping the unit.
1261) 16.764284, Floating point - Wikipedia, the free encyclopedia.txt#43, term: computer, content:Floating-point numbers are typically packed into a computer datum as the sign bit, the exponent field, and the significand or mantissa, from left to right. For the IEEE 754 binary formats (basic and extended) which have extant hardware implementations, they are apportioned as follows:
1262) 16.764284, Floating point - Wikipedia, the free encyclopedia.txt#45, term: computer, content:In the IEEE binary interchange formats the leading 1 bit of a normalized significand is not actually stored in the computer datum. It is called the "hidden" or "implicit" bit. Because of this, single precision format actually has a significand with 24 bits of precision, double precision format has 53, and quad has 113.
1263) 16.764284, FLOPS - Wikipedia, the free encyclopedia.txt#0, term: computer, content:In computing, FLOPS or flops (an acronym for floating-point operations per second) is a measure of computer performance, useful in fields of scientific calculations that make heavy use of floating-point calculations. For such cases it is a more accurate measure than the generic instructions per second.
1264) 16.764284, FLOPS - Wikipedia, the free encyclopedia.txt#3, term: computer, content:In late 1996 Intel's ASCI Red was the world's first computer to achieve oneTFLOPS and beyond. Sandia director Bill Camp said that ASCI Red had the best reliability of any supercomputer ever built, and was supercomputings high-water mark in longevity, price, and performance. [2]
1265) 16.764284, FLOPS - Wikipedia, the free encyclopedia.txt#5, term: computer, content:For comparison, a handheld calculator performs relatively few FLOPS. A computer response time below 0.1second in a calculation context is usually perceived as instantaneous by a human operator,[3] so a simple calculator needs only about 10FLOPS to be considered functional.
1266) 16.764284, Free Software Foundation - Wikipedia, the free encyclopedia.txt#12, term: computer, content:The FSF maintains a list of "high priority projects" to which the Foundation claims that "there is a vital need to draw the free software community's attention".[33] The FSF considers these projects "important because computer users are continually being seduced into using non-free software, because there is no adequate free replacement."[33]
1267) 16.764284, GNU - Wikipedia, the free encyclopedia.txt#0, term: computer, content:GNU i/nu/[3][4] is an operating system[5][6][7] and an extensive collection of computer software. GNU is composed wholly of free software,[8][9][10] most of which is licensed under GNU's own GPL.
1268) 16.764284, Grace Hopper - Wikipedia, the free encyclopedia.txt#33, term: computer, content:Throughout much of her later career, Grace Hopper was much in demand as a speaker at various computer-related events. She was well known for her lively and irreverent speaking style, as well as a rich treasury of early war stories. She also received the nickname "Grandma COBOL".
1269) 16.764284, Graphical user interface - Wikipedia, the free encyclopedia.txt#13, term: computer, content:Human interface devices, for the efficient interaction with a GUI include a computer keyboard, especially used together with keyboard shortcuts, pointing devices for the cursor (or rather pointer) control: mouse, pointing stick, touchpad, trackball, joystick, virtual keyboards, and head-up displays (translucent information devices at the eye level).
1270) 16.764284, Graphical user interface - Wikipedia, the free encyclopedia.txt#18, term: computer, content:Following PARC the first GUI-centric computer operating model was the Xerox 8010 Information System (Xerox Star) in 1981,[15][16] followed by the Apple Lisa (which presented the concept of menu bar and window controls) in 1983, the Apple Macintosh 128K in 1984, and the Atari ST and Commodore Amiga in 1985.
1271) 16.764284, Graphical user interface - Wikipedia, the free encyclopedia.txt#32, term: computer, content:Some environments use the methods of 3D graphics to project virtual three dimensional user interface objects onto the screen. These are often shown in use in science fiction films (see below for examples). As the processing power of computer graphics hardware increases, this becomes less of an obstacle to a smooth user experience.
1272) 16.764284, Graphics tablet - Wikipedia, the free encyclopedia.txt#1, term: computer, content:The device consists of a flat surface upon which the user may "draw" or trace an image using the attached stylus, a pen-like drawing apparatus. The image is displayed on the computer monitor, though some graphics tablets now also incorporate an LCD screen for a more realistic or natural experience and usability.
1273) 16.764284, Graphics tablet - Wikipedia, the free encyclopedia.txt#6, term: computer, content:Digitizers were popularized in the mid 1970s and early 1980s by the commercial success of the ID (Intelligent Digitizer) and BitPad manufactured by the Summagraphics Corp. These digitizers were used as the input device for many high-end CAD (Computer Aided Design) systems as well as bundled with PCs and PC-based CAD software like AutoCAD.
1274) 16.764284, Hang (computing) - Wikipedia, the free encyclopedia.txt#0, term: computer, content:In computing, a hang or freeze occurs when either a computer program or system ceases to respond to inputs. A typical example is a graphical user interface that no longer responds to the user's keyboard or mouse, but the term covers a wide range of behaviors in both clients and servers, and is not limited to graphical user interface issues.
1275) 16.764284, Hans Meuer - Wikipedia, the free encyclopedia.txt#0, term: computer, content:Hans Meuer was a professor of computer science at the University of Mannheim, managing director of Prometeus GmbH and general chair of the International Supercomputing Conference. In 1986, he became co-founder and organizer of the first Mannheim Supercomputer Conference, which has been held annually but known as the International Supercomputing Conference [1] since 2001.
1276) 16.764284, Hard disk drive - Wikipedia, the free encyclopedia.txt#57, term: computer, content:Defragmentation is a procedure used to minimize delay in retrieving data by moving related items to physically proximate areas on the disk.[131] Some computer operating systems perform defragmentation automatically. Although automatic defragmentation is intended to reduce access delays, performance will be temporarily reduced while the procedure is in progress.[132]
1277) 16.764284, Harvard Mark I - Wikipedia, the free encyclopedia.txt#7, term: computer, content:The enclosure for the Mark I was designed by futuristic American industrial designer Norman Bel Geddes. Aiken considered the elaborate case to be a waste of resources, since computing power was in high demand during the war and the funds ($50,000 or more according to Grace Hopper) could have been used to build additional computer equipment.[7]
1278) 16.764284, History of computing hardware - Wikipedia, the free encyclopedia.txt#36, term: computer, content:The Z2 was one of the earliest examples of an electromechanical relay computer, and was created by German engineer Konrad Zuse in 1939. It was an improvement on his earlier Z1; although it used the same mechanical memory, it replaced the arithmetic and control logic with electrical relay circuits.[49]
1279) 16.764284, History of computing hardware - Wikipedia, the free encyclopedia.txt#59, term: computer, content:Turing felt that speed and size of memory were crucial and he proposed a high-speed memory of what would today be called 25 KB, accessed at a speed of 1 MHz. The ACE implemented subroutine calls, whereas the EDVAC did not, and the ACE also used Abbreviated Computer Instructions, an early form of programming language.
1280) 16.764284, History of computing hardware - Wikipedia, the free encyclopedia.txt#60, term: computer, content:The Manchester Small-Scale Experimental Machine, nicknamed Baby, was the world's first stored-program computer. It was built at the Victoria University of Manchester by Frederic C. Williams, Tom Kilburn and Geoff Tootill, and ran its first program on 21June 1948.[77]
1281) 16.764284, History of computing hardware - Wikipedia, the free encyclopedia.txt#79, term: computer, content:The bipolar transistor was invented in 1947. From 1955 onwards transistors replaced vacuum tubes in computer designs,[104] giving rise to the "second generation" of computers. Initially the only devices available were germanium point-contact transistors.[105]
1282) 16.764284, History of computing hardware - Wikipedia, the free encyclopedia.txt#84, term: computer, content:The Transistor Computer's design was adopted by the local engineering firm of Metropolitan-Vickers in their Metrovick 950, the first commercial transistor computer anywhere.[116] Six Metrovick 950s were built, the first completed in 1956. They were successfully deployed within various departments of the company and were in use for about five years.[109]
1283) 16.764284, Home computer - Wikipedia, the free encyclopedia.txt#43, term: computer, content:Throughout the 1990s and 1st decade of the 21st century, many home computer systems were available inexpensively at garage sales and on eBay. Many enthusiasts started to collect home computers, with older and rarer systems being much sought after. Sometimes the collections turned into a virtual museum presented on web sites.[71]
1284) 16.764284, Honeywell, Inc. v. Sperry Rand Corp. - Wikipedia, the free encyclopedia.txt#5, term: computer, content:The computer played a major role in the prosecution of the case for plaintiff Honeywell. A computerized record of documents pertaining to the case, known as Electronic Legal Files (or ELF), allowed Honeywell attorneys to store, sort, recall, and print information on hundreds of different subjects.
1285) 16.764284, HP 2100 - Wikipedia, the free encyclopedia.txt#2, term: computer, content:The 2116A is a 16-bit word-addressed general purpose computer. Main memory is 4096 words (4K), expandable to 8K of magnetic core in the mainframe, or 16K with a memory extender. The 2116A features 16 I/O slots in the mainframe, a 10MHz clock and a memory cycle time of 1.6 microseconds.
1286) 16.764284, HP 2100 - Wikipedia, the free encyclopedia.txt#3, term: computer, content:The 2116A had two subsequent revisions: the 2116B added support for up to 32K with a memory extender, and the 2116C incorporated a more compact model of core memory, allowing the full 32K to be housed within the computer mainframe. (32K 16-bit words, equivalent to 64K 8-bit bytes of memory.)
1287) 16.764284, HP-UX - Wikipedia, the free encyclopedia.txt#1, term: computer, content:Earlier versions of HP-UX supported the HP Integral PC and HP 9000 Series 200, 300, and 400 computer systems based on the Motorola 68000 series of processors, as well as the HP 9000 Series 500 computers based on HP's proprietary FOCUS processor architecture.
1288) 16.764284, Human–computer interaction - Wikipedia, the free encyclopedia.txt#0, term: computer, content:Humancomputer interaction (HCI) researches the design and use of computer technology, focusing on the interfaces between people (users) and computers. Researchers in the field of HCI both observe the ways in which humans interact with computers and design technologies that let humans interact with computers in novel ways.
1289) 16.764284, Human–computer interaction - Wikipedia, the free encyclopedia.txt#39, term: computer, content:In human and computer interactions, there usually exists a semantic gap between human and computer's understandings towards mutual behaviors. Ontology (information science), as a formal representation of domain-specific knowledge, can be used to address this problem, through solving the semantic ambiguities between the two parties.[22]
1290) 16.764284, Hybrid computer - Wikipedia, the free encyclopedia.txt#5, term: computer, content:Hybrid computers should be distinguished from hybrid systems. The latter may be no more than a digital computer equipped with an analog-to-digital converter at the input and/or a digital-to-analog converter at the output, to convert analog signals for ordinary digital signal processing, and conversely, e.g., for driving physical control systems, such as servomechanisms.
1291) 16.764284, IBM 604 - Wikipedia, the free encyclopedia.txt#3, term: computer, content:The 604 and a modified version, the 605, were used as components of the Card Programmed Electronic Calculators (CPC and CPC II). The 604 was also a component of the Test Assembly, a precursor to IBM's early computers. The circuit module design and packaging was also used for the IBM 650, a popular early computer.
1292) 16.764284, IBM 702 - Wikipedia, the free encyclopedia.txt#0, term: computer, content:The IBM 702 was IBM's response to the UNIVACthe first mainframe computer using magnetic tapes. Because these machines had less computational power than the IBM 701 and ERA 1103, which were favored for scientific computing, the 702 was aimed at business computing.[1]
1293) 16.764284, IBM PC compatible - Wikipedia, the free encyclopedia.txt#5, term: computer, content:Rumors of "lookalike", compatible computers, created without IBM's approval, began almost immediately after the IBM PC's release.[2][3] By June 1983 PC Magazine defined "PC 'clone'" as "a computer [that can] accommodate the user who takes a disk home from an IBM PC, walks across the room, and plugs it into the 'foreign' machine".[4] Because of a shortage of IBM PCs that year, many customers purchased clones instead.[5][6] Columbia Data Products produced the first computer more or less compatible to the IBM PC standard during June 1982, soon followed by Eagle Computer. Compaq announced its first IBM PC compatible in November 1982, the Compaq Portable. The Compaq was the first sewing machine-sized portable computer that was essentially 100% PC-compatible. The company could not copy the BIOS directly as a result of the court decision in Apple v. Franklin, but it could reverse-engineer the IBM BIOS and then write its own BIOS using clean room design.
1294) 16.764284, IBM System i - Wikipedia, the free encyclopedia.txt#4, term: computer, content:Realizing the importance of compatibility with the thousands of programs written in legacy code, IBM launched the AS/400 midrange computer line in 1988. AS stands for "Application System." Great effort was made during development of the AS/400 to enable programs written for the System/34 and System/36 to be moved to the AS/400.
1295) 16.764284, IBM System i - Wikipedia, the free encyclopedia.txt#10, term: computer, content:The IBM System i platform extended the System/38 architecture of an object-based system with an integrated DB2 relational database. Equally important are the virtual machine and single-level storage concepts which established the platform as an advanced business computer.
1296) 16.764284, IBM System i - Wikipedia, the free encyclopedia.txt#29, term: computer, content:The AS/400 was one of the first general-purpose computer systems to attain a C2 security rating from the NSA (Gould UTX/C2, a UNIX-based system was branded in 1986[5]), and in 1995 was extended to employ a 64-bit processor and operating system.
1297) 16.764284, Image scanner - Wikipedia, the free encyclopedia.txt#25, term: computer, content:Color scanners typically read RGB (red-green-blue color) data from the array. This data is then processed with some proprietary algorithm to correct for different exposure conditions, and sent to the computer via the device's input/output interface (usually USB, previous to which was SCSI or bidirectional parallel port in older units).
1298) 16.764284, Image scanner - Wikipedia, the free encyclopedia.txt#38, term: computer, content:During the early 1990s professional flatbed scanners were available over a local computer network. This proved useful to publishers, print shops, etc. This functionality largely fell out of use as the cost of flatbed scanners reduced enough to make sharing unnecessary.
1299) 16.764284, Information system - Wikipedia, the free encyclopedia.txt#20, term: computer, content:The field of study called information systems encompasses a variety of topics including systems analysis and design, computer networking, information security, database management and decision support systems. Information management deals with the practical and theoretical problems of collecting and analyzing information in a business function area including business productivity tools, applications programming and implementation, electronic commerce, digital media production, data mining, and decision support. Communications and networking deals with the telecommunication technologies. Information systems bridges business and computer science using the theoretical foundations of information and computation to study various business models and related algorithmic processes [24] on building the IT systems [25][26] within a computer science discipline.[27][28][29][30][31][32][33][34][35][36][37][38][39] Computer information system(s) (CIS) is a field studying computers and algorithmic processes, including their principles, their software and hardware designs, their applications, and their impact on society,[40][41][42] whereas IS emphasizes functionality over design.[43]
1300) 16.764284, Installation (computer programs) - Wikipedia, the free encyclopedia.txt#6, term: computer, content:As mentioned earlier, some computer programs need no installation. This was once usual for many programs which run on DOS, Mac OS, Atari TOS and AmigaOS. As computing environments grew more complex and fixed hard drives replaced floppy disks, the need for tangible installation presented itself.
1301) 16.764284, Instant messaging - Wikipedia, the free encyclopedia.txt#39, term: computer, content:With rapid adoption of IM in the workplace, demand for IM security products began to grow in the mid-2000s. By 2007, the preferred platform for the purchase of security software had become the "computer appliance", according to IDC, who estimate that by 2008, 80% of network security products will be delivered via an appliance.[19]
1302) 16.764284, Integrated circuit - Wikipedia, the free encyclopedia.txt#22, term: computer, content:Integrated circuits such as 1K-bit RAMs, calculator chips, and the first microprocessors, that began to be manufactured in moderate quantities in the early 1970s, had under 4000 transistors. True LSI circuits, approaching 10,000 transistors, began to be produced around 1974, for computer main memories and second-generation microprocessors.
1303) 16.764284, Integrated development environment - Wikipedia, the free encyclopedia.txt#0, term: computer, content:An integrated development environment (IDE) is a software application that provides comprehensive facilities to computer programmers for software development. An IDE normally consists of a source code editor, build automation tools and a debugger. Most modern IDEs have intelligent code completion.
1304) 16.764284, Intel 8008 - Wikipedia, the free encyclopedia.txt#10, term: computer, content:For controller and CRT terminal use, this was an acceptable design, but it was rather cumbersome to use for most other tasks, at least compared to the next generations of microprocessors. A few early computer designs were based on it, but most would use the later and greatly improved Intel 8080 instead.
1305) 16.764284, Intel 8080 - Wikipedia, the free encyclopedia.txt#14, term: computer, content:The interrupt system state (enabled or disabled) was also output on a separate pin. For simple systems, where the interrupts were not used, it is possible to find cases where this pin is used as an additional single-bit output port (the popular Radio86RK computer made in the Soviet Union, for instance).
1306) 16.764284, Intel MCS-51 - Wikipedia, the free encyclopedia.txt#37, term: computer, content:The 8051's predecessor, the 8048, was used in the keyboard of the first IBM PC, where it converted keypresses into the serial data stream which is sent to the main unit of the computer. The 8048 and derivatives are still used today[update] for basic model keyboards.
1307) 16.764284, International Federation for Information Processing - Wikipedia, the free encyclopedia.txt#8, term: computer, content:The current IFIP TC1, which focuses on Foundations of Computer Science, was established in 1997. There was an earlier TC1, covering Terminology, which was IFIPs first Technical Committee. Formed in 1961, it produced a multilingual dictionary of information-processing terminology, but was later disbanded.
1308) 16.764284, Internet - Wikipedia, the free encyclopedia.txt#33, term: computer, content:Many computer scientists describe the Internet as a "prime example of a large-scale, highly engineered, yet highly complex system".[54] The structure was found to be highly robust to random failures,[55] yet, very vulnerable to intentional attacks.[56]
1309) 16.764284, Internet - Wikipedia, the free encyclopedia.txt#57, term: computer, content:The prevalent language for communication on the Internet has been English. This may be a result of the origin of the Internet, as well as the language's role as a lingua franca. Early computer systems were limited to the characters in the American Standard Code for Information Interchange (ASCII), a subset of the Latin alphabet.
1310) 16.764284, Interpreter (computing) - Wikipedia, the free encyclopedia.txt#32, term: computer, content:Clive Gifford introduced[citation needed] a measure quality of self-interpreter (the eigenratio), the limit of the ratio between computer time spent running a stack of N self-interpreters and time spent to run a stack of N1 self-interpreters as N goes to infinity. This value does not depend on the program being run.
1311) 16.764284, Interrupt - Wikipedia, the free encyclopedia.txt#3, term: computer, content:Each interrupt has its own interrupt handler. The number of hardware interrupts is limited by the number of interrupt request (IRQ) lines to the processor, but there may be hundreds of different software interrupts. Interrupts are a commonly used technique for computer multitasking, especially in real-time computing. Such a system is said to be interrupt-driven.[3]
1312) 16.764284, Interrupt - Wikipedia, the free encyclopedia.txt#22, term: computer, content:A message-signalled interrupt does not use a physical interrupt line. Instead, a device signals its request for service by sending a short message over some communications medium, typically a computer bus. The message might be of a type reserved for interrupts, or it might be of some pre-existing type such as a memory write.
1313) 16.764284, IRIX - Wikipedia, the free encyclopedia.txt#10, term: computer, content:IRIX had strong support for real-time disk and graphics I/O. IRIX was one of the first Unix versions to feature a graphical user interface for the main desktop environment. IRIX was widely used in the computer animation industry and for scientific visualization due to its once-large application base.
1314) 16.764284, J. Lyons and Co. - Wikipedia, the free encyclopedia.txt#12, term: computer, content:The company was losing money in the 1960s but remained under the control of the Salmon family, descended from a founding partner. Lyons began to close some of its London tea shops and hotels; in 1963 it also merged its LEO Computers business with English Electric's computer interests to form the jointly owned English Electric LEO.
1315) 16.764284, Jack Kilby - Wikipedia, the free encyclopedia.txt#5, term: computer, content:Jack Kilby went on to pioneer military, industrial, and commercial applications of microchip technology. He headed teams that built both the first military system and the first computer incorporating integrated circuits. He later co-invented both the hand-held calculator and the thermal printer that was used in portable data terminals.
1316) 16.764284, Konrad Zuse - Wikipedia, the free encyclopedia.txt#11, term: computer, content:The Z3, the first fully operational electromechanical computer, was partially financed by German government-supported DVL, which wanted their extensive calculations automated. A request by his co-worker Helmut Schreyerwho had helped Zuse build the Z3 prototype in 1938[19]for government funding for an electronic successor to the Z3 was denied as "strategically unimportant".
1317) 16.764284, Laptop - Wikipedia, the free encyclopedia.txt#24, term: computer, content:Intel, Asus, Compal, Quanta and some other laptop manufacturers have created the Common Building Block standard for laptop parts to address some of the inefficiencies caused by the lack of standards.[24] The following list summarizes the differences and distinguishing features of laptop components in comparison to desktop personal computer parts.[25]
1318) 16.764284, Laptop - Wikipedia, the free encyclopedia.txt#57, term: computer, content:The integrated screen often requires users to lean over for a better view, which can cause neck and/or spinal injuries. A larger and higher-quality external screen can be connected to almost any laptop to alleviate that and to provide additional screen space for more productive work. Another solution is to use a computer stand.
1319) 16.764284, LINC - Wikipedia, the free encyclopedia.txt#13, term: computer, content:The display screen was a CRT about 5inches square which was actually a standard Tektronix oscilloscope with special plug-in amplifiers. The special plug-ins could be replaced with standard oscilloscope plug-ins for use in diagnostic maintenance of the computer. Many LINCs were supplied as kits to be assembled by the end user, so the oscilloscope came in handy.
1320) 16.764284, LINC - Wikipedia, the free encyclopedia.txt#17, term: computer, content:The LINC connector module included bays for two plug-in chassis allowing custom interfacing to experimental setups. Analog-to-digital and digital-to-analog converters were built into the computer and each could be accessed by a single machine instruction. Six relays were also available.
1321) 16.764284, Linux - Wikipedia, the free encyclopedia.txt#59, term: computer, content:Linux distributions have become increasingly popular on mainframes in the last decade partly due to pricing and the open-source model.[21][citation needed] In December 2009, computer giant IBM reported that it would predominantly market and sell mainframe-based Enterprise Linux Server.[97]
1322) 16.764284, Linux - Wikipedia, the free encyclopedia.txt#79, term: computer, content:Linux Live CD sessions have long been used as a tool for recovering data from a broken computer system and for repairing the system. Building upon that idea, several Linux distributions tailored for this purpose have emerged, most of which use GParted as a partition editor, with additional data recovery and system repair software:
1323) 16.764284, Lisp (programming language) - Wikipedia, the free encyclopedia.txt#7, term: computer, content:Lisp was first implemented by Steve Russell on an IBM 704 computer. Russell had read McCarthy's paper and realized (to McCarthy's surprise) that the Lisp eval function could be implemented in machine code.[8] The result was a working Lisp interpreter which could be used to run Lisp programs, or more properly, 'evaluate Lisp expressions.'
1324) 16.764284, List of programming languages by type - Wikipedia, the free encyclopedia.txt#23, term: computer, content:In electronics, a Hardware description language or HDL is a specialized computer language used to describe the structure, design and operation of electronic circuits, and most commonly, digital logic circuits. The two most widely used and well-supported HDL varieties used in industry are Verilog and VHDL. Hardware description languages include:
1325) 16.764284, Logical conjunction - Wikipedia, the free encyclopedia.txt#14, term: computer, content:In high-level computer programming and digital electronics, logical conjunction is commonly represented by an infix operator, usually as a keyword such as "AND", an algebraic multiplication, or the ampersand symbol "&". Many languages also provide short-circuit control structures corresponding to logical conjunction.
1326) 16.764284, Lorenz cipher - Wikipedia, the free encyclopedia.txt#28, term: computer, content:In Germany, examples may be seen at the Heinz Nixdorf MuseumsForum, a computer museum in Paderborn and the Deutsches Museum, a museum of science and technology in Munich.[27] A Lorenz machine is also displayed at Bletchley Park in the United Kingdom and at the National Cryptologic Museum in the United States.
1327) 16.764284, Machine - Wikipedia, the free encyclopedia.txt#17, term: computer, content:Computers are machines to process information, often in the form of numbers. Charles Babbage designed various machines to tabulate logarithms and other functions in 1837. His Difference engine can be considered an advanced mechanical calculator and his Analytical Engine a forerunner of the modern computer, though none were built in Babbage's lifetime.
1328) 16.764284, Mainframe computer - Wikipedia, the free encyclopedia.txt#21, term: computer, content:A supercomputer is a computer that is at the frontline of current processing capacity, particularly speed of calculation. Supercomputers are used for scientific and engineering problems (high-performance computing) which are data crunching and number crunching,[25] while mainframes are used for transaction processing. The differences are as follows:
1329) 16.764284, Manchester Mark 1 - Wikipedia, the free encyclopedia.txt#1, term: computer, content:The machine's successful operation was widely reported in the British press, which used the phrase "electronic brain" in describing it to their readers. That description provoked a reaction from the head of the University of Manchester's Department of Neurosurgery, the start of a long-running debate as to whether an electronic computer could ever be truly creative.
1330) 16.764284, Manchester Small-Scale Experimental Machine - Wikipedia, the free encyclopedia.txt#0, term: computer, content:The Manchester Small-Scale Experimental Machine (SSEM), nicknamed Baby, was the world's first stored-program computer. It was built at the Victoria University of Manchester, England, by Frederic C. Williams, Tom Kilburn and Geoff Tootill, and ran its first program on 21 June 1948.[1]
1331) 16.764284, Max Newman - Wikipedia, the free encyclopedia.txt#18, term: computer, content:The Robinson machines were limited in speed and reliability. Tommy Flowers of the Post Office Research Station, Dollis Hill had experience of thermionic valves and built an electronic machine, the Colossus computer which was installed in the Newmanry. This was a great success and ten were in use by the end of the war.
1332) 16.764284, Max Newman - Wikipedia, the free encyclopedia.txt#22, term: computer, content:Now let's be clear before we go any further that neither Tom Kilburn nor I knew the first thing about computers when we arrived at Manchester University... Newman explained the whole business of how a computer works to us.
1333) 16.764284, Message transfer agent - Wikipedia, the free encyclopedia.txt#1, term: computer, content:The terms mail server, mail exchanger, and MX host may also refer to a computer performing the MTA function. The Domain Name System (DNS) associates a mail server to a domain with an MX record containing the domain name of the host(s) providing MTA services.
1334) 16.764284, Message transfer agent - Wikipedia, the free encyclopedia.txt#2, term: computer, content:A mail server is a computer that serves as an electronic post office for email. Mail exchanged across networks is passed between mail servers that run specially designed software. This software is built around agreed-upon, standardized protocols for handling mail messages and any data files (such as images, multimedia or documents) that might be attached to them.
1335) 16.764284, Microprocessor - Wikipedia, the free encyclopedia.txt#24, term: computer, content:In the NASA Apollo space missions to the moon in the 1960s and 1970s, all onboard computations for primary guidance, navigation and control were provided by a small custom processor called "The Apollo Guidance Computer". It used wire wrap circuit boards whose only logic elements were three-input NOR gates.[11]
1336) 16.764284, Microprocessor - Wikipedia, the free encyclopedia.txt#69, term: computer, content:In the mid-1980s to early 1990s, a crop of new high-performance reduced instruction set computer (RISC) microprocessors appeared, influenced by discrete RISC-like CPU designs such as the IBM 801 and others. RISC microprocessors were initially used in special-purpose machines and Unix workstations, but then gained wide acceptance in other roles.
1337) 16.764284, MIPS instruction set - Wikipedia, the free encyclopedia.txt#58, term: computer, content:Operating systems ported to the architecture include SGI's IRIX, Microsoft's Windows NT (through v4.0), Windows CE, Linux, FreeBSD, NetBSD, OpenBSD, UNIX System V, SINIX, QNX, and MIPS Computer Systems' own RISC/os.
1338) 16.764284, MIT Press - Wikipedia, the free encyclopedia.txt#4, term: computer, content:MIT Press primarily publishes academic titles in the fields of Art & Architecture, Visual & Cultural Studies, Cognitive Science, Philosophy, Linguistics, Computer Science, Economics, Finance & Business, Environmental Science, Political Science, Life Sciences, Neuroscience, New Media, and Science, Technology, & Society.[3]
1339) 16.764284, MOS Technology 6502 - Wikipedia, the free encyclopedia.txt#37, term: computer, content:The Western Design Center designed and produced the 65C816 processor, a 16-bit successor to the 65C02, with greatly enhanced features. The 65C816 architecture was the core of the popular Super Nintendo Entertainment System and the Apple IIGS computer. The W65C816S static core version is in current production and available through electronics distributors.
1340) 16.764284, Motorola 6800 - Wikipedia, the free encyclopedia.txt#3, term: computer, content:The 6800 was popular in computer peripherals, test equipment applications and point-of-sale terminals. The MC6802, introduced in 1977, included 128 bytes of RAM and an internal clock oscillator on chip. The MC6801 and MC6805 included with RAM, ROM and I/O on a single chip were popular in automotive applications.
1341) 16.764284, Motorola 6800 - Wikipedia, the free encyclopedia.txt#7, term: computer, content:Motorola did not chronicle the development of the 6800 microprocessor the way that Intel did for their microprocessors. In 2008 the Computer History Museum interviewed four members of the 6800 microprocessor design team. Their recollections can be confirmed and expanded by magazine and journal articles written at the time.
1342) 16.764284, Motorola 6800 - Wikipedia, the free encyclopedia.txt#46, term: computer, content:The Tektronix 4051 Graphics Computing System was introduced in October 1975. This was a professional desktop computer that had a 6800 microprocessor with up to 32KB of user RAM, 300KB magnetic tape storage, BASIC in ROM and a 1024 by 780 graphics display. The Tektronix 4051 sold for $7000, rather higher than the personal computers using the 6800.[89]
1343) 16.764284, Multimedia - Wikipedia, the free encyclopedia.txt#0, term: computer, content:Multimedia is content that uses a combination of different content forms such as text, audio, images, animation, video and interactive content. Multimedia contrasts with media that use only rudimentary computer displays such as text-only or traditional forms of printed or hand-produced material.
1344) 16.764284, Multimedia - Wikipedia, the free encyclopedia.txt#2, term: computer, content:Multimedia may be broadly divided into linear and non-linear categories. Linear active content progresses often without any navigational control for the viewer such as a cinema presentation. Non-linear uses interactivity to control progress as with a video game or self-paced computer based training. Hypermedia is an example of non-linear content.
1345) 16.764284, Multimedia - Wikipedia, the free encyclopedia.txt#21, term: computer, content:In Education, multimedia is used to produce computer-based training courses (popularly called CBTs) and reference books like encyclopedia and almanacs. A CBT lets the user go through a series of presentations, text about a particular topic, and associated illustrations in various information formats. Edutainment is the combination of education with entertainment, especially multimedia entertainment.
1346) 16.764284, OpenGL - Wikipedia, the free encyclopedia.txt#1, term: computer, content:Silicon Graphics Inc., (SGI) started developing OpenGL in 1991 and released it in January 1992;[6] applications use it extensively in the fields of computer-aided design (CAD), virtual reality, scientific visualization, information visualization, flight simulation, and video games. OpenGL is managed by the non-profit technology consortium Khronos Group.
1347) 16.764284, Operating system - Wikipedia, the free encyclopedia.txt#8, term: computer, content:Embedded operating systems are designed to be used in embedded computer systems. They are designed to operate on small machines like PDAs with less autonomy. They are able to operate with a limited number of resources. They are very compact and extremely efficient by design. Windows CE and Minix 3 are some examples of embedded operating systems.
1348) 16.764284, Operating system - Wikipedia, the free encyclopedia.txt#24, term: computer, content:Digital Equipment Corporation developed many operating systems for its various computer lines, including TOPS-10 and TOPS-20 time sharing systems for the 36-bit PDP-10 class systems. Before the widespread use of UNIX, TOPS-10 was a particularly popular system in universities, and in the early ARPANET community.
1349) 16.764284, Operating system - Wikipedia, the free encyclopedia.txt#41, term: computer, content:The Linux kernel originated in 1991, as a project of Linus Torvalds, while a university student in Finland. He posted information about his project on a newsgroup for computer students and programmers, and received support and assistance from volunteers who succeeded in creating a complete and functional kernel.
1350) 16.764284, Operating system - Wikipedia, the free encyclopedia.txt#50, term: computer, content:The components of an operating system all exist in order to make the different parts of a computer work together. All user software needs to go through the operating system in order to use any of the hardware, whether it be as simple as a mouse or keyboard or as complex as an Internet component.
1351) 16.764284, PA-RISC - Wikipedia, the free encyclopedia.txt#0, term: computer, content:PA-RISC is an instruction set architecture (ISA) developed by Hewlett-Packard. As the name implies, it is a reduced instruction set computer (RISC) architecture, where the PA stands for Precision Architecture. The design is also referred to as HP/PA for Hewlett Packard Precision Architecture.
1352) 16.764284, Pascal (programming language) - Wikipedia, the free encyclopedia.txt#20, term: computer, content:Apple Computer created its own Lisa Pascal for the Lisa Workshop in 1982, and ported the compiler to the Apple Macintosh and MPW in 1985. In 1985 Larry Tesler, in consultation with Niklaus Wirth, defined Object Pascal and these extensions were incorporated in both the Lisa Pascal and Mac Pascal compilers.
1353) 16.764284, PDP-11 - Wikipedia, the free encyclopedia.txt#1, term: computer, content:The PDP-11 is considered by some experts[6][7][8] to be the most popular minicomputer ever. Design features of the PDP-11 influenced the design of most late-1970s computer systems including the Intel x86[6] and the Motorola 68000.
1354) 16.764284, PDP-8 - Wikipedia, the free encyclopedia.txt#5, term: computer, content:The last commercial PDP-8 models in 1979 were called "CMOS-8s" and used custom complementary metal-oxide-semiconductor (CMOS) microprocessors. They were not priced competitively, and the offering failed. The IBM PC in 1981 cemented the doom of the CMOS-8s by making a legitimate, well-supported small microprocessor computer.
1355) 16.764284, PDP-8 - Wikipedia, the free encyclopedia.txt#77, term: computer, content:An engineering textbook popular in the 1980s, The Art of Digital Design by David Winkel and Franklin Prosser, contains an example problem spanning several chapters in which the authors demonstrate the process of designing a computer that is compatible with the PDP-8/I. The function of every component is explained. Although it is not a production design, the exercise provides a detailed description of the computer's operation.
1356) 16.764284, Perl - Wikipedia, the free encyclopedia.txt#73, term: computer, content:The Computer Language Benchmarks Game, a project hosted by Alioth, compares the performance of implementations of typical programming problems in several programming languages.[78] The submitted Perl implementations typically perform toward the high end of the memory-usage spectrum and give varied speed results. Perl's performance in the benchmarks game is typical for interpreted languages.[79]
1357) 16.764284, Personal computer - Wikipedia, the free encyclopedia.txt#52, term: computer, content:A pocket PC is a hardware specification for a handheld-sized computer (personal digital assistant, PDA) that runs the Microsoft Windows Mobile operating system. It may have the capability to run an alternative operating system like NetBSD or Linux. Pocket PCs have many of the capabilities of modern desktop PCs.
1358) 16.764284, Personal computer - Wikipedia, the free encyclopedia.txt#88, term: computer, content:Computer communications involve internal modem cards, modems, network adapter cards, and routers. Common peripherals and adapter cards include headsets, joysticks, microphones, printers, scanners, sound adapter cards (as a separate card rather than located on the motherboard), speakers and webcams.
1359) 16.764284, Personal digital assistant - Wikipedia, the free encyclopedia.txt#12, term: computer, content:While early PDAs connected to a user's personal computer via serial ports or another proprietary connection,[specify] many today connect via a USB cable. Older PDAs were unable to connect to each other via USB, as their implementations of USB didn't support acting as the "host".
1360) 16.764284, PowerPC - Wikipedia, the free encyclopedia.txt#34, term: computer, content:IBM's RS64 processors are a family of chips implementing the "Amazon" variant of the PowerPC architecture. These processors are used in the RS/6000 and AS/400 computer families; the Amazon architecture includes proprietary extensions used by AS/400. The POWER4 and later POWER processors implement the Amazon architecture and replaced the RS64 chips in the RS/6000 and AS/400 families.
1361) 16.764284, Program counter - Wikipedia, the free encyclopedia.txt#0, term: computer, content:The program counter (PC), commonly called the instruction pointer (IP) in Intel x86 and Itanium microprocessors, and sometimes called the instruction address register (IAR),[1] the instruction counter,[2] or just part of the instruction sequencer,[3] is a processor register that indicates where a computer is in its program sequence.
1362) 16.764284, Program counter - Wikipedia, the free encyclopedia.txt#2, term: computer, content:Instructions are usually fetched sequentially from memory, but control transfer instructions change the sequence by placing a new value in the PC. These include branches (sometimes called jumps), subroutine calls, and returns. A transfer that is conditional on the truth of some assertion lets the computer follow a different sequence under different conditions.
1363) 16.764284, Programmer - Wikipedia, the free encyclopedia.txt#17, term: computer, content:Computer programming, offshore outsourcing, and Foreign Worker Visas became a controversial topic after the crash of the dot-com bubble left many programmers without work or with lower wages. Programming was even mentioned in the 2004 US Presidential debate on the topic of offshore outsourcing.[citation needed]
1364) 16.764284, Programming language - Wikipedia, the free encyclopedia.txt#8, term: computer, content:The next step was development of so-called second-generation programming languages (2GL) or assembly languages, which were still closely tied to the instruction set architecture of the specific computer. These served to make the program much more human-readable, and relieved the programmer of tedious and error-prone address calculations.
1365) 16.764284, Programming language - Wikipedia, the free encyclopedia.txt#9, term: computer, content:The first high-level programming languages, or third-generation programming languages (3GL), were written in the 1950s. An early high-level programming language to be designed for a computer was Plankalkl, developed for the German Z3 by Konrad Zuse between 1943 and 1945. However, it was not implemented until 1998 and 2000.[26]
1366) 16.764284, Programming language - Wikipedia, the free encyclopedia.txt#11, term: computer, content:At the University of Manchester, Alick Glennie developed Autocode in the early 1950s. A programming language, it used a compiler to automatically convert the language into machine code. The first code and compiler was developed in 1952 for the Mark 1 computer at the University of Manchester and is considered to be the first compiled high-level programming language.[28][29]
1367) 16.764284, Puzzle video game - Wikipedia, the free encyclopedia.txt#8, term: computer, content:An early hidden object game was Alice: An Interactive Museum. Computer Gaming World reported in 1993 that "one disadvantage of searching through screen after screen for 'switches' is that after a while one develops a case of 'clickitus' of the fingers as one repeatedly punches that mouse button like a chicken pecking at a farmyard".[10]
1368) 16.764284, Puzzle video game - Wikipedia, the free encyclopedia.txt#15, term: computer, content:Puzzle video games owe their origins to brain teasers and puzzles throughout human history. Android Nim (TRS-80, 1979) is a computerized version of the mathematical strategy game Nim.[15] Other traditional, thinking games such as Hangman and Bulls and Cows (commercialized as Mastermind) were popular targets for computer implementation.
1369) 16.764284, Quantum computing - Wikipedia, the free encyclopedia.txt#14, term: computer, content:For more details on the sequences of operations used for various quantum algorithms, see universal quantum computer, Shor's algorithm, Grover's algorithm, DeutschJozsa algorithm, amplitude amplification, quantum Fourier transform, quantum gate, quantum adiabatic algorithm and quantum error correction.
1370) 16.764284, Quantum computing - Wikipedia, the free encyclopedia.txt#50, term: computer, content:In May 2013, Google announced that it was launching the Quantum Artificial Intelligence Lab, hosted by NASA's Ames Research Center, with a 512-qubit D-Wave quantum computer. The USRA (Universities Space Research Association) will invite researchers to share time on it with the goal of studying quantum computing for machine learning.[75]
1371) 16.764284, Quantum computing - Wikipedia, the free encyclopedia.txt#51, term: computer, content:In early 2014 it was reported, based on documents provided by former NSA contractor Edward Snowden, that the U.S. National Security Agency (NSA) is running a $79.7 million research program (titled "Penetrating Hard Targets") to develop a quantum computer capable of breaking vulnerable encryption.[76]
1372) 16.764284, Quantum computing - Wikipedia, the free encyclopedia.txt#54, term: computer, content:In April 2015 IBM scientists claimed two critical advances towards the realization of a practical quantum computer. They claimed the ability to detect and measure both kinds of quantum errors simultaneously, as well as a new, square quantum bit circuit design that could scale to larger dimensions.[80]
1373) 16.764284, Random-access machine - Wikipedia, the free encyclopedia.txt#0, term: computer, content:In computer science, random-access machine (RAM) is an abstract machine in the general class of register machines. The RAM is very similar to the counter machine but with the added capability of 'indirect addressing' of its registers. Like the counter machine the RAM has its instructions in the finite-state portion of the machine (the so-called Harvard architecture).
1374) 16.764284, Random-access machine - Wikipedia, the free encyclopedia.txt#29, term: computer, content:If we eschew the Minsky approach of one monster number in one register, and specify that our machine model will be "like a computer" we have to confront this problem of indirection if we are to compute the recursive functions (also called the -recursive functions ) both total and partial varieties.
1375) 16.764284, Random-access machine - Wikipedia, the free encyclopedia.txt#43, term: computer, content:Historically what has happened is these two CPY instructions have received distinctive names; however, no convention exists. Tradition (e.g. Knuth's (1973) imaginary MIX computer) uses two names called LOAD and STORE. Here we are adding the "i/d" parameter:
1376) 16.764284, Random-access memory - Wikipedia, the free encyclopedia.txt#22, term: computer, content:Software can "partition" a portion of a computer's RAM, allowing it to act as a much faster hard drive that is called a RAM disk. A RAM disk loses the stored data when the computer is shut down, unless memory is arranged to have a standby battery source.
1377) 16.764284, Register machine - Wikipedia, the free encyclopedia.txt#4, term: computer, content:In practical computer science, a similar concept known as a virtual machine is sometimes used to minimise dependencies on underlying machine architectures. Such machines are also used for teaching. The term "register machine" is sometimes used to refer to a virtual machine in textbooks.[1]
1378) 16.764284, Register machine - Wikipedia, the free encyclopedia.txt#29, term: computer, content:Minsky (1967) hints at the issue in his investigation of a counter machine (he calls them "program computer models") equipped with the instructions { CLR (r), INC (r), and RPT ("a" times the instructions m to n) }. He doesn't tell us how to fix the problem, but he does observe that:
1379) 16.764284, Sabre (computer system) - Wikipedia, the free encyclopedia.txt#17, term: computer, content:On one occasion, Sabre deliberately withheld Continental's discount fares on 49 routes where American competed.[12] A Sabre staffer had been directed to work on a program that would automatically suppress any discount fares loaded into the computer system.
1380) 16.764284, Semi-Automatic Ground Environment - Wikipedia, the free encyclopedia.txt#17, term: computer, content:In a test for the U.S. military at Bedford on 20 April 1951, data produced by a radar was transmitted through telephone lines to a computer for the first time, showing the detection of a mock enemy aircraft. This first test was directed by C. Robert Wieser.[16]
1381) 16.764284, Server (computing) - Wikipedia, the free encyclopedia.txt#13, term: computer, content:Since servers are usually accessed over a network, many run unattended without a computer monitor or input device, audio hardware and USB interfaces. Many servers do not have a graphical user interface (GUI). They are configured and managed remotely. Remote management include MMC, SSH or a web browser.
1382) 16.764284, Server (computing) - Wikipedia, the free encyclopedia.txt#16, term: computer, content:A server farm or server cluster is a collection of computer servers maintained by an organization to supply server functionality far beyond the capability of a single device. Modern data centers are now often built of very large clusters of much simpler servers,[9] and there is a collaborative effort, Open Compute Project around this concept.
1383) 16.764284, Shor's algorithm - Wikipedia, the free encyclopedia.txt#7, term: computer, content:In turn, finding such a     b   {\displaystyle b}   is reduced to finding an element     a   {\displaystyle a}   of even period with a certain additional property (as explained below, it is required that the condition of Step 6 of the classical part does not hold). The quantum algorithm is used for finding the period of randomly chosen elements     a   {\displaystyle a}  , as order-finding is a hard problem on a classical computer.
1384) 16.764284, Software bug - Wikipedia, the free encyclopedia.txt#4, term: computer, content:Use of the term "bug" to describe inexplicable defects has been a part of engineering jargon for many decades and predates computers and computer software; it may have originally been used in hardware engineering to describe mechanical malfunctions. For instance, Thomas Edison wrote the following words in a letter to an associate in 1878:
1385) 16.764284, Software bug - Wikipedia, the free encyclopedia.txt#38, term: computer, content:Malicious software may attempt to exploit known vulnerabilities in a systemwhich may or may not be bugs. Viruses and other malicious software are not bugs in themselvesthey are typically programs that are doing precisely what they were designed to do. In addition, it is often a security bug in a computer system that allows malicious software to work in the first place.
1386) 16.764284, Software synthesizer - Wikipedia, the free encyclopedia.txt#7, term: computer, content:Stand-alone softsynths run as a program on the computer so additional software is not required. Plug-in softsynths require a host application such as a digital audio workstation, which records the music that is played. Common plug-in technologies include VST, AU, LADSPA, DXi, and RTAS.
1387) 16.764284, Sound card - Wikipedia, the free encyclopedia.txt#35, term: computer, content:In the late 1990s many computer manufacturers began to replace plug-in soundcards with a "codec" chip (actually a combined audio AD/DA-converter) integrated into the motherboard. Many of these used Intel's AC'97 specification. Others used inexpensive ACR slot accessory cards.
1388) 16.764284, Sound card - Wikipedia, the free encyclopedia.txt#58, term: computer, content:Sound cards with a sampling rate of 192 kHz can be used to synchronize the clock of the computer with a time signal transmitter working on frequencies below 96 kHz like DCF 77 with a special software and a coil at the entrance of the soundcard, working as antenna [2], [3].
1389) 16.764284, Spreadsheet - Wikipedia, the free encyclopedia.txt#29, term: computer, content:SuperCalc was a spreadsheet application published by Sorcim in 1980, and originally bundled (along with WordStar) as part of the CP/M software package included with the Osborne 1 portable computer. It quickly became the de facto standard spreadsheet for CP/M and was ported to MS-DOS in 1982.
1390) 16.764284, Spreadsheet - Wikipedia, the free encyclopedia.txt#41, term: computer, content:A value can be entered from the computer keyboard by directly typing into the cell itself. Alternatively, a value can be based on a formula (see below), which might perform a calculation, display the current date or time, or retrieve external data such as a stock quote or a database value.
1391) 16.764284, Stan Frankel - Wikipedia, the free encyclopedia.txt#3, term: computer, content:In September, 1959, Frankel published a paper in IRE Transactions on Electronic Computers proposing a microwave computer that used travelling-wave tubes as digital storage devices, similar to, but faster than the acoustic delay lines used in the early 1950s. Frankel published a paper on measuring the thickness of soap films in the Journal of Applied Physics in 1966.[2]
1392) 16.764284, Stored-program computer - Wikipedia, the free encyclopedia.txt#4, term: computer, content:Many early computers, such as the AtanasoffBerry Computer, were not reprogrammable. They executed a single hardwired program. As there were no program instructions, no program storage was necessary. Other computers, though programmable, stored their programs on punched tape, which was physically fed into the machine as needed.
1393) 16.764284, SunOS - Wikipedia, the free encyclopedia.txt#0, term: computer, content:SunOS is a Unix-branded operating system developed by Sun Microsystems for their workstation and server computer systems. The SunOS name is usually only used to refer to versions 1.0 to 4.1.4, which were based on BSD, while versions 5.0 and later are based on UNIX System V Release 4, and are marketed under the brand name Solaris.
1394) 16.764284, Supercomputer - Wikipedia, the free encyclopedia.txt#26, term: computer, content:While in a traditional multi-user computer system job scheduling is, in effect, a tasking problem for processing and peripheral resources, in a massively parallel system, the job management system needs to manage the allocation of both computational and communication resources, as well as gracefully deal with inevitable hardware failures when tens of thousands of processors are present.[73]
1395) 16.764284, Supercomputer - Wikipedia, the free encyclopedia.txt#48, term: computer, content:Erik P. DeBenedictis of Sandia National Laboratories theorizes that a zettaFLOPS (1021, one sextillion FLOPS) computer is required to accomplish full weather modeling, which could cover a two-week time span accurately.[98][not in citation given] Such systems might be built around 2030.[99]
1396) 16.764284, Tablet computer - Wikipedia, the free encyclopedia.txt#46, term: computer, content: Some tablets are modified by adding physical gamepad buttons such as D-pad and thumb sticks for better gaming experience combined with the touchscreen and all other features of a typical tablet computer. Most of these tablets are targeted to run native OS games and emulator games.
1397) 16.764284, Tablet computer - Wikipedia, the free encyclopedia.txt#88, term: computer, content:Research firms Gartner and IDC both predict that tablet sales will exceed traditional personal computer (desktops, notebooks) sales in 2015.[159][160] As per the report from ABI Research in 2014 December, globally the average selling price of Ultrabooks and tablets declined 7.8 percent in 2014.
1398) 16.764284, Telecommunication - Wikipedia, the free encyclopedia.txt#64, term: computer, content:Wide area networks (WANs) are private computer networks that may extend for thousands of kilometers. Once again, some of their advantages include privacy and security. Prime users of private LANs and WANs include armed forces and intelligence agencies that must keep their information secure and secret.
1399) 16.764284, Telecommunications engineering - Wikipedia, the free encyclopedia.txt#17, term: computer, content:Optical fiber can be used as a medium for telecommunication and computer networking because it is flexible and can be bundled as cables. It is especially advantageous for long-distance communications, because light propagates through the fiber with little attenuation compared to electrical cables. This allows long distances to be spanned with few repeaters.
1400) 16.764284, Testbed - Wikipedia, the free encyclopedia.txt#1, term: computer, content:The term is used across many disciplines to describe experimental research and new product development platforms and environments. They may vary from hands-on prototype development in manufacturing industries such as automobiles (known as "mules"), aircraft engines or systems and to intellectual property refinement in such fields as computer software development shielded from the hazards of testing live.
1401) 16.764284, Text-based (computing) - Wikipedia, the free encyclopedia.txt#0, term: computer, content:Usually used in reference to a computer application, a text-based application is one whose primary input and output are based on text rather than graphics or sound. This does not mean that text-based applications do not have graphics or sound, just that the graphics or sound are secondary to the text.
1402) 16.764284, Theory of computation - Wikipedia, the free encyclopedia.txt#0, term: computer, content:In theoretical computer science and mathematics, the theory of computation is the branch that deals with how efficiently problems can be solved on a model of computation, using an algorithm. The field is divided into three major branches: automata theory and language, computability theory, and computational complexity theory, which are linked by the question: "What are the fundamental capabilities and limitations of computers?".[1]
1403) 16.764284, Theory of computation - Wikipedia, the free encyclopedia.txt#9, term: computer, content:Complexity theory considers not only whether a problem can be solved at all on a computer, but also how efficiently the problem can be solved. Two major aspects are considered: time complexity and space complexity, which are respectively how many steps does it take to perform a computation, and how much memory is required to perform that computation.
1404) 16.764284, Tommy Flowers - Wikipedia, the free encyclopedia.txt#0, term: computer, content:Thomas "Tommy" Harold Flowers, MBE (22 December 1905  28 October 1998) was a British engineer. During World War II, Flowers designed Colossus, the world's first programmable electronic computer, to help solve encrypted German messages.
1405) 16.764284, Torpedo Data Computer - Wikipedia, the free encyclopedia.txt#35, term: computer, content:As with the angle solver the equations implemented in the angle solver can be found in the Torpedo Data Computer manual.[39] Similar functions were implemented in the rangekeepers for surface ship-based fire control systems. For a general discussion of the principles behind the position keeper, see Rangekeeper.
1406) 16.764284, Touchscreen - Wikipedia, the free encyclopedia.txt#7, term: computer, content:In 1985, Sega released the Terebi Oekaki, also known as the Sega Graphic Board, for the SG-1000 video game console and SC-3000 home computer. It consisted of a plastic pen and a plastic board with a transparent window where the pen presses are detected. It was used primarily for a drawing software application.[14]
1407) 16.764284, Touchscreen - Wikipedia, the free encyclopedia.txt#15, term: computer, content:In 1990 the University of Maryland Human  Computer Interaction Lab demonstrated a touchscreen slider,[23] which was later cited as prior art in the lock screen patent litigation between Apple and other touchscreen mobile phone vendors (in relation to U.S. Patent 7,657,849).[24]
1408) 16.764284, Transistor - Wikipedia, the free encyclopedia.txt#11, term: computer, content:The transistor's low cost, flexibility, and reliability have made it a ubiquitous device. Transistorized mechatronic circuits have replaced electromechanical devices in controlling appliances and machinery. It is often easier and cheaper to use a standard microcontroller and write a computer program to carry out a control function than to design an equivalent mechanical control function.
1409) 16.764284, United States Navy - Wikipedia, the free encyclopedia.txt#64, term: computer, content:European operations revolve around facilities in Italy (NAS Sigonella and Naval Computer and Telecommunications Station Naples) with NSA Naples as the homeport for the Sixth Fleet and Command Naval Region Europe, Africa, Southwest Asia (CNREURAFSWA), and additional facilities in nearby Gaeta. There is also NS Rota in Spain and NSA Souda Bay in Greece.
1410) 16.764284, Universal Turing machine - Wikipedia, the free encyclopedia.txt#8, term: computer, content:Wang hoped that his paper would "connect the two approaches." Indeed, Minsky confirms this: "that the first formulation of Turing-machine theory in computer-like models appears in Wang (1957)" (Minsky 1967:200). Minsky goes on to demonstrate Turing equivalence of a counter machine.
1411) 16.764284, University of Manchester - Wikipedia, the free encyclopedia.txt#42, term: computer, content:Well-known figures among the university's current academic staff include computer scientist Steve Furber, economist Richard Nelson,[61] novelist Jeanette Winterson[62] (who succeeded Colm Tibn in 2012)[63] and biochemist Sir John Sulston, Nobel laureate of 2002.
1412) 16.764284, UNIX System V - Wikipedia, the free encyclopedia.txt#33, term: computer, content:Project Monterey was started in 1998 to combine major features of existing commercial Unix platforms, as a joint project of Compaq, IBM, Intel, SCO, and Sequent Computer Systems. The target platform was meant to be Intel's new IA-64 architecture and Itanium line of processors. However, the project was abruptly canceled in 2001 after little progress.[29]
1413) 16.764284, VAX - Wikipedia, the free encyclopedia.txt#1, term: computer, content:A 32-bit complex instruction set computer based on DEC's earlier PDP-11, VAX ("virtual address extension") was designed to extend or replace DEC's various PDP ISAs. The VAX architecture's primary features were virtual addressing (for example demand paged virtual memory) and its orthogonal instruction set.
1414) 16.764284, Version control - Wikipedia, the free encyclopedia.txt#4, term: computer, content:In computer software engineering, revision control is any kind of practice that tracks and provides control over changes to source code. Software developers sometimes use revision control software to maintain documentation and configuration files as well as source code.
1415) 16.764284, Video game - Wikipedia, the free encyclopedia.txt#37, term: computer, content:Many early computer games for non-PC descendant based platforms featured multiplayer support. Personal computer systems from Atari and Commodore both regularly featured at least two game ports. PC-based computer games started with a lower availability of multiplayer options because of technical limitations. PCs typically had either one or no game ports at all. Network games for these early personal computers were generally limited to only text based adventures or MUDs that were played remotely on a dedicated server. This was due both to the slow speed of modems (300-1200-bit/s), and the prohibitive cost involved with putting a computer online in such a way where multiple visitors could make use of it. However, with the advent of widespread local area networking technologies and Internet based online capabilities, the number of players in modern games can be 32 or higher, sometimes featuring integrated text and/or voice chat. Massively multiplayer online game (MMOs) can offer extremely high numbers of simultaneous players; Eve Online set a record with 65,303 players on a single server in 2013.[66]
1416) 16.764284, Video game industry - Wikipedia, the free encyclopedia.txt#1, term: computer, content:Considered by some as a curiosity in the mid-1970s, the computer and video game industries have grown from focused markets to mainstream. They took in about US$9.5 billion in the US in 2007, 11.7 billion in 2008, and 25.1 billion in 2010 (ESA annual report).
1417) 16.764284, Wearable computer - Wikipedia, the free encyclopedia.txt#23, term: computer, content:Though perhaps not technically "wearable," in 1986 Steve Roberts built Winnebiko-II, a recumbent bicycle with on-board computer and chorded keyboard. Winnebiko II was the first of Steve Roberts' forays into nomadic computing that allowed him to type while riding.[24]
1418) 16.764284, Web server - Wikipedia, the free encyclopedia.txt#0, term: computer, content:A web server is a computer system that processes requests via HTTP, the basic network protocol used to distribute information on the World Wide Web. The term can refer to the entire system, or specifically to the software that accepts and supervises the HTTP requests.[1]
1419) 16.764284, Webcam - Wikipedia, the free encyclopedia.txt#33, term: computer, content:Various companies sell sliding lens covers and stickers that allow users to retrofit a computer or smartphone to close access to the camera lens as needed.[19] One such company reported having sold more than 250,000 such items from 2013 to 2016.[19] Prominent users include FBI director James Comey.[19]
1420) 16.764284, Williams tube - Wikipedia, the free encyclopedia.txt#0, term: computer, content:The Williams tube, or the WilliamsKilburn tube after inventors Freddie Williams and Tom Kilburn,[1][2] developed in 1946 and 1947, was a cathode ray tube used as a computer memory to electronically store binary data. It was the first random-access digital storage device,[3] and was used successfully in several early computers.
1421) 16.764284, Windows 2000 - Wikipedia, the free encyclopedia.txt#55, term: computer, content:Remote Installation Services (RIS) are a means to automatically install Windows 2000 Professional (and not Windows 2000 Server) to a local computer over a network from a central server. Images do not have to support specific hardware configurations and the security settings can be configured after the computer reboots as the service generates a new unique security ID (SID) for the machine. This is required so that local accounts are given the right identifier and do not clash with other Windows 2000 Professional computers on a network.[101] RIS requires that client computers are able to boot over the network via either a network interface card that has a Pre-Boot Execution Environment (PXE) boot ROM installed or that the client computer has a network card installed that is supported by the remote boot disk generator. The remote computer must also meet the Net PC specification. The server that RIS runs on must be Windows 2000 Server and it must be able to access a network DNS Service, a DHCP service and the Active Directory services.[102]
1422) 16.764284, Windows 95 - Wikipedia, the free encyclopedia.txt#32, term: computer, content:In the UK, the largest computer chain PC World received a large number of oversized Windows 95 boxes, posters and point of sale material, and many branches opened at midnight to sell the first copies of the product. Copies of The Times were available for free, Microsoft paid for 1.5 million issues (twice the daily circulation at the time).[33]
1423) 16.764284, Windows 95 - Wikipedia, the free encyclopedia.txt#35, term: computer, content:A number of editions of Windows 95 have been released. Only the original release was sold as a shrink-wrapped product, later editions were provided only to computer original equipment manufacturers (OEMs) for installation on new PCs. For this reason these editions are known as OEM Service Releases (OSR).
1424) 16.764284, Windows 98 - Wikipedia, the free encyclopedia.txt#29, term: computer, content:System Configuration Utility (also known as Msconfig) is a new system utility used to disable programs and services that are not required to run the computer. A Maintenance Wizard is included that schedules and automates ScanDisk, Disk Defragmenter and Disk Cleanup. Windows Script Host, with VBScript and JScript engines is built-in and upgradeable to version 5.6.
1425) 16.764284, Windows Vista - Wikipedia, the free encyclopedia.txt#71, term: computer, content:Computer manufactures such as Dell, Lenovo, and Hewlett-Packard released their newest computers with Windows Vista pre-installed; however, after the negative reception of the operating system, they also began selling their computers with Windows XP CDs included because of a drop in sales.[162]
1426) 16.764284, Windows XP - Wikipedia, the free encyclopedia.txt#0, term: computer, content:Windows XP (stylised as Windowsxp; codenamed Whistler)[6] is a personal computer operating system that was produced by Microsoft as part of the Windows NT family of operating systems. The operating system was released to manufacturing on August 24, 2001, and generally released for retail sale on October 25, 2001.
1427) 16.764284, Word (computer architecture) - Wikipedia, the free encyclopedia.txt#0, term: computer, content:Incomputing, a word is the natural unit of data used by a particular processor design. A word is a fixed-sized piece of data handled as a unit by the instruction set or the hardware of the processor. The number of bits in a word (the word size, word width, or word length) is an important characteristic of any specific processor design or computer architecture.
1428) 16.764284, Word processor - Wikipedia, the free encyclopedia.txt#32, term: computer, content:In the early 1970s, computer scientist Harold Koplow was hired by Wang Laboratories to program calculators. One of his programs permitted a Wang calculator to interface with an IBM Selectric typewriter, which was at the time used to calculate and print the paperwork for auto sales.
1429) 16.764284, World Wide Web - Wikipedia, the free encyclopedia.txt#10, term: computer, content:The first web page may be lost, but Paul Jones of UNC-Chapel Hill in North Carolina announced in May 2013 that Berners-Lee gave him what he says is the oldest known web page during a 1991 visit to UNC. Jones stored it on a magneto-optical drive and on his NeXT computer.[14]
1430) 16.764284, World Wide Web - Wikipedia, the free encyclopedia.txt#44, term: computer, content:Many formal standards and other technical specifications and software define the operation of different aspects of the World Wide Web, the Internet, and computer information exchange. Many of the documents are the work of the World Wide Web Consortium (W3C), headed by Berners-Lee, but some are produced by the Internet Engineering Task Force (IETF) and other organizations.
1431) 16.764284, World Wide Web - Wikipedia, the free encyclopedia.txt#52, term: computer, content:A web cache is a server computer located either on the public Internet, or within an enterprise that stores recently accessed web pages to improve response time for users when the same content is requested within a certain time after the original request.
1432) 16.595795, 3D computer graphics - Wikipedia, the free encyclopedia.txt#2, term: computer, content:3D computer graphics are often referred to as 3D models. Apart from the rendered graphic, the model is contained within the graphical data file. However, there are differences: a 3D model is the mathematical representation of any three-dimensional object. A model is not technically a graphic until it is displayed. A model can be displayed visually as a two-dimensional image through a process called 3D rendering or used in non-graphical computer simulations and calculations. With 3D printing, 3D models are similarly rendered into a 3D physical representation of the model, with limitations to how accurate the rendering can match the virtual model.
1433) 16.595795, 3D computer graphics - Wikipedia, the free encyclopedia.txt#5, term: computer, content:The model describes the process of forming the shape of an object. The two most common sources of 3D models are those that an artist or engineer originates on the computer with some kind of 3D modeling tool, and models scanned into a computer from real-world objects. Models can also be produced procedurally or via physical simulation. Basically, a 3D model is formed from points called vertices (or vertexes) that define the shape and form polygons. A polygon is an area formed from at least three vertexes (a triangle). A polygon of n points is an n-gon[citation needed]. The overall integrity of the model and its suitability to use in animation depend on the structure of the polygons.
1434) 16.595795, Advanced Micro Devices - Wikipedia, the free encyclopedia.txt#3, term: computer, content:In September 1969, AMD moved from its temporary location in Santa Clara to Sunnyvale, California.[12] To immediately secure a customer base, AMD initially became a second source supplier of microchips designed by Fairchild and National Semiconductor.[13][14] AMD first focused on producing logic chips.[15] The company guaranteed quality control to United States Military Standard, an advantage in the early computer industry since unreliability in microchips was a distinct problem that customers  including computer manufacturers, the telecommunications industry, and instrument manufacturers  wanted to avoid.[13][16][17][18]
1435) 16.595795, Analog computer - Wikipedia, the free encyclopedia.txt#41, term: computer, content:Most practical mechanical analog computers of any significant complexity used rotating shafts to carry variables from one mechanism to another. Cables and pulleys were used in a Fourier synthesizer, a tide-predicting machine, which summed the individual harmonic components. Another category, not nearly as well known, used rotating shafts only for input and output, with precision racks and pinions. The racks were connected to linkages that performed the computation. At least one US Naval sonar fire control computer of the later 1950s, made by Librascope, was of this type, as was the principal computer in the Mk. 56 Gun Fire Control System.
1436) 16.595795, Antivirus software - Wikipedia, the free encyclopedia.txt#18, term: computer, content:In 1990, the Computer Antivirus Research Organization (CARO) was founded. In 1991, CARO released the "Virus Naming Scheme", originally written by Fririk Sklason and Vesselin Bontchev.[44] Although this naming scheme is now outdated, it remains the only existing standard that most computer security companies and researchers ever attempted to adopt. CARO members includes: Alan Solomon, Costin Raiu, Dmitry Gryaznov, Eugene Kaspersky, Fririk Sklason, Igor Muttik, Mikko Hyppnen, Morton Swimmer, Nick FitzGerald, Padgett Peterson, Peter Ferrie, Righard Zwienenberg and Dr. Vesselin Bontchev.[45][46]
1437) 16.595795, ARPANET - Wikipedia, the free encyclopedia.txt#33, term: computer, content:Unlike modern Internet datagrams, the ARPANET was designed to reliably transmit 1822 messages, and to inform the host computer when it loses a message; the contemporary IP is unreliable, whereas the TCP is reliable. Nonetheless, the 1822 protocol proved inadequate for handling multiple connections among different applications residing in a host computer. This problem was addressed with the Network Control Program (NCP), which provided a standard method to establish reliable, flow-controlled, bidirectional communications links among different processes in different host computers. The NCP interface allowed application software to connect across the ARPANET by implementing higher-level communication protocols, an early example of the protocol layering concept incorporated to the OSI model.[40]
1438) 16.595795, Artificial intelligence - Wikipedia, the free encyclopedia.txt#28, term: computer, content:Affective computing is the study and development of systems and devices that can recognize, interpret, process, and simulate human affects.[84][85] It is an interdisciplinary field spanning computer sciences, psychology, and cognitive science.[86] While the origins of the field may be traced as far back as to early philosophical inquiries into emotion,[87] the more modern branch of computer science originated with Rosalind Picard's 1995 paper[88] on affective computing.[89][90] A motivation for the research is the ability to simulate empathy. The machine should interpret the emotional state of humans and adapt its behaviour to them, giving an appropriate response for those emotions.
1439) 16.595795, BASIC - Wikipedia, the free encyclopedia.txt#6, term: computer, content:The original BASIC language was released on May 1, 1964 by John Kemeny and Thomas Kurtz[3] and implemented under their direction by a team of Dartmouth College students.[4][5] The acronym BASIC comes from the name of an unpublished paper by Thomas Kurtz.[6] BASIC was designed to allow students to write mainframe computer programs for the Dartmouth Time-Sharing System. It was intended specifically for less technical users who did not have or want the mathematical background previously expected. Being able to use a computer to support teaching and research was quite novel at the time.
1440) 16.595795, BASIC - Wikipedia, the free encyclopedia.txt#11, term: computer, content:One of the first BASICs to appear was Tiny BASIC, a simple BASIC variant designed by Dennis Allison at the urging of Bob Albrecht of the Homebrew Computer Club. He had seen BASIC on minicomputers and felt it would be the perfect match for new machines like the MITS Altair 8800. How to design and implement a stripped-down version of an interpreter for the BASIC language was covered in articles by Allison in the first three quarterly issues of the People's Computer Company newsletter published in 1975 and implementations with source code published in Dr. Dobb's Journal of Tiny BASIC Calisthenics & Orthodontia: Running Light Without Overbyte. Versions were written by Li-Chen Wang and Tom Pittman.[11]
1441) 16.595795, BIOS - Wikipedia, the free encyclopedia.txt#75, term: computer, content:In a December 2013 interview with CBS 60 Minutes, Deborah Plunkett, Information Assurance Director for the US National Security Agency claimed that NSA analysts had uncovered and thwarted a possible BIOS attack by a foreign nation state. The attack on the world's computers could have allegedly "literally taken down the US economy." The segment further cites anonymous cyber security experts briefed on the operation as alleging the plot was conceived in China.[36] A later article in The Guardian cast doubt on the likelihood of such a threat, quoting Berkeley computer-science researcher Nicholas Weaver, Matt Blaze, a computer and information sciences professor at the University of Pennsylvania, and cybersecurity expert Robert David Graham in an analysis of the NSA's claims.[37]
1442) 16.595795, Branch (computer science) - Wikipedia, the free encyclopedia.txt#0, term: computer, content:A branch is an instruction in a computer program that can cause a computer to begin executing a different instruction sequence and thus deviate from its default behavior of executing instructions in order.[a] Branch (or branching, branched) may also refer to the act of switching execution to a different instruction sequence as a result of executing a branch instruction. A branch instruction can be either an unconditional branch, which always results in branching, or a conditional branch, which may or may not cause branching, depending on some condition. Branch instructions are used to implement control flow in program loops and conditionals (i.e., executing a particular sequence of instructions only if certain conditions are satisfied).
1443) 16.595795, COBOL - Wikipedia, the free encyclopedia.txt#12, term: computer, content:The short-range committee was made up of members representing six computer manufacturers and three government agencies. The six computer manufacturers were Burroughs Corporation, IBM, Minneapolis-Honeywell (Honeywell Labs), RCA, Sperry Rand, and Sylvania Electric Products. The three government agencies were the US Air Force, the Navy's David Taylor Model Basin, and the National Bureau of Standards (now the National Institute of Standards and Technology).[27] The committee was chaired by Joseph Wegstein of the US National Bureau of Standards. Work began by investigating data description, statements, existing applications and user experiences.[28]
1444) 16.595795, Colossus computer - Wikipedia, the free encyclopedia.txt#29, term: computer, content:Being not widely known, Colossus had little direct influence on the development of later computers; it was EDVAC that was the early design which had the most influence on subsequent computer architecture. However, the technology of Colossus, and the knowledge that reliable high-speed electronic digital computing devices were feasible, did have a significant influence on the development of some early computers in the United Kingdom and probably in the US. A number of people who were associated with the project and knew all about Colossus played significant roles in early computer work in the UK. In 1972, Herman Goldstine wrote that:
1445) 16.595795, Computer - Simple English Wikipedia, the free encyclopedia.txt#27, term: computer, content:There are bigger computers that many people at a time can use. These are called "Mainframes," and these computers do all the things that make things like the internet work. You can think of a personal computer like this: the personal computer is like your skin: you can see it, other people can see it, and through your skin you feel wind, water, air, and the rest of the world. A mainframe is more like your internal organs: you never see them, and you barely even think about them, but if they suddenly went missing, you would have some very big problems.
1446) 16.595795, Computer - Wikipedia, the free encyclopedia.txt#34, term: computer, content:At the University of Manchester, a team under the leadership of Tom Kilburn designed and built a machine using the newly developed transistors instead of valves.[42] Their first transistorised computer and the first in the world, was operational by 1953, and a second version was completed there in April 1955. However, the machine did make use of valves to generate its 125kHz clock waveforms and in the circuitry to read and write on its magnetic drum memory, so it was not the first completely transistorized computer. That distinction goes to the Harwell CADET of 1955,[43] built by the electronics division of the Atomic Energy Research Establishment at Harwell.[43][44]
1447) 16.595795, Computer - Wikipedia, the free encyclopedia.txt#53, term: computer, content:Errors in computer programs are called "bugs". They may be benign and not affect the usefulness of the program, or have only subtle effects. But in some cases, they may cause the program or the entire system to "hang", becoming unresponsive to input such as mouse clicks or keystrokes, to completely fail, or to crash. Otherwise benign bugs may sometimes be harnessed for malicious intent by an unscrupulous user writing an exploit, code designed to take advantage of a bug and disrupt a computer's proper execution. Bugs are usually not the fault of the computer. Since computers merely execute the instructions they are given, bugs are nearly always the result of programmer error or an oversight made in the program's design.[58]
1448) 16.595795, Computer - Wikipedia, the free encyclopedia.txt#67, term: computer, content:A computer's memory can be viewed as a list of cells into which numbers can be placed or read. Each cell has a numbered "address" and can store a single number. The computer can be instructed to "put the number 123 into the cell numbered 1357" or to "add the number that is in cell 1357 to the number that is in cell 2468 and put the answer into cell 1595." The information stored in memory may represent practically anything. Letters, numbers, even computer instructions can be placed into memory with equal ease. Since the CPU does not differentiate between different types of information, it is the software's responsibility to give significance to what the memory sees as nothing but a series of numbers.
1449) 16.595795, Computer architecture - Wikipedia, the free encyclopedia.txt#2, term: computer, content:The term architecture in computer literature can be traced to the work of Lyle R. Johnson, Mohammad Usman Khan and Frederick P. Brooks, Jr., members in 1959 of the Machine Organization department in IBMs main research center. Johnson had the opportunity to write a proprietary research communication about the Stretch, an IBM-developed supercomputer for Los Alamos National Laboratory (at the time known as Los Alamos Scientific Laboratory). To describe the level of detail for discussing the luxuriously embellished computer, he noted that his description of formats, instruction types, hardware parameters, and speed enhancements were at the level of system architecture  a term that seemed more useful than machine organization.
1450) 16.595795, Computer architecture - Wikipedia, the free encyclopedia.txt#13, term: computer, content:ISAs vary in quality and completeness. A good ISA compromises between programmer convenience (how easy the code is to understand), size of the code (how much code is required to do a specific action), cost of the computer to interpret the instructions (more complexity means more space needed to disassemble the instructions), and speed of the computer (with larger dissemblers comes longer disassemble time). For example, single-instruction ISAs like an ISA that subtracts one from a value and if the value is zero then the value returns to a higher value are both inexpensive, and fast, however ISAs like that are not convenient or helpful when looking at the size of the ISA. Memory organization defines how instructions interact with the memory, and how memory interacts with itself.
1451) 16.595795, Computer architecture - Wikipedia, the free encyclopedia.txt#21, term: computer, content:Modern computer performance is often described in IPC (instructions per cycle). This measures the efficiency of the architecture at any refresh rate. Since a faster rate can make a faster computer, this is a useful measurement. Older computers had IPC counts as low as 0.1 instructions per cycle. Simple modern processors easily reach near 1. Superscalar processors may reach three to five IPC by executing several instructions per refresh. Multicore and vector processing CPUs can multiply this further by acting on a lot of data per instruction, which have several CPU cores executing in parallel.
1452) 16.595795, Computer architecture - Wikipedia, the free encyclopedia.txt#28, term: computer, content:Benchmarking takes all these factors into account by measuring the time a computer takes to run through a series of test programs. Although benchmarking shows strengths, it shouldn't be how you choose a computer. Often the measured machines split on different measures. For example, one system might handle scientific applications quickly, while another might render video games more smoothly. Furthermore, designers may target and add special features to their products, through hardware or software, that permit a specific benchmark to execute quickly but don't offer similar advantages to general tasks.
1453) 16.595795, Computer data storage - Wikipedia, the free encyclopedia.txt#4, term: computer, content:A modern digital computer represents data using the binary numeral system. Text, numbers, pictures, audio, and nearly any other form of information can be converted into a string of bits, or binary digits, each of which has a value of 1 or 0. The most common unit of storage is the byte, equal to 8 bits. A piece of information can be handled by any computer or device whose storage space is large enough to accommodate the binary representation of the piece of information, or simply data. For example, the complete works of Shakespeare, about 1250 pages in print, can be stored in about five megabytes (40 million bits) with one byte per character.
1454) 16.595795, Computer data storage - Wikipedia, the free encyclopedia.txt#20, term: computer, content:Secondary storage (also known as external memory or auxiliary storage), differs from primary storage in that it is not directly accessible by the CPU. The computer usually uses its input/output channels to access secondary storage and transfers the desired data using intermediate area in primary storage. Secondary storage does not lose the data when the device is powered downit is non-volatile. Per unit, it is typically also two orders of magnitude less expensive than primary storage. Modern computer systems typically have two orders of magnitude more secondary storage than primary storage and data are kept for a longer time there.
1455) 16.595795, Computer data storage - Wikipedia, the free encyclopedia.txt#31, term: computer, content:Off-line storage is used to transfer information, since the detached medium can be easily physically transported. Additionally, in case a disaster, for example a fire, destroys the original data, a medium in a remote location will probably be unaffected, enabling disaster recovery. Off-line storage increases general information security, since it is physically inaccessible from a computer, and data confidentiality or integrity cannot be affected by computer-based attack techniques. Also, if the information stored for archival purposes is rarely accessed, off-line storage is less expensive than tertiary storage.
1456) 16.595795, Computer graphics - Wikipedia, the free encyclopedia.txt#24, term: computer, content:The field began to see the first rendered graphics that could truly pass as photorealistic to the untrained eye (though they could not yet do so with a trained CGI artist) and 3D graphics became far more popular in gaming, multimedia and animation. At the end of the 1980s and the beginning of the nineties were created, in France, the very first computer graphics TV series: La Vie des btes by studio Mac Guff Ligne (1988), Les Fables Gomtriques (1989-1991) by studio Fantme, and Quarxs, the first HDTV computer graphics series by Maurice Benayoun and Franois Schuiten (studio Z-A production, 19901993).
1457) 16.595795, Computer hardware - Wikipedia, the free encyclopedia.txt#2, term: computer, content:The template for all modern computers is the Von Neumann architecture, detailed in a 1945 paper by Hungarian mathematician John von Neumann. This describes a design architecture for an electronic digital computer with subdivisions of a processing unit consisting of an arithmetic logic unit and processor registers, a control unit containing an instruction register and program counter, a memory to store both data and instructions, external mass storage, and input and output mechanisms.[3] The meaning of the term has evolved to mean a stored-program computer in which an instruction fetch and a data operation cannot occur at the same time because they share a common bus. This is referred to as the Von Neumann bottleneck and often limits the performance of the system.[4]
1458) 16.595795, Computer keyboard - Wikipedia, the free encyclopedia.txt#13, term: computer, content:Standard alphanumeric keyboards have keys that are on three-quarter inch centers (0.750inches, 19.05mm)[citation needed], and have a key travel of at least 0.150inches (3.81mm). Desktop computer keyboards, such as the 101-key US traditional keyboards or the 104-key Windows keyboards, include alphabetic characters, punctuation symbols, numbers and a variety of function keys. The internationally common 102/104 key keyboards have a smaller left shift key and an additional key with some more symbols between that and the letter to its right (usually Z or Y). Also the enter key is usually shaped differently. Computer keyboards are similar to electric-typewriter keyboards but contain additional keys, such as the command or Windows keys.
1459) 16.595795, Computer monitor - Wikipedia, the free encyclopedia.txt#2, term: computer, content:Early electronic computers were fitted with a panel of light bulbs where the state of each particular bulb would indicate the on/off state of a particular register bit inside the computer. This allowed the engineers operating the computer to monitor the internal state of the machine, so this panel of lights came to be known as the 'monitor'. As early monitors were only capable of displaying a very limited amount of information, and were very transient, they were rarely considered for programme output. Instead, a line printer was the primary output device, while the monitor was limited to keeping track of the programme's operation.
1460) 16.595795, Computer monitor - Wikipedia, the free encyclopedia.txt#20, term: computer, content:The resolution for computer monitors has increased over time. From 320x200 during the early 1980s, to 800x600 during the late 1990s. Since 2009, the most commonly sold resolution for computer monitors is 1920x1080.[11] Before 2013 top-end consumer products were limited to 2560x1600 at 30in (76cm), excluding Apple products.[12] Apple introduced 2880x1800 with Retina MacBook Pro at 15.4in (39cm) on June 12, 2012, and introduced a 5120x2880 Retina iMac at 27in (69cm) on October 16, 2014. By 2015 all major display manufacturers had released 3840x2160 resolution displays.
1461) 16.595795, Computer mouse - Wikipedia, the free encyclopedia.txt#3, term: computer, content:The online Oxford Dictionaries entry for mouse states the plural for the small rodent is mice, while the plural for the small computer connected device is either mice or mouses. The dictionary's usage section states that the more common plural is mice and claims the first recorded use of the plural is mice[2] (though it cites a 1984 use of mice when there were actually several earlier ones, such as J. C. R. Licklider's "The Computer as a Communication Device" of 1968[3]). According to the fifth edition of The American Heritage Dictionary of the English Language the plural can be either "mice" or "mouses".[4]
1462) 16.595795, Computer mouse - Wikipedia, the free encyclopedia.txt#25, term: computer, content:Simple logic circuits interpret the relative timing to indicate which direction the wheel is rotating. This incremental rotary encoder scheme is sometimes called quadrature encoding of the wheel rotation, as the two optical sensor produce signals that are in approximately quadrature phase. The mouse sends these signals to the computer system via the mouse cable, directly as logic signals in very old mice such as the Xerox mice, and via a data-formatting IC in modern mice. The driver software in the system converts the signals into motion of the mouse cursor along X and Y axes on the computer screen.
1463) 16.595795, Computer music - Wikipedia, the free encyclopedia.txt#19, term: computer, content:Many systems for generating musical scores actually existed well before the time of computers. One of these was Musikalisches Wrfelspiel (Musical dice game; 18th century), a system which used throws of the dice to randomly select measures from a large collection of small phrases. When patched together, these phrases combined to create musical pieces which could be performed by human players. Although these works were not actually composed with a computer in the modern sense, it uses a rudimentary form of the random combinatorial techniques sometimes used in computer-generated composition.[citation needed]
1464) 16.595795, Computer programming - Wikipedia, the free encyclopedia.txt#0, term: computer, content:Computer programming (often shortened to programming) is a process that leads from an original formulation of a computing problem to executable computer programs. Programming involves activities such as analysis, developing understanding, generating algorithms, verification of requirements of algorithms including their correctness and resources consumption, and implementation (commonly referred to as coding[1][2]) of algorithms in a target programming language. Source code is written in one or more programming languages. The purpose of programming is to find a sequence of instructions that will automate performing a specific task or solving a given problem. The process of programming thus often requires expertise in many different subjects, including knowledge of the application domain, specialized algorithms and formal logic.
1465) 16.595795, Computer science - Wikipedia, the free encyclopedia.txt#1, term: computer, content:Its fields can be divided into a variety of theoretical and practical disciplines. Some fields, such as computational complexity theory (which explores the fundamental properties of computational and intractable problems), are highly abstract, while fields such as computer graphics emphasize real-world visual applications. Still other fields focus on challenges in implementing computation. For example, programming language theory considers various approaches to the description of computation, while the study of computer programming itself investigates various aspects of the use of programming language and complex systems. Humancomputer interaction considers the challenges in making computers and computations useful, usable, and universally accessible to humans.
1466) 16.595795, Computer science - Wikipedia, the free encyclopedia.txt#2, term: computer, content:The earliest foundations of what would become computer science predate the invention of the modern digital computer. Machines for calculating fixed numerical tasks such as the abacus have existed since antiquity, aiding in computations such as multiplication and division. Further, algorithms for performing computations have existed since antiquity, even before the development of sophisticated computing equipment. The ancient Sanskrit treatise Shulba Sutras, or "Rules of the Chord", is a book of algorithms written in 800 BC for constructing geometric objects like altars using a peg and chord, an early precursor of the modern field of computational geometry.
1467) 16.595795, Computer-aided design - Wikipedia, the free encyclopedia.txt#4, term: computer, content:CAD is an important industrial art extensively used in many applications, including automotive, shipbuilding, and aerospace industries, industrial and architectural design, prosthetics, and many more. CAD is also widely used to produce computer animation for special effects in movies, advertising and technical manuals, often called DCC digital content creation. The modern ubiquity and power of computers means that even perfume bottles and shampoo dispensers are designed using techniques unheard of by engineers of the 1960s. Because of its enormous economic importance, CAD has been a major driving force for research in computational geometry, computer graphics (both hardware and software), and discrete differential geometry.[6]
1468) 16.595795, Computer-aided design - Wikipedia, the free encyclopedia.txt#20, term: computer, content:Originally software for Computer-Aided Design systems was developed with computer languages such as Fortran, ALGOL but with the advancement of object-oriented programming methods this has radically changed. Typical modern parametric feature based modeler and freeform surface systems are built around a number of key C modules with their own APIs. A CAD system can be seen as built up from the interaction of a graphical user interface (GUI) with NURBS geometry and/or boundary representation (B-rep) data via a geometric modeling kernel. A geometry constraint engine may also be employed to manage the associative relationships between geometry, such as wireframe geometry in a sketch or components in an assembly.
1469) 16.595795, Conditional (computer programming) - Wikipedia, the free encyclopedia.txt#28, term: computer, content:This was the only conditional control statement in the original implementation of Fortran on the IBM 704 computer. On that computer the test-and-branch op-code had three addresses for those three states. Other computers would have "flag" registers such as positive, zero, negative, even, overflow, carry, associated with the last arithmetic operations and would use instructions such as 'Branch if accumulator negative' then 'Branch if accumulator zero' or similar. Note that the expression is evaluated once only, and in cases such as integer arithmetic where overflow may occur, the overflow or carry flags would be considered also.
1470) 16.595795, Cryptography - Wikipedia, the free encyclopedia.txt#0, term: computer, content:Cryptography or cryptology (from Greek  krypts, "hidden, secret"; and  graphein, "writing", or - -logia, "study", respectively[1]) is the practice and study of techniques for secure communication in the presence of third parties called adversaries.[2] More generally, cryptography is about constructing and analyzing protocols that prevent third parties or the public from reading private messages;[3] various aspects in information security such as data confidentiality, data integrity, authentication, and non-repudiation[4] are central to modern cryptography. Modern cryptography exists at the intersection of the disciplines of mathematics, computer science, and electrical engineering. Applications of cryptography include ATM cards, computer passwords, and electronic commerce.
1471) 16.595795, Digital Equipment Corporation - Wikipedia, the free encyclopedia.txt#6, term: computer, content:Ken Olsen and Harlan Anderson were two engineers who had been working at MIT Lincoln Laboratory on the lab's various computer projects. The Lab is best known for their work on what would today be known as "interactivity", and their machines were among the first where operators had direct control over programs running in real time. These had started in 1944 with the famed Whirlwind, which was originally developed to make a flight simulator for the US Navy, although this was never completed.[3] Instead, this effort evolved into the SAGE system for the US Air Force, which used large screens and light guns to allow operators to interact with radar data stored in the computer.[4]
1472) 16.595795, Digital Equipment Corporation - Wikipedia, the free encyclopedia.txt#10, term: computer, content:The pair returned with an updated business plan that outlined two phases for the company's development. They would start by selling computer modules as stand-alone devices that could be purchased separately and wired together to produce a number of different digital systems for lab use. Then, if these "digital modules" were able to build a self-sustaining business, the company would be free to use them to develop a complete computer in their Phase II.[8] The newly christened "Digital Equipment Corporation" received $70,000 from AR&D for a 70% share of the company,[7] and began operations in a Civil War era textile mill in Maynard, Massachusetts, where plenty of inexpensive manufacturing space was available.
1473) 16.595795, Digital Equipment Corporation - Wikipedia, the free encyclopedia.txt#73, term: computer, content:At its peak in the late 1980s, Digital had $14 billion in sales and ranked among the most profitable companies in the US. With its strong staff of engineers, Digital was expected to usher in the age of personal computers, but the autocratic and trend-resistant Mr. Olsen was openly skeptical of the desktop machines, saying the personal computer will fall flat on its face in business, and regarding them as toys used for playing video games. Digital's fortunes declined after missing out on some critical market shifts, particularly toward the personal computer. The board forced Olsen to resign as president in July 1992.[68]
1474) 16.595795, Electrical engineering - Wikipedia, the free encyclopedia.txt#20, term: computer, content:The invention of the transistor in late 1947 by William B. Shockley, John Bardeen, and Walter Brattain of the Bell Telephone Laboratories opened the door for more compact devices and led to the development of the integrated circuit in 1958 by Jack Kilby and independently in 1959 by Robert Noyce.[23] Starting in 1968, Ted Hoff and a team at the Intel Corporation invented the first commercial microprocessor, which foreshadowed the personal computer. The Intel 4004 was a four-bit processor released in 1971, but in 1973 the Intel 8080, an eight-bit processor, made the first personal computer, the Altair 8800, possible.[24]
1475) 16.595795, ENIAC - Wikipedia, the free encyclopedia.txt#32, term: computer, content:Like the Colossus, ENIAC required rewiring to reprogram until September 1948. Three months earlier, in June 1948, the Manchester Small-Scale Experimental Machine (SSEM) ran its first program and earned the distinction of first stored-program computer. Though the idea of a stored-program computer with combined memory for program and data was conceived during the development of ENIAC, it was not initially implemented in ENIAC because World War II priorities required the machine to be completed quickly, and ENIAC's 20 storage locations would be too small to hold data and programs.
1476) 16.595795, Ferranti Pegasus - Wikipedia, the free encyclopedia.txt#2, term: computer, content:At least two Pegasus machines survive, one in The Science Museum, London and one in The Manchester Museum of Science and Industry. The Pegasus in The Science Museum ran its first program in December 1959 and was regularly demonstrated until 2009 when it developed a severe electrical fault. In early 2014, the Science Museum decided to retire it permanently,[4] effectively ending the life of one of the world's oldest working computers. The Pegasus officially held the title of the world's oldest computer until 2012, when the restoration of the Harwell computer was completed at the National Museum of Computing.
1477) 16.595795, Floppy disk - Wikipedia, the free encyclopedia.txt#17, term: computer, content:Corporate computer environments may still make use of floppy disks for older machines that do not support the current company networks and in the case of laptops where Wi-Fi is not considered secure.[citation needed] The floppy disk provides for a controlled means of file transfer by permitting only a few files to be transmitted. This is as USB ports on enterprise computer terminals/workstations are often disabled in order to prevent employees from using a flash memory drive to take large amounts of data for unauthorized use. In addition, the loss of a floppy disk has less consequence than the loss of any modern piece of removable flash storage.
1478) 16.595795, Fortran - Wikipedia, the free encyclopedia.txt#4, term: computer, content:In late 1953, John W. Backus submitted a proposal to his superiors at IBM to develop a more practical alternative to assembly language for programming their IBM 704 mainframe computer. Backus' historic FORTRAN team consisted of programmers Richard Goldberg, Sheldon F. Best, Harlan Herrick, Peter Sheridan, Roy Nutt, Robert Nelson, Irving Ziller, Lois Haibt, and David Sayre.[6] Its concepts included easier entry of equations into a computer, an idea developed by J. Halcombe Laning and demonstrated in his GEORGE compiler of 1952.[7]
1479) 16.595795, Graphics processing unit - Wikipedia, the free encyclopedia.txt#0, term: computer, content:A graphics processing unit (GPU), also occasionally called visual processing unit (VPU), is a specialized electronic circuit designed to rapidly manipulate and alter memory to accelerate the creation of images in a frame buffer intended for output to a display. GPUs are used in embedded systems, mobile phones, personal computers, workstations, and game consoles. Modern GPUs are very efficient at manipulating computer graphics and image processing, and their highly parallel structure makes them more efficient than general-purpose CPUs for algorithms where the processing of large blocks of data is done in parallel. In a personal computer, a GPU can be present on a video card, or it can be embedded on the motherboard orin certain CPUson the CPU die.[1]
1480) 16.595795, Graphics processing unit - Wikipedia, the free encyclopedia.txt#8, term: computer, content:In 1987, the IBM 8514 graphics system was released as one of[vague] the first video cards for IBM PC compatibles to implement fixed-function 2D primitives in electronic hardware. The same year, Sharp released the X68000, which used a custom graphics chipset[18] that was powerful for a home computer at the time, with a 65,536 color palette and hardware support for sprites, scrolling and multiple playfields,[19] eventually serving as a development machine for Capcom's CP System arcade board. Fujitsu later competed with the FM Towns computer, released in 1989 with support for a full 16,777,216 color palette.[20]
1481) 16.595795, Harvard architecture - Wikipedia, the free encyclopedia.txt#3, term: computer, content:Under pure von Neumann architecture the CPU can be either reading an instruction or reading/writing data from/to the memory. Both cannot occur at the same time since the instructions and data use the same bus system. In a computer using the Harvard architecture, the CPU can both read an instruction and perform a data memory access at the same time,[1] even without a cache. A Harvard architecture computer can thus be faster for a given circuit complexity because instruction fetches and data access do not contend for a single memory pathway.
1482) 16.595795, History of computing hardware - Wikipedia, the free encyclopedia.txt#33, term: computer, content:The principle of the modern computer was first described by computer scientist Alan Turing, who set out the idea in his seminal 1936 paper,[47] On Computable Numbers. Turing reformulated Kurt Gdel's 1931 results on the limits of proof and computation, replacing Gdel's universal arithmetic-based formal language with the formal and simple hypothetical devices that became known as Turing machines. He proved that some such machine would be capable of performing any conceivable mathematical computation if it were representable as an algorithm. He went on to prove that there was no solution to the Entscheidungsproblem by first showing that the halting problem for Turing machines is undecidable: in general, it is not possible to decide algorithmically whether a given Turing machine will ever halt.
1483) 16.595795, Home computer - Wikipedia, the free encyclopedia.txt#18, term: computer, content:Toward the end of the 1980s, clones also became popular with non-corporate customers. Inexpensive, highly compatible clones succeeded where the PCjr had failed. Replacing the hobbyists who had made up the majority of the home computer market were, as Compute! described them, "people who want to take work home from the office now and then, play a game now and then, learn more about computers, and help educate their children". By 1986 industry experts predicted an "MS-DOS Christmas", and the magazine stated that clones threatened Commodore, Atari, and Apple's domination of the home-computer market.[31]
1484) 16.595795, Home computer - Wikipedia, the free encyclopedia.txt#26, term: computer, content:Eventually mass production of 5.25" drives resulted in lower prices, and after about 1984 they pushed cassette drives out of the US home computer market. 5.25" floppy disk drives would remain standard until the end of the 8-bit era. Though external 3.5" drives were made available for home computer systems toward the latter part of the 1980s, almost all software sold for 8-bit home computers remained on 5.25" disks; 3.5" drives were used for data storage. Standardization of disk formats was not common; sometimes even different models from the same manufacturer used different disk formats.
1485) 16.595795, Home computer - Wikipedia, the free encyclopedia.txt#27, term: computer, content:Toward the end of the home computer era, drives for a number of home computer models appeared offering disk-format compatibility with the IBM PC. The disk drives sold with the Commodore 128, Amiga and Atari ST were all able to read and write PC disks, which themselves were undergoing the transition from 5.25" to 3.5" format at the time. 5.25" drives were made available for the ST, Amiga and Macintosh, otherwise 3.5" based systems with no other use for a 5.25" format. Hard drives were never popular on home computers, remaining an expensive, niche product mainly for BBS sysops and the few business users.
1486) 16.595795, Home computer - Wikipedia, the free encyclopedia.txt#44, term: computer, content:As their often-inexpensively manufactured hardware ages and the supply of replacement parts dwindles, it has become popular among enthusiasts[72] to emulate these machines, recreating their software environments[73] on modern computers. One of the more well-known emulators is the Multi Emulator Super System (MESS) which can emulate most of the better-known home computers. A more or less complete list of home computer emulators can be found in the List of computer system emulators article. Games for many 8 and 16 bit home computers are becoming available for the Wii Virtual Console.
1487) 16.595795, Home computer - Wikipedia, the free encyclopedia.txt#46, term: computer, content:The most popular home computers in the USA up to 1985 were: the TRS-80 (1977), various models of the Apple II family (first introduced in 1977), the Atari 400/800 (1979) along with its follow up models the 800XL and 130XE, and the Commodore VIC-20 (1980) and the Commodore 64 (1982). The VIC was the first computer of any type to sell over one million units, and the 64 is still the highest-selling single model of personal computer ever, with over 17 million produced before production stopped in 1994 a 12-year run with only minor changes.[74] At one point in 1983 Commodore was selling as many 64s as the rest of the industry's computers combined.[75]
1488) 16.595795, Home computer - Wikipedia, the free encyclopedia.txt#52, term: computer, content:Until the introduction of the IBM PC in 1981, computers such as the Apple II and TRS 80 also found considerable use in office work.[80][81] In 1983 IBM introduced the PCjr in an attempt to continue their business computer success in the home computer market, but incompatibilities between it and the standard PC kept users away.[82][83] Assisted by a large public domain software library and promotional offers from Commodore, the PET had a sizable presence in the North American education market until that segment was largely ceded to the Apple II as Commodore focused on the C-64's success in the mass retail market.[84]
1489) 16.595795, HP 2100 - Wikipedia, the free encyclopedia.txt#1, term: computer, content:HP entered the minicomputer market in 1966, along with Varian Data Machines. Later, General Automation, Computer Automation, Data General, Micro Systems, and Lockheed would also be competitors. The 2116A was the first model of the series, demonstrated November 7-10, 1966 at the Joint Computer Conference in San Francisco.[1][2] It was designed by HP's Dymec division, after absorbing Data Systems Inc. (DSI), a subsidiary of Union Carbide. DSI had designs for a 16-bit minicomputer called the DSI-1000, which would eventually evolve into the 2116A through HP's involvement.
1490) 16.595795, Human computer - Wikipedia, the free encyclopedia.txt#11, term: computer, content:The term "human computer" has been recently used by a group of researchers who refer to their work as "human computation" (Law, 2011) In this usage, "human computer" refers to activities of humans in the context of human-based computation (HBC). This usage is questionable for the following reason. HBC is a computational technique where a machine outsources certain (not necessarily algorithmic) tasks to humans. In fact, most of the time humans in the context of HBC are not provided with a sequence of exact steps that needs to be executed to yield an answer. HBC is agnostic about how humans solve the problem. This is why the term outsourcing is used in the definition. The use of humans as "human computers" in the context of HBC is very rare.
1491) 16.595795, Hybrid computer - Wikipedia, the free encyclopedia.txt#6, term: computer, content:Recently in 2015, researchers at Columbia University published a paper[2] on a small scale hybrid computer in 65nm CMOS technology. This 4th-order VLSI hybrid computer contains 4 integrator blocks, 8 multiplier/gain-setting blocks, 8 fanout blocks for distributing current-mode signals, 2 ADCs, 2 DACs and 2 SRAMs blocks. Digital controllers are also implemented on the chip for executing the external instructions. A robot experiment in the paper demonstrates the use of the hybrid computing chip in today's emerging low-power embedded applications.
1492) 16.595795, IBM 650 - Wikipedia, the free encyclopedia.txt#1, term: computer, content:The 650 was a two-address, bi-quinary coded decimal computer (both data and addresses were decimal), with memory on a rotating magnetic drum. Character support was provided by the input/output units converting alphabetical and special characters to/from a two-digit decimal code. The 650 was marketed to scientific and engineering users as well as to users of punched card machines who were upgrading from calculating punches, such as the IBM 604, to computers.[5] Because of its relatively low cost and ease of programming, the 650 was used to pioneer a wide variety of applications, from modeling submarine crew performance[6] to teaching high school and college students computer programming.
1493) 16.595795, IBM PC compatible - Wikipedia, the free encyclopedia.txt#8, term: computer, content:This expectation seemed reasonable in the computer marketplace of the time. Until then Microsoft was based primarily on computer languages such as BASIC. The established small system operating software was CP/M from Digital Research which was in use both at the hobbyist level and by the more professional of those using microcomputers. To achieve such widespread use, and thus make the product viable economically, the OS had to operate across a range of machines from different vendors that had widely varying hardware. Those customers who needed other applications than the starter programs could reasonably expect publishers to offer their products for a variety of computers, on suitable media for each.
1494) 16.595795, Instruction set - Wikipedia, the free encyclopedia.txt#19, term: computer, content:The design of instruction sets is a complex issue. There were two stages in history for the microprocessor. The first was the CISC (Complex Instruction Set Computer), which had many different instructions. In the 1970s, however, places like IBM did research and found that many instructions in the set could be eliminated. The result was the RISC (Reduced Instruction Set Computer), an architecture that uses a smaller set of instructions. A simpler instruction set may offer the potential for higher speeds, reduced processor size, and reduced power consumption. However, a more complex set may optimize common operations, improve memory/cache efficiency, or simplify programming.
1495) 16.595795, Integer - Wikipedia, the free encyclopedia.txt#27, term: computer, content:An integer is often a primitive data type in computer languages. However, integer data types can only represent a subset of all integers, since practical computers are of finite capacity. Also, in the common two's complement representation, the inherent definition of sign distinguishes between "negative" and "non-negative" rather than "negative, positive, and0". (It is, however, certainly possible for a computer to determine whether an integer value is truly positive.) Fixed length integer approximation data types (or subsets) are denoted int or Integer in several programming languages (such as Algol68, C, Java, Delphi, etc.).
1496) 16.595795, Intel - Wikipedia, the free encyclopedia.txt#2, term: computer, content:Intel was an early developer of SRAM and DRAM memory chips, which represented the majority of its business until 1981. Although Intel created the world's first commercial microprocessor chip in 1971, it was not until the success of the personal computer (PC) that this became its primary business. During the 1990s, Intel invested heavily in new microprocessor designs fostering the rapid growth of the computer industry. During this period Intel became the dominant supplier of microprocessors for PCs, and was known for aggressive and anti-competitive tactics in defense of its market position, particularly against Advanced Micro Devices (AMD), as well as a struggle with Microsoft for control over the direction of the PC industry.[4][5]
1497) 16.595795, Intel - Wikipedia, the free encyclopedia.txt#125, term: computer, content:In July 2007, the European Commission accused Intel of anti-competitive practices, mostly against AMD.[254] The allegations, going back to 2003, include giving preferential prices to computer makers buying most or all of their chips from Intel, paying computer makers to delay or cancel the launch of products using AMD chips, and providing chips at below standard cost to governments and educational institutions.[255] Intel responded that the allegations were unfounded and instead qualified its market behavior as consumer-friendly.[255] General counsel Bruce Sewell responded that the Commission had misunderstood some factual assumptions as to pricing and manufacturing costs.[256]
1498) 16.595795, Job (computing) - Wikipedia, the free encyclopedia.txt#3, term: computer, content:In this sense of "job", a programmable computer performs "jobs", as each one can be different from the last. The term "job" is also common in operations research, predating its use in computing, in such uses as job shop scheduling; see for example Baker & Dzielinski (1960) and references thereof from throughout the 1950s, including several System Research Department Reports from IBM Research Center. This analogy is applied to computer systems, where the system resources are analogous to machines in a job shop, and the goal of scheduling is to minimize the total time from beginning to end (makespan). The term "job" for computing work dates to the mid 1950s, as in this use from 1955:
1499) 16.595795, John Mauchly - Wikipedia, the free encyclopedia.txt#8, term: computer, content:In 1942 Mauchly wrote a memo proposing the building of a general-purpose electronic computer.[6] The proposal, which circulated within the Moore School (but the significance of which was not immediately recognized), emphasized the enormous speed advantage that could be gained by using digital electronics with no moving parts. Lieutenant Herman Goldstine, who was the liaison between the United States Army and Moore School, picked up on the idea and asked Mauchly to write a formal proposal. In April 1943, the Army contracted with the Moore School to build the Electronic Numerical Integrator and Computer (ENIAC). Mauchly led the conceptual design while Eckert led the hardware engineering on ENIAC. A number of other talented engineers contributed to the confidential "Project PX".
1500) 16.595795, John Mauchly - Wikipedia, the free encyclopedia.txt#21, term: computer, content:Dr. Mauchly stayed involved in computers for the rest of his life. He was a founding member and president of the Association for Computing Machinery (ACM) and also helped found the Society for Industrial and Applied Mathematics (SIAM), serving as its fourth president. The Eckert-Mauchly Corporation was bought by Remington Rand in 1950 and for ten years Dr. Mauchly remained as Director of Univac Applications Research. Leaving in 1959 he formed Mauchly Associates, a consulting company that later introduced the critical path method (CPM) for construction scheduling by computer. In 1967 he founded Dynatrend, a computer consulting organization. In 1973 he became a consultant to Sperry Univac.
1501) 16.595795, Logic in computer science - Wikipedia, the free encyclopedia.txt#1, term: computer, content:Logic plays a fundamental role in computer science. Some of the key areas of logic that are particularly significant are computability theory (formerly called recursion theory), modal logic and category theory. The theory of computation is based on concepts defined by logicians and mathematicians such as Alonzo Church and Alan Turing.[1][2] Church first showed the existence of algorithmically unsolvable problems using his notion of lambda-definability. Turing gave the first compelling analysis of what can be called a mechanical procedure and Gdel asserted that he found Turing's analysis "perfect."[3] In addition some other major areas of theoretical overlap between logic and computer science are:
1502) 16.595795, Magnetic-core memory - Wikipedia, the free encyclopedia.txt#4, term: computer, content:The basic concept of using the square hysteresis loop of certain magnetic materials as a storage or switching device was known from the earliest days of computer development. Much of this knowledge had developed due to an understanding of transformers, which allowed amplification and switch-like performance when built using certain materials. The stable switching behavior was well known in the electrical engineering field, and its application in computer systems was immediate. For example, J. Presper Eckert and Jeffrey Chuan Chu had done some development work on the concept in 1945 at the Moore School during the ENIAC efforts.[2]
1503) 16.595795, Manchester Mark 1 - Wikipedia, the free encyclopedia.txt#4, term: computer, content:In 1936, mathematician Alan Turing published a definition of a theoretical "universal computing machine", a computer which held its program on tape, along with the data being worked on. Turing proved that such a machine was capable of solving any conceivable mathematical problem for which an algorithm could be written.[3] During the 1940s, Turing and others such as Konrad Zuse developed the idea of using the computer's own memory to hold both the program and data, instead of tape,[4] but it was mathematician John von Neumann who became widely credited with defining that stored-program computer architecture, on which the Manchester Mark 1 was based.[5]
1504) 16.595795, Manchester Small-Scale Experimental Machine - Wikipedia, the free encyclopedia.txt#3, term: computer, content:The first design for a program-controlled computer was Charles Babbage's Analytical Engine in the 1830s. A century later, in 1936, mathematician Alan Turing published his description of what became known as a Turing machine, a theoretical concept intended to explore the limits of mechanical computation. Turing was not imagining a physical machine, but a person he called a "computer", who acted according to the instructions provided by a tape on which symbols could be read and written sequentially as the tape moved under a tape head. Turing proved that if an algorithm can be written to solve a mathematical problem, then a Turing machine can execute that algorithm.[4]
1505) 16.595795, Massachusetts Institute of Technology - Wikipedia, the free encyclopedia.txt#53, term: computer, content:In electronics, magnetic core memory, radar, single electron transistors, and inertial guidance controls were invented or substantially developed by MIT researchers.[238][239] Harold Eugene Edgerton was a pioneer in high speed photography and sonar.[240][241] Claude E. Shannon developed much of modern information theory and discovered the application of Boolean logic to digital circuit design theory.[242] In the domain of computer science, MIT faculty and researchers made fundamental contributions to cybernetics, artificial intelligence, computer languages, machine learning, robotics, and cryptography.[239][243] At least nine Turing Award laureates and seven recipients of the Draper Prize in engineering have been or are currently associated with MIT.[244][245]
1506) 16.595795, Microcode - Wikipedia, the free encyclopedia.txt#30, term: computer, content:Some vertical microcode is just the assembly language of a simple conventional computer that is emulating a more complex computer. Some processors, such as DEC Alpha processors and the CMOS microprocessors on later IBM System/390 mainframes and z/Architecture mainframes, have PALcode (the term used on Alpha processors) or millicode (the term used on IBM mainframe microprocessors). This is a form of machine code, with access to special registers and other hardware resources not available to regular machine code, used to implement some instructions and other functions, such as page table walks on Alpha processors.[15][16][17]
1507) 16.595795, Microprocessor - Wikipedia, the free encyclopedia.txt#22, term: computer, content:By the late-1960s, designers were striving to integrate the central processing unit (CPU) functions of a computer onto a handful of MOS LSI chips, called microprocessor unit (MPU) chip sets. Building on 8-bit arithmetic logic units (3800/3804) he designed earlier at Fairchild, in 1969 Lee Boysel created the Four-Phase Systems Inc. AL-1 an 8-bit CPU slice that was expandable to 32-bits. In 1970, Steve Geller and Ray Holt of Garrett AiResearch designed the MP944 chip set to implement the F-14A Central Air Data Computer on six metal-gate chips fabricated by AMI.
1508) 16.595795, MIMD - Wikipedia, the free encyclopedia.txt#0, term: computer, content:In computing, MIMD (multiple instruction, multiple data) is a technique employed to achieve parallelism. Machines using MIMD have a number of processors that function asynchronously and independently. At any time, different processors may be executing different instructions on different pieces of data. MIMD architectures may be used in a number of application areas such as computer-aided design/computer-aided manufacturing, simulation, modeling, and as communication switches. MIMD machines can be of either shared memory or distributed memory categories. These classifications are based on how MIMD processors access memory. Shared memory machines may be of the bus-based, extended, or hierarchical type. Distributed memory machines may have hypercube or mesh interconnection schemes.
1509) 16.595795, Minicomputer - Wikipedia, the free encyclopedia.txt#3, term: computer, content:The definition of minicomputer is vague with the consequence that there are a number of candidates for the first minicomputer.[6] An early and highly successful minicomputer was Digital Equipment Corporation's (DEC) 12-bit PDP-8, which was built using discrete transistors and cost from US$16,000 upwards when launched in 1964. Later versions of the PDP-8 took advantage of small-scale integrated circuits. The important precursors of the PDP-8 include the PDP-5, LINC, the TX-0, the TX-2, and the PDP-1. DEC gave rise to a number of minicomputer companies along Massachusetts Route 128, including Data General, Wang Laboratories, Apollo Computer, and Prime Computer.
1510) 16.595795, MIPS instruction set - Wikipedia, the free encyclopedia.txt#0, term: computer, content:MIPS (originally an acronym for Microprocessor without Interlocked Pipeline Stages) is a reduced instruction set computer (RISC) instruction set architecture (ISA) developed by MIPS Technologies (formerly MIPS Computer Systems, Inc.). The early MIPS architectures were 32-bit, with 64-bit versions added later. Multiple revisions of the MIPS instruction set exist, including MIPS I, MIPS II, MIPS III, MIPS IV, MIPS V, MIPS32, and MIPS64. The current revisions are MIPS32 (for 32-bit implementations) and MIPS64 (for 64-bit implementations).[1][2] MIPS32 and MIPS64 define a control register set as well as the instruction set.
1511) 16.595795, Operating system - Wikipedia, the free encyclopedia.txt#13, term: computer, content:In the early 1950s, a computer could execute only one program at a time. Each user had sole use of the computer for a limited period of time and would arrive at a scheduled time with program and data on punched paper cards or punched tape. The program would be loaded into the machine, and the machine would be set to work until the program completed or crashed. Programs could generally be debugged via a front panel using toggle switches and panel lights. It is said that Alan Turing was a master of this on the early Manchester Mark 1 machine, and he was already deriving the primitive conception of an operating system from the principles of the Universal Turing machine.[6]
1512) 16.595795, Operating system - Wikipedia, the free encyclopedia.txt#97, term: computer, content:Every computer that is to be operated by an individual requires a user interface. The user interface is usually referred to as a shell and is essential if human interaction is to be supported. The user interface views the directory structure and requests services from the operating system that will acquire data from input hardware devices, such as a keyboard, mouse or credit card reader, and requests operating system services to display prompts, status messages and such on output hardware devices, such as a video monitor or printer. The two most common forms of a user interface have historically been the command-line interface, where computer commands are typed out line-by-line, and the graphical user interface, where a visual environment (most commonly a WIMP) is present.
1513) 16.595795, Optical computing - Wikipedia, the free encyclopedia.txt#1, term: computer, content:Most research projects focus on replacing current computer components with optical equivalents, resulting in an optical digital computer system processing binary data. This approach appears to offer the best short-term prospects for commercial optical computing, since optical components could be integrated into traditional computers to produce an optical-electronic hybrid. However, optoelectronic devices lose 30% of their energy converting electronic energy into photons and back; this conversion also slows the transmission of messages. All-optical computers eliminate the need for optical-electrical-optical (OEO) conversions.[1]
1514) 16.595795, Personal computer - Wikipedia, the free encyclopedia.txt#35, term: computer, content:Prior to the widespread usage of PCs, a computer that could fit on a desk was remarkably small, leading to the "desktop" nomenclature. More recently, the phrase usually indicates a particular style of computer case. Desktop computers come in a variety of styles ranging from large vertical tower cases to small models which can be tucked behind an LCD monitor. In this sense, the term "desktop" refers specifically to a horizontally oriented case, usually intended to have the display screen placed on top to save desk space. Most modern desktop computers have separate screens and keyboards.
1515) 16.595795, Personal computer - Wikipedia, the free encyclopedia.txt#39, term: computer, content:A home theater PC (HTPC) is a convergence device that combines the functions of a personal computer and a digital video recorder. It is connected to a TV set or an appropriately sized computer display, and is often used as a digital photo viewer, music and video player, TV receiver, and digital video recorder. HTPCs are also referred to as media center systems or media servers. The general goal in a HTPC is usually to combine many or all components of a home theater setup into one box. More recently, HTPCs gained the ability to connect to services providing on-demand movies and TV shows.
1516) 16.595795, Portable computer - Wikipedia, the free encyclopedia.txt#4, term: computer, content:The MIT Suitcase Computer, constructed in 1975, was the first known microprocessor-based portable computer. It was based on the Motorola 6800. Constructed in a Samsonite suitcase approximately 20"x30"x8" and weighing approximately 20lbs., it had 4K of SRAM, a serial port to accept downloaded software and connect to a modem, a keyboard and a 40-column thermal printer taken from a cash register. Built by student David Emberson in the MIT Digital Systems Laboratory as a thesis project, it never entered production. It is currently in the collection of Dr. Hoo-Min D. Toong.[citation needed]
1517) 16.595795, Printer (computing) - Wikipedia, the free encyclopedia.txt#32, term: computer, content:Line printers were the fastest of all impact printers and were used for bulk printing in large computer centres. A line printer could print at 1100 lines per minute or faster, frequently printing pages more rapidly than many current laser printers. On the other hand, the mechanical components of line printers operated with tight tolerances and required regular preventive maintenance (PM) to produce top quality print. They were virtually never used with personal computers and have now been replaced by high-speed laser printers. The legacy of line printers lives on in many computer operating systems, which use the abbreviations "lp", "lpr", or "LPT" to refer to printers.
1518) 16.595795, Programmer - Wikipedia, the free encyclopedia.txt#11, term: computer, content:Computer programmers often are grouped into two broad types: application programmers and systems programmers. Application programmers write programs to handle a specific job, such as a program to track inventory within an organization. They also may revise existing packaged software or customize generic applications which are frequently purchased from independent software vendors. Systems programmers, in contrast, write programs to maintain and control computer systems software, such as operating systems and database management systems. These workers make changes in the instructions that determine how the network, workstations, and CPU of the system handle the various jobs they have been given and how they communicate with peripheral equipment such as printers and disk drives.
1519) 16.595795, Programming language - Wikipedia, the free encyclopedia.txt#1, term: computer, content:The earliest known programmable machine preceded the invention of the digital computer and is the automatic flute player described in the 9th century by the brothers Musa in Baghdad, at the time a major centre of knowledge.[1] From the early 1800s, "programs" were used to direct the behavior of machines such as Jacquard looms and player pianos.[2] Thousands of different programming languages have been created, mainly in the computer field, and many more still are being created every year. Many programming languages require computation to be specified in an imperative form (i.e., as a sequence of operations to perform), while other languages use other forms of program specification such as the declarative form (i.e. the desired result is specified, not how to achieve it).
1520) 16.595795, Programming language - Wikipedia, the free encyclopedia.txt#6, term: computer, content:Another usage regards programming languages as theoretical constructs for programming abstract machines, and computer languages as the subset thereof that runs on physical computers, which have finite hardware resources.[24] John C. Reynolds emphasizes that formal specification languages are just as much programming languages as are the languages intended for execution. He also argues that textual and even graphical input formats that affect the behavior of a computer are programming languages, despite the fact they are commonly not Turing-complete, and remarks that ignorance of programming language concepts is the reason for many flaws in input formats.[25]
1521) 16.595795, Programming language - Wikipedia, the free encyclopedia.txt#53, term: computer, content:One common trend in the development of programming languages has been to add more ability to solve problems using a higher level of abstraction. The earliest programming languages were tied very closely to the underlying hardware of the computer. As new programming languages have developed, features have been added that let programmers express ideas that are more remote from simple translation into underlying hardware instructions. Because programmers are less tied to the complexity of the computer, their programs can do more computing with less effort from the programmer. This lets them write more functionality per time unit.[55]
1522) 16.595795, Register machine - Wikipedia, the free encyclopedia.txt#5, term: computer, content:Two trends appeared in the early 1950sthe first to characterize the computer as a Turing machine, the second to define computer-like modelsmodels with sequential instruction sequences and conditional jumpswith the power of a Turing machine, i.e. a so-called Turing equivalence. Need for this work was carried out in context of two "hard" problems: the unsolvable word problem posed by Emil Posthis problem of "tag"and the very "hard" problem of Hilbert's problemsthe 10th question around Diophantine equations. Researchers were questing for Turing-equivalent models that were less "logical" in nature and more "arithmetic" (cf Melzak (1961) p.281, Shepherdson-Sturgis (1963) p.218).
1523) 16.595795, RS-232 - Wikipedia, the free encyclopedia.txt#0, term: computer, content:In telecommunications, RS-232 is a standard for serial communication transmission of data. It formally defines the signals connecting between a DTE (data terminal equipment) such as a computer terminal, and a DCE (data circuit-terminating equipment or data communication equipment), such as a modem. The RS-232 standard is commonly used in computer serial ports. The standard defines the electrical characteristics and timing of signals, the meaning of signals, and the physical size and pinout of connectors. The current version of the standard is TIA-232-F Interface Between Data Terminal Equipment and Data Circuit-Terminating Equipment Employing Serial Binary Data Interchange, issued in 1997.
1524) 16.595795, Software - Wikipedia, the free encyclopedia.txt#3, term: computer, content:At the lowest level, executable code consists of machine language instructions specific to an individual processortypically a central processing unit (CPU). A machine language consists of groups of binary values signifying processor instructions that change the state of the computer from its preceding state. For example, an instruction may change the value stored in a particular storage location in the computeran effect that is not directly observable to the user. An instruction may also (indirectly) cause something to appear on a display of the computer systema state change which should be visible to the user. The processor carries out the instructions in the order they are provided, unless it is instructed to "jump" to a different instruction, or interrupted.
1525) 16.595795, Software bug - Wikipedia, the free encyclopedia.txt#20, term: computer, content:Finding and fixing bugs, or "debugging", has always been a major part of computer programming. Maurice Wilkes, an early computing pioneer, described his realization in the late 1940s that much of the rest of his life would be spent finding mistakes in his own programs.[20] As computer programs grow more complex, bugs become more common and difficult to fix. Often programmers spend more time and effort finding and fixing bugs than writing new code. Software testers are professionals whose primary task is to find bugs, or write code to support testing. On some projects, more resources may be spent on testing than in developing the program.
1526) 16.595795, Spreadsheet - Wikipedia, the free encyclopedia.txt#13, term: computer, content:A batch "spreadsheet" is indistinguishable from a batch compiler with added input data, producing an output report, i.e., a 4GL or conventional, non-interactive, batch computer program. However, this concept of an electronic spreadsheet was outlined in the 1961 paper "Budgeting Models and System Simulation" by Richard Mattessich.[7] The subsequent work by Mattessich (1964a, Chpt. 9, Accounting and Analytical Methods) and its companion volume, Mattessich (1964b, Simulation of the Firm through a Budget Computer Program) applied computerized spreadsheets to accounting and budgeting systems (on mainframe computers programmed in FORTRAN IV). These batch Spreadsheets dealt primarily with the addition or subtraction of entire columns or rows (of input variables), rather than individual cells.
1527) 16.595795, Spreadsheet - Wikipedia, the free encyclopedia.txt#14, term: computer, content:In 1962 this concept of the spreadsheet, called BCL for Business Computer Language, was implemented on an IBM 1130 and in 1963 was ported to an IBM 7040 by R. Brian Walsh at Marquette University, Wisconsin. This program was written in Fortran. Primitive timesharing was available on those machines. In 1968 BCL was ported by Walsh to the IBM 360/67 timesharing machine at Washington State University. It was used to assist in the teaching of finance to business students. Students were able to take information prepared by the professor and manipulate it to represent it and show ratios etc. In 1964, a book entitled Business Computer Language was written by Kimball, Stoffells and Walsh and both the book and program were copyrighted in 1966 and years later that copyright was renewed[8]
1528) 16.595795, Supercomputer - Wikipedia, the free encyclopedia.txt#14, term: computer, content:Systems with a massive number of processors generally take one of two paths. In the grid computing approach, the processing power of many computers, organised as distributed, diverse administrative domains, is opportunistically used whenever a computer is available.[6] In another approach, a large number of processors are used in proximity to each other, e.g. in a computer cluster. In such a centralized massively parallel system the speed and flexibility of the interconnect becomes very important and modern supercomputers have used various approaches ranging from enhanced Infiniband systems to three-dimensional torus interconnects.[34][35] The use of multi-core processors combined with centralization is an emerging direction, e.g. as in the Cyclops64 system.[8][9]
1529) 16.595795, Tablet computer - Wikipedia, the free encyclopedia.txt#0, term: computer, content:A tablet computer, commonly shortened to tablet, is a mobile computer with a touchscreen display, circuitry, and battery in a single device. Tablets come equipped with sensors, including cameras, a microphone, and an accelerometer, and the touchscreen display uses the recognition of finger or stylus gestures replacing the usage of the mouse and keyboard. They usually feature on-screen, pop-up virtual keyboards for typing. Tablets may have physical buttons for basic features such as speaker volume and power, and ports for network communications and battery charging. Tablets are typically larger than smartphones or personal digital assistants with screens 7 inches (18cm) or larger, measured diagonally.[1][2][3][4]
1530) 16.595795, Telecommunications engineering - Wikipedia, the free encyclopedia.txt#15, term: computer, content:On 11 September 1940, George Stibitz was able to transmit problems using teleprinter to his Complex Number Calculator in New York and receive the computed results back at Dartmouth College in New Hampshire.[15] This configuration of a centralized computer or mainframe computer with remote "dumb terminals" remained popular throughout the 1950s and into the 1960s. However, it was not until the 1960s that researchers started to investigate packet switching  a technology that allows chunks of data to be sent between different computers without first passing through a centralized mainframe. A four-node network emerged on 5 December 1969. This network soon became the ARPANET, which by 1981 would consist of 213 nodes.[16]
1531) 16.595795, Theory of computation - Wikipedia, the free encyclopedia.txt#10, term: computer, content:In order to analyze how much time and space a given algorithm requires, computer scientists express the time or space required to solve the problem as a function of the size of the input problem. For example, finding a particular number in a long list of numbers becomes harder as the list of numbers grows larger. If we say there are n numbers in the list, then if the list is not sorted or indexed in any way we may have to look at every number in order to find the number we're seeking. We thus say that in order to solve this problem, the computer needs to perform a number of steps that grows linearly in the size of the problem.
1532) 16.595795, Tom Kilburn - Wikipedia, the free encyclopedia.txt#11, term: computer, content:Kilburn became a professor of computing engineering in the Department of Electrical Engineering at Manchester in 1960. He was instrumental in forming the School of Computer Science in 1964, becoming the first head of the department, and served as Dean of the Faculty of Science from 1970 to 1972, and pro-vice-chancellor of the university from 1976 to 1979.[2] His final computer project was the MU5, which was designed to facilitate the running of programs in high-level programming languages. An analysis of code written for the Atlas gave an insight into the frequency of different operands and control structures. The project was assisted by a 630,000 Science Research Council (SRC) grant awarded over five-years. The design heavily influenced the successful ICL 2900 Series.[2][12]
1533) 16.595795, Transistor computer - Wikipedia, the free encyclopedia.txt#9, term: computer, content:First generation computers were largely out of reach of schools and hobbyists who wished to build their own, largely because of the cost of the large number of vacuum tubes required (though relay-based computer projects were undertaken[17]). The fourth generation (VLSI) was also largely out of reach, too, due to most of the design work being inside the integrated circuit package (though this barrier, too, was later removed[18]). So, second and third generation computer design (transistors and SSI) were perhaps the best suited to being undertaken by schools and hobbyists.[19]
1534) 16.595795, Turing completeness - Wikipedia, the free encyclopedia.txt#1, term: computer, content:A closely related concept is that of Turing equivalence  two computers P and Q are called equivalent if P can simulate Q and Q can simulate P. The ChurchTuring thesis conjectures that any function whose values can be computed by an algorithm can be computed by a Turing machine, and therefore that if any real-world computer can be simulated by a Turing machine, it is Turing equivalent to a Turing machine. A Universal Turing machine can be used to simulate any Turing machine and by extension the computational aspects of any possible real-world computer.[NB 1]
1535) 16.595795, Unconventional computing - Wikipedia, the free encyclopedia.txt#2, term: computer, content:Unconventional computing is, according to a recent conference description,[2] "an interdisciplinary research area with the main goal to enrich or go beyond the standard models, such as the Von Neumann computer architecture and the Turing machine, which have dominated computer science for more than half a century". These methods model their computational operations based on non-standard paradigms, and are currently mostly in the research and development stage. This computing behavior can be "simulated" using the classical silicon-based micro-transistors or solid state computing technologies, but aim to achieve a new kind of computing engineering inspired in nature.
1536) 16.595795, Von Neumann architecture - Wikipedia, the free encyclopedia.txt#12, term: computer, content:Von Neumann was involved in the Manhattan Project at the Los Alamos National Laboratory, which required huge amounts of calculation. This drew him to the ENIAC project, during the summer of 1944. There he joined into the ongoing discussions on the design of this stored-program computer, the EDVAC. As part of that group, he wrote up a description titled First Draft of a Report on the EDVAC[1] based on the work of Eckert and Mauchly. It was unfinished when his colleague Herman Goldstine circulated it with only von Neumann's name on it, to the consternation of Eckert and Mauchly.[10] The paper was read by dozens of von Neumann's colleagues in America and Europe, and influenced the next round of computer designs.
1537) 16.595795, Wearable computer - Wikipedia, the free encyclopedia.txt#22, term: computer, content:The 1980s saw the rise of more general-purpose wearable computers that fit the modern definition of "computer" by going beyond task-specific hardware to more general-purpose (e.g. re-programmable by the user) devices. In 1981, Steve Mann designed and built a backpack-mounted 6502-based wearable multimedia computer with text, graphics, and multimedia capability, as well as video capability (cameras and other photographic systems). Mann went on to be an early and active researcher in the wearables field, especially known for his 1994 creation of the Wearable Wireless Webcam, the first example of Lifelogging.[22][23]
1538) 16.425577, Free software - Wikipedia, the free encyclopedia.txt#3, term: computer, content:From the 1950s up until the early 1970s, it was normal for computer users to have the software freedoms associated with free software, which was typically public domain software.[12] Software was commonly shared by individuals who used computers and by hardware manufacturers who welcomed the fact that people were making software that made their hardware useful. Organizations of users and suppliers, for example, SHARE, were formed to facilitate exchange of software. As software was often written in an interpreted language such as BASIC, the source code was distributed to use a software. Software was also shared and distributed as printed source code (Type-in program) in computer magazines (like Creative Computing, SoftSide, Compute!, Byte etc) and books, like the bestseller BASIC Computer Games.[14] By the early 1970s, the picture changed: software costs were dramatically increasing, a growing software industry was competing with the hardware manufacturer's bundled software products (free in that the cost was included in the hardware cost), leased machines required software support while providing no revenue for software, and some customers able to better meet their own needs did not want the costs of "free" software bundled with hardware product costs. In United States vs. IBM, filed January 17, 1969, the government charged that bundled software was anti-competitive.[15] While some software might always be free, there would henceforth be a growing amount of software produced primarily for sale. In the 1970s and early 1980s, the software industry began using technical measures (such as only distributing binary copies of computer programs) to prevent computer users from being able to study or adapt the software as they saw fit. In 1980 copyright law was extended to computer programs.
1539) 14.518295, BASIC - Wikipedia, the free encyclopedia.txt#14, term: computer, content:As the popularity of BASIC grew in this period, magazines published complete source code in BASIC for games, utilities, and other programs. Given BASIC's straightforward nature, it was a simple matter to type in the code from the magazine and execute the program. Different magazines were published featuring programs for specific computers, though some BASIC programs were considered universal and could be used in machines running any variant of BASIC (sometimes with minor adaptations). Many books of type-in programs were also available, and in particular, Ahl published versions of the original 101 BASIC games converted into the Microsoft dialect and published it from Creative Computing as BASIC Computer Games. This book, and its sequels, provided hundreds of ready-to-go programs that could be easily converted to practically any BASIC-running platform.[13][14][15] The book reached the stores in 1978, just as the home computer market was starting off, and it became the first million-selling computer book. Later packages, such as Learn to Program BASIC would also have gaming as an introductory focus.
1540) 14.518295, Computer - Simple English Wikipedia, the free encyclopedia.txt#34, term: computer, content:Computers can become obsolete quickly, depending on what programs the user runs. Very often, they are thrown away within two or three years, because newer programs require a more powerful computer. This makes the problem worse, so computer recycling happens a lot. Many projects try to send working computers to developing nations so they can be re-used and will not become waste as quickly, as most people do not need to run new programs. Some computer parts, such as hard drives, can break easily. When these parts end up in the landfill, they can put poisonous chemicals like lead into the ground water. Hard drives can also contain secret information like credit card numbers. If the hard drive is not erased before being thrown away, an identity thief can get the information off of the hard drive, even if the drive doesn't work, and use it to steal money from the previous owner's bank account.
1541) 14.518295, Computer - Wikipedia, the free encyclopedia.txt#46, term: computer, content:In most computers, individual instructions are stored as machine code with each instruction being given a unique number (its operation code or opcode for short). The command to add two numbers together would have one opcode; the command to multiply them would have a different opcode, and so on. The simplest computers are able to perform any of a handful of different instructions; the more complex computers have several hundred to choose from, each with a unique numerical code. Since the computer's memory is able to store numbers, it can also store the instruction codes. This leads to the important fact that entire programs (which are just lists of these instructions) can be represented as lists of numbers and can themselves be manipulated inside the computer in the same way as numeric data. The fundamental concept of storing programs in the computer's memory alongside the data they operate on is the crux of the von Neumann, or stored program[citation needed], architecture. In some cases, a computer might store some or all of its program in memory that is kept separate from the data it operates on. This is called the Harvard architecture after the Harvard Mark I computer. Modern von Neumann computers display some traits of the Harvard architecture in their designs, such as in CPU caches.
1542) 14.518295, Computer animation - Wikipedia, the free encyclopedia.txt#23, term: computer, content:The goal of computer animation is not always to emulate live action as closely as possible, so many animated films instead feature characters who are anthropomorphic animals (Finding Nemo, Ice Age, Bolt, Madagascar, Over the Hedge, Rio, Kung Fu Panda, Alpha and Omega, Zootopia), machines (Cars, WALL-E, Robots), insects (Antz, A Bug's Life, The Ant Bully, Bee Movie), fantasy creatures and characters (Monsters, Inc., Shrek, TMNT, Brave, Epic), or humans with non-realistic, cartoon-like proportions (Despicable Me, Up, Megamind, The Incredibles, Jimmy Neutron: Boy Genius, Planet 51, Hotel Transylvania, Cloudy with a Chance of Meatballs). Computer animation can also be tailored to mimic or substitute for other kinds of animation, traditional stop-motion animation (as shown in Flushed Away or The Lego Movie). Some of the long-standing basic principles of animation, like squash & stretch, call for movement that is not strictly realistic, and such principles still see widespread application in computer animation.
1543) 14.518295, Computer graphics - Wikipedia, the free encyclopedia.txt#5, term: computer, content:The precursor sciences to the development of modern computer graphics were the advances in electrical engineering, electronics, and television that took place during the first half of the twentieth century. Screens could display art since the Lumiere brothers' use of mattes to create special effects for the earliest films dating from 1895, but such displays were limited and not interactive. The first cathode ray tube, the Braun tube, was invented in 1897 - it in turn would permit the oscilloscope and the military control panel - the more direct precursors of the field, as they provided the first two-dimensional electronic displays that responded to programmatic or user input. Nevertheless, computer graphics remained relatively unknown as a discipline until the 1950s and the post-World War II period - during which time, the discipline emerged from a combination of both pure university and laboratory academic research into more advanced computers and the United States military's further development of technologies like radar, advanced aviation, and rocketry developed during the war. New kinds of displays were needed to process the wealth of information resulting from such projects, leading to the development of computer graphics as a discipline.
1544) 14.518295, Computer graphics - Wikipedia, the free encyclopedia.txt#54, term: computer, content:Rendering is the generation of a 2D image from a 3D model by means of computer programs. A scene file contains objects in a strictly defined language or data structure; it would contain geometry, viewpoint, texture, lighting, and shading information as a description of the virtual scene. The data contained in the scene file is then passed to a rendering program to be processed and output to a digital image or raster graphics image file. The rendering program is usually built into the computer graphics software, though others are available as plug-ins or entirely separate programs. The term "rendering" may be by analogy with an "artist's rendering" of a scene. Though the technical details of rendering methods vary, the general challenges to overcome in producing a 2D image from a 3D representation stored in a scene file are outlined as the graphics pipeline along a rendering device, such as a GPU. A GPU is a device able to assist the CPU in calculations. If a scene is to look relatively realistic and predictable under virtual lighting, the rendering software should solve the rendering equation. The rendering equation does not account for all lighting phenomena, but is a general lighting model for computer-generated imagery. 'Rendering' is also used to describe the process of calculating effects in a video editing file to produce final video output.
1545) 14.518295, Computer music - Wikipedia, the free encyclopedia.txt#6, term: computer, content:Early computer-music programs typically did not run in real time. Programs would run for hours or days, on multimillion-dollar computers, to generate a few minutes of music.[6][7] One way around this was to use a 'hybrid system', most notably the Roland MC-8 Microcomposer, where a microprocessor-based system controls an analog synthesizer, released in 1978.[5] John Chowning's work on FM synthesis from the 1960s to the 1970s allowed much more efficient digital synthesis,[8] eventually leading to the development of the affordable FM synthesis-based Yamaha DX7 digital synthesizer, released in 1983.[9] In addition to the Yamaha DX7, the advent of inexpensive digital chips and microcomputers opened the door to real-time generation of computer music.[9] In the 1980s, Japanese personal computers such as the NEC PC-88 came installed with FM synthesis sound chips and featured audio programming languages such as Music Macro Language (MML) and MIDI interfaces, which were most often used to produce video game music, or chiptunes.[5] By the early 1990s, the performance of microprocessor-based computers reached the point that real-time generation of computer music using more general programs and algorithms became possible.[10]
1546) 14.518295, Computer science - Wikipedia, the free encyclopedia.txt#8, term: computer, content:Although first proposed in 1956,[14] the term "computer science" appears in a 1959 article in Communications of the ACM,[26] in which Louis Fein argues for the creation of a Graduate School in Computer Sciences analogous to the creation of Harvard Business School in 1921,[27] justifying the name by arguing that, like management science, the subject is applied and interdisciplinary in nature, while having the characteristics typical of an academic discipline.[26] His efforts, and those of others such as numerical analyst George Forsythe, were rewarded: universities went on to create such programs, starting with Purdue in 1962.[28] Despite its name, a significant amount of computer science does not involve the study of computers themselves. Because of this, several alternative names have been proposed.[29] Certain departments of major universities prefer the term computing science, to emphasize precisely that difference. Danish scientist Peter Naur suggested the term datalogy,[30] to reflect the fact that the scientific discipline revolves around data and data treatment, while not necessarily involving computers. The first scientific institution to use the term was the Department of Datalogy at the University of Copenhagen, founded in 1969, with Peter Naur being the first professor in datalogy. The term is used mainly in the Scandinavian countries. An alternative term, also proposed by Naur, is data science; this is now used for a distinct field of data analysis, including statistics and databases.
1547) 14.518295, Computer-aided design - Wikipedia, the free encyclopedia.txt#29, term: computer, content:It is argued that a turning point was the development of the SKETCHPAD system at MIT[22][23] by Ivan Sutherland (who later created a graphics technology company with Dr. David Evans). The distinctive feature of SKETCHPAD was that it allowed the designer to interact with his computer graphically: the design can be fed into the computer by drawing on a CRT monitor with a light pen. Effectively, it was a prototype of graphical user interface, an indispensable feature of modern CAD. Sutherland presented his paper Sketchpad: A Man-Machine Graphical Communication System in 1963 at a Joint Computer Conference having worked on it as his PhD thesis paper for a few years. Quoting,"For drawings where motion of the drawing, or analysis of a drawn problem is of value to the user, Sketchpad excels. For highly repetitive drawings or drawings where accuracy is required, Sketchpad is sufficiently faster than conventional techniques to be worthwhile. For drawings which merely communicate with shops, it is probably better to use conventional paper and pencil." Over time efforts would be directed toward the goal of having the designers drawings communicate not just with shops but with the shop tool itself. This goal would be a long time arriving.
1548) 14.518295, Digital video - Wikipedia, the free encyclopedia.txt#8, term: computer, content:QuickTime, Apple Computer's architecture for time-based and streaming data formats appeared in June, 1991. Initial consumer-level content creation tools were crude, requiring an analog video source to be digitized to a computer-readable format. While low-quality at first, consumer digital video increased rapidly in quality, first with the introduction of playback standards such as MPEG-1 and MPEG-2 (adopted for use in television transmission and DVD media), and then the introduction of the DV tape format allowing recordings in the format to be transferred direct to digital video files (containing the same video data recorded on the transferred DV tape) on an editing computer and simplifying the editing process, allowing non-linear editing systems (NLE) to be deployed cheaply and widely on desktop computers with no external playback/recording equipment needed, save for the computer simply requiring a FireWire port to interface to the DV-format camera or VCR. The widespread adoption of digital video has also drastically reduced the bandwidth needed for a high-definition video signal (with HDV and AVCHD, as well as several commercial variants such as DVCPRO-HD, all using less bandwidth than a standard definition analog signal) and tapeless camcorders based on flash memory and often a variant of MPEG-4.
1549) 14.518295, Honeywell, Inc. v. Sperry Rand Corp. - Wikipedia, the free encyclopedia.txt#9, term: computer, content:Charges of derivation stemmed from testimony and correspondence describing meetings between Atanasoff and Mauchly in December 1940 and June 1941, the first at the University of Pennsylvania where Atanasoff attended a talk given by Mauchly at a meeting of the American Association for the Advancement of Science on use of Mauchly's harmonic analyzer (a simple analog computer) to speed the calculation of meteorological data to test for periodicities in precipitation, and the second in Ames, Iowa where Mauchly had driven to visit Atanasoff for a period of five days and to examine his progress on a special-purpose computing machine whose construction Atanasoff had described for Mauchly at the prior meeting. (In the discovery process leading up to Honeywell v. Sperry Rand, this device came to be called the AtanasoffBerry Computer, or ABC; Clifford Berry had been Atanasoff's graduate student assistant in the computer development project in the basement of the physics building at Iowa State College and in 1942 the two of them left Iowa State for positions in war researchAtanasoff in Washington, D.C. and Berry in Pasadena, California.)
1550) 14.518295, IBM PC compatible - Wikipedia, the free encyclopedia.txt#30, term: computer, content:By the late 1990s, the success of Microsoft Windows had driven rival commercial operating systems into near-extinction, and had ensured that the IBM PC compatible computer was the dominant computing platform. This meant that if a developer made their software only for the Wintel platform, they would still be able to reach the vast majority of computer users. By the late 1980s, the only major competitor to Windows with more than a few percentage points of market share was Apple Inc.'s Macintosh. The Mac started out billed as "the computer for the rest of us" but the Mac's high prices and closed architecture meant the DOS/Windows onslaught quickly drove the Macintosh into an education and desktop publishing niche, from which it only emerged in the mid-2000s. By the mid-1990s the Mac's market share had dwindled to around 5% and introducing a new rival operating system had become too risky a commercial venture. Experience had shown that even if an operating system was technically superior to Windows, it would be a failure in the market (BeOS and OS/2 for example). In 1989 Steve Jobs said of his new NeXT system, "It will either be the last new hardware platform to succeed, or the first to fail." Four years later in 1993 NeXT announced it was ending production of the NeXTcube and porting NeXTSTEP to Intel processors.
1551) 14.518295, Intel 8008 - Wikipedia, the free encyclopedia.txt#2, term: computer, content:In order to address the heating and other issues, a re-design started that featured the CPU part of the internal circuitry re-implemented on a single chip. Looking for a company able to produce their chip design, Roche turned to Intel, then primarily a vendor of memory chips.[3] Roche met with Bob Noyce, who expressed concern with the concept; John Frassanito recalls that "Noyce said it was an intriguing idea, and that Intel could do it, but it would be a dumb move. He said that if you have a computer chip, you can only sell one chip per computer, while with memory, you can sell hundreds of chips per computer."[3] Another major concern was that Intel's existing customer base purchased their memory chips for use with their own processor designs; if Intel introduced their own processor, they might be seen as a competitor, and their customers might look elsewhere for memory. Nevertheless, Noyce agreed to a $50,000 development contract in early 1970. Texas Instruments (TI) was also brought in as a second supplier.
1552) 14.518295, John von Neumann - Wikipedia, the free encyclopedia.txt#72, term: computer, content:Stochastic computing was first introduced in a pioneering paper by von Neumann in 1953.[131] However, the theory could not be implemented until advances in computing of the 1960s.[132][133] He also created the field of cellular automata without the aid of computers, constructing the first self-replicating automata with pencil and graph paper. The concept of a universal constructor based on the von Neumann cellular automaton was fleshed out in his posthumous work Theory of Self Reproducing Automata.[134] The von Neumann neighborhood, in which each cell in a two-dimensional grid has the four orthogonally adjacent grid cells as neighbors, continues to be used for other cellular automata. Von Neumann proved that the most effective way of performing large-scale mining operations such as mining an entire moon or asteroid belt would be by using self-replicating spacecraft, taking advantage of their exponential growth.[135] His rigorous mathematical analysis of the structure of self-replication (of the semiotic relationship between constructor, description and that which is constructed), preceded the discovery of the structure of DNA.[136] Beginning in 1949, von Neumann's design for a self-reproducing computer program is considered the world's first computer virus, and he is considered to be the theoretical father of computer virology.[137]
1553) 14.518295, Magnetic-core memory - Wikipedia, the free encyclopedia.txt#7, term: computer, content:The MIT Whirlwind computer required a fast memory system for real-time aircraft tracking use. At first, Williams tubesa storage system based on cathode ray tubeswere used, but these devices were always temperamental and unreliable. Several researchers in the late 1940s conceived the idea of using magnetic cores for computer memory, but Jay Forrester received the principal patent for his invention of the coincident core memory that enabled the 3D storage of information.[5] William Papian of Project Whirlwind cited one of these efforts, Harvard's "Static Magnetic Delay Line", in an internal memo. The first core memory of 32 x 32 x 16 bits was installed on Whirlwind in the summer of 1953. Papian, described: "Magnetic-Core Storage has two big advantages: (1) greater reliability with a consequent reduction in maintenance time devoted to storage; (2) shorter access time (core access time is 9 microseconds: tube access time is approximately 25 microseconds) thus increasing the speed of computer operation."[6]
1554) 14.518295, Microphone - Wikipedia, the free encyclopedia.txt#17, term: computer, content:Due to their good performance and ease of manufacture, hence low cost, the vast majority of microphones made today are electret microphones; a semiconductor manufacturer[22] estimates annual production at over one billion units. Nearly all cell-phone, computer, PDA and headset microphones are electret types. They are used in many applications, from high-quality recording and lavalier use to built-in microphones in small sound recording devices and telephones. Though electret microphones were once considered low quality, the best ones can now rival traditional condenser microphones in every respect and can even offer the long-term stability and ultra-flat response needed for a measurement microphone. Unlike other capacitor microphones, they require no polarizing voltage, but often contain an integrated preamplifier that does require power (often incorrectly called polarizing power or bias). This preamplifier is frequently phantom powered in sound reinforcement and studio applications. Monophonic microphones designed for personal computer (PC) use, sometimes called multimedia microphones, use a 3.5mm plug as usually used, without power, for stereo; the ring, instead of carrying the signal for a second channel, carries power via a resistor from (normally) a 5V supply in the computer. Stereophonic microphones use the same connector; there is no obvious way to determine which standard is used by equipment and microphones.
1555) 14.518295, Quantum computing - Wikipedia, the free encyclopedia.txt#15, term: computer, content:Integer factorization, which underpins the security of public key cryptographic systems, is believed to be computationally infeasible with an ordinary computer for large integers if they are the product of few prime numbers (e.g., products of two 300-digit primes).[13] By comparison, a quantum computer could efficiently solve this problem using Shor's algorithm to find its factors. This ability would allow a quantum computer to decrypt many of the cryptographic systems in use today, in the sense that there would be a polynomial time (in the number of digits of the integer) algorithm for solving the problem. In particular, most of the popular public key ciphers are based on the difficulty of factoring integers or the discrete logarithm problem, both of which can be solved by Shor's algorithm. In particular the RSA, Diffie-Hellman, and Elliptic curve Diffie-Hellman algorithms could be broken. These are used to protect secure Web pages, encrypted email, and many other types of data. Breaking these would have significant ramifications for electronic privacy and security, but in exchange, would render malicious usage of cryptography, such as Ransomware useless.
1556) 14.518295, Video card - Wikipedia, the free encyclopedia.txt#20, term: computer, content:The RAMDAC, or Random Access Memory Digital-to-Analog Converter, converts digital signals to analog signals for use by a computer display that uses analog inputs such as Cathode ray tube (CRT) displays. The RAMDAC is a kind of RAM chip that regulates the functioning of the graphics card. Depending on the number of bits used and the RAMDAC-data-transfer rate, the converter will be able to support different computer-display refresh rates. With CRT displays, it is best to work over 75Hz and never under 60Hz, in order to minimize flicker.[24] (With LCD displays, flicker is not a problem.[citation needed]) Due to the growing popularity of digital computer displays and the integration of the RAMDAC onto the GPU die, it has mostly disappeared as a discrete component. All current LCD/plasma monitors and TVs and projectors with only digital connections, work in the digital domain and do not require a RAMDAC for those connections. There are displays that feature analog inputs (VGA, component, SCART etc.) only. These require a RAMDAC, but they reconvert the analog signal back to digital before they can display it, with the unavoidable loss of quality stemming from this digital-to-analog-to-digital conversion.[25] With VGA standard being phased out in favor of digital, RAMDACs will begin to disappear from video cards.
1557) 14.518295, Wearable computer - Wikipedia, the free encyclopedia.txt#29, term: computer, content:In 1994, Edgar Matias and Mike Ruicci of the University of Toronto, debuted a "wrist computer." Their system presented an alternative approach to the emerging head-up display plus chord keyboard wearable. The system was built from a modified HP 95LX palmtop computer and a Half-QWERTY one-handed keyboard. With the keyboard and display modules strapped to the operator's forearms, text could be entered by bringing the wrists together and typing.[30] The same technology was used by IBM researchers to create the half-keyboard "belt computer.[31] Also in 1994, Mik Lamming and Mike Flynn at Xerox EuroPARC demonstrated the Forget-Me-Not, a wearable device that would record interactions with people and devices and store this information in a database for later query.[32] It interacted via wireless transmitters in rooms and with equipment in the area to remember who was there, who was being talked to on the telephone, and what objects were in the room, allowing queries like "Who came by my office while I was on the phone to Mark?". As with the Toronto system, Forget-Me-Not was not based on a head-mounted display.
1558) 14.518295, Wearable computer - Wikipedia, the free encyclopedia.txt#47, term: computer, content:Evidence of weak market acceptance was demonstrated when Panasonic Computer Solutions Company's product failed. Panasonic has specialized in mobile computing with their Toughbook line for over 10 years and has extensive market research into the field of portable, wearable computing products. In 2002, Panasonic introduced a wearable brick computer coupled with a handheld or a touchscreen worn on the arm. The "Brick" Computer is the CF-07 Toughbook, dual batteries, screen used same batteries as the base, 800 x 600 resolution, optional GPS and WWAN. Has one M-PCI slot and one PCMCIA slot for expansion. CPU used is a 600MHz Pentium 3 factory under clocked to 300MHz so it can stay cool passively as it has no fan. Micro DIM RAM is upgradeable. The screen can be used wirelessly on other computers. The brick would communicate wirelessly to the screen, and concurrently the brick would communicate wirelessly out to the internet or other networks. The wearable brick was quietly pulled from the market in 2005, while the screen evolved to a thin client touchscreen used with a handstrap.
1559) 14.224966, 64-bit computing - Wikipedia, the free encyclopedia.txt#6, term: computer, content:Many computer instruction sets are designed so that a single integer register can store the address to any location in the computer's physical or virtual memory. Therefore, the total number of addresses to memory is often determined by the width of these registers. The IBM System/360 of the 1960s was an early 32-bit computer; it had 32-bit integer registers, although it only used the low order 24 bits of a word for addresses, resulting in a 16MiB [16  10242 bytes] address space. 32-bit superminicomputers, such as the DEC VAX, became common in the 1970s, and 32-bit microprocessors, such as the Motorola 68000 family and the 32-bit members of the x86 family starting with the Intel 80386, appeared in the mid-1980s, making 32 bits something of a de facto consensus as a convenient register size.
1560) 14.224966, 86-DOS - Wikipedia, the free encyclopedia.txt#2, term: computer, content:86-DOS was created because sales of the Seattle Computer Products 8086 computer kit, demonstrated in June 1979 and shipped in November,[1] were languishing due to the absence of an operating system. The only software which SCP could sell with the board was Microsoft's Standalone Disk BASIC-86, which Microsoft had developed on a prototype of SCP's hardware.[1] SCP wanted to offer the 8086-version of CP/M that Digital Research had announced, but its release date was uncertain.[2] This was not the first time Digital Research had lagged behind hardware developments; two years earlier it had been slow to adapt CP/M for new floppy disk formats and hard disks. In April 1980 SCP assigned 24-year-old Tim Paterson to develop a substitute for CP/M-86.[1]
1561) 14.224966, Advanced Micro Devices - Wikipedia, the free encyclopedia.txt#62, term: computer, content:In August 2003, AMD also purchased the Geode business which was originally the Cyrix MediaGX from National Semiconductor to augment its existing line of embedded x86 processor products. During the second quarter of 2004, it launched new low-power Geode NX processors based on the K7 Thoroughbred architecture with speeds of fanless processors 667 MHz and 1 GHz, and 1.4 GHz processor with fan, of TDP 25W. This technology is used in a variety of embedded systems (Casino slot machines and customer kiosks for instance), several UMPC designs in Asia markets, as well as the OLPC XO-1 computer, an inexpensive laptop computer intended to be distributed to children in developing countries around the world. The Geode LX processor was announced in 2005 and is said will continue to be available through 2015.
1562) 14.224966, Alan Turing - Wikipedia, the free encyclopedia.txt#43, term: computer, content:In 1948 Turing was appointed Reader in the Mathematics Department at the Victoria University of Manchester. In 1949, he became Deputy Director of the Computing Machine Laboratory there, working on software for one of the earliest stored-programme computersthe Manchester Mark 1. During this time he continued to do more abstract work in mathematics,[91] and in "Computing Machinery and Intelligence" (Mind, October 1950), Turing addressed the problem of artificial intelligence, and proposed an experiment that became known as the Turing test, an attempt to define a standard for a machine to be called "intelligent". The idea was that a computer could be said to "think" if a human interrogator could not tell it apart, through conversation, from a human being.[92] In the paper, Turing suggested that rather than building a programme to simulate the adult mind, it would be better rather to produce a simpler one to simulate a child's mind and then to subject it to a course of education. A reversed form of the Turing test is widely used on the Internet; the CAPTCHA test is intended to determine whether the user is a human or a computer.
1563) 14.224966, Alan Turing - Wikipedia, the free encyclopedia.txt#59, term: computer, content:Turing has been honoured in various ways in Manchester, the city where he worked towards the end of his life. In 1994, a stretch of the A6010 road (the Manchester city intermediate ring road) was named "Alan Turing Way". A bridge carrying this road was widened, and carries the name Alan Turing Bridge. A statue of Turing was unveiled in Manchester on 23 June 2001 in Sackville Park, between the University of Manchester building on Whitworth Street and Canal Street. The memorial statue depicts the "father of computer science" sitting on a bench at a central position in the park. Turing is shown holding an apple. The cast bronze bench carries in relief the text 'Alan Mathison Turing 19121954', and the motto 'Founder of Computer Science' as it could appear if encoded by an Enigma machine: 'IEKYF ROMSI ADXUO KVKZC GUBJ'.
1564) 14.224966, Algorithm - Wikipedia, the free encyclopedia.txt#76, term: computer, content:The clock: Bolter credits the invention of the weight-driven clock as "The key invention [of Europe in the Middle Ages]", in particular the verge escapement[68] that provides us with the tick and tock of a mechanical clock. "The accurate automatic machine"[69] led immediately to "mechanical automata" beginning in the 13th century and finally to "computational machines"the difference engine and analytical engines of Charles Babbage and Countess Ada Lovelace, mid-19th century.[70] Lovelace is credited with the first creation of an algorithm intended for processing on a computer  Babbage's analytical engine, the first device considered a real Turing-complete computer instead of just a calculator  and is sometimes called "history's first programmer" as a result, though a full implementation of Babbage's second device would not be realized until decades after her lifetime.
1565) 14.224966, Analog computer - Wikipedia, the free encyclopedia.txt#8, term: computer, content:Many mechanical aids to calculation and measurement were constructed for astronomical and navigation use. The planisphere was a star chart invented by Ab Rayn al-Brn in the early 11th century.[3] The astrolabe was invented in the Hellenistic world in either the 1st or 2nd centuries BC and is often attributed to Hipparchus. A combination of the planisphere and dioptra, the astrolabe was effectively an analog computer capable of working out several different kinds of problems in spherical astronomy. An astrolabe incorporating a mechanical calendar computer[4][5] and gear-wheels was invented by Abi Bakr of Isfahan, Persia in 1235.[6] Ab Rayhn al-Brn invented the first mechanical geared lunisolar calendar astrolabe,[7] an early fixed-wired knowledge processing machine[8] with a gear train and gear-wheels,[9] circa 1000 AD.
1566) 14.224966, Analog computer - Wikipedia, the free encyclopedia.txt#20, term: computer, content:Educational analog computers illustrated the principles of analog calculation. The Heathkit EC-1, a $199 educational analog computer, was made by the Heath Company, USA c. 1960.[17] It was programmed using patch cords that connected nine operational amplifiers and other components.[18] General Electric also marketed an "educational" analog computer kit of a simple design in the early 1960s consisting of a two transistor tone generator and three potentiometers wired such that the frequency of the oscillator was nulled when the potentiometer dials were positioned by hand to satisfy an equation. The relative resistance of the potentiometer was then equivalent to the formula of the equation being solved. Multiplication or division could be performed depending on which dials were considered inputs and which was the output. Accuracy and resolution was limited and a simple slide rule was more accurate; however, the unit did demonstrate the basic principle.
1567) 14.224966, Antivirus software - Wikipedia, the free encyclopedia.txt#70, term: computer, content:Active anti-virus software can interfere with a firmware update process.[137] Any writeable firmware in the computer can be infected by malicious code.[138] This is a major concern, as an infected BIOS could require the actual BIOS chip to be replaced to ensure the malicious code is completely removed.[139] Anti-virus software is not effective at protecting firmware and the motherboard BIOS from infection.[140] In 2014, security researchers discovered that USB devices contain writeable firmware which can be modified with malicious code (dubbed "BadUSB"), which anti-virus software cannot detect or prevent. The malicious code can run undetected on the computer and could even infect the operating system prior to it booting up.[141][142]
1568) 14.224966, Binary number - Wikipedia, the free encyclopedia.txt#15, term: computer, content:In November 1937, George Stibitz, then working at Bell Labs, completed a relay-based computer he dubbed the "Model K" (for "Kitchen", where he had assembled it), which calculated using binary addition.[21] Bell Labs thus authorized a full research program in late 1938 with Stibitz at the helm. Their Complex Number Computer, completed 8 January 1940, was able to calculate complex numbers. In a demonstration to the American Mathematical Society conference at Dartmouth College on 11 September 1940, Stibitz was able to send the Complex Number Calculator remote commands over telephone lines by a teletype. It was the first computing machine ever used remotely over a phone line. Some participants of the conference who witnessed the demonstration were John von Neumann, John Mauchly and Norbert Wiener, who wrote about it in his memoirs.[22][23][24]
1569) 14.224966, Bit - Wikipedia, the free encyclopedia.txt#12, term: computer, content:In the earliest non-electronic information processing devices, such as Jacquard's loom or Babbage's Analytical Engine, a bit was often stored as the position of a mechanical lever or gear, or the presence or absence of a hole at a specific point of a paper card or tape. The first electrical devices for discrete logic (such as elevator and traffic light control circuits, telephone switches, and Konrad Zuse's computer) represented bits as the states of electrical relays which could be either "open" or "closed". When relays were replaced by vacuum tubes, starting in the 1940s, computer builders experimented with a variety of storage methods, such as pressure pulses traveling down a mercury delay line, charges stored on the inside surface of a cathode-ray tube, or opaque spots printed on glass discs by photolithographic techniques.
1570) 14.224966, Bit - Wikipedia, the free encyclopedia.txt#16, term: computer, content:Multiple bits may be expressed and represented in several ways. For convenience of representing commonly reoccurring groups of bits in information technology, several units of information have traditionally been used. The most common is the unit byte, coined by Werner Buchholz in July 1956, which historically was used to represent the group of bits used to encode a single character of text (until UTF-8 multibyte encoding took over) in a computer[10][11] and for this reason it was used as the basic addressable element in many computer architectures. The trend in hardware design converged on the most common implementation of using eightbits per byte, as it is widely used today. However, because of the ambiguity of relying on the underlying hardware design, the unit octet was defined to explicitly denote a sequence of eightbits.
1571) 14.224966, Booting - Wikipedia, the free encyclopedia.txt#1, term: computer, content:A boot loader is a computer program that loads an operating system or some other system software for the computer after completion of the power-on self-tests; it is the loader for the operating system itself. Within the hard reboot process, it runs after completion of the self-tests, then loads and runs the software. A boot loader is loaded into main memory from persistent memory, such as a hard disk drive or, in some older computers, from a medium such as punched cards, punched tape, or magnetic tape. The boot loader then loads and executes the processes that finalize the boot. Like POST processes, the boot loader code comes from a "hard-wired" and persistent location; if that location is too limited for some reason, that primary boot loader calls a second-stage boot loader or a secondary program loader.
1572) 14.224966, Byte - Wikipedia, the free encyclopedia.txt#0, term: computer, content:The byte (/bat/) is a unit of digital information that most commonly consists of eight bits. Historically, the byte was the number of bits used to encode a single character of text in a computer[1][2] and for this reason it is the smallest addressable unit of memory in many computer architectures. The size of the byte has historically been hardware dependent and no definitive standards existed that mandated the size. The de facto standard of eight bits is a convenient power of two permitting the values 0 through 255 for one byte. The international standard IEC 80000-13 codified this common meaning. Many types of applications use information representable in eight or fewer bits and processor designers optimize for this common usage. The popularity of major commercial computing architectures has aided in the ubiquitous acceptance of the 8-bit size.[3]
1573) 14.224966, Central processing unit - Wikipedia, the free encyclopedia.txt#16, term: computer, content:In the 1970s, the fundamental inventions by Federico Faggin (Silicon Gate MOS ICs with self-aligned gates along with his new random logic design methodology) changed the design and implementation of CPUs forever. Since the introduction of the first commercially available microprocessor (the Intel 4004) in 1970, and the first widely used microprocessor (the Intel 8080) in 1974, this class of CPUs has almost completely overtaken all other central processing unit implementation methods. Mainframe and minicomputer manufacturers of the time launched proprietary IC development programs to upgrade their older computer architectures, and eventually produced instruction set compatible microprocessors that were backward-compatible with their older hardware and software. Combined with the advent and eventual success of the ubiquitous personal computer, the term CPU is now applied almost exclusively[a] to microprocessors. Several CPUs (denoted cores) can be combined in a single processing chip.[39]
1574) 14.224966, COBOL - Wikipedia, the free encyclopedia.txt#105, term: computer, content:In the 1970s, programmers began moving away from unstructured spaghetti code to the structured programming paradigm. In his letter to an editor in 1975 entitled "How do we tell truths that might hurt?" which was critical of several of COBOL's contemporaries, computer scientist and Turing Award recipient Edsger Dijkstra remarked that "The use of COBOL cripples the mind; its teaching should, therefore, be regarded as a criminal offense."[136] In his dissenting response to Dijkstra's article and the above "offensive statement," computer scientist Howard E. Tompkins defended structured COBOL: "COBOL programs with convoluted control flow indeed tend to 'cripple the mind'," but this was because "There are too many such business application programs written by programmers that have never had the benefit of structured COBOL taught well..."[137]
1575) 14.224966, Computational science - Wikipedia, the free encyclopedia.txt#13, term: computer, content:Most scientific journals do not accept software papers because a description of a reasonably mature software usually does not meet the criterion of novelty.[citation needed] Outside computer science itself, there are only few journals dedicated to scientific software. Established journals like Elsevier's Computer Physics Communications publish papers that are not open-access (though the described software usually is). To fill this gap, a new journal entitled Open research computation was announced in 2010;[14] it closed in 2012 without having published a single paper, for a lack of submissions probably due to excessive quality requirements.[15] A new initiative was launched in 2012, the Journal of Open Research Software.[16] In 2015, a new journal [17] dedicated to the replication of computational results has been started on GitHub.
1576) 14.224966, Computer - Simple English Wikipedia, the free encyclopedia.txt#12, term: computer, content:Historians disagree on which early machines are "computers". Many say the "castle clock", an astronomical clock invented by Al-Jazari in 1206, is the first known programmable analog computer. Others say the first computer was made by Charles Babbage.[2] Al - Jazari's showed the zodiac, the solar and lunar orbits, a crescent moon-shaped pointer travelling across a gateway that made some doors to open every hour,[3][4] and five robotic musicians who play music when levers hit them. The length of day and night could be changed (AKA re-programmed) every day in order to account for the changing lengths of day and night throughout the year.[2] Ada Lovelace is considered to be the first programmer.[5][6][7]
1577) 14.224966, Computer - Simple English Wikipedia, the free encyclopedia.txt#21, term: computer, content:Several developers of ENIAC saw its problems. They invented a way to for a computer to remember what they had told it, and a way to change what it remembered. This is known as "stored program architecture" or von Neumann architecture. John von Neumann talked about this design in the paper First Draft of a Report on the EDVAC, distributed in 1945. A number of projects to develop computers based on the stored-program architecture started around this time. The first of these was completed in Great Britain. The first to be demonstrated working was the Manchester Small-Scale Experimental Machine (SSEM or "Baby"), while the EDSAC, completed a year after SSEM, was the first really useful computer that used the stored program design. Shortly afterwards, the machine originally described by von Neumann's paperEDVACwas completed but was not ready for two years.
1578) 14.224966, Computer - Wikipedia, the free encyclopedia.txt#6, term: computer, content:Many mechanical aids to calculation and measurement were constructed for astronomical and navigation use. The planisphere was a star chart invented by Ab Rayhn al-Brn in the early 11th century.[5] The astrolabe was invented in the Hellenistic world in either the 1st or 2nd centuries BC and is often attributed to Hipparchus. A combination of the planisphere and dioptra, the astrolabe was effectively an analog computer capable of working out several different kinds of problems in spherical astronomy. An astrolabe incorporating a mechanical calendar computer[6][7] and gear-wheels was invented by Abi Bakr of Isfahan, Persia in 1235.[8] Ab Rayhn al-Brn invented the first mechanical geared lunisolar calendar astrolabe,[9] an early fixed-wired knowledge processing machine[10] with a gear train and gear-wheels,[11] circa 1000 AD.
1579) 14.224966, Computer - Wikipedia, the free encyclopedia.txt#22, term: computer, content:Purely electronic circuit elements soon replaced their mechanical and electromechanical equivalents, at the same time that digital calculation replaced analog. The engineer Tommy Flowers, working at the Post Office Research Station in London in the 1930s, began to explore the possible use of electronics for the telephone exchange. Experimental equipment that he built in 1934 went into operation 5 years later, converting a portion of the telephone exchange network into an electronic data processing system, using thousands of vacuum tubes.[17] In the US, John Vincent Atanasoff and Clifford E. Berry of Iowa State University developed and tested the AtanasoffBerry Computer (ABC) in 1942,[25] the first "automatic electronic digital computer".[26] This design was also all-electronic and used about 300 vacuum tubes, with capacitors fixed in a mechanically rotating drum for memory.[27]
1580) 14.224966, Computer - Wikipedia, the free encyclopedia.txt#42, term: computer, content:In most cases, computer instructions are simple: add one number to another, move some data from one location to another, send a message to some external device, etc. These instructions are read from the computer's memory and are generally carried out (executed) in the order they were given. However, there are usually specialized instructions to tell the computer to jump ahead or backwards to some other place in the program and to carry on executing from there. These are called "jump" instructions (or branches). Furthermore, jump instructions may be made to happen conditionally so that different sequences of instructions may be used depending on the result of some previous calculation or some external event. Many computers directly support subroutines by providing a type of jump that "remembers" the location it jumped from and another instruction to return to the instruction following that jump instruction.
1581) 14.224966, Computer - Wikipedia, the free encyclopedia.txt#64, term: computer, content:The set of arithmetic operations that a particular ALU supports may be limited to addition and subtraction, or might include multiplication, division, trigonometry functions such as sine, cosine, etc., and square roots. Some can only operate on whole numbers (integers) whilst others use floating point to represent real numbers, albeit with limited precision. However, any computer that is capable of performing just the simplest operations can be programmed to break down the more complex operations into simple steps that it can perform. Therefore, any computer can be programmed to perform any arithmetic operationalthough it will take more time to do so if its ALU does not directly support the operation. An ALU may also compare numbers and return boolean truth values (true or false) depending on whether one is equal to, greater than or less than the other ("is 64 greater than 65?").
1582) 14.224966, Computer - Wikipedia, the free encyclopedia.txt#76, term: computer, content:One means by which this is done is with a special signal called an interrupt, which can periodically cause the computer to stop executing instructions where it was and do something else instead. By remembering where it was executing prior to the interrupt, the computer can return to that task later. If several programs are running "at the same time". then the interrupt generator might be causing several hundred interrupts per second, causing a program switch each time. Since modern computers typically execute instructions several orders of magnitude faster than human perception, it may appear that many programs are running at the same time even though only one is ever executing in any given instant. This method of multitasking is sometimes termed "time-sharing" since each program is allocated a "slice" of time in turn.[68]
1583) 14.224966, Computer animation - Wikipedia, the free encyclopedia.txt#11, term: computer, content:In most 3D computer animation systems, an animator creates a simplified representation of a character's anatomy, which is analogous to a skeleton or stick figure.[12] The position of each segment of the skeletal model is defined by animation variables, or Avars for short. In human and animal characters, many parts of the skeletal model correspond to the actual bones, but skeletal animation is also used to animate other things, with facial features (though other methods for facial animation exist).[13] The character "Woody" in Toy Story, for example, uses 700 Avars (100 in the face alone). The computer doesn't usually render the skeletal model directly (it is invisible), but it does use the skeletal model to compute the exact position and orientation of that certain character, which is eventually rendered into an image. Thus by changing the values of Avars over time, the animator creates motion by making the character move from frame to frame.
1584) 14.224966, Computer data storage - Wikipedia, the free encyclopedia.txt#6, term: computer, content:By adding bits to each encoded unit, redundancy allows the computer to both detect errors in coded data and correct them based on mathematical algorithms. Errors generally occur in low probabilities due to random bit value flipping, or "physical bit fatigue", loss of the physical bit in storage its ability to maintain distinguishable value (0 or 1), or due to errors in inter or intra-computer communication. A random bit flip (e.g., due to random radiation) is typically corrected upon detection. A bit, or a group of malfunctioning physical bits (not always the specific defective bit is known; group definition depends on specific storage device) is typically automatically fenced-out, taken out of use by the device, and replaced with another functioning equivalent group in the device, where the corrected bit values are restored (if possible). The cyclic redundancy check (CRC) method is typically used in communications and storage for error detection. A detected error is then retried.
1585) 14.224966, Computer graphics - Wikipedia, the free encyclopedia.txt#29, term: computer, content:The continued rise and increasing sophistication of the graphics processing unit was crucial to this decade, and 3D rendering capabilities became a standard feature as 3D-graphics GPUs became considered a necessity for desktop computer makers to offer. The Nvidia GeForce line of graphics cards dominated the market in the early decade with occasional significant competing presence from ATI.[13] As the decade progressed, even low-end machines usually contained a 3D-capable GPU of some kind as Nvidia and AMD both introduced low-priced chipsets and continued to dominate the market. Shaders which had been introduced in the 1980s to perform specialized processing on the GPU would by the end of the decade become supported on most consumer hardware, speeding up graphics considerably and allowing for greatly improved texture and shading in computer graphics via the widespread adoption of normal mapping, bump mapping, and a variety of other techniques allowing the simulation of a great amount of detail.
1586) 14.224966, Computer graphics - Wikipedia, the free encyclopedia.txt#30, term: computer, content:Computer graphics used in films and video games gradually began to be realistic to the point of entering the uncanny valley. CGI movies proliferated, with traditional animated cartoon films like Ice Age and Madagascar as well as numerous Pixar offerings like Finding Nemo dominating the box office in this field. The Final Fantasy: The Spirits Within, released in 2001, was the first fully computer-generated feature film to use photorealistic CGI characters and be fully made with motion capture.[14] The film was not a box-office success, however.[15] Some commentators have suggested this may be partly because the lead CGI characters had facial features which fell into the "uncanny valley".[16] Other animated films like The Polar Express drew attention at this time as well. Star Wars also resurfaced with its prequel trilogy and the effects continued to set a bar for CGI in film.
1587) 14.224966, Computer graphics - Wikipedia, the free encyclopedia.txt#57, term: computer, content:3D modeling is the process of developing a mathematical, wireframe representation of any three-dimensional object, called a "3D model", via specialized software. Models may be created automatically or manually; the manual modeling process of preparing geometric data for 3D computer graphics is similar to plastic arts such as sculpting. 3D models may be created using multiple approaches: use of NURBS curves to generate accurate and smooth surface patches, polygonal mesh modeling (manipulation of faceted geometry), or polygonal mesh subdivision (advanced tessellation of polygons, resulting in smooth surfaces similar to NURBS models). A 3D model can be displayed as a two-dimensional image through a process called 3D rendering, used in a computer simulation of physical phenomena, or animated directly for other purposes. The model can also be physically created using 3D Printing devices.
1588) 14.224966, Computer keyboard - Wikipedia, the free encyclopedia.txt#63, term: computer, content:Wireless keyboards have become popular for their increased user freedom. A wireless keyboard often includes a required combination transmitter and receiver unit that attaches to the computer's keyboard port. The wireless aspect is achieved either by radio frequency (RF) or by infrared (IR) signals sent and received from both the keyboard and the unit attached to the computer. A wireless keyboard may use an industry standard RF, called Bluetooth. With Bluetooth, the transceiver may be built into the computer. However, a wireless keyboard needs batteries to work and may pose a security problem due to the risk of data "eavesdropping" by hackers. Wireless solar keyboards charge their batteries from small solar panels using sunlight or standard artificial lighting. An early example of a consumer wireless keyboard is that of the Olivetti Envision.
1589) 14.224966, Computer monitor - Wikipedia, the free encyclopedia.txt#17, term: computer, content:Until about 2003, most computer monitors had a 4:3 aspect ratio and some had 5:4. Between 2003 and 2006, monitors with 16:9 and mostly 16:10 (8:5) aspect ratios became commonly available, first in laptops and later also in standalone monitors. Reasons for this transition was productive uses for such monitors, i.e. besides widescreen computer game play and movie viewing, are the word processor display of two standard letter pages side by side, as well as CAD displays of large-size drawings and CAD application menus at the same time.[7][8] In 2008 16:10 became the most common sold aspect ratio for LCD monitors and the same year 16:10 was the mainstream standard for laptops and notebook computers.[9]
1590) 14.224966, Computer program - Wikipedia, the free encyclopedia.txt#30, term: computer, content:Computer programs may be categorized along functional lines. The main functional categories are application software and system software. System software includes the operating system which couples computer hardware with application software.[23] The purpose of the operating system is to provide an environment in which application software executes in a convenient and efficient manner.[23] In addition to the operating system, system software includes embedded programs, boot programs, and micro programs. Application software designed for end users have a user interface. Application software not designed for the end user includes middleware, which couples one application with another. Application software also includes utility programs. The distinction between system software and application software is under debate.
1591) 14.224966, Computer security - Wikipedia, the free encyclopedia.txt#53, term: computer, content:Within computer systems, two of many security models capable of enforcing privilege separation are access control lists (ACLs) and capability-based security. Using ACLs to confine programs has been proven to be insecure in many situations, such as if the host computer can be tricked into indirectly allowing restricted file access, an issue known as the confused deputy problem. It has also been shown that the promise of ACLs of giving access to an object to only one person can never be guaranteed in practice. Both of these problems are resolved by capabilities. This does not mean practical flaws exist in all ACL-based systems, but only that the designers of certain utilities must take responsibility to ensure that they do not introduce flaws.[citation needed]
1592) 14.224966, Cray - Wikipedia, the free encyclopedia.txt#10, term: computer, content:New vendors introduced small supercomputers, known as minisupercomputers (as opposed to superminis) during the late 1980s and early 1990s, which out-competed low-end Cray machines in the market. The Convex Computer series, as well as a number of small-scale parallel machines from companies like Pyramid Technology and Alliant Computer Systems were particularly popular. One such vendor was Supertek, whose S-1 machine was an air-cooled CMOS implementation of the X-MP processor. Cray purchased Supertek in 1990 and sold the S-1 as the Cray XMS, but the machine proved problematic; meanwhile, their not-yet-completed S-2, a Y-MP clone, was later offered as the Cray Y-MP EL (later becoming the EL90 series) which started to sell in reasonable numbers in 1991-92to mostly smaller companies, notably in the oil exploration business. This line evolved into the Cray J90 and eventually the Cray SV1 in 1998.
1593) 14.224966, Desktop computer - Wikipedia, the free encyclopedia.txt#5, term: computer, content:Early personal computers, like the original IBM Personal Computer, were enclosed in a "desktop case", horizontally oriented to have the display screen placed on top, thus saving space on the user's actual desk, although these cases had to be sturdy enough to support the weight of CRT displays that were widespread at the time. Over the course of the 1990s, desktop cases gradually became less common than the more-accessible tower cases that may be located on the floor under or beside a desk rather than on a desk. Not only these tower cases had more room for expansion, but also they have freed up desk space for monitors which were becoming larger every year. Desktop cases, particularly the compact form factors, remain popular for corporate computing environments and kiosks. Some computer cases can be interchangeably positioned either horizontally (desktop) or upright (mini-tower).
1594) 14.224966, Digital Equipment Corporation - Wikipedia, the free encyclopedia.txt#43, term: computer, content:RT-11 provided a practical real-time operating system in minimal memory, allowing the PDP-11 to continue Digital's critical role as a computer supplier for embedded systems. Historically, RT-11 also served as the inspiration for many microcomputer OS's, as these were generally being written by programmers who cut their teeth on one of the many PDP-11 models. For example, CP/M used a command syntax similar to RT-11's, and even retained the awkward PIP program used to copy data from one computer device to another. As another historical footnote, DEC's use of "/" for "switches" (command-line options) would lead to the adoption of "\" for pathnames in MS-DOS and Microsoft Windows as opposed to "/" in Unix.[44]
1595) 14.224966, Digital Equipment Corporation - Wikipedia, the free encyclopedia.txt#45, term: computer, content:In 1976, DEC decided to extend the PDP-11 architecture to 32 bits while adding a complete virtual memory system to the simple paging and memory protection of the PDP-11. The result was the VAX architecture, where VAX stands for Virtual Address eXtension (from 16 to 32 bits). The first computer to use a VAX CPU was the VAX-11/780, which DEC referred to as a superminicomputer. Although it was not the first 32-bit minicomputer, the VAX-11/780's combination of features, price, and marketing almost immediately propelled it to a leadership position in the market after it was released in 1978. VAX systems were so successful that in 1983, DEC canceled its Jupiter project, which had been intended to build a successor to the PDP-10 mainframe, and instead focused on promoting the VAX as the single computer architecture for the company.[45]
1596) 14.224966, Digital Equipment Corporation - Wikipedia, the free encyclopedia.txt#48, term: computer, content:When a DEC research group demonstrated two prototype microcomputers in 1974before the debut of the MITS AltairOlsen chose to not proceed with the project. The company similarly rejected another personal computer proposal in 1977.[47] At the time these systems were of limited utility, and Olsen famously derided them in 1977, stating "There is no reason for any individual to have a computer in his home."[48] Unsurprisingly, DEC did not put much effort into the microcomputer area in the early days of the market. Interestingly in 1977, the Heathkit H11 was announced; a PDP-11 in kit form. At the beginning of the 1980s, DEC built the VT180 (codenamed "Robin"), which was a VT100 terminal with an added Z80-based microcomputer running CP/M, but this product was initially available only to DEC employees.[49]
1597) 14.224966, Digital Equipment Corporation - Wikipedia, the free encyclopedia.txt#58, term: computer, content:At its peak, Digital was the second-largest computer company in the world, with over 100,000 employees. It was during this time that the company branched out development into a wide variety of projects that were far from its core business in computer equipment. The company invested heavily in custom software. In the 1970s and earlier most software was custom-written to serve a specific task, but by the 1980s the introduction of relational databases and similar systems allowed powerful software to be built in a modular fashion, potentially saving enormous amounts of development time. Software companies like Oracle became the new darlings of the industry, and DEC started their own efforts in every "hot" niche, in some cases several projects for the same niche. Some of these products competed with DEC's own partners, notably Rdb which competed with Oracle's products on the VAX, part of a major partnership only a few years earlier.
1598) 14.224966, Digital video - Wikipedia, the free encyclopedia.txt#27, term: computer, content:Digital video can be copied with no degradation in quality. No matter how many generations of a digital source is copied, it will still be as clear as the original first generation of digital footage. However a change in parameters like frame size as well as a change of the digital format can decrease the quality of the video due to new calculations that have to be made. Digital video can be manipulated and edited to follow an order or sequence on an NLE, or non-linear editing workstation, a computer-based device intended to edit video and audio. More and more, videos are edited on readily available, increasingly affordable consumer-grade computer hardware and software. However, such editing systems require ample disk space for video footage. The many video formats and parameters to be set make it quite impossible to come up with a specific number for how many minutes need how much time.
1599) 14.224966, Ferranti Pegasus - Wikipedia, the free encyclopedia.txt#5, term: computer, content:Pegasus had eight accumulators, seven of which could also be used as index registers. (It was the first computer to allow this dual use.) Accumulators 6 and 7 were known as p and q and were involved in multiply and divide and some double length shift instructions. It had 56 words of fast memory stored in nickel delay lines, which was supplemented by a magnetic drum holding 5120 words. A word was 40 bits, of which one bit was for parity checking. Two 19-bit instructions were packed into one word and the extra bit (not counting the parity bit) could be used to indicate a breakpoint (optional stop), to assist in debugging. It had a relatively generous instruction set for a computer of its time, but there was no explicit hardware provision for handling either characters or floating point numbers.
1600) 14.224966, Floppy disk - Wikipedia, the free encyclopedia.txt#12, term: computer, content:Little-used floppy disk drives sitting unused for years in computers meanwhile can actually damage disks, due to computer case air cooling that sucks air through the drive openings. This pulls dust into the mechanism, which coats all surfaces including the exposed read/write head. Upon loading a disk on the rare occasion the drive is needed, the dust is stirred up when the disk is inserted into the mechanism, and is then deposited onto the disk through the open shutters, leading to nearly instant fouling of the media. The dust also gets into the drive mechanism lubricant, turning it into a sticky slime, which may jam the mechanism and prevent head movement. This causes the computer user to see read errors on every disk they try to use, and may also lead to data-corrupting writing in incorrect locations.
1601) 14.224966, Hard disk drive - Wikipedia, the free encyclopedia.txt#62, term: computer, content:As of 2010[update], a typical 7,200-rpm desktop HDD has a sustained "disk-to-buffer" data transfer rate up to 1,030Mbit/s.[136] This rate depends on the track location; the rate is higher for data on the outer tracks (where there are more data sectors per rotation) and lower toward the inner tracks (where there are fewer data sectors per rotation); and is generally somewhat higher for 10,000-rpm drives. A current widely used standard for the "buffer-to-computer" interface is 3.0Gbit/s SATA, which can send about 300 megabyte/s (10-bit encoding) from the buffer to the computer, and thus is still comfortably ahead of today's disk-to-buffer transfer rates. Data transfer rate (read/write) can be measured by writing a large file to disk using special file generator tools, then reading back the file. Transfer rate can be influenced by file system fragmentation and the layout of the files.[131]
1602) 14.224966, History of computing hardware - Wikipedia, the free encyclopedia.txt#28, term: computer, content:An important advance in analog computing was the development of the first fire-control systems for long range ship gunlaying. When gunnery ranges increased dramatically in the late 19th century it was no longer a simple matter of calculating the proper aim point, given the flight times of the shells. Various spotters on board the ship would relay distance measures and observations to a central plotting station. There the fire direction teams fed in the location, speed and direction of the ship and its target, as well as various adjustments for Coriolis effect, weather effects on the air, and other adjustments; the computer would then output a firing solution, which would be fed to the turrets for laying. In 1912, British engineer Arthur Pollen developed the first electrically powered mechanical analogue computer (called at the time the Argo Clock).[43] It was used by the Imperial Russian Navy in World War I.[citation needed] The alternative Dreyer Table fire control system was fitted to British capital ships by mid-1916.
1603) 14.224966, History of computing hardware - Wikipedia, the free encyclopedia.txt#58, term: computer, content:Meanwhile, John von Neumann at the Moore School of Electrical Engineering, University of Pennsylvania, circulated his First Draft of a Report on the EDVAC in 1945. Although substantially similar to Turing's design and containing comparatively little engineering detail, the computer architecture it outlined became known as the "von Neumann architecture". Turing presented a more detailed paper to the National Physical Laboratory (NPL) Executive Committee in 1946, giving the first reasonably complete design of a stored-program computer, a device he called the Automatic Computing Engine (ACE). However, the better-known EDVAC design of John von Neumann, who knew of Turing's theoretical work, received more publicity, despite its incomplete nature and questionable lack of attribution of the sources of some of the ideas.[41]
1604) 14.224966, History of computing hardware - Wikipedia, the free encyclopedia.txt#81, term: computer, content:At the University of Manchester, a team under the leadership of Tom Kilburn designed and built a machine using the newly developed transistors instead of valves. Initially the only devices available were germanium point-contact transistors, less reliable than the valves they replaced but which consumed far less power.[107] Their first transistorised computer and the first in the world, was operational by 1953,[108] and a second version was completed there in April 1955.[109] The 1955 version used 200 transistors, 1,300 solid-state diodes, and had a power consumption of 150 watts. However, the machine did make use of valves to generate its 125kHz clock waveforms and in the circuitry to read and write on its magnetic drum memory, so it was not the first completely transistorized computer.
1605) 14.224966, History of computing hardware - Wikipedia, the free encyclopedia.txt#102, term: computer, content:In the 21st century, multi-core CPUs became commercially available.[139] Content-addressable memory (CAM)[140] has become inexpensive enough to be used in networking, and is frequently used for on-chip cache memory in modern microprocessors, although no computer system has yet implemented hardware CAMs for use in programming languages. Currently, CAMs (or associative arrays) in software are programming-language-specific. Semiconductor memory cell arrays are very regular structures, and manufacturers prove their processes on them; this allows price reductions on memory products. During the 1980s, CMOS logic gates developed into devices that could be made as fast as other circuit types; computer power consumption could therefore be decreased dramatically. Unlike the continuous current draw of a gate based on other logic types, a CMOS gate only draws significant current during the 'transition' between logic states, except for leakage.
1606) 14.224966, Home computer - Wikipedia, the free encyclopedia.txt#14, term: computer, content:During the peak years of the home computer market, scores of models were produced, usually as individual design projects with little or no thought given to compatibility between different manufacturers or even within product lines of the same manufacturer.[22] Except for the Japanese MSX standard,[23] the concept of a computer platform was still forming, with most companies considering rudimentary BASIC language and disk format compatibility sufficient to claim a model as "compatible". Things were different in the business world, where cost-conscious small business owners had been using CP/M running on Z80 based computers from Osborne, Kaypro, Morrow Designs and a host of other manufacturers. For many of these businesses, the development of the microcomputer made computing and business software affordable where they had not been before.
1607) 14.224966, Home computer - Wikipedia, the free encyclopedia.txt#19, term: computer, content:The declining cost of IBM compatibles on the one hand, and the greatly increased graphics, sound, and storage abilities of fourth generation video game consoles such as the Sega Genesis and Super Nintendo Entertainment System on the other, combined to cause the market segment for home computers to vanish by the early 1990s in the US. In Europe, the home computer remained a distinct presence for a few years more, with the low-end models of the 16-bit Amiga and Atari ST families being the dominant players, but by the mid-1990s even the European market had dwindled.[33] The Dutch government even ran a program that allowed businesses to sell computers tax-free to its employees, often accompanied by home training programs. Naturally, these businesses chose to equip their employees with the same systems they themselves were using. Today a computer bought for home use anywhere will be very similar to those used in offices made by the same manufacturers, with compatible peripherals, operating systems, and application software.
1608) 14.224966, Home computer - Wikipedia, the free encyclopedia.txt#29, term: computer, content:In another defining characteristic of the home computer, instead of a command line, the BASIC interpreter served double duty as a user interface. Coupled to a character-based screen or line editor, BASIC's file management commands could be entered in direct mode. In contrast to modern computers, home computers most often had their operating system (OS) stored in ROM chips. This made startup times very fast no more than a few seconds but made OS upgrades difficult or impossible without buying a new unit. Usually only the most severe bugs were fixed by issuing new ROMs to replace the old ones at the user's cost. Also, the small size and limited scope of home computer "operating systems" (really little more than what today would be called a kernel) left little room for bugs to hide.
1609) 14.224966, Human computer - Wikipedia, the free encyclopedia.txt#9, term: computer, content:Human computers played integral roles in the World War II war effort in the United States, and because of the depletion of the male labor force due to the draft, many computers during World War II were women, frequently with degrees in mathematics. In the Manhattan Project, human computers, working with a variety of mechanical aids, assisted numerical studies of the complex formulas related to nuclear fission.[7] Because the six people responsible for setting up problems on the ENIAC (the premiere general-purpose electronic digital computer built at the University of Pennsylvania during World War II) were drafted from a corps of human computers, the world's first professional computer programmers were women. These were Kay McNulty, Betty Snyder, Marlyn Wescoff, Ruth Lichterman, Betty Jean Jennings, and Fran Bilas.
1610) 14.224966, Human–computer interaction - Wikipedia, the free encyclopedia.txt#2, term: computer, content:Humans interact with computers in many ways; and the interface between humans and the computers they use is crucial to facilitating this interaction. Desktop applications, internet browsers, handheld computers, and computer kiosks make use of the prevalent graphical user interfaces (GUI) of today.[5] Voice user interfaces (VUI) are used for speech recognition and synthesising systems, and the emerging multi-modal and gestalt User Interfaces (GUI) allow humans to engage with embodied character agents in a way that cannot be achieved with other interface paradigms. The growth in human-computer interaction field has been in quality of interaction, and in different branching in its history. Instead of designing regular interfaces, the different research branches have had different focus on the concepts of multimodality rather than unimodality, intelligent adaptive interfaces rather than command/action based ones, and finally active rather than passive interfaces[citation needed]
1611) 14.224966, IBM PC DOS - Wikipedia, the free encyclopedia.txt#5, term: computer, content:Microsoft first licensed,[6] then purchased[7] 86-DOS from Seattle Computer Products (SCP), which was modified for the IBM PC by Microsoft employee Bob O'Rear with assistance from SCP (later Microsoft) employee Tim Paterson.[8] O'Rear got 86-DOS to run on the prototype PC in February 1981. 86-DOS had to be converted from 8-inch to 5.25-inch floppy disks and integrated with the BIOS, which Microsoft was helping IBM to write.[9] IBM had more people writing requirements for the computer than Microsoft had writing code. O'Rear often felt overwhelmed by the number of people he had to deal with at the ESD (Entry Systems Division) facility in Boca Raton.
1612) 14.224966, IBM PC DOS - Wikipedia, the free encyclopedia.txt#6, term: computer, content:Perhaps the first public mention of the operating system was in July 1981, when Byte discussed rumors of a forthcoming personal computer with "a CP/M-like DOS ... to be called, simply, 'IBM Personal Computer DOS.'"[10] 86-DOS was rebranded IBM PC DOS 1.0 for its August 1981 release with the IBM PC. The initial version of DOS was largely based on CP/M and many of its function calls as well as the file system were copied directly from the older OS. Unlike all later DOS versions, the DATE and TIME commands were separate executables rather than part of COMMAND.COM. Single-sided 160 kilobyte (kB) 5.25" floppies were the only disk format supported.
1613) 14.224966, Information technology - Wikipedia, the free encyclopedia.txt#7, term: computer, content:Early electronic computers such as Colossus made use of punched tape, a long strip of paper on which data was represented by a series of holes, a technology now obsolete.[15] Electronic data storage, which is used in modern computers, dates from World War II, when a form of delay line memory was developed to remove the clutter from radar signals, the first practical application of which was the mercury delay line.[16] The first random-access digital storage device was the Williams tube, based on a standard cathode ray tube,[17] but the information stored in it and delay line memory was volatile in that it had to be continuously refreshed, and thus was lost once power was removed. The earliest form of non-volatile computer storage was the magnetic drum, invented in 1932[18] and used in the Ferranti Mark 1, the world's first commercially available general-purpose electronic computer.[19]
1614) 14.224966, Institute of Electrical and Electronics Engineers - Wikipedia, the free encyclopedia.txt#6, term: computer, content:IEEE's Constitution defines the purposes of the organization as "scientific and educational, directed toward the advancement of the theory and practice of Electrical, Electronics, Communications and Computer Engineering, as well as Computer Science, the allied branches of engineering and the related arts and sciences."[1] In pursuing these goals, the IEEE serves as a major publisher of scientific journals and organizer of conferences, workshops, and symposia (many of which have associated published proceedings). It is also a leading standards development organization for the development of industrial standards (having developed over 900 active industry technical standards) in a broad range of disciplines, including electric power and energy, biomedical technology and healthcare, information technology, information assurance, telecommunications, consumer electronics, transportation, aerospace, and nanotechnology. IEEE develops and participates in educational activities such as accreditation of electrical engineering programs in institutes of higher learning. The IEEE logo is a diamond-shaped design which illustrates the right hand grip rule embedded in Benjamin Franklin's kite, and it was created at the time of the 1963 merger.[6]
1615) 14.224966, Interrupt - Wikipedia, the free encyclopedia.txt#27, term: computer, content:In a push button analogy applied to computer systems, the term doorbell or doorbell interrupt is often used to describe a mechanism whereby a software system can signal or notify a computer hardware device that there is some work to be done. Typically, the software system will place data in some well known and mutually agreed upon memory location(s), and "ring the doorbell" by writing to a different memory location. This different memory location is often called the doorbell region, and there may even be multiple doorbells serving different purposes in this region. It is this act of writing to the doorbell region of memory that "rings the bell" and notifies the hardware device that the data are ready and waiting. The hardware device would now know that the data are valid and can be acted upon. It would typically write the data to a hard disk drive, or send them over a network, or encrypt them, etc.
1616) 14.224966, Laptop - Wikipedia, the free encyclopedia.txt#3, term: computer, content:The terms laptop and notebook are used fairly interchangeably to describe a portable computer in English, although in some parts of the world one or the other may be preferred. There is some question as to the original etymology and specificity of either termthe term laptop appears to have been coined in the early 1980s to describe a mobile computer which can be used on one's lap, and to distinguish these devices from earlier, much heavier, portable computers (often called "luggables" in retrospect). The term "notebook" appears to have gained currency somewhat later as manufacturers started producing even smaller portable devices, further reducing their weight and size and incorporating a display roughly the size of A4 paper; these were marketed as notebooks to distinguish them from bulkier laptops.[4] Regardless of the etymology, by the late 1990s, the terms were interchangeable.
1617) 14.224966, Laptop - Wikipedia, the free encyclopedia.txt#75, term: computer, content:The ruggedized Grid Compass computer was used since the early days of the Space Shuttle program. The first commercial laptop used in space was a Macintosh portable in 1991 aboard Space Shuttle mission STS-43.[88][89][90] Mac and other laptop computers continue to be flown aboard manned spaceflights though the only long duration flight certified computer for the International Space Station is the ThinkPad.[91] As of 2011 over 100 ThinkPads were aboard the ISS. Laptops used aboard the International Space Station and other spaceflights are generally the same ones that can be purchased by the general public but needed modifications are made to allow them to be used safely and effectively in a weightless environment such as updating the cooling systems to function without relying on hot air rising and accommodation for the lower cabin air pressure.[92]
1618) 14.224966, LEO (computer) - Wikipedia, the free encyclopedia.txt#3, term: computer, content:J. Lyons and Co., one of the UK's leading catering and food manufacturing companies in the first half of the 20th century, sent two of its senior managers, Oliver Standingford and Raymond Thompson, to the USA in 1947 to look at new business methods developed during World War II. During their visit they met Herman Goldstine, one of the original developers of ENIAC, the first general-purpose electronic computer (although it had no stored program). Standingford and Thompson saw the potential of computers to help solve the problem of administering a major business enterprise. They also learned from Goldstine that, back in the UK, Douglas Hartree and Maurice Wilkes were actually building another such machine, the pioneering EDSAC computer, at the University of Cambridge.[2]
1619) 14.224966, Linux - Wikipedia, the free encyclopedia.txt#80, term: computer, content:SpaceX uses multiple redundant flight computers in a fault-tolerant design in the Falcon9 rocket. Each Merlin engine is controlled by three voting computers, with two physical processors per computer that constantly check each other's operation. Linux is not inherently fault-tolerant (no operating system is, as it is a function of the whole system including the hardware), but the flight computer software makes it so for its purpose.[119] For flexibility, commercial off-the-shelf parts and system-wide "radiation-tolerant" design are used instead of radiation hardened parts.[119] As of June 2015[update], SpaceX has made 19 launches of the Falcon9 since 2010, out of which 18 have successfully delivered their primary payloads to Earth orbit, including some support missions for the International Space Station.
1620) 14.224966, Lisp (programming language) - Wikipedia, the free encyclopedia.txt#20, term: computer, content:The Scheme community actively maintains over twenty implementations. Several significant new implementations (Chicken, Gambit, Gauche, Ikarus, Larceny, Ypsilon) have been developed in the last few years. The Revised5 Report on the Algorithmic Language Scheme[28] standard of Scheme was widely accepted in the Scheme community. The Scheme Requests for Implementation process has created a lot of quasi standard libraries and extensions for Scheme. User communities of individual Scheme implementations continue to grow. A new language standardization process was started in 2003 and led to the R6RS Scheme standard in 2007. Academic use of Scheme for teaching computer science seems to have declined somewhat. Some universities, such as MIT, are no longer using Scheme in their computer science introductory courses.[29][30]
1621) 14.224966, Logic in computer science - Wikipedia, the free encyclopedia.txt#3, term: computer, content:There has always been a strong influence from mathematical logic on the field of Artificial Intelligence (AI). From the beginning of the field it was realized that technology to automate logical inferences could have great potential to solve problems and draw conclusions from facts. Ron Brachman has described First Order Logic (FOL) as metric by which all AI knowledge representation formalism should be evaluated. There is no more general or powerful known method for describing and analyzing information than FOL. The reason FOL itself is simply not used as a computer language is that it is actually too expressive, in the sense that FOL can easily express statements that no computer, no matter how powerful, could ever solve. For this reason every form of knowledge representation is in some sense a trade off between expressivity and computability. The more expressive the language is, the closer it is to FOL, the more likely it is to be slower and prone to an infinite loop.[8]
1622) 14.224966, Logic synthesis - Wikipedia, the free encyclopedia.txt#2, term: computer, content:The first step toward automation of logic minimization was the introduction of the QuineMcCluskey algorithm that could be implemented on a computer. This exact minimization technique presented the notion of prime implicants and minimum cost covers that would become the cornerstone of two-level minimization. Nowadays, the much more efficient Espresso heuristic logic minimizer has become the standard tool for this operation. Another area of early research was in state minimization and encoding of finite state machines (FSMs), a task that was the bane of designers. The applications for logic synthesis lay primarily in digital computer design. Hence, IBM and Bell Labs played a pivotal role in the early automation of logic synthesis. The evolution from discrete logic components to programmable logic arrays (PLAs) hastened the need for efficient two-level minimization, since minimizing terms in a two-level representation reduces the area in a PLA.
1623) 14.224966, Mainframe computer - Wikipedia, the free encyclopedia.txt#18, term: computer, content:In 1991, AT&T Corporation briefly owned NCR. During the same period, companies found that servers based on microcomputer designs could be deployed at a fraction of the acquisition price and offer local users much greater control over their own systems given the IT policies and practices at that time. Terminals used for interacting with mainframe systems were gradually replaced by personal computers. Consequently, demand plummeted and new mainframe installations were restricted mainly to financial services and government. In the early 1990s, there was a rough consensus among industry analysts that the mainframe was a dying market as mainframe platforms were increasingly replaced by personal computer networks. InfoWorld's Stewart Alsop famously predicted that the last mainframe would be unplugged in 1996; in 1993, he cited Cheryl Currid, a computer industry analyst as saying that the last mainframe "will stop working on December 31, 1999",[19] a reference to the anticipated Year 2000 problem (Y2K).
1624) 14.224966, Manchester Small-Scale Experimental Machine - Wikipedia, the free encyclopedia.txt#7, term: computer, content:Turing joined the National Physical Laboratory (NPL) in October 1945,[11] by which time scientists within the Ministry of Supply had concluded that Britain needed a National Mathematical Laboratory to coordinate machine-aided computation.[12] A Mathematics Division was set up at the NPL, and on 19February 1946 Alan Turing presented a paper outlining his design for an electronic stored-program computer to be known as the Automatic Computing Engine (ACE).[12] This was one of several projects set up in the years following the Second World War with the aim of constructing a stored-program computer. At about the same time, EDVAC was under development at the University of Pennsylvania's Moore School of Electrical Engineering, and the University of Cambridge Mathematical Laboratory was working on EDSAC.[13]
1625) 14.224966, Message transfer agent - Wikipedia, the free encyclopedia.txt#7, term: computer, content:At its most basic, an MUA using POP3 downloads messages from the server mailbox onto the local computer for display in the MUA. Messages are generally removed from the server at the same time but most systems also allow a copy to be left behind as a backup. In contrast, an MUA using IMAP displays messages directly from the server, although a download option for archive purposes is usually also available. One advantage this gives IMAP is that the same messages are visible from any computer accessing the email account, since messages aren't routinely downloaded and deleted from the server. If set up properly, sent mail can be saved to the server also, in contrast with POP mail, where sent messages exist only in the local MUA and are not visible by other MUAs accessing the same account.
1626) 14.224966, Microcode - Wikipedia, the free encyclopedia.txt#23, term: computer, content:Programmers develop microprograms, using basic software tools. A microassembler allows a programmer to define the table of bits symbolically. Because of its close relationship to the underlying architecture, "microcode has several properties that make it difficult to generate using a compiler."[1] A simulator program is intended to execute the bits in the same way as the electronics, and allows much more freedom to debug the microprogram. After the microprogram is finalized, and extensively tested, it is sometimes used as the input to a computer program that constructs logic to produce the same data. This program is similar to those used to optimize a programmable logic array. No known computer program can produce optimal logic, but even good logic can vastly reduce the number of transistors from the number required for a ROM control store. This reduces the cost of producing, and the electricity consumed by, a CPU.
1627) 14.224966, Microcomputer - Wikipedia, the free encyclopedia.txt#12, term: computer, content:In 1972, a French team headed by Franois Gernelle within a small company, Ralisations & Etudes Electroniqes (R2E), developed and patented a computer based on a microprocessor  the Intel 8008 8-bit microprocessor. This Micral-N was marketed in early 1973 as a "Micro-ordinateur" or microcomputer, mainly for scientific and process-control applications. About a hundred Micral-N were installed in the next two years, followed by a new version based on the Intel 8080. Meanwhile, another French team developed the Alvan, a small computer for office automation which found clients in banks and other sectors. The first version was based on LSI chips with an Intel 8008 as peripheral controller (keyboard, monitor and printer), before adopting the Zilog Z80 as main processor.
1628) 14.224966, MOS Technology 6502 - Wikipedia, the free encyclopedia.txt#3, term: computer, content:Motorola's target customers were established electronics companies such as Hewlett-Packard, Tektronix, TRW and Chrysler.[15] In May 1972 Motorola's engineers began visiting select customers and sharing the details of their proposed 8-bit microprocessor system with ROM, RAM, parallel and serial interfaces.[16] In early 1974 they provided engineering samples of the chips so customers could prototype their designs. Motorola's "total product family" strategy did not focus on the price of the microprocessor but on reducing the customer's total design cost. They offered development software on a timeshare computer, the "EXORciser" system debugging system, onsite training and field application engineer support.[17][18] Both Intel and Motorola had initially announced a $360 price for a single microprocessor.[19][20] (The IBM System/360 mainframe was a well known computer at the time.) The actual price for production quantities was much less. Motorola offered a design kit containing the 6800 with six support chips for $300.[21]
1629) 14.224966, MOS Technology 6502 - Wikipedia, the free encyclopedia.txt#16, term: computer, content:With legal troubles behind them, MOS was still left with the problem of getting developers to try their processor, prompting Chuck Peddle to design the MDT-650 ("microcomputer development terminal") single-board computer. Another group inside the company designed the KIM-1, which was sold semi-complete and could be turned into a usable system with the addition of a 3rd party computer terminal and compact cassette drive. Much to their amazement, the KIM-1 sold well to hobbyists and tinkerers, as well as to the engineers to which it had been targeted. The related Rockwell AIM 65 control/training/development system also did well. The software in the AIM 65 was based on that in the MDT. Another roughly similar product was the Synertek SYM-1.
1630) 14.224966, Motorola 6800 - Wikipedia, the free encyclopedia.txt#26, term: computer, content:Link Young was the product marketer that developed the total system approach for the M6800 family release. In addition to releasing a full set of support chips with the 6800 microprocessor, Motorola offered a software and hardware development system. The software development tools were available on remote time-sharing computers or the source code was available so the customer could use an in-house computer system. The software that would run on a microprocessor system was typically written in assembly language. The development system consisted of a text editor, assembler and a simulator.[50] This allowed the developer to test the software before the target system was complete. The hardware development was a desktop computer built with M6800 family CPU and peripherals known as the EXORcisor.[42] Motorola offered a three to five-day microprocessor design course for the 6800 hardware and software.[51] This systems-oriented approach became the standard way new microprocessor were introduced.[52]
1631) 14.224966, Motorola 6800 - Wikipedia, the free encyclopedia.txt#39, term: computer, content:A series of peripheral chip were introduced by 1978. The MC6840 programmable counter had three 16-bit binary counters that could be used for frequency measurement, event counting, or interval measurement. The MC6844 Direct Memory Access Controller could transfer data from an I/O controller to RAM without loading down the MC6800 microprocessor. The MC6845 CRT Controller provided the control logic for a character based computer terminal. The 6845 had support for a light pen, an alternative to a computer mouse. This was a very popular chip and was even used in the original IBM PC Monochrome Display Adapter with the Intel 8088 16-bit microprocessor in 1981, and in the follow-up IBM Color Graphics Adapter for the original PC and successors; the IBM Enhanced Graphics Adapter card contained custom IBM chips that emulated the Motorola 6845, with minor differences.
1632) 14.224966, MS-DOS - Wikipedia, the free encyclopedia.txt#2, term: computer, content:During its life, several competing products were released for the x86 platform,[10] and MS-DOS went through eight versions, until development ceased in 2000.[11] Initially MS-DOS was targeted at Intel 8086 processors running on computer hardware using floppy disks to store and access not only the operating system, but application software and user data as well. Progressive version releases delivered support for other mass storage media in ever greater sizes and formats, along with added feature support for newer processors and rapidly evolving computer architectures. Ultimately it was the key product in Microsoft's growth from a programming languages company to a diverse software development firm, providing the company with essential revenue and marketing resources. It was also the underlying basic operating system on which early versions of Windows ran as a GUI. It is a flexible operating system, and consumes negligible installation space.
1633) 14.224966, MS-DOS - Wikipedia, the free encyclopedia.txt#3, term: computer, content:MS-DOS was a renamed form of 86-DOS[12]  owned by Seattle Computer Products, written by Tim Paterson. Development of 86-DOS took only six weeks, as it was basically a clone of Digital Research's CP/M (for 8080/Z80 processors), ported to run on 8086 processors and with two notable differences compared to CP/M; an improved disk sector buffering logic and the introduction of FAT12 instead of the CP/M filesystem. This first version was shipped in August 1980.[5] Microsoft, which needed an operating system for the IBM Personal Computer[7][8] hired Tim Paterson in May 1981 and bought 86-DOS 1.10 for $75,000 in July of the same year. Microsoft kept the version number, but renamed it MS-DOS. They also licensed MS-DOS 1.10/1.14 to IBM, who, in August 1981, offered it as PC DOS 1.0 as one of three operating systems[13] for the IBM 5150, or the IBM PC.[5]
1634) 14.224966, Operating system - Wikipedia, the free encyclopedia.txt#27, term: computer, content:The first microcomputers did not have the capacity or need for the elaborate operating systems that had been developed for mainframes and minis; minimalistic operating systems were developed, often loaded from ROM and known as monitors. One notable early disk operating system was CP/M, which was supported on many early microcomputers and was closely imitated by Microsoft's MS-DOS, which became widely popular as the operating system chosen for the IBM PC (IBM's version of it was called IBM DOS or PC DOS). In the 1980s, Apple Computer Inc. (now Apple Inc.) abandoned its popular Apple II series of microcomputers to introduce the Apple Macintosh computer with an innovative Graphical User Interface (GUI) to the Mac OS.
1635) 14.224966, Operating system - Wikipedia, the free encyclopedia.txt#93, term: computer, content:External security involves a request from outside the computer, such as a login at a connected console or some kind of network connection. External requests are often passed through device drivers to the operating system's kernel, where they can be passed onto applications, or carried out directly. Security of operating systems has long been a concern because of highly sensitive data held on computers, both of a commercial and military nature. The United States Government Department of Defense (DoD) created the Trusted Computer System Evaluation Criteria (TCSEC) which is a standard that sets basic requirements for assessing the effectiveness of security. This became of vital importance to operating system makers, because the TCSEC was used to evaluate, classify and select trusted operating systems being considered for the processing, storage and retrieval of sensitive or classified information.
1636) 14.224966, Personal computer - Wikipedia, the free encyclopedia.txt#22, term: computer, content:In 2001, 125 million personal computers were shipped in comparison to 48,000 in 1977.[16] More than 500 million personal computers were in use in 2002 and one billion personal computers had been sold worldwide from the mid-1970s up to this time. Of the latter figure, 75% were professional or work related, while the rest were sold for personal or home use. About 81.5% of personal computers shipped had been desktop computers, 16.4% laptops and 2.1% servers. The United States had received 38.8% (394 million) of the computers shipped, Europe 25% and 11.7% had gone to the Asia-Pacific region, the fastest-growing market as of 2002. The second billion was expected to be sold by 2008.[17] Almost half of all households in Western Europe had a personal computer and a computer could be found in 40% of homes in United Kingdom, compared with only 13% in 1985.[18]
1637) 14.224966, Personal computer - Wikipedia, the free encyclopedia.txt#41, term: computer, content:A laptop computer, also called a notebook, is a small personal computer designed for portability. Usually, all of the hardware and interfaces needed to operate a laptop, such as the graphics card, audio devices or USB ports (previously parallel and serial ports), are built into a single unit. Laptops contain high-capacity batteries that can power the device for extensive periods of time, enhancing portability. Once the battery charge is depleted, it will have to be recharged through a power outlet. In the interests of saving power, weight and space, laptop graphics cards are in many cases integrated into the CPU or chipset and use system RAM, resulting in reduced graphics performance when compared to an equivalent desktop machine. For this reason, desktop or gaming computers are usually preferred to laptop PCs for gaming purposes.
1638) 14.224966, Portable computer - Wikipedia, the free encyclopedia.txt#6, term: computer, content:An early portable computer was manufactured in 1979 by GM Research,[6] a small company in Santa Monica, California. The machine which was designed and patented by James Murez. It was called the Micro Star and later changed the name to The Small One. Although Xerox claims to have designed the first such system, the machine by Murez predated anything on the market or that had been documented in any publication at the time - hence the patent was issued. As early as 1979 the U.S. Government was contracting to purchase these machines. Other major customers included Sandia Labs, General Dynamics, BBN (featured on the cover of their annual report in 1980 as the C.A.T. system) and several dozen private individuals and companies around the world. In 1979, Adam Osborne viewed the machine along with several hundred other visitors at the first computer show that was sponsored by the IEEE Westec in Los Angeles. Later that year the machine was also shown at the first COMDEX show.
1639) 14.224966, Printer (computing) - Wikipedia, the free encyclopedia.txt#1, term: computer, content:The world's first computer printer was a 19th-century mechanically driven apparatus invented by Charles Babbage for his difference engine.[2] This system used a series of metal rods with characters printed on them and stuck a roll of paper against the rods to print the characters. The first commercial printers generally used mechanisms from electric typewriters and Teletype machines, which operated in a similar fashion. The demand for higher speed led to the development of new systems specifically for computer use. Among the systems widely used through the 1980s were daisy wheel systems similar to typewriters, line printers that produced similar output but at much higher speed, and dot matrix systems that could mix text and graphics but produced relatively low-quality output. The plotter was used for those requiring high quality line art like blueprints.
1640) 14.224966, Quantum computing - Wikipedia, the free encyclopedia.txt#2, term: computer, content:Large-scale quantum computers would theoretically be able to solve certain problems much more quickly than any classical computers that use even the best currently known algorithms, like integer factorization using Shor's algorithm or the simulation of quantum many-body systems. There exist quantum algorithms, such as Simon's algorithm, that run faster than any possible probabilistic classical algorithm.[9] Given sufficient computational resources, a classical computer could in theory be made to simulate any quantum algorithm, as quantum computation does not violate the ChurchTuring thesis.[10] On the other hand, quantum computers may be able to efficiently solve problems that no classical computer would be able to solve within a reasonable amount of time.
1641) 14.224966, Quantum computing - Wikipedia, the free encyclopedia.txt#13, term: computer, content:Finally, upon termination of the algorithm, the result needs to be read off. In the case of a classical computer, we sample from the probability distribution on the three-bit register to obtain one definite three-bit string, say 000. Quantum mechanically, we measure the three-qubit state, which is equivalent to collapsing the quantum state down to a classical distribution (with the coefficients in the classical state being the squared magnitudes of the coefficients for the quantum state, as described above), followed by sampling from that distribution. Note that this destroys the original quantum state. Many algorithms will only give the correct answer with a certain probability. However, by repeatedly initializing, running and measuring the quantum computer's results, the probability of getting the correct answer can be increased. In contrast, counterfactual quantum computation allows the correct answer to be inferred when the quantum computer is not actually running in a technical sense, though earlier initialization and frequent measurements are part of the counterfactual computation protocol.
1642) 14.224966, Quantum computing - Wikipedia, the free encyclopedia.txt#61, term: computer, content:Although quantum computers may be faster than classical computers for some problem types, those described above can't solve any problem that classical computers can't already solve. A Turing machine can simulate these quantum computers, so such a quantum computer could never solve an undecidable problem like the halting problem. The existence of "standard" quantum computers does not disprove the ChurchTuring thesis.[88] It has been speculated that theories of quantum gravity, such as M-theory or loop quantum gravity, may allow even faster computers to be built. Currently, defining computation in such theories is an open problem due to the problem of time, i.e., there currently exists no obvious way to describe what it means for an observer to submit input to a computer and later receive output.[89]
1643) 14.224966, Sabre (computer system) - Wikipedia, the free encyclopedia.txt#5, term: computer, content:American Airlines had already attacked the problem to some degree, and was in the process of introducing their new Magnetronic Reservisor, an electromechanical computer, in 1952 to replace the card files. This computer consisted of a single magnetic drum, each memory location holding the number of seats left on a particular flight. Using this system, a large number of operators could look up information simultaneously, so the ticket agents could be told over the phone whether a seat was available. On the downside, a staff member was still needed at each end of the phone line, and actually handling the ticket still took considerable effort and filing. Something much more highly automated was needed if American Airlines was going to enter the jet age, booking many times more seats.[citation needed]
1644) 14.224966, Scripting language - Wikipedia, the free encyclopedia.txt#10, term: computer, content:The first interactive shells were developed in the 1960s to enable remote operation of the first time-sharing systems, and these used shell scripts, which controlled running computer programs within a computer program, the shell. Calvin Mooers in his TRAC language is generally credited with inventing command substitution, the ability to embed commands in scripts that when interpreted insert a character string into the script.[8] Multics calls these active functions.[9] Louis Pouzin wrote an early processor for command scripts called RUNCOM for CTSS around 1964. Stuart Madnick at MIT wrote a scripting language for IBM's CP/CMS in 1966. He originally called this processor COMMAND, later named EXEC.[10] Multics included an offshoot of CTSS RUNCOM, also called RUNCOM.[11] EXEC was eventually replaced by EXEC 2 and REXX.
1645) 14.224966, SCSI - Wikipedia, the free encyclopedia.txt#27, term: computer, content:On a parallel SCSI bus, a device (e.g. host adapter, disk drive) is identified by a "SCSI ID", which is a number in the range 07 on a narrow bus and in the range 015 on a wide bus. On earlier models a physical jumper or switch controls the SCSI ID of the initiator (host adapter). On modern host adapters (since about 1997), doing I/O to the adapter sets the SCSI ID; for example, the adapter often contains a BIOS program that runs when the computer boots up and that program has menus that let the operator choose the SCSI ID of the host adapter. Alternatively, the host adapter may come with software that must be installed on the host computer to configure the SCSI ID. The traditional SCSI ID for a host adapter is 7, as that ID has the highest priority during bus arbitration (even on a 16 bit bus).
1646) 14.224966, Semi-Automatic Ground Environment - Wikipedia, the free encyclopedia.txt#7, term: computer, content:The idea of using a computer to handle the task of taking reports and developing tracks had been explored beginning late in the war. By 1944, analog computers had been installed at the CH stations to automatically convert radar readings into map locations, eliminating two people. Meanwhile, the Royal Navy began experimenting with the Comprehensive Display System (CDS), another analog computer that took X and Y locations from a map and automatically generated tracks from repeated inputs. Similar systems began development with the Royal Canadian Navy, DATAR, and the US Navy, the Naval Tactical Data System. A similar system was also specified for the Nike SAM project, specifically referring to a US version of CDS,[11] coordinating the defense over a battle area so that multiple batteries did not fire on a single target. However, all of these systems were relatively small in geographic scale, generally tracking within a city-sized area.
1647) 14.224966, Semi-Automatic Ground Environment - Wikipedia, the free encyclopedia.txt#23, term: computer, content:The AN/FSQ-7 was developed by the Lincoln Laboratory's Digital Computer Laboratory and Division 6, working closely with IBM as the manufacturer. Each FSQ-7 actually consisted of two nearly identical computers operating in "duplex"[26] for redundancy. The design used an improved version of the Whirlwind I magnetic core memory and was an extension of the Whirlwind II computer program, renamed AN/FSQ-7 in 1953 to comply with Air Force nomenclature. It has been suggested the FSQ-7 was based on the IBM 701 but, while the 701 was investigated by MIT engineers, its design was ultimately rejected due to high error rates and generally being "inadequate to the task."[27] IBM's contributions were essential to the success of the FSQ-7 but IBM benefited immensely from its association with the SAGE project, most evidently during development of the IBM 704.[28][29]
1648) 14.224966, Shor's algorithm - Wikipedia, the free encyclopedia.txt#1, term: computer, content:On a quantum computer, to factor an integer N, Shor's algorithm runs in polynomial time (the time taken is polynomial in log N, which is the size of the input).[1] Specifically it takes quantum gates of order O((log N)2(log log N)(log log log N)) using fast multiplication,[2] demonstrating that the integer factorization problem can be efficiently solved on a quantum computer and is thus in the complexity class BQP. This is substantially faster than the most efficient known classical factoring algorithm, the general number field sieve, which works in sub-exponential time  about O(e1.9 (log N)1/3 (log log N)2/3).[3] The efficiency of Shor's algorithm is due to the efficiency of the quantum Fourier transform, and modular exponentiation by repeated squarings.
1649) 14.224966, Software engineering - Wikipedia, the free encyclopedia.txt#10, term: computer, content:Knowledge of computer programming is a prerequisite to becoming a software engineer. In 2004 the IEEE Computer Society produced the SWEBOK, which has been published as ISO/IEC Technical Report 1979:2004, describing the body of knowledge that they recommend to be mastered by a graduate software engineer with four years of experience.[25] Many software engineers enter the profession by obtaining a university degree or training at a vocational school. One standard international curriculum for undergraduate software engineering degrees was defined by the CCSE, and updated in 2004.[26] A number of universities have Software Engineering degree programs; as of 2010[update], there were 244 Campus Bachelor of Software Engineering programs, 70 Online programs, 230 Masters-level programs, 41 Doctorate-level programs, and 69 Certificate-level programs in the United States.[27]
1650) 14.224966, Software synthesizer - Wikipedia, the free encyclopedia.txt#4, term: computer, content:The major downside of using softsynths can often be more latency (delay between playing the note and hearing the corresponding sound). Decreasing latency requires increasing the demand on the computer's processor. When the soft synthesizer is running as a plug-in for a host sequencer, both the soft synth and the sequencer are competing for processor time. Multi-processor computers can handle this better than single-processor computers. As the processor becomes overloaded, sonic artifacts such as "clicks" and "pops" can be heard during performance or playback. When the processor becomes completely overloaded, the host sequencer or computer can lock up or crash. Increasing buffer size helps, but also increases latency. However modern professional audio interfaces can frequently operate with extremely low latency, so in recent years this has become much less of a problem than in the early days of computer music.
1651) 14.224966, Spreadsheet - Wikipedia, the free encyclopedia.txt#21, term: computer, content:In 1968, three former employees from the General Electric computer company headquartered in Phoenix, Arizona set out to start their own software development house. A. Leroy Ellison, Harry N. Cantrell, and Russell E. Edwards found themselves doing a large number of calculations when making tables for the business plans that they were presenting to venture capitalists. They decided to save themselves a lot of effort and wrote a computer program that produced their tables for them. This program, originally conceived as a simple utility for their personal use, would turn out to be the first software product offered by the company that would become known as Capex Corporation. "AutoPlan" ran on GEs Time-sharing service; afterward, a version that ran on IBM mainframes was introduced under the name AutoTab. (National CSS offered a similar product, CSSTAB, which had a moderate timesharing user base by the early 1970s. A major application was opinion research tabulation.)
1652) 14.224966, Supercomputer - Wikipedia, the free encyclopedia.txt#4, term: computer, content:Systems with massive numbers of processors generally take one of the two paths: in one approach (e.g., in distributed computing), hundreds or thousands of discrete computers (e.g., laptops) distributed across a network (e.g., the Internet) devote some or all of their time to solving a common problem; each individual computer (client) receives and completes many small tasks, reporting the results to a central server which integrates the task results from all the clients into the overall solution.[6][7] In another approach, thousands of dedicated processors are placed in proximity to each other (e.g., in a computer cluster); this saves considerable time moving data around and makes it possible for the processors to work together (rather than on separate tasks), for example in mesh and hypercube architectures.
1653) 14.224966, Supercomputer - Wikipedia, the free encyclopedia.txt#39, term: computer, content:No single number can reflect the overall performance of a computer system, yet the goal of the Linpack benchmark is to approximate how fast the computer solves numerical problems and it is widely used in the industry.[83] The FLOPS measurement is either quoted based on the theoretical floating point performance of a processor (derived from manufacturer's processor specifications and shown as "Rpeak" in the TOP500 lists) which is generally unachievable when running real workloads, or the achievable throughput, derived from the LINPACK benchmarks and shown as "Rmax" in the TOP500 list. The LINPACK benchmark typically performs LU decomposition of a large matrix. The LINPACK performance gives some indication of performance for some real-world problems, but does not necessarily match the processing requirements of many other supercomputer workloads, which for example may require more memory bandwidth, or may require better integer computing performance, or may need a high performance I/O system to achieve high levels of performance.[83]
1654) 14.224966, Telecommunication - Wikipedia, the free encyclopedia.txt#57, term: computer, content:The Internet works in part because of protocols that govern how the computers and routers communicate with each other. The nature of computer network communication lends itself to a layered approach where individual protocols in the protocol stack run more-or-less independently of other protocols. This allows lower-level protocols to be customized for the network situation while not changing the way higher-level protocols operate. A practical example of why this is important is because it allows an Internet browser to run the same code regardless of whether the computer it is running on is connected to the Internet through an Ethernet or Wi-Fi connection. Protocols are often talked about in terms of their place in the OSI reference model (pictured on the right), which emerged in 1983 as the first step in an unsuccessful attempt to build a universally adopted networking protocol suite.[84]
1655) 14.224966, Telecommunications engineering - Wikipedia, the free encyclopedia.txt#5, term: computer, content:Telecommunication systems are generally designed by telecommunication engineers which sprang from technological improvements in the telegraph industry in the late 19th century and the radio and the telephone industries in the early 20th century. Today, telecommunication is widespread and devices that assist the process, such as the television, radio and telephone, are common in many parts of the world. There are also many networks that connect these devices, including computer networks, public switched telephone network (PSTN),[citation needed] radio networks, and television networks. Computer communication across the Internet is one of many examples of telecommunication.[citation needed] Telecommunication plays a vital role in the part of world economy and the telecommunication industry's revenue has been placed at just under 3% of the gross world product.[citation needed]
1656) 14.224966, Texas Instruments - Wikipedia, the free encyclopedia.txt#34, term: computer, content:In 1979, TI entered the home computer market with the TI99/4, a competitor to such entries as the Apple II, Tandy/RadioShack TRS-80 and the later Atari 400/800 series and Commodore VIC-20. It discontinued the TI-99/4A (1981), the sequel to the 99/4, in late 1983 amidst an intense price war waged primarily against Commodore. At the 1983 Winter CES, TI showed models 99/2 and the Compact Computer 40 (CC-40), the latter aimed at professional users. The TI Professional (1983) ultimately joined the ranks of the many unsuccessful DOS and x86-basedbut non-compatible[34]competitors to the IBM PC (the founders of Compaq, an early leader in PC compatibles, all came from TI). The company for years successfully made and sold PC-compatible laptops before withdrawing from the market and selling its product line to Acer in 1997.
1657) 14.224966, Two's complement - Wikipedia, the free encyclopedia.txt#5, term: computer, content:Many early computers, including the CDC 6600, the LINC, the PDP-1, and the UNIVAC 1107, use ones' complement notation; the descendants of the UNIVAC 1107, the UNIVAC 1100/2200 series, continue to do so. The IBM 700/7000 series scientific machines use sign/magnitude notation, except for the index registers which are two's complement. Early commercial two's complement computers include the Digital Equipment Corporation PDP-5 and the 1963 PDP-6. The System/360, introduced in 1964 by IBM, then the dominant player in the computer industry, made two's complement the most widely used binary representation in the computer industry. The first minicomputer, the PDP-8 introduced in 1965, uses two's complement arithmetic as do the 1969 Data General Nova, the 1970 PDP-11, and almost all subsequent minicomputers and microcomputers.
1658) 14.224966, User interface - Wikipedia, the free encyclopedia.txt#17, term: computer, content:Early batch systems gave the currently running job the entire computer; program decks and tapes had to include what we would now think of as operating-system code to talk to I/O devices and do whatever other housekeeping was needed. Midway through the batch period, after 1957, various groups began to experiment with so-called load-and-go systems. These used a monitor program which was always resident on the computer. Programs could call the monitor for services. Another function of the monitor was to do better error checking on submitted jobs, catching errors earlier and more intelligently and generating more useful feedback to the users. Thus, monitors represented a first step towards both operating systems and explicitly designed user interfaces.
1659) 14.224966, Video game - Wikipedia, the free encyclopedia.txt#58, term: computer, content:There are many video game museums around the world, including the Computer Games Museum in Berlin[88] and the Museum of Soviet Arcade Machines in Moscow and Saint-Petersburg.[89][90] The Museum of Art and Digital Entertainment in Oakland, California is a dedicated video game museum focusing on playable exhibits of console and computer games.[91] The Video Game Museum in Rome is also dedicated to preserving video games and their history.[92] The International Center for the History of Electronic Games at The Strong in Rochester, New York contains one of the largest collections of electronic games and game-related historical materials in the world, including a 5,000-square-foot (460m2) exhibit which allows guests to play their way through the history of video games.[93][94][95] The Smithsonian Institution in Washington, D.C. has three video games on permanent display: Pac-Man, Dragon's Lair, and Pong.[96]
1660) 14.224966, Von Neumann architecture - Wikipedia, the free encyclopedia.txt#14, term: computer, content:I know that in or about 1943 or '44 von Neumann was well aware of the fundamental importance of Turing's paper of 1936 Von Neumann introduced me to that paper and at his urging I studied it with care. Many people have acclaimed von Neumann as the "father of the computer" (in a modern sense of the term) but I am sure that he would never have made that mistake himself. He might well be called the midwife, perhaps, but he firmly emphasized to me, and to others I am sure, that the fundamental conception is owing to Turing in so far as not anticipated by Babbage Both Turing and von Neumann, of course, also made substantial contributions to the "reduction to practice" of these concepts but I would not regard these as comparable in importance with the introduction and explication of the concept of a computer able to store in its memory its program of activities and of modifying that program in the course of these activities.
1661) 14.224966, Williams tube - Wikipedia, the free encyclopedia.txt#9, term: computer, content:The Williams tube tended to become unreliable with age, and most working installations had to be "tuned" by hand. By contrast, mercury delay line memory was slower and not truly random access, as the bits were presented serially, which complicated programming. Delay lines also needed hand tuning, but did not age as badly and enjoyed some success in early digital electronic computing despite their data rate, weight, cost, thermal and toxicity problems. However, the Manchester Mark 1, which used Williams tubes, was successfully commercialised as the Ferranti Mark 1. Some early computers in the USA also used the Williams tube, including the IAS machine (originally designed for Selectron tube memory), the UNIVAC 1103, Whirlwind, IBM 701, IBM 702 and the Standards Western Automatic Computer (SWAC). Williams tubes were also used in the Soviet Strela-1 and in the Japan TAC (Tokyo Automatic Computer).
1662) 14.224966, Windows 2000 - Wikipedia, the free encyclopedia.txt#35, term: computer, content:The main tools that come with Windows 2000 can be found in the Computer Management console (in Administrative Tools in the Control Panel).[85] This contains the Event Viewera means of seeing events and the Windows equivalent of a log file,[86] a system information utility, a backup utility, Task Scheduler and management consoles to view open shared folders and shared folder sessions, configure and manage COM+ applications, configure Group Policy,[87] manage all the local users and user groups, and a device manager.[88] It contains Disk Management and Removable Storage snap-ins,[89] a disk defragmenter as well as a performance diagnostic console, which displays graphs of system performance and configures data logs and alerts. It also contains a service configuration console, which allows users to view all installed services and to stop and start them, as well as configure what those services should do when the computer starts. CHKDSK has significant performance improvements.[90]
1663) 14.224966, Word processor - Wikipedia, the free encyclopedia.txt#41, term: computer, content:The mid-to-late 1980s saw the spread of laser printers, a "typographic" approach to word processing, and of true WYSIWYG bitmap displays with multiple fonts (pioneered by the Xerox Alto computer and Bravo word processing program), PostScript, and graphical user interfaces (another Xerox PARC innovation, with the Gypsy word processor which was commercialised in the Xerox Star product range). Standalone word processors adapted by getting smaller and replacing their CRTs with small character-oriented LCD displays. Some models also had computer-like features such as floppy disk drives and the ability to output to an external printer. They also got a name change, now being called "electronic typewriters" and typically occupying a lower end of the market, selling for under $200 USD.
1664) 14.224966, World Wide Web - Wikipedia, the free encyclopedia.txt#18, term: computer, content:The World Wide Web Consortium (W3C) was founded by Tim Berners-Lee after he left the European Organization for Nuclear Research (CERN) in October 1994. It was founded at the Massachusetts Institute of Technology Laboratory for Computer Science (MIT/LCS) with support from the Defense Advanced Research Projects Agency (DARPA), which had pioneered the Internet; a year later, a second site was founded at INRIA (a French national computer research lab) with support from the European Commission DG InfSo; and in 1996, a third continental site was created in Japan at Keio University. By the end of 1994, the total number of websites was still relatively small, but many notable websites were already active that foreshadowed or inspired today's most popular services.
1665) 14.224966, Zilog Z8000 - Wikipedia, the free encyclopedia.txt#10, term: computer, content:The Zilog S8000 computer came out with a version of Unix called ZEUS (Zilog Enhanced Unix System). This version of Unix was not based on Xenix but on Interactive's Unix. There may have been a Xenix version from some other company as, in 1979, Xenix was Intel 8088/8086 based. ZEUS was a port of Unix Ver 7 and included what were referred to as 'the Berkeley Enhancements'. ZEUS included a version of COBOL called RM/COBOL (Ryan McFarland COBOL). The availability of RM/COBOL allowed many commercial applications to be quickly ported to the S8000 computer although this did not help its long term success. The S8000 did find some success with the IRS and tax preparers in United States, who used the model for processing of electronically filed tax returns.[7]
1666) 13.4114275, 3D computer graphics - Wikipedia, the free encyclopedia.txt#7, term: computer, content:Rendering converts a model into an image either by simulating light transport to get photo-realistic images, or by applying an art style as in non-photorealistic rendering. The two basic operations in realistic rendering are transport (how much light gets from one place to another) and scattering (how surfaces interact with light). This step is usually performed using 3D computer graphics software or a 3D graphics API. Altering the scene into a suitable form for rendering also involves 3D projection, which displays a three-dimensional image in two dimensions.
1667) 13.4114275, 3D computer graphics - Wikipedia, the free encyclopedia.txt#12, term: computer, content:Some video games use restricted projections of three-dimensional environments, such as isometric graphics or virtual cameras with fixed angles, either as a way to improve performance of the game engine, or for stylistic and gameplay concerns. Such games are said to use pseudo-3D graphics. By contrast, games using 3D computer graphics without such restrictions are said to use true 3D.
1668) 13.4114275, 4-bit - Wikipedia, the free encyclopedia.txt#7, term: computer, content:While 32- and 64-bit processors are more prominent in modern consumer electronics, 4-bit CPUs continue to be used (usually as part of a microcontroller) in cost-sensitive applications which require minimal computing power. For example, one bicycle computer specifies that it uses a "4 bit 1-chip microcomputer".[14] Other typical uses include coffee makers, infrared remote controls,[15] and security alarms.[16]
1669) 13.4114275, 4-bit - Wikipedia, the free encyclopedia.txt#8, term: computer, content:Use of 4-bit processors has declined relative to 8- or even 32-bit processors as they are hard to find cheaper in general computer supplier's stores. The simplest kinds are not available in any of them and others are "Non-stock" and more expensive[17] (a few expensive ones can be found, as of 2014[update], on eBay[18][19][20]).
1670) 13.4114275, 64-bit computing - Wikipedia, the free encyclopedia.txt#0, term: computer, content:In computer architecture, 64-bit computing is the use of processors that have datapath widths, integer size, and memory address widths of 64 bits (eight octets). Also, 64-bit CPU and ALU architectures are those that are based on registers, address buses, or data buses of that size. From the software perspective, 64-bit computing means the use of code with 64-bit virtual memory addresses.
1671) 13.4114275, 64-bit computing - Wikipedia, the free encyclopedia.txt#3, term: computer, content:Without further qualification, a 64-bit computer architecture generally has integer and addressing registers that are 64 bits wide, allowing direct support for 64-bit data types and addresses. However, a CPU might have external data buses or address buses with different sizes from the registers, even larger (the 32-bit Pentium had a 64-bit data bus, for instance[4]). The term may also refer to the size of low-level data types, such as 64-bit floating-point numbers.
1672) 13.4114275, 86-DOS - Wikipedia, the free encyclopedia.txt#6, term: computer, content:Microsoft purchased a non-exclusive license for 86-DOS from Seattle Computer Products[7] in December 1980 for $25,000.[citation needed] In May 1981, it hired Tim Paterson to port the system to the IBMPC,[1] which used the slower and less expensive Intel 8088 processor and had its own specific family of peripherals. IBM watched the developments daily,[1] submitted over 300 change requests before it accepted the product and wrote the user manual for it.
1673) 13.4114275, 86-DOS - Wikipedia, the free encyclopedia.txt#11, term: computer, content:Seattle Computer Products' 86-DOS supported the FAT12 filesystem on a range of 8.0" and 5.25" floppy disk drives on S-100 floppy disk controller hardware manufactured by Cromemco, Tarbell Electronics and North Star Computers. The Western Digital FD1771-based Cromemco and Tarbell boards supported one-sided single-density soft-sectored drives. A Tarbell double-density board utilizing the FD1791 was supported as well. At a later stage, SCP offered advanced floppy disk controllers like the Disk Master series themselves.
1674) 13.4114275, 86-DOS - Wikipedia, the free encyclopedia.txt#17, term: computer, content:Disk formats supported by one of the last versions developed by Tim Paterson at Microsoft, MS-DOS 1.25[18][22][23] [24] (March 1982) for the SCP Gazelle computer with SCP controller or Cromemco 16FDC controller (by default, this version only supported the MS-DOS-compatible variants of the 8.0" formats with a single reserved sector but it could be built to provide two extra drive letters to read and write floppies in the previous SCP 86-DOS 8.0" disk formats since 0.42 as well):
1675) 13.4114275, Ada (programming language) - Wikipedia, the free encyclopedia.txt#1, term: computer, content:Ada was originally designed by a team led by Jean Ichbiah of CII Honeywell Bull under contract to the United States Department of Defense (DoD) from 1977 to 1983 to supersede over 450 programming languages used by the DoD at that time.[9] Ada was named after Ada Lovelace (18151852), who is credited with being the first computer programmer.[10]
1676) 13.4114275, Ada (programming language) - Wikipedia, the free encyclopedia.txt#18, term: computer, content:The first validated Ada implementation was the NYU Ada/Ed translator,[17] certified on April 11, 1983. NYU Ada/Ed is implemented in the high-level set language SETL.[18] A number of commercial companies began offering Ada compilers and associated development tools, including Alsys, TeleSoft, DDC-I, Advanced Computer Techniques, Tartan Laboratories, TLD Systems, Verdix, and others.[19]
1677) 13.4114275, Advanced Micro Devices - Wikipedia, the free encyclopedia.txt#0, term: computer, content:Advanced Micro Devices, Inc. (AMD) is an American multinational semiconductor company based in Sunnyvale, California, United States, that develops computer processors and related technologies for business and consumer markets. While initially it manufactured its own processors, the company became fabless after GlobalFoundries was spun off in 2009. AMD's main products include microprocessors, motherboard chipsets, embedded processors and graphics processors for servers, workstations and personal computers, and embedded systems applications.
1678) 13.4114275, Advanced Micro Devices - Wikipedia, the free encyclopedia.txt#83, term: computer, content:On August 31, 2011, in Austin, Texas, AMD achieved a Guinness World Record for the "Highest frequency of a computer processor": 8.429GHz.[156] The company ran an 8-core FX-8150 processor with only one active module (two cores), and cooled with liquid helium.[157] The previous record was 8.308GHz, with an Intel Celeron 352 (one core).
1679) 13.4114275, Alan Turing - Wikipedia, the free encyclopedia.txt#2, term: computer, content:After the war, he worked at the National Physical Laboratory, where he designed the ACE, among the first designs for a stored-program computer. In 1948 Turing joined Max Newman's Computing Machine Laboratory at the Victoria University of Manchester, where he helped develop the Manchester computers[7] and became interested in mathematical biology. He wrote a paper on the chemical basis of morphogenesis, and predicted oscillating chemical reactions such as the BelousovZhabotinsky reaction, first observed in the 1960s.
1680) 13.4114275, Alan Turing - Wikipedia, the free encyclopedia.txt#42, term: computer, content:According to the memoirs of the German computer pioneer Heinz Billing from the Max Planck Institute for Physics, published by Genscher, Dsseldorf, there was a meeting between Alan Turing and Konrad Zuse.[90] It took place in Gttingen in 1947. The interrogation had the form of a colloquium. Participants were Womersley, Turing, Porter from England and a few German researchers like Zuse, Walther, and Billing. (For more details see Herbert Bruderer, Konrad Zuse und die Schweiz).
1681) 13.4114275, Algorithm - Wikipedia, the free encyclopedia.txt#18, term: computer, content:Algorithms can be expressed in many kinds of notation, including natural languages, pseudocode, flowcharts, drakon-charts, programming languages or control tables (processed by interpreters). Natural language expressions of algorithms tend to be verbose and ambiguous, and are rarely used for complex or technical algorithms. Pseudocode, flowcharts, drakon-charts and control tables are structured ways to express algorithms that avoid many of the ambiguities common in natural language statements. Programming languages are primarily intended for expressing algorithms in a form that can be executed by a computer, but are often used as a way to define or document algorithms.
1682) 13.4114275, Algorithm - Wikipedia, the free encyclopedia.txt#28, term: computer, content:Computers (and computors), models of computation: A computer (or human "computor"[25]) is a restricted type of machine, a "discrete deterministic mechanical device"[26] that blindly follows its instructions.[27] Melzak's and Lambek's primitive models[28] reduced this notion to four elements: (i) discrete, distinguishable locations, (ii) discrete, indistinguishable counters[29] (iii) an agent, and (iv) a list of instructions that are effective relative to the capability of the agent.[30]
1683) 13.4114275, Analog computer - Wikipedia, the free encyclopedia.txt#7, term: computer, content:The Antikythera mechanism was an orrery and is also claimed to be the earliest known mechanical analog computer, according to Derek J. de Solla Price.[2] It was designed to calculate astronomical positions. It was discovered in 1901 in the Antikythera wreck off the Greek island of Antikythera, between Kythera and Crete, and has been dated to circa 100 BC. Devices of a level of complexity comparable to that of the Antikythera mechanism would not reappear until a thousand years later.
1684) 13.4114275, Analog computer - Wikipedia, the free encyclopedia.txt#11, term: computer, content:The slide rule was invented around 16201630, shortly after the publication of the concept of the logarithm. It is a hand-operated analog computer for doing multiplication and division. As slide rule development progressed, added scales provided reciprocals, squares and square roots, cubes and cube roots, as well as transcendental functions such as logarithms and exponentials, circular and hyperbolic trigonometry and other functions. Aviation is one of the few fields where slide rules are still in widespread use, particularly for solving timedistance problems in light aircraft.
1685) 13.4114275, Analog computer - Wikipedia, the free encyclopedia.txt#13, term: computer, content:The differential analyser, a mechanical analog computer designed to solve differential equations by integration, used wheel-and-disc mechanisms to perform the integration. In 1876 Lord Kelvin had already discussed the possible construction of such calculators, but he had been stymied by the limited output torque of the ball-and-disk integrators.[10] In a differential analyzer, the output of one integrator drove the input of the next integrator, or a graphing output. The torque amplifier was the advance that allowed these machines to work. Starting in the 1920s, Vannevar Bush and others developed mechanical differential analyzers.
1686) 13.4114275, Analog computer - Wikipedia, the free encyclopedia.txt#14, term: computer, content:The Dumaresq was a mechanical calculating device invented around 1902 by Lieutenant John Dumaresq of the Royal Navy. It was an analog computer which related vital variables of the fire control problem to the movement of one's own ship and that of a target ship. It was often used with other devices, such as a Vickers range clock to generate range and deflection data so the gun sights of the ship could be continuously set. A number of versions of the Dumaresq were produced of increasing complexity as development proceeded.
1687) 13.4114275, Analog computer - Wikipedia, the free encyclopedia.txt#55, term: computer, content:A special type of integrator, used at a point where only moderate accuracy was needed, was based on a steel ball, instead of a disc. It had two inputs, one to rotate the ball, and the other to define the angle of the ball's rotating axis. That axis was always in a plane that contained the axes of two movement-pickoff rollers, quite similar to the mechanism of a rolling-ball computer mouse (in this mechanism, the pickoff rollers were roughly the same diameter as the ball). The pickoff roller axes were at right angles.
1688) 13.4114275, Analog computer - Wikipedia, the free encyclopedia.txt#69, term: computer, content:Any physical process which models some computation can be interpreted as an analog computer. Some examples, invented for the purpose of illustrating the concept of analog computation, include using a bundle of spaghetti as a model of sorting numbers; a board, a set of nails, and a rubber band as a model of finding the convex hull of a set of points; and strings tied together as a model of finding the shortest path in a network. These are all described in Dewdney (1984).
1689) 13.4114275, Analog computer - Wikipedia, the free encyclopedia.txt#83, term: computer, content:These idealized analog computers may in theory solve problems that are intractable on digital computers; however as mentioned, in reality, analog computers are far from attaining this ideal, largely because of noise minimization problems. In theory, ambient noise is limited by quantum noise (caused by the quantum movements of ions). Ambient noise may be severely reduced but never to zero by using cryogenically cooled parametric amplifiers. Moreover, given unlimited time and memory, the (ideal) digital computer may also solve real number problems.
1690) 13.4114275, Analytical Engine - Wikipedia, the free encyclopedia.txt#7, term: computer, content:In 1842, the Italian mathematician Luigi Federico Menabrea published a description of the engine based on a lecture by Babbage in French. In 1843, the description was translated into English and extensively annotated by Ada Lovelace, who had become interested in the engine eight years earlier. In recognition of her additions to Menabrea's paper, which included a way to calculate Bernoulli numbers using the machine, she has been described as the first computer programmer.
1691) 13.4114275, Analytical Engine - Wikipedia, the free encyclopedia.txt#21, term: computer, content:Babbage understood that the existence of an automatic computer would kindle interest in the field now known as algorithmic efficiency, writing in his Passages from the Life of a Philosopher, "As soon as an Analytical Engine exists, it will necessarily guide the future course of the science. Whenever any result is sought by its aid, the question will then ariseBy what course of calculation can these results be arrived at by the machine in the shortest time?"[28]
1692) 13.4114275, Antikythera - Wikipedia, the free encyclopedia.txt#6, term: computer, content:Antikythera is most famous for being the location of the 1900 discovery of the Antikythera wreck,[7] from which the Antikythera Ephebe and Antikythera Mechanism were recovered. The Antikythera mechanism is an ancient mechanical calculator (sometimes described as the first mechanical computer) designed to calculate astronomical positions which has been dated to about 205 BC.[8] Technological artifacts of similar complexity did not appear until a thousand years later.
1693) 13.4114275, Antikythera wreck - Wikipedia, the free encyclopedia.txt#7, term: computer, content:On 17 May 1902, however, the former Minister of Education Spyridon Stais made the most celebrated find at the National Archaeological Museum in Athens. When examining the artifacts that had been recovered, he noticed that a severely corroded piece of bronze had inscriptions and a gear wheel embedded in it. The object would come to be known as the Antikythera mechanism or astrolabe. Originally thought to be one of the first forms of a mechanised clock or an astrolabe, it is at times referred to as the worlds oldest known analog computer.[5]
1694) 13.4114275, Antikythera wreck - Wikipedia, the free encyclopedia.txt#11, term: computer, content:Further evidence for an early 1st-century BC sinking date came in 1974, when Yale University Professor Derek de Solla Price published his interpretation of the Antikythera mechanism. He argued that the object was a calendar computer. From gear settings and inscriptions on the mechanism's faces, he concluded that the mechanism was made about 87 BC and lost only a few years later.
1695) 13.4114275, Antivirus software - Wikipedia, the free encyclopedia.txt#7, term: computer, content:Before internet connectivity was widespread, computer viruses were typically spread by infected floppy disks. Antivirus software came into use, but was updated relatively infrequently. During this time, virus checkers essentially had to check executable files and the boot sectors of floppy disks and hard disks. However, as internet usage became common, viruses began to spread online.[21]
1696) 13.4114275, Antivirus software - Wikipedia, the free encyclopedia.txt#46, term: computer, content:Anti-virus software can attempt to scan for rootkits. A rootkit is a type of malware designed to gain administrative-level control over a computer system without being detected. Rootkits can change how the operating system functions and in some cases can tamper with the anti-virus program and render it ineffective. Rootkits are also difficult to remove, in some cases requiring a complete re-installation of the operating system.[95]
1697) 13.4114275, Antivirus software - Wikipedia, the free encyclopedia.txt#58, term: computer, content:The functionality of a few computer programs can be hampered by active anti-virus software. For example, TrueCrypt, a disk encryption program, states on its troubleshooting page that anti-virus programs can conflict with TrueCrypt and cause it to malfunction or operate very slowly.[119] Anti-virus software can impair the performance and stability of games running in the Steam platform.[120]
1698) 13.4114275, Antivirus software - Wikipedia, the free encyclopedia.txt#60, term: computer, content:Studies in December 2007 showed that the effectiveness of antivirus software had decreased in the previous year, particularly against unknown or zero day attacks. The computer magazine c't found that detection rates for these threats had dropped from 40-50% in 2006 to 20-30% in 2007. At that time, the only exception was the NOD32 antivirus, which managed a detection rate of 68 percent.[122] According to the ZeuS tracker website the average detection rate for all variants of the well-known ZeuS trojan is as low as 40%.[123]
1699) 13.4114275, Antivirus software - Wikipedia, the free encyclopedia.txt#69, term: computer, content:If a file has been infected by a computer virus, anti-virus software will attempt to remove the virus code from the file during disinfection, but it is not always able to restore the file to its undamaged state.[133][134] In such circumstances, damaged files can only be restored from existing backups or shadow copies (this is also true for ransomware[135]); installed software that is damaged requires re-installation[136] (however, see System File Checker).
1700) 13.4114275, Antivirus software - Wikipedia, the free encyclopedia.txt#72, term: computer, content:Furthermore, inexperienced users can be lulled into a false sense of security when using the computer, considering themselves to be invulnerable, and may have problems understanding the prompts and decisions that antivirus software presents them with. An incorrect decision may lead to a security breach. If the antivirus software employs heuristic detection, it must be fine-tuned to minimize misidentifying harmless software as malicious (false positive).[144]
1701) 13.4114275, Antivirus software - Wikipedia, the free encyclopedia.txt#75, term: computer, content:Network firewalls prevent unknown programs and processes from accessing the system. However, they are not antivirus systems and make no attempt to identify or remove anything. They may protect against infection from outside the protected computer or network, and limit the activity of any malicious software which is present by blocking incoming or outgoing requests on certain TCP/IP ports. A firewall is designed to deal with broader system threats that come from network connections into the system and is not an alternative to a virus protection system.
1702) 13.4114275, Arithmometer - Wikipedia, the free encyclopedia.txt#14, term: computer, content:Because it was the first mass-marketed and the first widely copied calculator, its design marks the starting point of the mechanical calculator industry, which eventually morphed into the electronic calculator industry and which, through the accidental design of the first microprocessor to be commercialized, the Intel 4004, for one of Busicom's calculator in 1971, led to the first commercially available personal computer, the Altair in 1975.
1703) 13.4114275, ARM architecture - Wikipedia, the free encyclopedia.txt#0, term: computer, content:ARM, originally Acorn RISC Machine, later Advanced RISC Machine, is a family of reduced instruction set computing (RISC) architectures for computer processors, configured for various environments. British company ARM Holdings develops the architecture and licenses it to other companies, who design their own products that implement one of those architecturesincluding systems-on-chips (SoC) that incorporate memory, interfaces, radios, etc. It also designs cores that implement this instruction set and licenses these designs to a number of companies that incorporate those core designs into their own products.
1704) 13.4114275, ARM architecture - Wikipedia, the free encyclopedia.txt#1, term: computer, content:A RISC-based computer design approach means processors require fewer transistors than typical complex instruction set computing (CISC) x86 processors in most personal computers. This approach reduces costs, heat and power use. Such reductions are desirable traits for light, portable, battery-powered devicesincluding smartphones, laptops, tablet and notepad computers, and other embedded systems.[3][4][5]
1705) 13.4114275, ARM architecture - Wikipedia, the free encyclopedia.txt#12, term: computer, content:In the late 1980s Apple Computer and VLSI Technology started working with Acorn on newer versions of the ARM core. In 1990, Acorn spun off the design team into a new company named Advanced RISC Machines Ltd.,[24][25][26] which became ARM Ltd when its parent company, ARM Holdings plc, floated on the London Stock Exchange and NASDAQ in 1998.[27]
1706) 13.4114275, ARPANET - Wikipedia, the free encyclopedia.txt#1, term: computer, content:The packet switching methodology employed in the ARPANET was based on concepts and designs by Americans Leonard Kleinrock and Paul Baran, British scientist Donald Davies, and Lawrence Roberts of the Lincoln Laboratory.[6] The TCP/IP communications protocols were developed for ARPANET by computer scientists Robert Kahn and Vint Cerf, and incorporated concepts by Louis Pouzin for the French CYCLADES project.
1707) 13.4114275, ARPANET - Wikipedia, the free encyclopedia.txt#31, term: computer, content:Senator Albert Gore, Jr. began to craft the High Performance Computing and Communication Act of 1991 (commonly referred to as "The Gore Bill") after hearing the 1988 report toward a National Research Network submitted to Congress by a group chaired by Leonard Kleinrock, professor of computer science at UCLA. The bill was passed on 9 December 1991 and led to the National Information Infrastructure (NII) which Al Gore called the "information superhighway". ARPANET was the subject of two IEEE Milestones, both dedicated in 2009.[46][47]
1708) 13.4114275, Artificial intelligence - Wikipedia, the free encyclopedia.txt#5, term: computer, content:Mechanical or "formal" reasoning has been developed by philosophers and mathematicians since antiquity. The study of logic led directly to the invention of the programmable digital electronic computer, based on the work of mathematician Alan Turing and others. Turing's theory of computation suggested that a machine, by shuffling symbols as simple as "0" and "1", could simulate any conceivable act of mathematical deduction.[17][18] This, along with concurrent discoveries in neurology, information theory and cybernetics, inspired a small group of researchers to begin to seriously consider the possibility of building an electronic brain.[19]
1709) 13.4114275, Artificial intelligence - Wikipedia, the free encyclopedia.txt#29, term: computer, content:Emotion and social skills[91] play two roles for an intelligent agent. First, it must be able to predict the actions of others, by understanding their motives and emotional states. (This involves elements of game theory, decision theory, as well as the ability to model human emotions and the perceptual skills to detect emotions.) Also, in an effort to facilitate human-computer interaction, an intelligent machine might want to be able to display emotionseven if it does not actually experience them itselfin order to appear sensitive to the emotional dynamics of human interaction.
1710) 13.4114275, Artificial intelligence - Wikipedia, the free encyclopedia.txt#82, term: computer, content:Joseph Weizenbaum wrote that AI applications can not, by definition, successfully simulate genuine human empathy and that the use of AI technology in fields such as customer service or psychotherapy[221] was deeply misguided. Weizenbaum was also bothered that AI researchers (and some philosophers) were willing to view the human mind as nothing more than a computer program (a position now known as computationalism). To Weizenbaum these points suggest that AI research devalues human life.[222]
1711) 13.4114275, Assembly language - Wikipedia, the free encyclopedia.txt#22, term: computer, content:Each computer architecture has its own machine language. Computers differ in the number and type of operations they support, in the different sizes and numbers of registers, and in the representations of data in storage. While most general-purpose computers are able to carry out essentially the same functionality, the ways they do so differ; the corresponding assembly languages reflect these differences.
1712) 13.4114275, Assembly language - Wikipedia, the free encyclopedia.txt#34, term: computer, content:Assembly languages, like most other computer languages, allow comments to be added to program source code that will be ignored during assembly. Judicious commenting is essential in assembly language programs, as the meaning and purpose of a sequence of binary machine instructions can be difficult to determine. The "raw" (uncommented) assembly language generated by compilers or disassemblers is quite difficult to read when changes must be made.
1713) 13.4114275, Association for Computing Machinery - Wikipedia, the free encyclopedia.txt#28, term: computer, content:The President of the ACM for 20142016 is Alexander L. Wolf of Imperial College London, UK. He is the successor of Vint Cerf (20122014), an American computer scientist who is recognized as one of "the fathers of the Internet"; Alain Chesnais (20102012), a French citizen living in Toronto, Canada, where he runs his company named Visual Transitions; and Dame Wendy Hall of the University of Southampton, UK (2008-2010).[26]
1714) 13.4114275, Atanasoff–Berry computer - Wikipedia, the free encyclopedia.txt#5, term: computer, content:It was not a Turing complete computer, which distinguishes it from more general machines, like contemporary Konrad Zuse's Z3 (1941), or later machines like the 1946 ENIAC, 1949 EDVAC, the University of Manchester designs, or Alan Turing's post-War design of ACE at NPL and elsewhere. Nor did it implement the stored program architecture that made practical fully general-purpose, reprogrammable computers.
1715) 13.4114275, Atanasoff–Berry computer - Wikipedia, the free encyclopedia.txt#10, term: computer, content:Although the AtanasoffBerry Computer was an important step up from earlier calculating machines, it was not able to run entirely automatically through an entire problem. An operator was needed to operate the control switches to set up its functions, much like the electro-mechanical calculators and unit record equipment of the time. Selection of the operation to be performed, reading, writing, converting to or from binary to decimal, or reducing a set of equations was made by front panel switches and in some cases jumpers.
1716) 13.4114275, Audio editing software - Wikipedia, the free encyclopedia.txt#0, term: computer, content:Audio editing software is software which allows editing and generating of audio data. Audio editing software can be implemented completely or partly as library, as computer application, as Web application or as a loadable kernel module. Wave Editors are digital audio editors and there are many sources of software available to perform this function. Most can edit music, apply effects and filters, adjust stereo channels etc.
1717) 13.4114275, Ball-and-disk integrator - Wikipedia, the free encyclopedia.txt#5, term: computer, content:Similar devices soon appeared in other navies and for other roles. The US Navy used a somewhat simpler device known as the Rangekeeper, but this also saw continual modification over time and eventually turned into a system of equal or greater sophistication to the UK versions. A similar calculator formed the basis of the Torpedo Data Computer, which solved the more demanding problem of the very long engagement times of torpedo fire.
1718) 13.4114275, BASIC - Wikipedia, the free encyclopedia.txt#36, term: computer, content:The ubiquity of BASIC interpreters on personal computers was such that textbooks once included simple "Try It In BASIC" exercises that encouraged students to experiment with mathematical and computational concepts on classroom or home computers. Popular computer magazines of the day typically included type-in programs. Futurist and sci-fi writer David Brin mourned the loss of ubiquitous BASIC in a 2006 Salon article[23] as have others who first used computers during this era. In turn, the article prompted Microsoft to develop and release Small Basic.[24]
1719) 13.4114275, Berkeley Software Distribution - Wikipedia, the free encyclopedia.txt#0, term: computer, content:Berkeley Software Distribution (BSD) is a Unix operating system derivative developed and distributed by the Computer Systems Research Group (CSRG) of the University of California, Berkeley, from 1977 to 1995. Today the term "BSD" is often used non-specifically to refer to any of the BSD descendants which together form a branch of the family of Unix-like operating systems. Operating systems derived from the original BSD code remain actively developed and widely used.
1720) 13.4114275, Berkeley Software Distribution - Wikipedia, the free encyclopedia.txt#17, term: computer, content:After 4.3BSD, it was determined that BSD would move away from the aging VAX platform. The Power 6/32 platform (codenamed "Tahoe") developed by Computer Consoles Inc. seemed promising at the time, but was abandoned by its developers shortly thereafter. Nonetheless, the 4.3BSD-Tahoe port (June 1988) proved valuable, as it led to a separation of machine-dependent and machine-independent code in BSD which would improve the system's future portability.
1721) 13.4114275, Berkeley Software Distribution - Wikipedia, the free encyclopedia.txt#35, term: computer, content:Most university and government computer centers that use UNIX use Berkeley UNIX, rather than System V. There are several reasons for this, but perhaps the two most significant are that Berkeley UNIX provides networking capabilities that until recently (Release 3.0) were completely unavailable in System V, and that Berkeley UNIX is much more suited to a research environment, which requires a faster file system, better virtual memory handling, and a larger variety of programming languages.
1722) 13.4114275, Binary number - Wikipedia, the free encyclopedia.txt#0, term: computer, content:In mathematics and digital electronics, a binary number is a number expressed in the binary numeral system or base-2 numeral system which represents numeric values using two different symbols: typically 0 (zero) and 1 (one). The base-2 system is a positional notation with a radix of 2. Because of its straightforward implementation in digital electronic circuitry using logic gates, the binary system is used internally by almost all modern computers and computer-based devices. Each digit is referred to as a bit.
1723) 13.4114275, Bletchley Park - Wikipedia, the free encyclopedia.txt#52, term: computer, content:The museum, which opened in 2007, is an independent voluntary organisation that is governed by its own board of trustees. Its aim is "To collect and restore computer systems particularly those developed in Britain and to enable people to explore that collection for inspiration, learning and enjoyment."[122] Through its many exhibits, the museum displays the story of computing through the mainframes of the 1960s and 1970s, and the rise of personal computing in the 1980s. It has a policy of having as many of the exhibits as possible in full working order.
1724) 13.4114275, Bombe - Wikipedia, the free encyclopedia.txt#65, term: computer, content:In 1994 a group led by John Harper of the BCS Computer Conservation Society started a project to build a working replica of a bombe.[62] The project required detailed research, and took 13 years of effort before the replica was completed, which was then put on display at the Bletchley Park museum.[63] In March 2009 it won an Engineering Heritage Award.[64]
1725) 13.4114275, Booting - Wikipedia, the free encyclopedia.txt#5, term: computer, content:Early computers in the 1940s and 1950s were one-of-a-kind engineering efforts that could take weeks to program and program loading was one of many problems that had to be solved. An early computer, ENIAC, had no "program" stored in memory, but was set up for each problem by a configuration of interconnecting cables. Bootstrapping did not apply to ENIAC, whose hardware configuration was ready for solving problems as soon as power was applied.
1726) 13.4114275, Booting - Wikipedia, the free encyclopedia.txt#20, term: computer, content:Following the older approach, the earlier PDP-1 has a hardware loader, such that an operator need only push the "load" switch to instruct the paper tape reader to load a program directly into core memory. The Data General Supernova used front panel switches to cause the computer to automatically load instructions into memory from a device specified by the front panel's data switches, and then jump to loaded code; the Nova 800 and 1200 had a switch that loaded a program into main memory from a special read-only memory and jumped to it.[15]
1727) 13.4114275, Booting - Wikipedia, the free encyclopedia.txt#56, term: computer, content:Some systems (particularly newer Macintoshes and new editions of Microsoft Windows) use Intel's EFI. Also coreboot allows a computer to boot without having the firmware/BIOS constantly running in system management mode. 16-bit BIOS interfaces are required by certain x86 operating systems, such as DOS and Windows 3.1/95/98 (and all when not booted via UEFI). However, most boot loaders retain 16-bit BIOS call support.[36][37][38]
1728) 13.4114275, Branch (computer science) - Wikipedia, the free encyclopedia.txt#3, term: computer, content:Depending on computer architecture, the assembly language mnemonic for a jump instruction is typically some shortened form of the word jump or the word branch, often along with other informative letters (or an extra parameter) representing the condition. Sometimes other details are included as well, such as the range of the jump (the offset size) or a special addressing mode that should be used to locate the actual effective offset.
1729) 13.4114275, Brian Randell - Wikipedia, the free encyclopedia.txt#5, term: computer, content:Brian Randell's main research interests are in the field of "computer science, specifically on system dependability and fault tolerance. His interest in the history of computing was kick-started by coming across the then almost unknown work of Percy Ludgate. This was over thirty years ago, when he was preparing an inaugural lecture, and led on to producing the book "The Origins of Computers". This triggered him to further investigate the Colossus wartime code-breaking machines".[1]
1730) 13.4114275, British Computer Society - Wikipedia, the free encyclopedia.txt#2, term: computer, content:BCS is a member institution of Engineering Council UK, through which it is licensed to award the designation of Incorporated Engineer and Chartered Engineer and therefore is responsible for regulation of ICT and computer science fields within the UK. The BCS is also a member of the Council of European Professional Informatics Societies (CEPIS) and the Seoul Accord for international tertiary degree recognition. BCS is also a member organisation of the Science Council through which it is licensed to award the designation of Chartered Scientist.
1731) 13.4114275, British Computer Society - Wikipedia, the free encyclopedia.txt#20, term: computer, content:The advanced course of ECDL ("Advanced ECDL") has four sections, each a qualification in its own right. Upon achieving all four advanced qualifications, the individual will receive a qualification as an "ECDL Expert"  in the UK, this confers upon the person Associate Membership of The British Computer Society, should that person wish to sign up to a code of conduct and join BCS.
1732) 13.4114275, British Computer Society - Wikipedia, the free encyclopedia.txt#22, term: computer, content:e-type is a qualification that allows individuals to improve and certify their typing skills. The average user can save up to 21 days a year by improving their typing speed as well as preventing repetitive strain injury (RSI). e-type comes with full support materials and computer-based courseware before allowing the user to assess their skills using a simple online test.[20]
1733) 13.4114275, Bus (computing) - Wikipedia, the free encyclopedia.txt#3, term: computer, content:In most traditional computer architectures, the CPU and main memory tend to be tightly coupled. A microprocessor conventionally is a single chip which has a number of electrical connections on its pins that can be used to select an "address" in the main memory and another set of pins to read and write the data stored at that location. In most cases, the CPU and memory share signalling characteristics and operate in synchrony. The bus connecting the CPU and memory is one of the defining characteristics of the system, and often referred to simply as the system bus.
1734) 13.4114275, Bus (computing) - Wikipedia, the free encyclopedia.txt#23, term: computer, content:Increasing the speed of the CPU becomes harder, because the speed of all the devices must increase as well. When it is not practical or economical to have all devices as fast as the CPU, the CPU must either enter a wait state, or work at a slower clock frequency temporarily,[7] to talk to other devices in the computer. While acceptable in embedded systems, this problem was not tolerated for long in general-purpose, user-expandable computers.
1735) 13.4114275, C (programming language) - Wikipedia, the free encyclopedia.txt#0, term: computer, content:C (/si/, as in the letter c) is a general-purpose, imperative computer programming language, supporting structured programming, lexical variable scope and recursion, while a static type system prevents many unintended operations. By design, C provides constructs that map efficiently to typical machine instructions, and therefore it has found lasting use in applications that had formerly been coded in assembly language, including operating systems, as well as various application software for computers ranging from supercomputers to embedded systems.
1736) 13.4114275, C (programming language) - Wikipedia, the free encyclopedia.txt#1, term: computer, content:C was originally developed by Dennis Ritchie between 1969 and 1973 at Bell Labs,[5] and used to re-implement the Unix operating system.[6] It has since become one of the most widely used programming languages of all time,[7][8] with C compilers from various vendors available for the majority of existing computer architectures and operating systems. C has been standardized by the American National Standards Institute (ANSI) since 1989 (see ANSI C) and subsequently by the International Organization for Standardization (ISO).
1737) 13.4114275, C (programming language) - Wikipedia, the free encyclopedia.txt#3, term: computer, content:Despite its low-level capabilities, the language was designed to encourage cross-platform programming. A standards-compliant and portably written C program can be compiled for a very wide variety of computer platforms and operating systems with few changes to its source code. The language has become available on a very wide range of platforms, from embedded microcontrollers to supercomputers.
1738) 13.4114275, C++ - Wikipedia, the free encyclopedia.txt#10, term: computer, content:According to Stroustrup: "the name signifies the evolutionary nature of the changes from C".[13] This name is credited to Rick Mascitti (mid-1983)[10] and was first used in December 1983. When Mascitti was questioned informally in 1992 about the naming, he indicated that it was given in a tongue-in-cheek spirit. The name comes from C's "++" operator (which increments the value of a variable) and a common naming convention of using "+" to indicate an enhanced computer program.
1739) 13.4114275, Calculator - Wikipedia, the free encyclopedia.txt#5, term: computer, content:In addition to general purpose calculators, there are those designed for specific markets; for example, there are scientific calculators which include trigonometric and statistical calculations. Some calculators even have the ability to do computer algebra. Graphing calculators can be used to graph functions defined on the real line, or higher-dimensional Euclidean space. Currently, basic calculators are inexpensive, but the scientific and graphing models tend to be higher priced.
1740) 13.4114275, Calculator - Wikipedia, the free encyclopedia.txt#10, term: computer, content:Calculators also have the ability to store numbers into computer memory. Basic types of these store only one number at a time; more specific types are able to store many numbers represented in variables. The variables can also be used for constructing formulas. Some models have the ability to extend memory capacity to store more numbers; the extended memory address is referred to as an array index.
1741) 13.4114275, Calculator - Wikipedia, the free encyclopedia.txt#11, term: computer, content:Power sources of calculators are: batteries, solar cells or mains electricity (for old models), turning on with a switch or button. Some models even have no turn-off button but they provide some way to put off (for example, leaving no operation for a moment, covering solar cell exposure, or closing their lid). Crank-powered calculators were also common in the early computer era.
1742) 13.4114275, Calculator - Wikipedia, the free encyclopedia.txt#67, term: computer, content:The two leading manufacturers, HP and TI, released increasingly feature-laden calculators during the 1980s and 1990s. At the turn of the millennium, the line between a graphing calculator and a handheld computer was not always clear, as some very advanced calculators such as the TI-89, the Voyage 200 and HP-49G could differentiate and integrate functions, solve differential equations, run word processing and PIM software, and connect by wire or IR to other calculators/computers.
1743) 13.4114275, Cellular architecture - Wikipedia, the free encyclopedia.txt#0, term: computer, content:A cellular architecture is a type of computer architecture prominent in parallel computing. Cellular architectures are relatively new, with IBM's Cell microprocessor being the first one to reach the market. Cellular architecture takes multi-core architecture design to its logical conclusion, by giving the programmer the ability to run large numbers of concurrent threads within a single processor. Each 'cell' is a compute node containing thread units, memory, and communication. Speed-up is achieved by exploiting thread-level parallelism inherent in many applications.
1744) 13.4114275, Charles Babbage - Wikipedia, the free encyclopedia.txt#70, term: computer, content:The London Science Museum has constructed two Difference Engines according to Babbage's plans for the Difference Engine No 2. One is owned by the museum. The other, owned by the technology multimillionaire Nathan Myhrvold, went on exhibition at the Computer History Museum[148] in Mountain View, California on 10 May 2008.[149] The two models that have been constructed are not replicas; Myhrvold's engine is the first design by Babbage, and the London Science Museum's is a later model.
1745) 13.4114275, Charles Babbage - Wikipedia, the free encyclopedia.txt#71, term: computer, content:After the attempt at making the first difference engine fell through, Babbage worked to design a more complex machine called the Analytical Engine. He hired C. G. Jarvis, who had previously worked for Clement as a draughtsman.[150] The Analytical Engine marks the transition from mechanised arithmetic to fully-fledged general purpose computation. It is largely on it that Babbage's standing as computer pioneer rests.[151]
1746) 13.4114275, Chemical computer - Wikipedia, the free encyclopedia.txt#4, term: computer, content:In 2014, a chemical computing system was developed by an international team headed by the Swiss Federal Laboratories for Materials Science and Technology (Empa). The chemical computer used surface tension calculations derived from the Marangoni Effect using an acidic gel to find the most efficient route between points A and B, outpacing a conventional Satellite Navigation system attempting to calculate the same route.[6][7]
1747) 13.4114275, Church–Turing thesis - Wikipedia, the free encyclopedia.txt#1, term: computer, content:Church[2] and Turing[3] proved that these three formally defined classes of computable functions coincide: a function is -computable if and only if it is Turing computable if and only if it is general recursive. This has led mathematicians and computer scientists to believe that the concept of computability is accurately characterized by these three equivalent processes. Other formal attempts to characterize computability have subsequently strengthened this belief (see below).
1748) 13.4114275, Church–Turing thesis - Wikipedia, the free encyclopedia.txt#3, term: computer, content:Since its inception, variations on the original thesis have arisen, including statements about what can physically be realized by a computer in our universe (Physical Church-Turing Thesis) and what can be efficiently computed (Complexity-Theoretic ChurchTuring Thesis). These variations are not due to Church or Turing, but arise from later work in complexity theory and digital physics. The thesis also has implications for the philosophy of mind.
1749) 13.4114275, Church–Turing thesis - Wikipedia, the free encyclopedia.txt#32, term: computer, content:If BQP is shown to be a strict superset of BPP, it would invalidate the Complexity-Theoretic ChurchTuring Thesis. In other words, there would be efficient quantum algorithms that perform tasks that do not have efficient probabilistic algorithms. This would not however invalidate the original ChurchTuring thesis, since a quantum computer can always be simulated by a Turing machine, but it would invalidate the classical Complexity-Theoretic ChurchTuring thesis for efficiency reasons. Consequently, the Quantum Complexity-Theoretic ChurchTuring thesis states:[49]
1750) 13.4114275, Clock rate - Wikipedia, the free encyclopedia.txt#12, term: computer, content:The clock rate alone is generally considered to be an inaccurate measure of performance when comparing different CPUs families. Software benchmarks are more useful. Clock rates can sometimes be misleading since the amount of work different CPUs can do in one cycle varies. For example, superscalar processors can execute more than one instruction per cycle (on average), yet it is not uncommon for them to do "less" in a clock cycle. In addition, subscalar CPUs or use of parallelism can also affect the performance of the computer regardless of clock rate.
1751) 13.4114275, COBOL - Wikipedia, the free encyclopedia.txt#4, term: computer, content:In the late 1950s, computer users and manufacturers were becoming concerned about the rising cost of programming. A 1959 survey had found that in any data processing installation, the programming cost US$800,000 on average and that translating programs to run on new hardware would cost $600,000. At a time when new programming languages were proliferating at an ever increasing rate, the same survey suggested that if a common business-oriented language were used, conversion would be far cheaper and faster.[14]
1752) 13.4114275, Command-line interface - Wikipedia, the free encyclopedia.txt#18, term: computer, content:Early microcomputers themselves were based on a command-line interface such as CP/M, MS-DOS or AppleSoft BASIC. Throughout the 1980s and 1990sespecially after the introduction of the Apple Macintosh and Microsoft Windowscommand line interfaces were replaced in popular usage by the Graphical User Interface. The command line remains in use, however, by system administrators and other advanced users for system administration, computer programming, and batch processing.
1753) 13.4114275, Command-line interface - Wikipedia, the free encyclopedia.txt#21, term: computer, content:A CLI is used whenever a large vocabulary of commands or queries, coupled with a wide (or arbitrary) range of options, can be entered more rapidly as text than with a pure GUI. This is typically the case with operating system command shells. CLIs are also used by systems with insufficient resources to support a graphical user interface. Some computer language systems (such as Python, Forth, LISP, Rexx, and many dialects of BASIC) provide an interactive command-line mode to allow for rapid evaluation of code.
1754) 13.4114275, Command-line interface - Wikipedia, the free encyclopedia.txt#81, term: computer, content:The earliest computers did not support interactive input/output devices, often relying on sense switches and lights to communicate with the computer operator. This was adequate for batch systems that ran one program at a time, often with the programmer acting as operator. This also had the advantage of low overhead, since lights and switches could be tested and set with one machine instruction. Later a single system console was added to allow the operator to communicate with the system.
1755) 13.4114275, Compiler - Wikipedia, the free encyclopedia.txt#1, term: computer, content:The name "compiler" is primarily used for programs that translate source code from a high-level programming language to a lower level language (e.g., assembly language or machine code). If the compiled program can run on a computer whose CPU or operating system is different from the one on which the compiler runs, the compiler is known as a cross-compiler. More generally, compilers are a specific type of translator.
1756) 13.4114275, Compiler - Wikipedia, the free encyclopedia.txt#9, term: computer, content:Compilers enabled the development of programs that are machine-independent. Before the development of FORTRAN, the first high-level language, in the 1950s,[5] machine-dependent assembly language was widely used. While assembly language produces more abstraction than machine code on the same architecture, just as with machine code, it has to be modified or rewritten if the program is to be executed on different computer hardware architecture.
1757) 13.4114275, Compiler - Wikipedia, the free encyclopedia.txt#11, term: computer, content:Compiler construction and compiler optimization are taught at universities and schools as part of a computer science curriculum.[6] Such courses are usually supplemented with the implementation of a compiler for an educational programming language. A well-documented example is Niklaus Wirth's PL/0 compiler, which Wirth used to teach compiler construction in the 1970s.[7] In spite of its simplicity, the PL/0 compiler introduced several influential concepts to the field:
1758) 13.4114275, Computability theory - Wikipedia, the free encyclopedia.txt#2, term: computer, content:Recursion theorists in mathematical logic often study the theory of relative computability, reducibility notions and degree structures described in this article. This contrasts with the theory of subrecursive hierarchies, formal methods and formal languages that is common in the study of computability theory in computer science. There is considerable overlap in knowledge and methods between these two research communities, however, and no firm line can be drawn between them.
1759) 13.4114275, Computable function - Wikipedia, the free encyclopedia.txt#9, term: computer, content:The basic characteristic of a computable function is that there must be a finite procedure (an algorithm) telling how to compute the function. The models of computation listed above give different interpretations of what a procedure is and how it is used, but these interpretations share many properties. The fact that these models give equivalent classes of computable functions stems from the fact that each model is capable of reading and mimicking a procedure for any of the other models, much as a compiler is able to read instructions in one computer language and emit instructions in another language.
1760) 13.4114275, Computable function - Wikipedia, the free encyclopedia.txt#21, term: computer, content:In computability theory in computer science, it is common to consider formal languages. An alphabet is an arbitrary set. A word on an alphabet is a finite sequence of symbols from the alphabet; the same symbol may be used more than once. For example, binary strings are exactly the words on the alphabet {0, 1} . A language is a subset of the collection of all words on a fixed alphabet. For example, the collection of all binary strings that contain exactly 3 ones is a language over the binary alphabet.
1761) 13.4114275, Computation - Wikipedia, the free encyclopedia.txt#2, term: computer, content:A computation can be seen as a purely physical phenomenon occurring inside a closed physical system called a computer. Examples of such physical systems include digital computers, mechanical computers, quantum computers, DNA computers, molecular computers, microfluidics-based computers, analog computers or wetware computers. This point of view is the one adopted by the branch of theoretical physics called the physics of computation.
1762) 13.4114275, Computation - Wikipedia, the free encyclopedia.txt#6, term: computer, content:Gualtiero Piccinini proposes an account of computation based in mechanical philosophy. It states that physical computing systems are types of mechanisms that, by design, perform physical computation, or the manipulation (by a functional mechanism) of a medium-independent vehicle according to a rule. Medium-independence allows for the use of physical variables with traits other than voltage (as in typical digital computers); this is imperative in considering other types of computation, such as that that occurs in the brain or in a quantum computer. A rule, in this sense, provides a mapping among inputs, outputs, and internal states of the physical computing system. [6]
1763) 13.4114275, Computational science - Wikipedia, the free encyclopedia.txt#14, term: computer, content:Scientific computation is most often studied through an applied mathematics or computer science program, or within a standard mathematics, sciences, or engineering program. At some institutions a specialization in scientific computation can be earned as a "minor" within another program (which may be at varying levels). However, there are increasingly many bachelor's and master's programs in computational science. Some schools also offer the Ph.D. in computational science, computational engineering, computational science and engineering, or scientific computation.
1764) 13.4114275, Computer - Simple English Wikipedia, the free encyclopedia.txt#1, term: computer, content:Modern computers are very different from early computers. They are now very powerful electronic machines that are able to do billions of calculations per second. Most people have used a personal computer in their home or at work. Computers are useful for many different jobs where automation is useful. Some examples are controlling traffic lights, vehicle computers, security systems, washing machines and digital televisions.
1765) 13.4114275, Computer - Simple English Wikipedia, the free encyclopedia.txt#24, term: computer, content:In 2005 Nokia started to call some of its mobile phones (the N-series) "multimedia computers" and after the launch of the Apple iPhone in 2007, many are now starting to add the smartphone category among "real" computers. In 2008, if the category of smartphones are included in the numbers of computers in the world, the biggest computer maker by units sold, is no longer Hewlett-Packard, but rather Nokia.[source?]
1766) 13.4114275, Computer - Wikipedia, the free encyclopedia.txt#1, term: computer, content:Conventionally, a computer consists of at least one processing element, typically a central processing unit (CPU), and some form of memory. The processing element carries out arithmetic and logic operations, and a sequencing and control unit can change the order of operations in response to stored information. Peripheral devices allow information to be retrieved from an external source, and the result of operations saved and retrieved.
1767) 13.4114275, Computer - Wikipedia, the free encyclopedia.txt#5, term: computer, content:The Antikythera mechanism is believed to be the earliest mechanical analog "computer", according to Derek J. de Solla Price.[4] It was designed to calculate astronomical positions. It was discovered in 1901 in the Antikythera wreck off the Greek island of Antikythera, between Kythera and Crete, and has been dated to circa 100 BC. Devices of a level of complexity comparable to that of the Antikythera mechanism would not reappear until a thousand years later.
1768) 13.4114275, Computer - Wikipedia, the free encyclopedia.txt#12, term: computer, content:The differential analyser, a mechanical analog computer designed to solve differential equations by integration, used wheel-and-disc mechanisms to perform the integration. In 1876 Lord Kelvin had already discussed the possible construction of such calculators, but he had been stymied by the limited output torque of the ball-and-disk integrators.[13] In a differential analyzer, the output of one integrator drove the input of the next integrator, or a graphing output. The torque amplifier was the advance that allowed these machines to work. Starting in the 1920s, Vannevar Bush and others developed mechanical differential analyzers.
1769) 13.4114275, Computer - Wikipedia, the free encyclopedia.txt#20, term: computer, content:Early digital computers were electromechanical; electric switches drove mechanical relays to perform the calculation. These devices had a low operating speed and were eventually superseded by much faster all-electric computers, originally using vacuum tubes. The Z2, created by German engineer Konrad Zuse in 1939, was one of the earliest examples of an electromechanical relay computer.[18]
1770) 13.4114275, Computer - Wikipedia, the free encyclopedia.txt#27, term: computer, content:The principle of the modern computer was proposed by Alan Turing, in his seminal 1936 paper,[34] On Computable Numbers. Turing proposed a simple device that he called "Universal Computing machine" that is later known as a Universal Turing machine. He proved that such machine is capable of computing anything that is computable by executing instructions (program) stored on tape, allowing the machine to be programmable.
1771) 13.4114275, Computer - Wikipedia, the free encyclopedia.txt#33, term: computer, content:The bipolar transistor was invented in 1947. From 1955 onwards transistors replaced vacuum tubes in computer designs, giving rise to the "second generation" of computers. Compared to vacuum tubes, transistors have many advantages: they are smaller, and require less power than vacuum tubes, so give off less heat. Silicon junction transistors were much more reliable than vacuum tubes and had longer, indefinite, service life. Transistorized computers could contain tens of thousands of binary logic circuits in a relatively compact space.
1772) 13.4114275, Computer - Wikipedia, the free encyclopedia.txt#44, term: computer, content:Comparatively, a person using a pocket calculator can perform a basic arithmetic operation such as adding two numbers with just a few button presses. But to add together all of the numbers from 1 to 1,000 would take thousands of button presses and a lot of time, with a near certainty of making a mistake. On the other hand, a computer may be programmed to do this with just a few simple instructions. The following example is written in the MIPS assembly language:
1773) 13.4114275, Computer - Wikipedia, the free encyclopedia.txt#60, term: computer, content:Since the program counter is (conceptually) just another set of memory cells, it can be changed by calculations done in the ALU. Adding 100 to the program counter would cause the next instruction to be read from a place 100 locations further down the program. Instructions that modify the program counter are often known as "jumps" and allow for loops (instructions that are repeated by the computer) and often conditional instruction execution (both examples of control flow).
1774) 13.4114275, Computer - Wikipedia, the free encyclopedia.txt#78, term: computer, content:Seemingly, multitasking would cause a computer that is switching between several programs to run more slowly, in direct proportion to the number of programs it is running, but most programs spend much of their time waiting for slow input/output devices to complete their tasks. If a program is waiting for the user to click on the mouse or press a key on the keyboard, then it will not take a "time slice" until the event it is waiting for has occurred. This frees up time for other programs to execute so that many programs may be run simultaneously without unacceptable speed loss.
1775) 13.4114275, Computer animation - Wikipedia, the free encyclopedia.txt#12, term: computer, content:There are several methods for generating the Avar values to obtain realistic motion. Traditionally, animators manipulate the Avars directly.[14] Rather than set Avars for every frame, they usually set Avars at strategic points (frames) in time and let the computer interpolate or tween between them in a process called keyframing. Keyframing puts control in the hands of the animator and has roots in hand-drawn traditional animation.
1776) 13.4114275, Computer animation - Wikipedia, the free encyclopedia.txt#25, term: computer, content:The popularity of websites that allow members to upload their own movies for others to view has created a growing community of amateur computer animators. With utilities and programs often included free with modern operating systems, many users can make their own animated movies and shorts. Several free and open source animation software applications exist as well. A popular amateur approach to animation is via the animated GIF format, which can be uploaded and seen on the web easily.
1777) 13.4114275, Computer animation - Wikipedia, the free encyclopedia.txt#27, term: computer, content:Computer animation uses different techniques to produce animations. Most frequently, sophisticated mathematics is used to manipulate complex three-dimensional polygons, apply "textures", lighting and other effects to the polygons and finally rendering the complete image. A sophisticated graphical user interface may be used to create the animation and arrange its choreography. Another technique called constructive solid geometry defines objects by conducting boolean operations on regular shapes, and has the advantage that animations may be accurately produced at any resolution.
1778) 13.4114275, Computer architecture - Wikipedia, the free encyclopedia.txt#9, term: computer, content:The purpose is to design a computer that maximizes performance while keeping power consumption in check, costs low relative to the amount of expected performance, and is also very reliable. For this, many aspects are to be considered which includes Instruction Set Design, Functional Organization, Logic Design, and Implementation. The implementation involves Integrated Circuit Design, Packaging, Power, and Cooling. Optimization of the design requires familiarity with Compilers, Operating Systems to Logic Design and Packaging.
1779) 13.4114275, Computer architecture - Wikipedia, the free encyclopedia.txt#15, term: computer, content:Computer organization helps optimize performance-based products. For example, software engineers need to know the processing power of processors. They may need to optimize software in order to gain the most performance for the lowest price. This can require quite detailed analysis of the computer's organization. For example, in a SD card, the designers might need to arrange the card so that the most data can be processed in the fastest possible way.
1780) 13.4114275, Computer architecture - Wikipedia, the free encyclopedia.txt#27, term: computer, content:Performance is affected by a very wide range of design choices  for example, pipelining a processor usually makes latency worse, but makes throughput better. Computers that control machinery usually need low interrupt latencies. These computers operate in a real-time environment and fail if an operation is not completed in a specified amount of time. For example, computer-controlled anti-lock brakes must begin braking within a predictable, short time after the brake pedal is sensed or else failure of the brake will occur.
1781) 13.4114275, Computer cluster - Wikipedia, the free encyclopedia.txt#3, term: computer, content:Computer clusters emerged as a result of convergence of a number of computing trends including the availability of low-cost microprocessors, high speed networks, and software for high-performance distributed computing.[citation needed] They have a wide range of applicability and deployment, ranging from small business clusters with a handful of nodes to some of the fastest supercomputers in the world such as IBM's Sequoia.[5]
1782) 13.4114275, Computer cluster - Wikipedia, the free encyclopedia.txt#5, term: computer, content:The computer clustering approach usually (but not always) connects a number of readily available computing nodes (e.g. personal computers used as servers) via a fast local area network.[6] The activities of the computing nodes are orchestrated by "clustering middleware", a software layer that sits atop the nodes and allows the users to treat the cluster as by and large one cohesive computing unit, e.g. via a single system image concept.[6]
1783) 13.4114275, Computer cluster - Wikipedia, the free encyclopedia.txt#8, term: computer, content:Although a cluster may consist of just a few personal computers connected by a simple network, the cluster architecture may also be used to achieve very high levels of performance. The TOP500 organization's semiannual list of the 500 fastest supercomputers often includes many clusters, e.g. the world's fastest machine in 2011 was the K computer which has a distributed memory, cluster architecture.[9][10]
1784) 13.4114275, Computer cluster - Wikipedia, the free encyclopedia.txt#9, term: computer, content:Greg Pfister has stated that clusters were not invented by any specific vendor but by customers who could not fit all their work on one computer, or needed a backup.[11] Pfister estimates the date as some time in the 1960s. The formal engineering basis of cluster computing as a means of doing parallel work of any sort was arguably invented by Gene Amdahl of IBM, who in 1967 published what has come to be regarded as the seminal paper on parallel processing: Amdahl's Law.
1785) 13.4114275, Computer cluster - Wikipedia, the free encyclopedia.txt#23, term: computer, content:Computer clusters have historically run on separate physical computers with the same operating system. With the advent of virtualization, the cluster nodes may run on separate physical computers with different operating systems which are painted above with a virtual layer to look similar.[19][citation needed][clarification needed] The cluster may also be virtualized on various configurations as maintenance takes place. An example implementation is Xen as the virtualization manager with Linux-HA.[20]
1786) 13.4114275, Computer cluster - Wikipedia, the free encyclopedia.txt#29, term: computer, content:One of the challenges in the use of a computer cluster is the cost of administrating it which can at times be as high as the cost of administrating N independent machines, if the cluster has N nodes.[24] In some cases this provides an advantage to shared memory architectures with lower administration costs.[24] This has also made to virtual machines popular, due to the ease of administration.[24]
1787) 13.4114275, Computer cluster - Wikipedia, the free encyclopedia.txt#34, term: computer, content:Load balancing clusters such as web servers use cluster architectures to support a large number of users and typically each user request is routed to a specific node, achieving task parallelism without multi-node cooperation, given that the main goal of the system is providing rapid user access to shared data. However, "computer clusters" which perform complex computations for a small number of users need to take advantage of the parallel processing capabilities of the cluster and partition "the same computation" among several nodes.[28]
1788) 13.4114275, Computer data storage - Wikipedia, the free encyclopedia.txt#1, term: computer, content:The central processing unit (CPU) of a computer is what manipulates data by performing computations. In practice, almost all computers use a storage hierarchy, which puts fast but expensive and small storage options close to the CPU and slower but larger and cheaper options farther away. Generally the fast volatile technologies (which lose data when off power) are referred to as "memory", while slower persistent technologies are referred to as "storage"; however, "memory" is sometimes also used when referring to persistent storage.
1789) 13.4114275, Computer data storage - Wikipedia, the free encyclopedia.txt#25, term: computer, content:Most computer operating systems use the concept of virtual memory, allowing utilization of more primary storage capacity than is physically available in the system. As the primary memory fills up, the system moves the least-used chunks (pages) to secondary storage devices (to a swap file or page file), retrieving them later when they are needed. As more of these retrievals from slower secondary storage are necessary, the more the overall system performance is degraded.
1790) 13.4114275, Computer graphics - Wikipedia, the free encyclopedia.txt#52, term: computer, content:Graphics are visual presentations on a surface, such as a computer screen. Examples are photographs, drawing, graphics designs, maps, engineering drawings, or other images. Graphics often combine text and illustration. Graphic design may consist of the deliberate selection, creation, or arrangement of typography alone, as in a brochure, flier, poster, web site, or book without any other element. Clarity or effective communication may be the objective, association with other cultural elements may be sought, or merely, the creation of a distinctive style.
1791) 13.4114275, Computer hardware - Wikipedia, the free encyclopedia.txt#12, term: computer, content:Data is stored by a computer using a variety of media. Hard disk drives are found in virtually all older computers, due to their high capacity and low cost, but solid-state drives are faster and more power efficient, although currently more expensive than hard drives, so are often found in more expensive computers. Some systems may use a disk array controller for greater performance or reliability.
1792) 13.4114275, Computer keyboard - Wikipedia, the free encyclopedia.txt#1, term: computer, content:A keyboard typically has characters engraved or printed on the keys and each press of a key typically corresponds to a single written symbol. However, to produce some symbols requires pressing and holding several keys simultaneously or in sequence. While most keyboard keys produce letters, numbers or signs (characters), other keys or simultaneous key presses can produce actions or execute computer commands.
1793) 13.4114275, Computer keyboard - Wikipedia, the free encyclopedia.txt#21, term: computer, content:Multifunctional keyboards provide additional function beyond the standard keyboard. Many are programmable, configurable computer keyboards and some control multiple PCs, workstations (incl. SUN) and other information sources (incl. Thomson Reuters FXT/Eikon, Bloomberg, EBS, etc.) usually in multi-screen work environments. Users have additional key functions as well as the standard functions and can typically use a single keyboard and mouse to access multiple sources.
1794) 13.4114275, Computer keyboard - Wikipedia, the free encyclopedia.txt#55, term: computer, content:Computer keyboards include control circuitry to convert key presses into key codes (usually scancodes) that the computer's electronics can understand. The key switches are connected via the printed circuit board in an electrical X-Y matrix where a voltage is provided sequentially to the Y lines and, when a key is depressed, detected sequentially by scanning the X lines.
1795) 13.4114275, Computer keyboard - Wikipedia, the free encyclopedia.txt#64, term: computer, content:Optical character recognition (OCR) is preferable to rekeying for converting existing text that is already written down but not in machine-readable format (for example, a Linotype-composed book from the 1940s). In other words, to convert the text from an image to editable text (that is, a string of character codes), a person could re-type it, or a computer could look at the image and deduce what each character is. OCR technology has already reached an impressive state (for example, Google Book Search) and promises more for the future.
1796) 13.4114275, Computer memory - Wikipedia, the free encyclopedia.txt#5, term: computer, content:The next significant advance in computer memory came with acoustic delay line memory, developed by J. Presper Eckert in the early 1940s. Through the construction of a glass tube filled with mercury and plugged at each end with a quartz crystal, delay lines could store bits of information in the form of sound waves propagating through mercury, with the quartz crystals acting as transducers to read and write bits. Delay line memory would be limited to a capacity of up to a few hundred thousand bits to remain efficient.
1797) 13.4114275, Computer memory - Wikipedia, the free encyclopedia.txt#23, term: computer, content:Without protected memory, it is possible that a bug in one program will alter the memory used by another program. This will cause that other program to run off of corrupted memory with unpredictable results. If the operating system's memory is corrupted, the entire computer system may crash and need to be rebooted. At times programs intentionally alter the memory used by other programs. This is done by viruses and malware to take over computers.
1798) 13.4114275, Computer monitor - Wikipedia, the free encyclopedia.txt#43, term: computer, content:Van Eck phreaking is the process of remotely displaying the contents of a CRT or LCD by detecting its electromagnetic emissions. It is named after Dutch computer researcher Wim van Eck, who in 1985 published the first paper on it, including proof of concept. Phreaking is the process of exploiting telephone networks, used here because of its connection to eavesdropping.[citation needed]
1799) 13.4114275, Computer mouse - Wikipedia, the free encyclopedia.txt#28, term: computer, content:Another type of mechanical mouse, the "analog mouse" (now generally regarded as obsolete), uses potentiometers rather than encoder wheels, and is typically designed to be plug compatible with an analog joystick. The "Color Mouse", originally marketed by RadioShack for their Color Computer (but also usable on MS-DOS machines equipped with analog joystick ports, provided the software accepted joystick input) was the best-known example.
1800) 13.4114275, Computer mouse - Wikipedia, the free encyclopedia.txt#67, term: computer, content:Mickeys per second is a unit of measurement for the speed and movement direction of a computer mouse.[57] But speed can also refer to the ratio between how many pixels the cursor moves on the screen and how far the mouse moves on the mouse pad, which may be expressed as pixels per Mickey, or pixels per inch, or pixels per cm. The directional movement is called the horizontal mickey count and the vertical mickey count.
1801) 13.4114275, Computer mouse - Wikipedia, the free encyclopedia.txt#87, term: computer, content:Many games, such as first- or third-person shooters, have a setting named "invert mouse" or similar (not to be confused with "button inversion", sometimes performed by left-handed users) which allows the user to look downward by moving the mouse forward and upward by moving the mouse backward (the opposite of non-inverted movement). This control system resembles that of aircraft control sticks, where pulling back causes pitch up and pushing forward causes pitch down; computer joysticks also typically emulate this control-configuration.
1802) 13.4114275, Computer multitasking - Wikipedia, the free encyclopedia.txt#0, term: computer, content:In computing, multitasking is a concept of performing multiple tasks (also known as processes) over a certain period of time by executing them concurrently. New tasks start and interrupt already started ones before they have reached completion, instead of executing the tasks sequentially so each started task needs to reach its end before a new one is started. As a result, a computer executes segments of multiple tasks in an interleaved manner, while the tasks share common processing resources such as central processing units (CPUs) and main memory.
1803) 13.4114275, Computer music - Wikipedia, the free encyclopedia.txt#16, term: computer, content:Operations such as these, and even more elaborate operations can also be performed in computer music programming languages such as Max/MSP, SuperCollider, Csound, Pure Data (Pd), Keykit, and ChucK. These programs now easily run on most personal computers, and are often capable of more complex functions than those which would have necessitated the most powerful mainframe computers several decades ago.[citation needed]
1804) 13.4114275, Computer music - Wikipedia, the free encyclopedia.txt#27, term: computer, content:The first system implementing interactive machine improvisation by means of Markov models and style modeling techniques is the Continuator, [1], developed by Franois Pachet at Sony CSL Paris in 2002[14][15] based on work on non-real time style modeling.[16][17] Matlab implementation of the Factor Oracle machine improvisation can be found as part of Computer Audition toolbox.
1805) 13.4114275, Computer network - Wikipedia, the free encyclopedia.txt#1, term: computer, content:Network computer devices that originate, route and terminate the data are called network nodes.[1] Nodes can include hosts such as personal computers, phones, servers as well as networking hardware. Two such devices can be said to be networked together when one device is able to exchange information with the other device, whether or not they have a direct connection to each other.
1806) 13.4114275, Computer network - Wikipedia, the free encyclopedia.txt#3, term: computer, content:Computer networks support an enormous number of applications such as access to the World Wide Web, video, digital audio, shared use of application and storage servers, printers, and fax machines, and use of email and instant messaging applications as well as many others. In most cases, application-specific communications protocols are layered (i.e. carried as payload) over other more general communications protocols.
1807) 13.4114275, Computer network - Wikipedia, the free encyclopedia.txt#12, term: computer, content:The transmission media (often referred to in the literature as the physical media) used to link devices to form a computer network include electrical cable (Ethernet, HomePNA, power line communication, G.hn), optical fiber (fiber-optic communication), and radio waves (wireless networking). In the OSI model, these are defined at layers 1 and 2  the physical layer and the data link layer.
1808) 13.4114275, Computer network - Wikipedia, the free encyclopedia.txt#33, term: computer, content:An overlay network is a virtual computer network that is built on top of another network. Nodes in the overlay network are connected by virtual or logical links. Each link corresponds to a path, perhaps through many physical links, in the underlying network. The topology of the overlay network may (and often does) differ from that of the underlying one. For example, many peer-to-peer networks are overlay networks. They are organized as nodes of a virtual system of links that run on top of the Internet.[11]
1809) 13.4114275, Computer network - Wikipedia, the free encyclopedia.txt#39, term: computer, content:A communications protocol is a set of rules for exchanging information over network links. In a protocol stack (also see the OSI model), each protocol leverages the services of the protocol below it. An important example of a protocol stack is HTTP (the World Wide Web protocol) running over TCP over IP (the Internet protocols) over IEEE 802.11 (the Wi-Fi protocol). This stack is used between the wireless router and the home user's personal computer when the user is surfing the web.
1810) 13.4114275, Computer network - Wikipedia, the free encyclopedia.txt#54, term: computer, content:A local area network (LAN) is a network that connects computers and devices in a limited geographical area such as a home, school, office building, or closely positioned group of buildings. Each computer or device on the network is a node. Wired LANs are most likely based on Ethernet technology. Newer standards such as ITU-T G.hn also provide a way to create a wired LAN using existing wiring, such as coaxial cables, telephone lines, and power lines.[18]
1811) 13.4114275, Computer network - Wikipedia, the free encyclopedia.txt#65, term: computer, content:A wide area network (WAN) is a computer network that covers a large geographic area such as a city, country, or spans even intercontinental distances. A WAN uses a communications channel that combines many types of media such as telephone lines, cables, and air waves. A WAN often makes use of transmission facilities provided by common carriers, such as telephone companies. WAN technologies generally function at the lower three layers of the OSI reference model: the physical layer, the data link layer, and the network layer.
1812) 13.4114275, Computer network - Wikipedia, the free encyclopedia.txt#74, term: computer, content:The Internet is the largest example of an internetwork. It is a global system of interconnected governmental, academic, corporate, public, and private computer networks. It is based on the networking technologies of the Internet Protocol Suite. It is the successor of the Advanced Research Projects Agency Network (ARPANET) developed by DARPA of the United States Department of Defense. The Internet is also the communications backbone underlying the World Wide Web (WWW).
1813) 13.4114275, Computer program - Wikipedia, the free encyclopedia.txt#5, term: computer, content:During a nine-month period in 184243, Ada Lovelace translated the memoir of Italian mathematician Luigi Menabrea. The memoir covered the Analytical Engine. The translation contained Note G which completely detailed a method for calculating Bernoulli numbers using the Analytical Engine. This note is recognized by some historians as the world's first written computer program.[6]
1814) 13.4114275, Computer program - Wikipedia, the free encyclopedia.txt#20, term: computer, content:Compilers are used to translate source code from a programming language into either object code or machine code.[19] Object code needs further processing to become machine code, and machine code consists of the central processing unit's native instructions, ready for execution. Compiled computer programs are commonly referred to as executables, binary images, or simply as binaries a reference to the binary file format used to store the executable code.
1815) 13.4114275, Computer programming - Wikipedia, the free encyclopedia.txt#1, term: computer, content:Related tasks include testing, debugging, and maintaining the source code, implementation of the build system, and management of derived artifacts such as machine code of computer programs. These might be considered part of the programming process, but often the term software development is used for this larger process with the term programming, implementation, or coding reserved for the actual writing of source code. Software engineering combines engineering techniques with software development practices.
1816) 13.4114275, Computer programming - Wikipedia, the free encyclopedia.txt#4, term: computer, content:Another ongoing debate is the extent to which the programming language used in writing computer programs affects the form that the final program takes.[citation needed] This debate is analogous to that surrounding the SapirWhorf hypothesis[4] in linguistics and cognitive science, which postulates that a particular spoken language's nature influences the habitual thought of its speakers. Different language patterns yield different patterns of thought. This idea challenges the possibility of representing the world perfectly with language, because it acknowledges that the mechanisms of any language condition the thoughts of its speaker community.
1817) 13.4114275, Computer programming - Wikipedia, the free encyclopedia.txt#7, term: computer, content:Charles Babbage adopted the use of punched cards around 1830 to control his Analytical Engine. Mathematician Ada Lovelace, a friend of Babbage, between 1842 and 1843 translated an article by Italian military engineer Luigi Menabrea on the engine,[8] which she supplemented with a set of notes, simply called Notes. These notes include an algorithm to calculate a sequence of Bernoulli numbers,[9] intended to be carried out by a machine. Despite controversy over scope of her contribution, many consider this algorithm to be the first computer program.[8]
1818) 13.4114275, Computer programming - Wikipedia, the free encyclopedia.txt#17, term: computer, content:The academic field and the engineering practice of computer programming are both largely concerned with discovering and implementing the most efficient algorithms for a given class of problem. For this purpose, algorithms are classified into orders using so-called Big O notation, which expresses resource use, such as execution time or memory consumption, in terms of the size of an input. Expert programmers are familiar with a variety of well-established algorithms and their respective complexities and use this knowledge to choose algorithms that are best suited to the circumstances.
1819) 13.4114275, Computer science - Wikipedia, the free encyclopedia.txt#7, term: computer, content:Despite its short history as a formal academic discipline, computer science has made a number of fundamental contributions to science and societyin fact, along with electronics, it is a founding science of the current epoch of human history called the Information Age and a driver of the Information Revolution, seen as the third major leap in human technological progress after the Industrial Revolution (17501850 CE) and the Agricultural Revolution (80005000 BC).
1820) 13.4114275, Computer science - Wikipedia, the free encyclopedia.txt#16, term: computer, content:Theoretical Computer Science is mathematical and abstract in spirit, but it derives its motivation from practical and everyday computation. Its aim is to understand the nature of computation and, as a consequence of this understanding, provide more efficient methodologies. All papers introducing or studying mathematical, logic and formal concepts and methods are welcome, provided that their motivation is clearly drawn from the field of computing.
1821) 13.4114275, Computer science - Wikipedia, the free encyclopedia.txt#17, term: computer, content:According to Peter Denning, the fundamental question underlying computer science is, "What can be (efficiently) automated?"[10] Theory of computation is focused on answering fundamental questions about what can be computed and what amount of resources are required to perform those computations. In an effort to answer the first question, computability theory examines which computational problems are solvable on various theoretical models of computation. The second question is addressed by computational complexity theory, which studies the time and space costs associated with different approaches to solving a multitude of computational problems.
1822) 13.4114275, Computer security - Wikipedia, the free encyclopedia.txt#5, term: computer, content:A backdoor in a computer system, a cryptosystem or an algorithm, is any secret method of bypassing normal authentication or security controls. They may exist for a number of reasons, including by original design or from poor configuration. They may also have been added later by an authorized party to allow some legitimate access, or by an attacker for malicious reasons; but regardless of the motives for their existence, they create a vulnerability.
1823) 13.4114275, Computer security - Wikipedia, the free encyclopedia.txt#7, term: computer, content:An unauthorized user gaining physical access to a computer is most likely able to directly copy data from it. They may also compromise security by making operating system modifications, installing software worms, keyloggers, covert listening devices or using wireless mice.[5] Even when the system is protected by standard security measures, these may be able to be by passed by booting another operating system or tool from a CD-ROM or other bootable media. Disk encryption and Trusted Platform Module are designed to prevent these attacks.
1824) 13.4114275, Computer security - Wikipedia, the free encyclopedia.txt#28, term: computer, content:Government and military computer systems are commonly attacked by activists[31][32][33][34] and foreign powers.[35][36][37][38] Local and regional government infrastructure such as traffic light controls, police and intelligence agency communications, personnel records, student records,[39] and financial systems are also potential targets as they are now all largely computerized. Passports and government ID cards that control access to facilities which use RFID can be vulnerable to cloning.
1825) 13.4114275, Computer security - Wikipedia, the free encyclopedia.txt#35, term: computer, content:A standard part of threat modelling for any particular system is to identify what might motivate an attack on that system, and who might be motivated to breach it. The level and detail of precautions will vary depending on the system to be secured. A home personal computer, bank, and classified military network face very different threats, even when the underlying technologies in use are similar.
1826) 13.4114275, Computer security - Wikipedia, the free encyclopedia.txt#40, term: computer, content:However, relatively few organisations maintain computer systems with effective detection systems, and fewer still have organised response mechanisms in place. As result, as Reuters points out: "Companies for the first time report they are losing more through electronic theft of data than physical stealing of assets".[55] The primary obstacle to effective eradication of cyber crime could be traced to excessive reliance on firewalls and other automated "detection" systems. Yet it is basic evidence gathering by using packet capture appliances that puts criminals behind bars.
1827) 13.4114275, Computer security - Wikipedia, the free encyclopedia.txt#58, term: computer, content:In 1988, only 60,000 computers were connected to the Internet, and most were mainframes, minicomputers and professional workstations. On November 2, 1988, many started to slow down, because they were running a malicious code that demanded processor time and that spread itself to other computers  the first internet "computer worm".[79] The software was traced back to 23-year-old Cornell University graduate student Robert Tappan Morris, Jr. who said 'he wanted to count how many machines were connected to the Internet'.[79]
1828) 13.4114275, Computer security - Wikipedia, the free encyclopedia.txt#66, term: computer, content:Conflict of laws in cyberspace has become a major cause of concern for computer security community. Some of the main challenges and complaints about the antivirus industry are the lack of global web regulations, a global base of common rules to judge, and eventually punish, cyber crimes and cyber criminals. There is no global cyber law and cybersecurity treaty that can be invoked for enforcing global cybersecurity issues.
1829) 13.4114275, Computer security - Wikipedia, the free encyclopedia.txt#68, term: computer, content:"[Computer viruses] switch from one country to another, from one jurisdiction to another  moving around the world, using the fact that we don't have the capability to globally police operations like this. So the Internet is as if someone [had] given free plane tickets to all the online criminals of the world."[94] Use of dynamic DNS, fast flux and bullet proof servers have added own complexities to this situation.
1830) 13.4114275, Computer security - Wikipedia, the free encyclopedia.txt#74, term: computer, content:The Department of Homeland Security has a dedicated division responsible for the response system, risk management program and requirements for cybersecurity in the United States called the National Cyber Security Division.[100][101] The division is home to US-CERT operations and the National Cyber Alert System.[101] The National Cybersecurity and Communications Integration Center brings together government organizations responsible for protecting computer networks and networked infrastructure.[102]
1831) 13.4114275, Computer security - Wikipedia, the free encyclopedia.txt#98, term: computer, content:In the future, wars will not just be fought by soldiers with guns or with planes that drop bombs. They will also be fought with the click of a mouse a half a world away that unleashes carefully weaponized computer programs that disrupt or destroy critical industries like utilities, transportation, communications, and energy. Such attacks could also disable military networks that control the movement of troops, the path of jet fighters, the command and control of warships.[140]
1832) 13.4114275, Computer security - Wikipedia, the free encyclopedia.txt#99, term: computer, content:This has led to new terms such as cyberwarfare and cyberterrorism. More and more critical infrastructure is being controlled via computer programs that, while increasing efficiency, exposes new vulnerabilities. The test will be to see if governments and corporations that control critical systems such as energy, communications and other information will be able to prevent attacks before they occur. As Jay Cross, the chief scientist of the Internet Time Group, remarked, "Connectedness begets vulnerability."[140]
1833) 13.4114275, Computer simulation - Wikipedia, the free encyclopedia.txt#21, term: computer, content:Vehicle manufacturers make use of computer simulation to test safety features in new designs. By building a copy of the car in a physics simulation environment, they can save the hundreds of thousands of dollars that would otherwise be required to build and test a unique prototype. Engineers can step through the simulation milliseconds at a time to determine the exact stresses being put upon each section of the prototype.[11]
1834) 13.4114275, Computer Technology Limited - Wikipedia, the free encyclopedia.txt#7, term: computer, content:The memory segmentation, together with two execution states (Normal State and non-interruptible privileged Special State) made possible the implementation of a self-protecting operating system kernel (known as the Executive, or Exec). Such ideas were popular in British computer academia at the time and later were adopted by some US designs such as the Intel 8086. Furthermore, the power system was set up as a peripheral with interrupt capabilities that gave the machine the ability to power down gracefully in an emergency.
1835) 13.4114275, Computer-aided design - Wikipedia, the free encyclopedia.txt#12, term: computer, content:CAD is also used for the accurate creation of photo simulations that are often required in the preparation of Environmental Impact Reports, in which computer-aided designs of intended buildings are superimposed into photographs of existing environments to represent what that locale will be like, where the proposed facilities are allowed to be built. Potential blockage of view corridors and shadow studies are also frequently analyzed through the use of CAD.
1836) 13.4114275, Computer-aided design - Wikipedia, the free encyclopedia.txt#21, term: computer, content:Unexpected capabilities of these associative relationships have led to a new form of prototyping called digital prototyping. In contrast to physical prototypes, which entail manufacturing time in the design. That said, CAD models can be generated by a computer after the physical prototype has been scanned using an industrial CT scanning machine. Depending on the nature of the business, digital or physical prototypes can be initially chosen according to specific needs.
1837) 13.4114275, Computer-aided design - Wikipedia, the free encyclopedia.txt#24, term: computer, content:The human-machine interface is generally via a computer mouse but can also be via a pen and digitizing graphics tablet. Manipulation of the view of the model on the screen is also sometimes done with the use of a Spacemouse/SpaceBall. Some systems also support stereoscopic glasses for viewing the 3D model.Technologies which in the past were limited to larger installations or specialist applications have become available to a wide group of users.These include the CAVE or HMD`s and interactive devices like motion-sensing technology
1838) 13.4114275, Computer-aided design - Wikipedia, the free encyclopedia.txt#25, term: computer, content:CAD software enables engineers and architects to design, inspect and manage engineering projects within an integrated graphical user interface (GUI) on a personal computer system. Most applications support solid modeling with boundary representation (B-Rep) and NURBS geometry, and enable the same to be published in a variety of formats. A geometric modeling kernel is a software component that provides solid modeling and surface modeling features to CAD applications.
1839) 13.4114275, Computer-aided design - Wikipedia, the free encyclopedia.txt#30, term: computer, content:The first commercial applications of CAD were in large companies in the automotive and aerospace industries, as well as in electronics. Only large corporations could afford the computers capable of performing the calculations. Notable company projects were, a joint project of GM (Dr. Patrick J.Hanratty) and IBM (Sam Matsa , Doug Ross`s MIT APT research assistant) to develop a prototype system for design engineers DAC-1 (Design Augmented by Computer) 1964; Lockheed projects; Bell GRAPHIC 1 and Renault.
1840) 13.4114275, Computer-aided manufacturing - Wikipedia, the free encyclopedia.txt#1, term: computer, content:Traditionally, CAM has been considered as a numerical control (NC) programming tool, where in two-dimensional (2-D) or three-dimensional (3-D) models of components generated in CADAs with other Computer-Aided technologies, CAM does not eliminate the need for skilled professionals such as manufacturing engineers, NC programmers, or machinists. CAM, in fact, leverages both the value of the most skilled manufacturing professionals through advanced productivity tools, while building the skills of new professionals through visualization, simulation and optimization tools.
1841) 13.4114275, Computer-aided manufacturing - Wikipedia, the free encyclopedia.txt#6, term: computer, content:At least in the United States, there is a shortage of young, skilled machinists entering the workforce able to perform at the extremes of manufacturing; high precision and mass production.[9][citation needed] As CAM software and machines become more complicated, the skills required of a machinist or machine operator advance to approach that of a computer programmer and engineer rather than eliminating the CNC machinist from the workforce.
1842) 13.4114275, Computing - Wikipedia, the free encyclopedia.txt#2, term: computer, content:"In a general way, we can define computing to mean any goal-oriented activity requiring, benefiting from, or creating computers. Thus, computing includes designing and building hardware and software systems for a wide range of purposes; processing, structuring, and managing various kinds of information; doing scientific studies using computers; making computer systems behave intelligently; creating and using communications and entertainment media; finding and gathering information relevant to any particular purpose, and so on. The list is virtually endless, and the possibilities are vast."
1843) 13.4114275, Computing - Wikipedia, the free encyclopedia.txt#11, term: computer, content:The earliest known tool for use in computation was the abacus, and it was thought to have been invented in Babylon circa 2400 BC. Its original style of usage was by lines drawn in sand with pebbles. Abaci, of a more modern design, are still used as calculation tools today. This was the first known computer and most advanced system of calculation known to date - preceding Greek methods by 2,000 years.
1844) 13.4114275, Computing - Wikipedia, the free encyclopedia.txt#22, term: computer, content:Communications protocols define the rules and data formats for exchanging information in a computer network, and provide the basis for network programming. Well-known communications protocols are Ethernet, a hardware and Link Layer standard that is ubiquitous in local area networks, and the Internet Protocol Suite, which defines a set of protocols for internetworking, i.e. for data communication between multiple networks, as well as host-to-host data transfer, and application-specific data transmission formats.
1845) 13.4114275, Computing - Wikipedia, the free encyclopedia.txt#24, term: computer, content:The Internet is a global system of interconnected computer networks that use the standard Internet protocol suite (TCP/IP) to serve billions of users that consists of millions of private, public, academic, business, and government networks, of local to global scope, that are linked by a broad array of electronic, wireless and optical networking technologies. The Internet carries an extensive range of information resources and services, such as the inter-linked hypertext documents of the World Wide Web (WWW) and the infrastructure to support email.
1846) 13.4114275, Control system - Wikipedia, the free encyclopedia.txt#39, term: computer, content:Since modern small microprocessors are so cheap (often less than $1 US), it's very common to implement control systems, including feedback loops, with computers, often in an embedded system. The feedback controls are simulated by having the computer make periodic measurements and then calculate from this stream of measurements (see digital signal processing, sampled data systems).
1847) 13.4114275, Control unit - Wikipedia, the free encyclopedia.txt#10, term: computer, content:The idea of microprogramming was introduced by Maurice Wilkes in 1951 as an intermediate level to execute computer program instructions. Microprograms were organized as a sequence of microinstructions and stored in special control memory. The algorithm for the microprogram control unit is usually specified by flowchart description.[4] The main advantage of the microprogram control unit is the simplicity of its structure. Outputs of the controller are organized in microinstructions and they can be easily replaced.[5]
1848) 13.4114275, Conventional PCI - Wikipedia, the free encyclopedia.txt#31, term: computer, content:The maximum width of a PCI card is 15.24mm (0.6inches). Two bracket heights have been specified, known as full-height and low-profile. The bracket or backplate is the part that fastens to the card cage to stabilize the card. It also usually contains external connectors, so it attaches in a window in the computer case so any connectors are accessible from outside. The backplate is typically fixed to the case by either a 6-32 or M3 screw, or with a separate hold-down bracket that is part of the case.
1849) 13.4114275, Conventional PCI - Wikipedia, the free encyclopedia.txt#60, term: computer, content:Finally, PCI configuration space provides access to 256 bytes of special configuration registers per PCI device. Each PCI slot gets its own configuration space address range. The registers are used to configure devices memory and I/O address ranges they should respond to from transaction initiators. When a computer is first turned on, all PCI devices respond only to their configuration space accesses. The computer's BIOS scans for devices and assigns Memory and I/O address ranges to them.
1850) 13.4114275, CPU cache - Wikipedia, the free encyclopedia.txt#0, term: computer, content:A CPU cache is a hardware cache used by the central processing unit (CPU) of a computer to reduce the average cost (time or energy) to access data from the main memory. The cache is a smaller, faster memory which stores copies of the data from frequently used main memory locations. Most CPUs have different independent caches, including instruction and data caches, where the data cache is usually organized as a hierarchy of more cache levels (L1, L2, etc.).
1851) 13.4114275, CPU cache - Wikipedia, the free encyclopedia.txt#43, term: computer, content:The graph to the right summarizes the cache performance seen on the Integer portion of the SPEC CPU2000 benchmarks, as collected by Hill and Cantin.[14] These benchmarks are intended to represent the kind of workload that an engineering workstation computer might see on any given day. The reader should keep in mind that finding benchmarks which are even usefully representative of many programs has been very difficult, and there will always be important programs with very different behavior than what is shown here.
1852) 13.4114275, CPU cache - Wikipedia, the free encyclopedia.txt#53, term: computer, content:Some early virtual memory systems were very slow because they required an access to the page table (held in main memory) before every programmed access to main memory.[NB 1] With no caches, this effectively cut the speed of memory access in half. The first hardware cache used in a computer system was not actually a data or instruction cache, but rather a TLB.[16]
1853) 13.4114275, CPU cache - Wikipedia, the free encyclopedia.txt#128, term: computer, content:There are several tools available to computer architects to help explore tradeoffs between the cache cycle time, energy, and area. These tools include the open-source CACTI cache simulator[45] and the open-source SimpleScalar instruction set simulator. Modeling of 2D and 3D SRAM, eDRAM, STT-RAM, ReRAM and PCM caches can be done using the DESTINY tool.[46]
1854) 13.4114275, Cray - Wikipedia, the free encyclopedia.txt#9, term: computer, content:Most sites with a Cray installation were considered a member of the "exclusive club" of Cray operators. Cray computers were considered quite prestigious because Crays were extremely expensive machines, and the number of units sold was small compared to ordinary mainframes. This perception extended to countries as well: to boost the perception of exclusivity, Cray Research's marketing department had promotional neckties made with a mosaic of tiny national flags illustrating the "club of Cray-operating countries".(Computer History Museum, Cray 1 30th Anniversary recorded presentation, 2006)
1855) 13.4114275, Cryptography - Wikipedia, the free encyclopedia.txt#18, term: computer, content:Cryptanalysis of the new mechanical devices proved to be both difficult and laborious. In the United Kingdom, cryptanalytic efforts at Bletchley Park during WWII spurred the development of more efficient means for carrying out repetitious tasks. This culminated in the development of the Colossus, the world's first fully electronic, digital, programmable computer, which assisted in the decryption of ciphers generated by the German Army's Lorenz SZ40/42 machine.
1856) 13.4114275, CSIRAC - Wikipedia, the free encyclopedia.txt#6, term: computer, content:In 1955, with the CSIRO's decision that computing research was outside its purview, the machine was transferred from its home at the Radiophysics Laboratory at the CSIRO in Sydney, to the University of Melbourne, where it formed Australia's only academic computing facility until late 1956. Many pioneers of computer use in Australia[who?] had their first exposure to computing there.[citation needed]
1857) 13.4114275, DARPA - Wikipedia, the free encyclopedia.txt#10, term: computer, content:This allowed ARPA to concentrate its efforts on the Project Defender (defense against ballistic missiles), Project Vela (nuclear test detection), and Project AGILE (counterinsurgency R&D) Programs, and to begin work on computer processing, behavioral sciences, and materials sciences. The DEFENDER and AGILE Programs formed the foundation of DARPA sensor, surveillance, and directed energy R&D, particularly in the study of radar, infrared sensing, and x-ray/gamma ray detection.
1858) 13.4114275, DARPA - Wikipedia, the free encyclopedia.txt#14, term: computer, content:DARPA supported the evolution of the ARPANET (the first wide-area packet switching network), Packet Radio Network, Packet Satellite Network and ultimately, the Internet and research in the artificial intelligence fields of speech recognition and signal processing, including parts of Shakey the robot.[18] DARPA also funded the development of the Douglas Engelbart's NLS computer system and The Mother of All Demos; and the Aspen Movie Map, which was probably the first hypermedia system and an important precursor of virtual reality.
1859) 13.4114275, DARPA - Wikipedia, the free encyclopedia.txt#28, term: computer, content:ARPA/DARPA is well known as a high-tech government agency, and as such has many appearances in popular fiction. Some realistic references to ARPA in fiction are in Tom Swift and the Visitor from Planet X (DARPA consults on a technical threat),[73] in episodes of television program The West Wing (the ARPA-DARPA distinction), the television program Numb3rs[74] (DARPA research into creating the first self-aware computer), and in the motion picture Executive Decision (use of a one-of-a-kind experimental prototype in an emergency).
1860) 13.4114275, Database - Wikipedia, the free encyclopedia.txt#2, term: computer, content:Formally, a "database" refers to a set of related data and the way it is organized. Access to these data is usually provided by a "database management system" (DBMS) consisting of an integrated set of computer software that allows users to interact with one or more databases and provides access to all of the data contained in the database (although restrictions may exist that limit access to particular data). The DBMS provides various functions that allow entry, storage and retrieval of large quantities of information and provides ways to manage how that information is organized.
1861) 13.4114275, Database - Wikipedia, the free encyclopedia.txt#28, term: computer, content:Linking the information back together is the key to this system. In the relational model, some bit of information was used as a "key", uniquely defining a particular record. When information was being collected about a user, information stored in the optional tables would be found by searching for this key. For instance, if the login name of a user is unique, addresses and phone numbers for that user would be recorded with the login name as its key. This simple "re-linking" of related data back into a single collection is something that traditional computer languages are not designed for.
1862) 13.4114275, Database - Wikipedia, the free encyclopedia.txt#34, term: computer, content:Another approach to hardware support for database management was ICL's CAFS accelerator, a hardware disk controller with programmable search capabilities. In the long term, these efforts were generally unsuccessful because specialized database machines could not keep pace with the rapid development and progress of general-purpose computers. Thus most database systems nowadays are software systems running on general-purpose hardware, using general-purpose computer data storage. However this idea is still pursued for certain applications by some companies like Netezza and Oracle (Exadata).
1863) 13.4114275, Debugger - Wikipedia, the free encyclopedia.txt#0, term: computer, content:A debugger or debugging tool is a computer program that is used to test and debug other programs (the "target" program). The code to be examined might alternatively be running on an instruction set simulator (ISS), a technique that allows great power in its ability to halt when specific conditions are encountered. but which will typically be somewhat slower than executing the code directly on the appropriate (or the same) processor. Some debuggers offer two modes of operationfull or partial simulationto limit this impact.
1864) 13.4114275, DEC Alpha - Wikipedia, the free encyclopedia.txt#4, term: computer, content:During development, the Palo Alto design team were working on a Unix-only workstation that originally included the PRISM. However, development of the workstation was well ahead of the PRISM, and the engineers proposed that they release the machines using the MIPS R2000 processor instead, moving its release date up considerably. DEC management doubted the need to produce a new computer architecture to replace their existing VAX and DECstation lines, and eventually ended the PRISM project in 1988.
1865) 13.4114275, DEC Alpha - Wikipedia, the free encyclopedia.txt#10, term: computer, content:A persistent report attributed to DEC insiders suggests the choice of the AXP tag for the processor was made by DEC's legal department, which was still smarting from the VAX trademark fiasco.[citation needed] After a lengthy search the tag "AXP" was found to be entirely unencumbered. Within the computer industry, a joke got started that the acronym AXP meant "Almost eXactly PRISM".
1866) 13.4114275, Desktop computer - Wikipedia, the free encyclopedia.txt#15, term: computer, content:Like laptops, some all-in-one desktop computers are characterized by an inability to customize or upgrade internal components, as the systems' cases do not provide convenient access to upgradable components, and faults in certain aspects of the hardware may require the entire computer to be replaced, regardless of the health of its remaining components.[22] There have been exceptions to this; the monitor portion of HP's Z1 workstation can be angled flat, and opened like a vehicle hood for access to internal hardware.[23]
1867) 13.4114275, Desktop computer - Wikipedia, the free encyclopedia.txt#20, term: computer, content:A desktop computer needs a UPS to handle electrical disturbances like short interruptions, blackouts and spikes; achieving an on-battery time of more than 2030 minutes for a desktop PC requires a large and expensive UPS.[24][25] A laptop with sufficiently charged battery can continue to be used for hours in case of a power outage and is not affected by short power interruptions and blackouts.
1868) 13.4114275, Difference engine - Wikipedia, the free encyclopedia.txt#8, term: computer, content:In addition to funding the construction of the output mechanism for the Science Museum's Difference Engine No. 2, Nathan Myhrvold commissioned the construction of a second complete Difference Engine No. 2, which was on exhibit at the Computer History Museum in Mountain View, California until 31 January 2016.[10][11][12] [13] It has since been transferred to Intellectual Ventures in Seattle where it is on display just outside the main lobby.
1869) 13.4114275, Differential analyser - Wikipedia, the free encyclopedia.txt#3, term: computer, content:One of the earliest practical uses of Thomson's concepts was a tide-predicting machine built by Kelvin starting in 1872-3. On Lord Kelvin's advice, Thomson's integrating machine was later incorporated into a fire-control system for naval gunnery being developed by Arthur Pollen, resulting in an electrically driven, mechanical analogue computer, which was completed by about 1912.[6] Italian mathematician Ernesto Pascal also developed integraphs for the mechanical integration of differential equations and published details in 1914.[7]
1870) 13.4114275, Digital camera - Wikipedia, the free encyclopedia.txt#65, term: computer, content:A common alternative is the use of a card reader which may be capable of reading several types of storage media, as well as high speed transfer of data to the computer. Use of a card reader also avoids draining the camera battery during the download process. An external card reader allows convenient direct access to the images on a collection of storage media. But if only one storage card is in use, moving it back and forth between the camera and the reader can be inconvenient. Many computers have a card reader built in, at least for SD cards.
1871) 13.4114275, Digital electronics - Wikipedia, the free encyclopedia.txt#79, term: computer, content:The earliest integrated circuits were a happy accident. They were constructed not to save money, but to save weight, and permit the Apollo Guidance Computer to control an inertial guidance system for a spacecraft. The first integrated circuit logic gates cost nearly $50 (in 1960 dollars, when an engineer earned $10,000/year). To everyone's surprise, by the time the circuits were mass-produced, they had become the least-expensive method of constructing digital logic. Improvements in this technology have driven all subsequent improvements in cost.
1872) 13.4114275, Digital electronics - Wikipedia, the free encyclopedia.txt#87, term: computer, content:Later, vacuum tubes were used. These were very fast, but generated heat, and were unreliable because the filaments would burn out. Fanouts were typically five to seven, limited by the heating from the tubes' current. In the 1950s, special "computer tubes" were developed with filaments that omitted volatile elements like silicon. These ran for hundreds of thousands of hours.
1873) 13.4114275, Digital Equipment Corporation - Wikipedia, the free encyclopedia.txt#13, term: computer, content:The original Laboratory Modules were soon supplemented with the "Digital Systems Module" line, which were identical internally but packaged differently. The Systems Modules were designed with all of the connections at the back of the module using 22-pin Amphenol connectors, and were attached to each other by plugging them into a backplane that could be mounted in a 19-inch rack. The backplanes allowed 25 modules in a single 5-1/4inch section of rack, and allowed the high densities needed to build a computer.[9]
1874) 13.4114275, Digital Equipment Corporation - Wikipedia, the free encyclopedia.txt#38, term: computer, content:The PDP-11 16-bit computer was designed in a crash program by Harold McFarland, Gordon Bell, Roger Cady, and others.[43] The project was able to leap forward in design with the arrival of Harold McFarland, who had been researching 16-bit designs at Carnegie Mellon University. One of his simpler designs became the PDP-11, although when they first viewed the proposal, management was not impressed and almost cancelled it.[43]
1875) 13.4114275, Digital Equipment Corporation - Wikipedia, the free encyclopedia.txt#94, term: computer, content:Digital, Intel and Xerox through their collaboration to create the DIX standard, were champions of Ethernet, but Digital is the company that made Ethernet commercially successful. Initially, Ethernet-based DECnet and LAT protocols interconnected VAXes with DECserver terminal servers. Starting with the Unibus to Ethernet adapter, multiple generations of Ethernet hardware from Digital were the de facto standard. The CI "computer interconnect" adapter was the industry's first network interface controller to use separate transmit and receive "rings".
1876) 13.4114275, Digital Equipment Corporation - Wikipedia, the free encyclopedia.txt#102, term: computer, content:Digital was one of the first businesses connected to the Internet, with dec.com, registered in 1985,[86] being one of the first of the now ubiquitous .com domains. DEC's gatekeeper.dec.com was a well-known software repository during the pre-World Wide Web days, and Digital was also the first computer vendor to open a public website, on 1 October 1993.[87] The popular AltaVista, created by Digital, was one of the first comprehensive Internet search engines. (Although Lycos was earlier, it was much more limited.)
1877) 13.4114275, Digital Equipment Corporation - Wikipedia, the free encyclopedia.txt#107, term: computer, content:Originally the users' group was called DECUS (Digital Equipment Computer User Society) during the 1960s to 1990s. When Compaq acquired Digital in 1998, the users group was renamed CUO, the Compaq Users' Organisation. When HP acquired Compaq in 2002, CUO became HP-Interex, although there are still DECUS groups in several countries. In the United States, the organization is represented by the Encompass organization; currently Connect.
1878) 13.4114275, Digital photography - Wikipedia, the free encyclopedia.txt#11, term: computer, content:Digital cameras can take pictures, and may also record sound and video. Some can be used as webcams, some can use the PictBridge standard to connect to a printer without using a computer, and some can display pictures directly on a television set. Similarly, many camcorders can take still photographs, and store them on videotape or on flash memorycards with the same functionality as digital cameras.
1879) 13.4114275, Digital photography - Wikipedia, the free encyclopedia.txt#22, term: computer, content:Because photographers rely on the integrity of image files, it is important to take proper care of memory cards. Common advocacy calls for formatting of the cards after transferring the images onto a computer. However, since all cameras only do quick formatting of cards, it is advisable to carry out a more thorough formatting using appropriate software on a PC once in a while. Effectively, this involves scanning of the cards to search for possible errors.
1880) 13.4114275, Digital signal processing - Wikipedia, the free encyclopedia.txt#21, term: computer, content:Often when the processing requirement is not real-time, processing is economically done with an existing general-purpose computer and the signal data (either input or output) exists in data files. This is essentially no different from any other data processing, except DSP mathematical techniques (such as the FFT) are used, and the sampled data is usually assumed to be uniformly sampled in time or space. For example: processing digital photographs with software such as Photoshop.
1881) 13.4114275, DirectX - Wikipedia, the free encyclopedia.txt#8, term: computer, content:The DirectX team faced the challenging task of testing each DirectX release against an array of computer hardware and software. A variety of different graphics cards, audio cards, motherboards, CPUs, input devices, games, and other multimedia applications were tested with each beta and final release. The DirectX team also built and distributed tests that allowed the hardware industry to confirm that new hardware designs and driver releases would be compatible with DirectX.
1882) 13.4114275, DNA computing - Wikipedia, the free encyclopedia.txt#0, term: computer, content:DNA computing is a branch of computing which uses DNA, biochemistry, and molecular biology hardware, instead of the traditional silicon-based computer technologies. Research and development in this area concerns theory, experiments, and applications of DNA computing. The term "molectronics" has sometimes been used, but this term had already been used for an earlier technology, a then-unsuccessful rival of the first integrated circuits;[1] this term has also been used more generally, for molecular-scale electronic technology.[2]
1883) 13.4114275, DNA computing - Wikipedia, the free encyclopedia.txt#2, term: computer, content:While the initial interest was in using this novel approach to tackle NP-hard problems, it was soon realized that they may not be best suited for this type of computation, and several proposals have been made to find a "killer application" for this approach. In 1997, computer scientist Mitsunori Ogihara working with biologist Animesh Ray suggested one to be the evaluation of Boolean circuits and described an implementation.[6][7]
1884) 13.4114275, DNA computing - Wikipedia, the free encyclopedia.txt#3, term: computer, content:In 2002, researchers from the Weizmann Institute of Science in Rehovot, Israel, unveiled a programmable molecular computing machine composed of enzymes and DNA molecules instead of silicon microchips.[8] On April 28, 2004, Ehud Shapiro, Yaakov Benenson, Binyamin Gil, Uri Ben-Dor, and Rivka Adar at the Weizmann Institute announced in the journal Nature that they had constructed a DNA computer coupled with an input and output module which would theoretically be capable of diagnosing cancerous activity within a cell, and releasing an anti-cancer drug upon diagnosis.[9]
1885) 13.4114275, E6B - Wikipedia, the free encyclopedia.txt#19, term: computer, content:Dalton's first popular computer was his 1933 Model B, the circular slide rule with True Airspeed (TAS) and Altitude corrections pilots know so well. In 1936 he put a double-drift diagram on its reverse to create what the U.S. Army Air Corps (USAAC) designated as the E-1, E-1A and E-1B.
1886) 13.4114275, E6B - Wikipedia, the free encyclopedia.txt#22, term: computer, content:The U.S. Army Air Corps decided the endless belt computer cost too much to manufacture, so later in 1937 Dalton morphed it to a simple, rigid, flat wind slide, with his old Model B circular slide rule included on the reverse. He called this prototype his Model H; the Army called it the E-6A.
1887) 13.4114275, E6B - Wikipedia, the free encyclopedia.txt#24, term: computer, content:The base name "E-6" was fairly arbitrary, as there were no standards for stock numbering at the time. For example, other USAAC computers of that time were the C-2, D-2, D-4, E-1 and G-1, and flight pants became E-1s as well. Most likely they chose "E" because Dalton's previously combined time and wind computer had been the E-1. The "B" simply meant it was the production model.
1888) 13.4114275, E6B - Wikipedia, the free encyclopedia.txt#27, term: computer, content:During World War II and into the early 1950s, The London Name Plate Mfg. Co. Ltd. produced a "Height & True Airspeed Computer Mk. IV" with the model reference "6B/345". The tool provided for calculation of the True Air Speed on the front side and Time-Speed calculations in relation to the altitude on the backside. They were still in use throughout the 1960s and 1970s in several European Air Forces, such as the German Air Force, until modern avionics made them obsolete.
1889) 13.4114275, Electrical engineering - Wikipedia, the free encyclopedia.txt#54, term: computer, content:The workplaces of engineers are just as varied as the types of work they do. Electrical engineers may be found in the pristine lab environment of a fabrication plant, the offices of a consulting firm or on site at a mine. During their working life, electrical engineers may find themselves supervising a wide range of individuals including scientists, electricians, computer programmers, and other engineers.[73]
1890) 13.4114275, Electronic delay storage automatic calculator - Wikipedia, the free encyclopedia.txt#1, term: computer, content:Later the project was supported by J. Lyons & Co. Ltd., a British firm, who were rewarded with the first commercially applied computer, LEO I, based on the EDSAC design. Work on EDSAC started at the end of 1946,[3] and it ran its first programs on 6 May 1949, when it calculated a table of squares[4] and a list of prime numbers. EDSAC 1 was finally shut down on 11 July 1958, having been superseded by EDSAC 2, which remained in use until 1965.[5]
1891) 13.4114275, Electronic delay storage automatic calculator - Wikipedia, the free encyclopedia.txt#19, term: computer, content:On 13 January 2011, the Computer Conservation Society announced that it planned to build a working replica of EDSAC, at the National Museum of Computing (TNMoC) in Bletchley Park.[14] The current target is to have the replica operational by the end of 2016 and, in common with most major exhibits at TNMoC, to run it regularly in public. The first parts of the recreation were switched on in November 2014.[15]
1892) 13.4114275, Electronic engineering - Wikipedia, the free encyclopedia.txt#1, term: computer, content:The term "electronic engineering" denotes a broad engineering field that covers subfields such as analog electronics, digital electronics, consumer electronics, embedded systems and power electronics. Electronics engineering deals with implementation of applications, principles and algorithms developed within many related fields, for example solid-state physics, radio engineering, telecommunications, control systems, signal processing, systems engineering, computer engineering, instrumentation engineering, electric power control, robotics, and many others.
1893) 13.4114275, Electronic engineering - Wikipedia, the free encyclopedia.txt#38, term: computer, content:Professional bodies of note for electrical engineers include the Institute of Electrical and Electronics Engineers (IEEE) and the Institution of Electrical Engineers (IEE) (now renamed the Institution of Engineering and Technology or IET). Member of the Institution of Engineering and Technology (MIET) is recognised in Europe as Electrical and computer (technology) engineer. The IEEE claims to produce 30 percent of the world's literature in electrical/electronic engineering, has over 370,000 members, and holds more than 450 IEEE sponsored or cosponsored conferences worldwide each year.
1894) 13.4114275, Electronic engineering - Wikipedia, the free encyclopedia.txt#40, term: computer, content:The workplaces of electronics engineers are just as varied as the types of work they do. Electronics engineers may be found in the pristine laboratory environment of a fabrication plant, the offices of a consulting firm or in a research laboratory. During their working life, electronics engineers may find themselves supervising a wide range of individuals including scientists, electricians, computer programmers and other engineers.
1895) 13.4114275, Electronics - Wikipedia, the free encyclopedia.txt#6, term: computer, content:In April 1955 the IBM 608 was the first IBM product to use transistor circuits without any vacuum tubes and is believed to be the world's first all-transistorized calculator to be manufactured for the commercial market.[2][3] The 608 contained more than 3,000 germanium transistors. Thomas J. Watson Jr. ordered all future IBM products to use transistors in their design. From that time on transistors were almost exclusively used for computer logic and peripherals.
1896) 13.4114275, Electronics - Wikipedia, the free encyclopedia.txt#16, term: computer, content:Heat generated by electronic circuitry must be dissipated to prevent immediate failure and improve long term reliability. Heat dissipation is mostly achieved by passive conduction/convection. Means to achieve greater dissipation include heat sinks and fans for air cooling, and other forms of computer cooling such as water cooling. These techniques use convection, conduction, and radiation of heat energy.
1897) 13.4114275, Email client - Wikipedia, the free encyclopedia.txt#3, term: computer, content:Emails are stored in the user's mailbox on the remote server until the user's email client requests them to be downloaded to the user's computer, or can otherwise access the user's mailbox on the possibly remote server. The email client can be set up to connect to multiple mailboxes at the same time and to request the download of emails either automatically, such as at pre-set intervals, or the request can be manually initiated by the user.
1898) 13.4114275, Embedded system - Wikipedia, the free encyclopedia.txt#0, term: computer, content:An embedded system is a computer system with a dedicated function within a larger mechanical or electrical system, often with real-time computing constraints.[1][2] It is embedded as part of a complete device often including hardware and mechanical parts. Embedded systems control many devices in common use today.[3] Ninety-eight percent of all microprocessors are manufactured as components of embedded systems.[4]
1899) 13.4114275, Embedded system - Wikipedia, the free encyclopedia.txt#6, term: computer, content:Since these early applications in the 1960s, embedded systems have come down in price and there has been a dramatic rise in processing power and functionality. An early microprocessor for example, the Intel 4004, was designed for calculators and other small systems but still required external memory and support chips. In 1978 National Engineering Manufacturers Association released a "standard" for programmable microcontrollers, including almost any computer-based controllers, such as single board computers, numerical, and event-based controllers.
1900) 13.4114275, Embedded system - Wikipedia, the free encyclopedia.txt#7, term: computer, content:As the cost of microprocessors and microcontrollers fell it became feasible to replace expensive knob-based analog components such as potentiometers and variable capacitors with up/down buttons or knobs read out by a microprocessor even in consumer products. By the early 1980s, memory, input and output system components had been integrated into the same chip as the processor forming a microcontroller. Microcontrollers find applications where a general-purpose computer would be too costly.
1901) 13.4114275, Embedded system - Wikipedia, the free encyclopedia.txt#25, term: computer, content:PC/104 and PC/104+ are examples of standards for ready made computer boards intended for small, low-volume embedded and ruggedized systems, mostly x86-based. These are often physically small compared to a standard PC, although still quite large compared to most simple (8/16-bit) embedded systems. They often use DOS, Linux, NetBSD, or an embedded real-time operating system such as MicroC/OS-II, QNX or VxWorks. Sometimes these boards use non-x86 processors.
1902) 13.4114275, ENIAC - Wikipedia, the free encyclopedia.txt#3, term: computer, content:ENIAC's design and construction was financed by the United States Army, Ordnance Corps, Research and Development Command, led by Major General Gladeon M. Barnes. The total cost was about $487,000, which equates to $6,816,000 in 2016.[10] The construction contract was signed on June 5, 1943; work on the computer began in secret at the University of Pennsylvania's Moore School of Electrical Engineering[11] the following month, under the code name "Project PX", with John Grist Brainerd as principal investigator.
1903) 13.4114275, ENIAC - Wikipedia, the free encyclopedia.txt#4, term: computer, content:ENIAC was designed by John Mauchly and J. Presper Eckert of the University of Pennsylvania, U.S.[12] The team of design engineers assisting the development included Robert F. Shaw (function tables), Jeffrey Chuan Chu (divider/square-rooter), Thomas Kite Sharpless (master programmer), Frank Mural (master programmer), Arthur Burks (multiplier), Harry Huskey (reader/printer) and Jack Davis (accumulators).[13] In 1946, the researchers resigned from the University of Pennsylvania and formed the Eckert-Mauchly Computer Corporation.
1904) 13.4114275, ENIAC - Wikipedia, the free encyclopedia.txt#22, term: computer, content:Although the Ballistic Research Laboratory was the sponsor of ENIAC, one year into this three-year project John von Neumann, a mathematician working on the hydrogen bomb at Los Alamos, became aware of this computer.[36] Los Alamos subsequently became so involved with ENIAC that the first test problem ran consisted of computations for the hydrogen bomb, not artillery tables.[6] The input/output for this test was one million cards.[37]
1905) 13.4114275, ENIAC - Wikipedia, the free encyclopedia.txt#31, term: computer, content:ENIAC was, like the Z3 and Harvard Mark I, able to run an arbitrary sequence of mathematical operations, but did not read them from a tape. Like the Colossus, it was programmed by plugboard and switches. ENIAC combined full, Turing complete programmability with electronic speed. The AtanasoffBerry Computer (ABC), ENIAC, and Colossus all used thermionic valves (vacuum tubes). ENIAC's registers performed decimal arithmetic, rather than binary arithmetic like the Z3, the ABC and Colossus.
1906) 13.4114275, Ethernet - Wikipedia, the free encyclopedia.txt#0, term: computer, content:Ethernet /irnt/ is a family of computer networking technologies commonly used in local area networks (LANs) and metropolitan area networks (MANs).[1] It was commercially introduced in 1980 and first standardized in 1983 as IEEE 802.3,[2] and has since been refined to support higher bit rates and longer link distances. Over time, Ethernet has largely replaced competing wired LAN technologies such as token ring, FDDI and ARCNET.
1907) 13.4114275, Ethernet - Wikipedia, the free encyclopedia.txt#18, term: computer, content:Since all communications happen on the same wire, any information sent by one computer is received by all, even if that information is intended for just one destination.[f] The network interface card interrupts the CPU only when applicable packets are received: The card ignores information not addressed to it.[g] Use of a single cable also means that the bandwidth is shared, such that, for example, available bandwidth to each device is halved when two stations are simultaneously active.[33]
1908) 13.4114275, Federico Faggin - Wikipedia, the free encyclopedia.txt#21, term: computer, content:Founded in 2011 the Federico and Elvia Faggin Foundation supports the scientific study of consciousness at US universities and research institutes. The purpose of the Foundation is to advance the understanding of consciousness through theoretical and experimental research. Faggins interest in consciousness has his roots in the study of artificial neural networks at Synaptics, a company he started in 1986, that prompted his inquiry into whether or not it is possible to build a conscious computer. [1]
1909) 13.4114275, Ferranti - Wikipedia, the free encyclopedia.txt#15, term: computer, content:In collaboration with the University of Manchester they built a new version of the famous Mark 1 that replaced valve diodes with solid state versions, which allowed the speed to be increased dramatically as well as increasing reliability.[21] Ferranti offered the result commercially as the Mercury starting in 1957, and eventually sold nineteen in total. Although a small part of Ferranti's empire, the computer division was nevertheless highly visible and operated out of a former steam locomotive factory in West Gorton.
1910) 13.4114275, Ferranti - Wikipedia, the free encyclopedia.txt#22, term: computer, content:In 1987 Ferranti purchased International Signal and Control (ISC), a US defence contractor based in Pennsylvania.[26] The company subsequently changed its name to Ferranti International plc. and restructured the combined business into the following divisions: Ferranti Computer Systems, Ferranti Defence Systems, Ferranti Dynamics, Ferranti Satcomms, Ferranti Telecoms, Ferranti Technologies and International Signal & Control.
1911) 13.4114275, Ferranti Mercury - Wikipedia, the free encyclopedia.txt#6, term: computer, content:The first of an eventual 19 Mercury computers was delivered in August 1957. Manchester University received one in February 1958, leasing half the time to commercial users via Ferranti's business unit. Both CERN at Geneva [2] and the Atomic Energy Research Establishment at Harwell also installed theirs in 1958. A Mercury bought in 1959 was the UK Met Office's first computer.[3] The University of Buenos Aires in Argentina received another one in 1960.[4]
1912) 13.4114275, Ferranti Pegasus - Wikipedia, the free encyclopedia.txt#6, term: computer, content:The speed of arithmetic operations was about the same as the Elliott 402 computer, which could add in 204 microseconds and multiply in 3366 microseconds. The Pegasus basic instruction cycle time for add/subtract/move and logical instructions was 128 microseconds. Multiply, divide, justify and shift instructions took a variable time to complete. Transfers to and from magnetic drum were synchronous and had to be optimised where possible. The layout of blocks on the magnetic drum was interleaved to allow some processing between transfers to/from consecutive blocks.
1913) 13.4114275, Fiber Distributed Data Interface - Wikipedia, the free encyclopedia.txt#4, term: computer, content:Designers normally constructed FDDI rings in a network topology such as a "dual ring of trees". A small number of devices, typically infrastructure devices such as routers and concentrators rather than host computers, were "dual-attached" to both rings. Host computers then connect as single-attached devices to the routers or concentrators. The dual ring in its most degenerate form simply collapses into a single device. Typically, a computer-room contained the whole dual ring, although some implementations deployed FDDI as a metropolitan area network.[4]
1914) 13.4114275, File manager - Wikipedia, the free encyclopedia.txt#0, term: computer, content:A file manager or file browser is a computer program that provides a user interface to manage files and folders. The most common operations performed on files or groups of files include creating, opening (e.g. viewing, playing, editing or printing), renaming, moving or copying, deleting and searching for files, as well as modifying file attributes, properties and file permissions. Folders and files may be displayed in a hierarchical tree based on their directory structure. Some file managers contain features inspired by web browsers, including forward and back navigational buttons.
1915) 13.4114275, Firmware - Wikipedia, the free encyclopedia.txt#0, term: computer, content:In electronic systems and computing, firmware[a] is a type of software that provides control, monitoring and data manipulation of engineered products and systems. Typical examples of devices containing firmware are embedded systems (such as traffic lights, consumer appliances, remote controls and digital watches), computers, computer peripherals, mobile phones, and digital cameras. The firmware contained in these devices provides the low-level control program for the device. As of 2013, most firmware can be updated.[2]
1916) 13.4114275, Firmware - Wikipedia, the free encyclopedia.txt#2, term: computer, content:Before integrated circuits, other firmware devices included a discrete semiconductor diode matrix. The Apollo guidance computer had firmware consisting of a specially manufactured core memory plane, called "core rope memory", where data were stored by physically threading wires through (1) or around (0) the core storing each data bit.[4]
1917) 13.4114275, First Draft of a Report on the EDVAC - Wikipedia, the free encyclopedia.txt#0, term: computer, content:The First Draft of a Report on the EDVAC (commonly shortened to First Draft) was an incomplete 101-page document written by John von Neumann and distributed on June 30, 1945 by Herman Goldstine, security officer on the classified ENIAC project. It contains the first published description of the logical design of a computer using the stored-program concept, which has controversially come to be known as the von Neumann architecture.
1918) 13.4114275, First Draft of a Report on the EDVAC - Wikipedia, the free encyclopedia.txt#9, term: computer, content:Von Neumann suggests (Sec. 5.6) keeping the computer as simple as possible, avoiding any attempt at improving performance by overlapping operations. Arithmetic operations are to be performed one binary digit at a time. He estimates addition of two binary digits as taking one microsecond and that therefore a 30-bit multiplication should take about 302 microseconds or about one millisecond, much faster than any computing device available at the time.
1919) 13.4114275, Floating point - Wikipedia, the free encyclopedia.txt#15, term: computer, content:The way in which the significand (including its sign) and exponent are stored in a computer is implementation-dependent. The common IEEE formats are described in detail later and elsewhere, but as an example, in the binary single-precision (32-bit) floating-point representation,     p = 24   {\displaystyle p=24}  , and so the significand is a string of 24 bits. For instance, the number 's first 33 bits are:
1920) 13.4114275, Floating point - Wikipedia, the free encyclopedia.txt#30, term: computer, content:On a typical computer system, a 'double precision' (64-bit) binary floating-point number has a coefficient of 53 bits (one of which is implied), an exponent of 11 bits, and one sign bit. Positive floating-point numbers in this format have an approximate range of 10308 to 10308, because the range of the exponent is [1022,1023] and 308 is approximately log10(21023). The complete range of the format is from about 10308 through +10308 (see IEEE 754).
1921) 13.4114275, Floating point - Wikipedia, the free encyclopedia.txt#36, term: computer, content:The IEEE has standardized the computer representation for binary floating-point numbers in IEEE 754 (a.k.a. IEC 60559). This standard is followed by almost all modern machines. IBM mainframes support IBM's own hexadecimal floating point format and IEEE 754-2008 decimal floating point in addition to the IEEE 754 binary format. The Cray T90 series had an IEEE version, but the SV1 still uses Cray floating-point format.
1922) 13.4114275, Floating point - Wikipedia, the free encyclopedia.txt#62, term: computer, content:When a number is represented in some format (such as a character string) which is not a native floating-point representation supported in a computer implementation, then it will require a conversion before it can be used in that implementation. If the number can be represented exactly in the floating-point format then the conversion is exact. If there is not an exact representation then the conversion requires a choice of which floating-point number to use to represent the original value. The representation chosen will have a different value to the original, and the value thus adjusted is called the rounded value.
1923) 13.4114275, Floppy disk - Wikipedia, the free encyclopedia.txt#19, term: computer, content:In May 2016 the United States Government Accountability Office released a report that covered the need to upgrade or replace legacy computer systems within Federal Agencies. According to this document, old IBM Series/1 minicomputers running on 8-inch floppy disks are still used to coordinate "the operational functions of the United States nuclear forces..." The government plans to update some of the technology by the end of the 2017 fiscal year.[13][14]
1924) 13.4114275, FLOPS - Wikipedia, the free encyclopedia.txt#12, term: computer, content:On May 25, 2008, an American supercomputer built by IBM, named 'Roadrunner', reached the computing milestone of one petaflops by processing more than 1.026 quadrillion calculations per second. It headed the June 2008 and November 2008 TOP500 list of the most powerful supercomputers (excluding grid computers).[9][10] The computer is located at Los Alamos National Laboratory in New Mexico, and the computer's name refers to the New Mexico state bird, the Greater Roadrunner.[11]
1925) 13.4114275, FLOPS - Wikipedia, the free encyclopedia.txt#19, term: computer, content:In November 2011 it was announced that Japan had achieved 10.51 petaflops with its K computer.[21] It is still under development and software performance tuning is currently underway. It has 88,128 SPARC64 VIIIfx processors in 864 racks, with theoretical performance of 11.28 petaflops. It is named after the Japanese word "kei", which stands for 10 quadrillion,[22] corresponding to the target speed of 10 petaFLOPS.
1926) 13.4114275, FLOPS - Wikipedia, the free encyclopedia.txt#27, term: computer, content:Given the current speed of progress, supercomputers are projected to reach 1exaFLOPS (EFLOPS) in 2018.[42] Cray, Inc. announced in December 2009 a plan to build a 1EFLOPS supercomputer before 2020.[43] Erik P. DeBenedictis of Sandia National Laboratories theorizes that a zettaFLOPS (ZFLOPS) computer is required to accomplish full weather modeling of two week time span.[44] Such systems might be built around 2030.[45]
1927) 13.4114275, Fortran - Wikipedia, the free encyclopedia.txt#14, term: computer, content:Before the development of disk files, text editors and terminals, programs were most often entered on a keypunch keyboard onto 80 column punched cards, one line to a card. The resulting deck of cards would be fed into a card reader to be compiled. Punched card codes included no lower case letters or many special characters, and special versions of the IBM 026 keypunch were offered that would correctly print the repurposed special characters used in Fortran. See Computer programming in the punched card era.
1928) 13.4114275, Fortran - Wikipedia, the free encyclopedia.txt#22, term: computer, content:Early FORTRAN compilers supported no recursion in subroutines. Early computer architectures supported no concept of a stack, and when they did directly support subroutine calls, the return location was often stored in one fixed location adjacent to the subroutine code, which does not permit a subroutine to be called again before a prior call of the subroutine has returned. Although not specified in Fortran 77, many F77 compilers supported recursion as an option, while it became a standard in Fortran 90.[13]
1929) 13.4114275, Fortran - Wikipedia, the free encyclopedia.txt#27, term: computer, content:Starting in 1961, as a result of customer demands, IBM began development of a FORTRAN IV that removed the machine-dependent features of FORTRAN II (such as READ INPUT TAPE), while adding new features such as a LOGICAL data type, logical Boolean expressions and the logical IF statement as an alternative to the arithmetic IF statement. FORTRAN IV was eventually released in 1962, first for the IBM 7030 ("Stretch") computer, followed by versions for the IBM 7090, IBM 7094, and later for the IBM 1401 in 1966.
1930) 13.4114275, Fortran - Wikipedia, the free encyclopedia.txt#31, term: computer, content:After the release of the FORTRAN 66 standard, compiler vendors introduced several extensions to Standard Fortran, prompting ANSI committee X3J3 in 1969 to begin work on revising the 1966 standard, under sponsorship of CBEMA, the Computer Business Equipment Manufacturers Association (formerly BEMA). Final drafts of this revised standard circulated in 1977, leading to formal approval of the new FORTRAN standard in April 1978. The new standard, called FORTRAN 77 and officially denoted X3.9-1978, added a number of significant features to address many of the shortcomings of FORTRAN 66:
1931) 13.4114275, Free software - Wikipedia, the free encyclopedia.txt#5, term: computer, content:In 1983, Richard Stallman, longtime member of the hacker community at the MIT Artificial Intelligence Laboratory, announced the GNU project, saying that he had become frustrated with the effects of the change in culture of the computer industry and its users.[16] Software development for the GNU operating system began in January 1984, and the Free Software Foundation (FSF) was founded in October 1985. An article outlining the project and its goals was published in March 1985 titled the GNU Manifesto. The manifesto included significant explanation of the GNU philosophy, Free Software Definition and "copyleft" ideas.
1932) 13.4114275, Free software - Wikipedia, the free encyclopedia.txt#10, term: computer, content:Thus, free software means that computer users have the freedom to cooperate with whom they choose, and to control the software they use. To summarize this into a remark distinguishing libre (freedom) software from gratis (zero price) software, the Free Software Foundation says: "Free software is a matter of liberty, not price. To understand the concept, you should think of 'free' as in 'free speech', not as in 'free beer'".[21] See Gratis versus libre.
1933) 13.4114275, Free software - Wikipedia, the free encyclopedia.txt#21, term: computer, content:There is debate over the security of free software in comparison to proprietary software, with a major issue being security through obscurity. A popular quantitative test in computer security is to use relative counting of known unpatched security flaws. Generally, users of this method advise avoiding products that lack fixes for known security flaws, at least until a fix is available.
1934) 13.4114275, Free Software Foundation - Wikipedia, the free encyclopedia.txt#0, term: computer, content:The Free Software Foundation (FSF) is a 501(c)(3) non-profit organization founded by Richard Stallman on 4 October 1985 to support the free software movement, which promotes the universal freedom to study, distribute, create, and modify computer software,[4] with the organization's preference for software being distributed under copyleft ("share alike") terms,[5] such as with its own GNU General Public License.[6] The FSF was incorporated in Massachusetts, USA, where it is also based.[7]
1935) 13.4114275, GNOME - Wikipedia, the free encyclopedia.txt#7, term: computer, content:GNOME addresses computer accessibility issues by using the Accessibility Toolkit (ATK) application programming interface, which allows enhancing user experience by using special input methods and speech synthesis and speech recognition software. Particular utilities are registered with ATK using Assistive Technology Service Provider Interface (AT-SPI), and become globally used throughout the desktop. Several assistive technology providers, including Orca screen reader and Dasher input method, were developed specifically for use with GNOME.
1936) 13.4114275, GNU - Wikipedia, the free encyclopedia.txt#2, term: computer, content:With the April 30, 2015 release of the Debian GNU/HURD 2015 distro,[19][20] GNU OS now provides the components to assemble an operating system that users can install and use on a computer.[21][22][23] This includes the GNU Hurd kernel, that is currently in a pre-production state. The Hurd status page states that "it may not be ready for production use, as there are still some bugs and missing features. However, it should be a good base for further development and non-critical application usage."[21]
1937) 13.4114275, GNU - Wikipedia, the free encyclopedia.txt#6, term: computer, content:The goal was to bring a wholly free software operating system into existence. Stallman wanted computer users to be "free", as most were in the 1960s and 1970s free to study the source code of the software they use, free to share the software with other people, free to modify the behavior of the software, and free to publish their modified versions of the software. This philosophy was later published as the GNU Manifesto in March 1985.[27]
1938) 13.4114275, Grace Hopper - Wikipedia, the free encyclopedia.txt#7, term: computer, content:In 1949, Hopper became an employee of the EckertMauchly Computer Corporation as a senior mathematician and joined the team developing the UNIVAC I.[17] In the early 1950s, the company was taken over by the Remington Rand corporation, and it was while she was working for them that her original compiler work was done. The compiler was known as the A compiler and its first version was A-0.[20]:11
1939) 13.4114275, Graphical Environment Manager - Wikipedia, the free encyclopedia.txt#9, term: computer, content:At this point, Apple Computer sued DRI[8] in what would turn into a long dispute over the "look and feel" of the GEM/1 system, which was an almost direct copy of the Macintosh (with some elements bearing a closer resemblance to those in the earlier Lisa, available since January 1983). This eventually led to DRI being forced to change several basic features of the system.[8] Apple would later go on to sue other companies for similar issues, including their copyright lawsuit against Microsoft and HP.[citation needed]
1940) 13.4114275, Graphical Environment Manager - Wikipedia, the free encyclopedia.txt#20, term: computer, content:Over the next seven years, from 1985 to 1992, new versions of TOS were released with each new generation of the ST line. Updates included support for more colors and higher resolutions in the raster-side of the system, but remained generally similar to the original in terms of GKS support. In 1992 Atari released TOS 4, or MultiTOS, along with their final computer system, the Falcon030. In combination with MiNT, TOS 4 allowed full multitasking support in GEM.
1941) 13.4114275, Graphical user interface - Wikipedia, the free encyclopedia.txt#15, term: computer, content:A precursor to GUIs was invented by researchers at the Stanford Research Institute, led by Douglas Engelbart. They developed the use of text-based hyperlinks manipulated with a mouse for the On-Line System (NLS). The concept of hyperlinks was further refined and extended to graphics by researchers at Xerox PARC and specifically Alan Kay, who went beyond text-based hyperlinks and used a GUI as the main interface for the Xerox Alto computer, released in 1973. Most modern general-purpose GUIs are derived from this system.
1942) 13.4114275, Graphical user interface - Wikipedia, the free encyclopedia.txt#22, term: computer, content:In 1984, Apple released a television commercial which introduced the Apple Macintosh during the telecast of Super Bowl XVIII by CBS,[21] with allusions to George Orwell's noted novel, Nineteen Eighty-Four. The commercial was aimed at making people think about computers, identifying the user-friendly interface as a personal computer which departed from prior business-oriented systems,[22] and becoming a signature representation of Apple products.[23]
1943) 13.4114275, Graphical user interface - Wikipedia, the free encyclopedia.txt#31, term: computer, content:For typical computer displays, three-dimensional is a misnomertheir displays are two-dimensional. Semantically, however, most graphical user interfaces use three dimensions. With height and width, they offer a third dimension of layering or stacking screen elements over one another. This may be represented visually on screen through an illusionary transparent effect, which offers the advantage that information in background windows may still be read, if not interacted with. Or the environment may simply hide the background information, possibly making the distinction apparent by drawing a drop shadow effect over it.
1944) 13.4114275, Graphics processing unit - Wikipedia, the free encyclopedia.txt#38, term: computer, content:An external GPU is a graphics processor located outside of the housing of the computer. External graphics processors are sometimes used with laptop computers. Laptops might have a substantial amount of RAM and a sufficiently powerful central processing unit (CPU), but often lack a powerful graphics processor (and instead have a less powerful but more energy-efficient on-board graphics chip). On-board graphics chips are often not powerful enough for playing the latest games, or for other tasks (video editing, ...).
1945) 13.4114275, Graphics tablet - Wikipedia, the free encyclopedia.txt#4, term: computer, content:The first graphics tablet resembling contemporary tablets and used for handwriting recognition by a computer was the Stylator in 1957.[2] Better known (and often misstated as the first digitizer tablet) is the RAND Tablet[3] also known as the Grafacon[4] (for Graphic Converter), introduced in 1964. The RAND Tablet employed a grid of wires under the surface of the pad that encoded horizontal and vertical coordinates in a small magnetic signal. The stylus would receive the magnetic signal, which could then be decoded back as coordinate information.
1946) 13.4114275, Graphics tablet - Wikipedia, the free encyclopedia.txt#7, term: computer, content:Summagraphics also made an OEM version of its BitPad which was sold by Apple Computer as the Apple Graphics Tablet accessory to their Apple II. These tablets used a magnetostriction technology which used wires made of a special alloy stretched over a solid substrate to accurately locate the tip of a stylus or the center of a digitizer cursor on the surface of the tablet. This technology also allowed Proximity or "Z" axis measurement.
1947) 13.4114275, Hang (computing) - Wikipedia, the free encyclopedia.txt#1, term: computer, content:Hangs have varied causes and symptoms, including software or hardware defects, such as an infinite loop or long-running uninterruptible computation, resource exhaustion (thrashing), under-performing hardware (throttling), external events such as a slow computer network, misconfiguration, and compatibility problems. The fundamental reason is typically resource exhaustion: resources necessary for some part of the system to run are not available, due to being in use by other processes or simply insufficient. Often the cause is an interaction of multiple factors, making "hang" a loose umbrella term rather than a technical one.
1948) 13.4114275, Hang (computing) - Wikipedia, the free encyclopedia.txt#10, term: computer, content:A computer may seem to hang when in fact it is simply processing very slowly. This can be caused by too many programs running at once, not enough memory (RAM), or memory fragmentation, slow hardware access (especially to remote devices), slow system APIs, etc. It can also be caused by hidden programs which were installed surreptitiously, such as spyware.
1949) 13.4114275, Hans Meuer - Wikipedia, the free encyclopedia.txt#2, term: computer, content:Hans Meuer studied mathematics, physics and politics at the universities of Marburg, Giessen and Vienna. In 1972, he received his doctorate in mathematics from the Rheinisch Westflische Technical University (RWTH) of Aachen. Since 1974, he was professor of mathematics and computer science at the University of Mannheim with specialization in software engineering. For more than 20 years, he has been involved intensively in the areas of supercomputing/parallel computing.
1950) 13.4114275, Hard disk drive - Wikipedia, the free encyclopedia.txt#5, term: computer, content:Hard disk drives were introduced in 1956 as data storage for an IBM real-time transaction processing computer and were developed for use with general-purpose mainframe and minicomputers. The first IBM drive, the 350 RAMAC, was approximately the size of two refrigerators and stored five million six-bit characters (3.75 megabytes)[10] on a stack of 50 disks.[17]
1951) 13.4114275, Hard disk drive - Wikipedia, the free encyclopedia.txt#7, term: computer, content:In 1962, IBM introduced the model 1311 disk drive, which was about the size of a washing machine and stored two million characters on a removable disk pack. Users could buy additional packs and interchange them as needed, much like reels of magnetic tape. Later models of removable pack drives, from IBM and others, became the norm in most computer installations and reached capacities of 300 megabytes by the early 1980s. Non-removable HDDs were called "fixed disk" drives.
1952) 13.4114275, Hard disk drive - Wikipedia, the free encyclopedia.txt#10, term: computer, content:Like the first removable pack drive, the first "Winchester" drives used platters 14 inches (360mm) in diameter. A few years later, designers were exploring the possibility that physically smaller platters might offer advantages. Drives with non-removable eight-inch platters appeared, and then drives that used a 514in (130mm) form factor (a mounting width equivalent to that used by contemporary floppy disk drives). The latter were primarily intended for the then-fledgling personal computer (PC) market.
1953) 13.4114275, Hard disk drive - Wikipedia, the free encyclopedia.txt#60, term: computer, content:Some desktop and laptop computer systems allow the user to make a tradeoff between seek performance and drive noise. Faster seek rates typically require more energy usage to quickly move the heads across the platter, causing louder noises from the pivot bearing and greater device vibrations as the heads are rapidly accelerated during the start of the seek motion and decelerated at the end of the seek motion. Quiet operation reduces movement speed and acceleration rates, but at a cost of reduced seek performance.
1954) 13.4114275, Hard disk drive - Wikipedia, the free encyclopedia.txt#79, term: computer, content:Features such as biometric security or multiple interfaces (for example, Firewire) are available at a higher cost.[156] There are pre-assembled external hard disk drives that, when taken out from their enclosures, cannot be used internally in a laptop or desktop computer due to embedded USB interface on their printed circuit boards, and lack of SATA (or Parallel ATA) interfaces.[157][158]
1955) 13.4114275, Harvard architecture - Wikipedia, the free encyclopedia.txt#7, term: computer, content:In recent years, the speed of the CPU has grown many times in comparison to the access speed of the main memory. Care needs to be taken to reduce the number of times main memory is accessed in order to maintain performance. If, for instance, every instruction run in the CPU requires an access to memory, the computer gains nothing for increased CPU speeda problem referred to as being memory bound.
1956) 13.4114275, Harwell CADET - Wikipedia, the free encyclopedia.txt#5, term: computer, content:By 1956, Brian Flowers, head of the theoretical physics division at AERE, was convinced that the CADET provided insufficient computing power for the needs of his numerical analysts and ordered a Ferranti Mercury computer. In 1958, Mercury number 4 became operational at AERE to accompany the CADET for another two years before the CADET was retired after four years' operation.
1957) 13.4114275, High-level programming language - Wikipedia, the free encyclopedia.txt#5, term: computer, content:However, with the growing complexity of modern microprocessor architectures, well-designed compilers for high-level languages frequently produce code comparable in efficiency to what most low-level programmers can produce by hand,[citation needed] and the higher abstraction may allow for more powerful techniques providing better overall results than their low-level counterparts in particular settings.[9] High-level languages are designed independent of structure of a specific computer. This facilitates executing a program written in such a language on different computers.
1958) 13.4114275, History of computing hardware - Wikipedia, the free encyclopedia.txt#16, term: computer, content:By the 20th century, earlier mechanical calculators, cash registers, accounting machines, and so on were redesigned to use electric motors, with gear position as the representation for the state of a variable. The word "computer" was a job title assigned to people who used these calculators to perform mathematical calculations. By the 1920s, British scientist Lewis Fry Richardson's interest in weather prediction led him to propose human computers and numerical analysis to model the weather; to this day, the most powerful computers on Earth are needed to adequately model its weather using the NavierStokes equations.[28]
1959) 13.4114275, History of computing hardware - Wikipedia, the free encyclopedia.txt#25, term: computer, content:In the first half of the 20th century, analog computers were considered by many to be the future of computing. These devices used the continuously changeable aspects of physical phenomena such as electrical, mechanical, or hydraulic quantities to model the problem being solved, in contrast to digital computers that represented varying quantities symbolically, as their numerical values change. As an analog computer does not use discrete values, but rather continuous values, processes cannot be reliably repeated with exact equivalence, as they can with Turing machines.[40]
1960) 13.4114275, History of computing hardware - Wikipedia, the free encyclopedia.txt#26, term: computer, content:The first modern analog computer was a tide-predicting machine, invented by Sir William Thomson, later Lord Kelvin, in 1872. It used a system of pulleys and wires to automatically calculate predicted tide levels for a set period at a particular location and was of great utility to navigation in shallow waters. His device was the foundation for further developments in analog computing.[41]
1961) 13.4114275, History of computing hardware - Wikipedia, the free encyclopedia.txt#27, term: computer, content:The differential analyser, a mechanical analog computer designed to solve differential equations by integration using wheel-and-disc mechanisms, was conceptualized in 1876 by James Thomson, the brother of the more famous Lord Kelvin. He explored the possible construction of such calculators, but was stymied by the limited output torque of the ball-and-disk integrators.[42] In a differential analyzer, the output of one integrator drove the input of the next integrator, or a graphing output.
1962) 13.4114275, History of computing hardware - Wikipedia, the free encyclopedia.txt#43, term: computer, content:Purely electronic circuit elements soon replaced their mechanical and electromechanical equivalents, at the same time that digital calculation replaced analog. Machines such as the Z3, the AtanasoffBerry Computer, the Colossus computers, and the ENIAC were built by hand, using circuits containing relays or valves (vacuum tubes), and often used punched cards or punched paper tape for input and as the main (non-volatile) storage medium.[61]
1963) 13.4114275, History of computing hardware - Wikipedia, the free encyclopedia.txt#45, term: computer, content:In the US, John Vincent Atanasoff and Clifford E. Berry of Iowa State University developed and tested the AtanasoffBerry Computer (ABC) in 1942,[62] the first electronic digital calculating device.[63] This design was also all-electronic, and used about 300 vacuum tubes, with capacitors fixed in a mechanically rotating drum for memory. However, its paper card writer/reader was unreliable, and work on the machine was discontinued. The machine's special-purpose nature and lack of a changeable, stored program distinguish it from modern computers.[64]
1964) 13.4114275, History of computing hardware - Wikipedia, the free encyclopedia.txt#61, term: computer, content:The machine was not intended to be a practical computer but was instead designed as a testbed for the Williams tube, the first random-access digital storage device.[78] Invented by Freddie Williams and Tom Kilburn[79][80] at the University of Manchester in 1946 and 1947, it was a cathode ray tube that used an effect called secondary emission to temporarily store electronic binary data, and was used successfully in several early computers.
1965) 13.4114275, History of computing hardware - Wikipedia, the free encyclopedia.txt#67, term: computer, content:EDSAC ran its first programs on 6May 1949, when it calculated a table of squares[87] and a list of prime numbers.The EDSAC also served as the basis for the first commercially applied computer, the LEO I, used by food manufacturing company J. Lyons & Co. Ltd. EDSAC 1 and was finally shut down on 11 July 1958, having been superseded by EDSAC 2 which stayed in use until 1965.[88]
1966) 13.4114275, History of computing hardware - Wikipedia, the free encyclopedia.txt#77, term: computer, content:A key feature of the American UNIVAC I system of 1951 was the implementation of a newly invented type of metal magnetic tape, and a high-speed tape unit, for non-volatile storage. Magnetic tape is still used in many computers.[102] In 1952, IBM publicly announced the IBM 701 Electronic Data Processing Machine, the first in its successful 700/7000 series and its first IBM mainframe computer. The IBM 704, introduced in 1954, used magnetic core memory, which became the standard for large machines.
1967) 13.4114275, History of computing hardware - Wikipedia, the free encyclopedia.txt#88, term: computer, content:During the second generation remote terminal units (often in the form of Teleprinters like a Friden Flexowriter) saw greatly increased use.[117] Telephone connections provided sufficient speed for early remote terminals and allowed hundreds of kilometers separation between remote-terminals and the computing center. Eventually these stand-alone computer networks would be generalized into an interconnected network of networksthe Internet.[118]
1968) 13.4114275, History of computing hardware - Wikipedia, the free encyclopedia.txt#90, term: computer, content:In the US, a series of computers at Control Data Corporation (CDC) were designed by Seymour Cray to use innovative designs and parallelism to achieve superior computational peak performance.[122] The CDC 6600, released in 1964, is generally considered the first supercomputer.[123][124] The CDC 6600 outperformed its predecessor, the IBM 7030 Stretch, by about a factor of three. With performance of about 1megaFLOPS, the CDC 6600 was the world's fastest computer from 1964 to 1969, when it relinquished that status to its successor, the CDC 7600.
1969) 13.4114275, History of computing hardware - Wikipedia, the free encyclopedia.txt#100, term: computer, content:A NeXT Computer and its object-oriented development tools and libraries were used by Tim Berners-Lee and Robert Cailliau at CERN to develop the world's first web server software, CERN httpd, and also used to write the first web browser, WorldWideWeb. These facts, along with the close association with Steve Jobs, secure the 68030 NeXT a place in history as one of the most significant computers of all time.[citation needed]
1970) 13.4114275, Home computer - Wikipedia, the free encyclopedia.txt#16, term: computer, content:Although the Apple II and Atari computers are functionally similar, Atari's home-oriented marketing resulted in a game-heavy library with much less business software.[26] In 1980 Wayne Green, the publisher of Kilobaud Microcomputing, recommended that companies avoid the term "home computer" in their advertising as "I feel is self-limiting for sales ... I prefer the term "microcomputers" since it doesn't limit the uses of the equipment in the imagination of the prospective customers".[27]
1971) 13.4114275, HP-UX - Wikipedia, the free encyclopedia.txt#0, term: computer, content:HP-UX (from "Hewlett Packard Unix") is Hewlett Packard Enterprise's proprietary implementation of the Unix operating system, based on UNIX System V (initially System III) and first released in 1984. Recent versions support the HP 9000 series of computer systems, based on the PA-RISC processor architecture, and HP Integrity systems, based on Intel's Itanium architecture.
1972) 13.4114275, Human–computer interaction - Wikipedia, the free encyclopedia.txt#11, term: computer, content:HCI differs from human factors and ergonomics as HCI focuses more on users working specifically with computers, rather than other kinds of machines or designed artifacts. There is also a focus in HCI on how to implement the computer software and hardware mechanisms to support humancomputer interaction. Thus, human factors is a broader term; HCI could be described as the human factors of computers  although some experts try to differentiate these areas.
1973) 13.4114275, Human–computer interaction - Wikipedia, the free encyclopedia.txt#38, term: computer, content:In recent years, there has been an explosion of social science research focusing on interactions as the unit of analysis. Much of this research draws from psychology, social psychology, and sociology. For example, one study found out that people expected a computer with a man's name to cost more than a machine with a woman's name.[20] Other research finds that individuals perceive their interactions with computers more positively than humans, despite behaving the same way towards these machines.[21]
1974) 13.4114275, Human–computer interaction - Wikipedia, the free encyclopedia.txt#43, term: computer, content:One of the main conferences for new research in human-computer interaction is the annually held Association for Computing Machinery's (ACM) Conference on Human Factors in Computing Systems, usually referred to by its short name CHI (pronounced kai, or khai). CHI is organized by ACM Special Interest Group on ComputerHuman Interaction (SIGCHI). CHI is a large conference, with thousands of attendants, and is quite broad in scope. It is attended by academics, practitioners and industry people, with company sponsors such as Google, Microsoft, and PayPal.
1975) 13.4114275, Hybrid computer - Wikipedia, the free encyclopedia.txt#0, term: computer, content:Hybrid computers are computers that exhibit features of analog computers and digital computers. The digital component normally serves as the controller and provides logical and numerical operations, while the analog component often serves as a solver of differential equations and other mathematically complex equations. The very first hybrid computer, HRS-100 was a prominent example in the 1970s. Late in the 20th century, hybrids dwindled with the increasing capabilities of digital computers including digital signal processors.[1]
1976) 13.4114275, Hypertext Transfer Protocol - Wikipedia, the free encyclopedia.txt#4, term: computer, content:HTTP functions as a requestresponse protocol in the clientserver computing model. A web browser, for example, may be the client and an application running on a computer hosting a web site may be the server. The client submits an HTTP request message to the server. The server, which provides resources such as HTML files and other content, or performs other functions on behalf of the client, returns a response message to the client. The response contains completion status information about the request and may also contain requested content in its message body.
1977) 13.4114275, IBM 650 - Wikipedia, the free encyclopedia.txt#0, term: computer, content:The IBM 650 Magnetic Drum Data-Processing Machine is one of IBM's early computers, and the worlds first mass-produced computer.[1][2] It was announced in 1953 and in 1956 enhanced as the IBM 650 RAMAC with the addition of up to four disk storage units.[3] Almost 2,000 systems were produced, the last in 1962.[4] Support for the 650 and its component units was withdrawn in 1969.
1978) 13.4114275, IBM 650 - Wikipedia, the free encyclopedia.txt#2, term: computer, content:The IBM 7070 (signed 10-digit decimal words), announced 1958, was expected to be a "common successor to at least the 650 and the [IBM] 705".[7] The IBM 1620 (variable length decimal), introduced in 1959, addressed the lower end of the market. The UNIVAC Solid State (a two-address computer, signed 10-digit decimal words) was announced by Sperry Rand in December 1958 as a response to the 650. None of these had a 650 compatible instruction set.
1979) 13.4114275, IBM 701 - Wikipedia, the free encyclopedia.txt#11, term: computer, content:"I think there is a world market for maybe five computers" is often attributed to Thomas Watson; Senior in 1943 and Junior at several dates in the 1950s. This misquote is from the 1953 IBM annual stockholders' meeting. Thomas Watson, Jr. was describing the market acceptance of the IBM 701 computer. Before production began, Watson visited with 20 companies that were potential customers. This is what he said at the stockholders' meeting, "as a result of our trip, on which we expected to get orders for five machines, we came home with orders for 18.[7]
1980) 13.4114275, IBM AIX - Wikipedia, the free encyclopedia.txt#0, term: computer, content:AIX (Advanced Interactive eXecutive, pronounced /eaks/[3]) is a series of proprietary Unix operating systems developed and sold by IBM for several of its computer platforms. Originally released for the IBM 6150 RISC workstation, AIX now supports or has supported a wide variety of hardware platforms, including the IBM RS/6000 series and later POWER and PowerPC-based systems, IBM System i, System/370 mainframes, PS/2 personal computers, and the Apple Network Server.
1981) 13.4114275, IBM AIX - Wikipedia, the free encyclopedia.txt#21, term: computer, content:The Apple Network Server systems were PowerPC-based systems designed by Apple Computer to have numerous high-end features that standard Apple hardware did not have, including swappable hard drives, redundant power supplies, and external monitoring capability. These systems were more or less based on the Power Macintosh hardware available at the time but were designed to use AIX (versions 4.1.4 or 4.1.5) as their native operating system in a specialized version specific to the ANS.
1982) 13.4114275, IBM PC compatible - Wikipedia, the free encyclopedia.txt#14, term: computer, content:Creative Computing in 1985 stated, "we reiterate our standard line regarding the IBM PC compatibles: try the package you want to use before you buy the computer."[16] Companies modified their computers' BIOS to work with newly discovered incompatible applications,[7] and reviewers and users developed stress tests to measure compatibility; by 1984 the ability to operate Lotus 1-2-3 and Flight Simulator became the standard.[5][17][7][18][16][19][9]
1983) 13.4114275, IBM PC compatible - Wikipedia, the free encyclopedia.txt#22, term: computer, content:No mass-market personal computer hardware vendor dared to be incompatible with the latest version of Windows, and Microsoft's annual WinHEC conferences provided a setting in which Microsoft could lobby forand in some cases dictatethe pace and direction of the hardware of the PC industry. Microsoft and Intel had become so important to the ongoing development of PC hardware that industry writers began using the portmanteau word Wintel to refer to the combined hardware-software system.
1984) 13.4114275, IBM SSEC - Wikipedia, the free encyclopedia.txt#3, term: computer, content:The February 1946 announcement of the fully electronic ENIAC energized the project.[8] The new machine, called the IBM Selective Sequence Electronic Calculator (SSEC), was ready to be installed by August 1947. Watson called such machines calculators because computer then referred to humans employed to perform calculations and he wanted to convey the message that IBM's machines were not designed to replace people. Rather they were designed to help people, by relieving them of drudgery..[6]:143
1985) 13.4114275, IBM SSEC - Wikipedia, the free encyclopedia.txt#14, term: computer, content:By 1951 the Ferranti Mark I was marketed in the UK as a commercial computer using Williams tube technology, followed by the UNIVAC I using delay line memory in the US. These memory technologies allowed stored-program features to be more practical. The stored-program concept had been first widely published in 1945 in the First Draft of a Report on the EDVAC and became known as the Von Neumann architecture. The EDVAC was the ENIAC successor (first working in 1949), designed by the team who then marketed the UNIVAC.
1986) 13.4114275, IBM SSEC - Wikipedia, the free encyclopedia.txt#15, term: computer, content:The SSEC ran until August 1952, when it was dismantled, having been made obsolete by fully electronic computers. An IBM 701 computer, known as the Defense Calculator, was installed in the same room for its April 7, 1953, public debut.[30] In July 1953 the much less expensive (and even better selling) IBM 650 was announced, which had been developed by the same Endicott team who developed the SSEC.[31]
1987) 13.4114275, Image scanner - Wikipedia, the free encyclopedia.txt#15, term: computer, content:Scanners are available that pull a flat sheet over the scanning element between rotating rollers. They can only handle single sheets up to a specified width (typically about 210mm, the width of many printed letters and documents), but can be very compact, just requiring a pair of narrow rollers between which the document is passed. Some are portable, powered by batteries and with their own storage, eventually transferring stored scans to a computer over a USB or other interface.
1988) 13.4114275, Information system - Wikipedia, the free encyclopedia.txt#4, term: computer, content:Some authors make a clear distinction between information systems, computer systems, and business processes. Information systems typically include an ICT component but are not purely concerned with ICT, focusing instead on the end use of information technology. Information systems are also different from business processes. Information systems help to control the performance of business processes.[9]
1989) 13.4114275, Information system - Wikipedia, the free encyclopedia.txt#21, term: computer, content:Several IS scholars have debated the nature and foundations of Information Systems which has its roots in other reference disciplines such as Computer Science, Engineering, Mathematics, Management Science, Cybernetics, and others.[44][45][46][47] Information systems also can be defined as a collection of hardware, software, data, people and procedures that work together to produce quality information.
1990) 13.4114275, Information system - Wikipedia, the free encyclopedia.txt#22, term: computer, content:Similar to computer science, other disciplines can be seen as both related and foundation disciplines of IS. The domain of study of IS involves the study of theories and practices related to the social and technological phenomena, which determine the development, use, and effects of information systems in organization and society.[48] But, while there may be considerable overlap of the disciplines at the boundaries, the disciplines are still differentiated by the focus, purpose, and orientation of their activities.[49]
1991) 13.4114275, Information system - Wikipedia, the free encyclopedia.txt#24, term: computer, content:One problem with that approach is that it prevents the IS field from being interested in non-organizational use of ICT, such as in social networking, computer gaming, mobile personal usage, etc. A different way of differentiating the IS field from its neighbours is to ask, "Which aspects of reality are most meaningful in the IS field and other fields?"[50] This approach, based on philosophy, helps to define not just the focus, purpose and orientation, but also the dignity, destiny and responsibility of the field among other fields. International Journal of Information Management, 30, 13-20.
1992) 13.4114275, Information technology - Wikipedia, the free encyclopedia.txt#4, term: computer, content:Devices have been used to aid computation for thousands of years, probably initially in the form of a tally stick.[8] The Antikythera mechanism, dating from about the beginning of the first century BC, is generally considered to be the earliest known mechanical analog computer, and the earliest known geared mechanism.[9] Comparable geared devices did not emerge in Europe until the 16th century,[10] and it was not until 1645 that the first mechanical calculator capable of performing the four basic arithmetical operations was developed.[11]
1993) 13.4114275, Information technology - Wikipedia, the free encyclopedia.txt#19, term: computer, content:In a business context, the Information Technology Association of America has defined information technology as "the study, design, development, application, implementation, support or management of computer-based information systems".[40] The responsibilities of those working in the field include network administration, software development and installation, and the planning and management of an organization's technology life cycle, by which hardware and software are maintained, upgraded and replaced.
1994) 13.4114275, Installation (computer programs) - Wikipedia, the free encyclopedia.txt#1, term: computer, content:Installation typically involves code being copied/generated from the installation files to new files on the local computer for easier access by the operating system. Because code is generally copied/generated in multiple locations, uninstallation usually involves more than just erasing the program folder. For example, registry files and other system code may need to be modified or deleted for a complete uninstallation.
1995) 13.4114275, Installation (computer programs) - Wikipedia, the free encyclopedia.txt#13, term: computer, content:A clean installation is one that is done in the absence of any interfering elements such as old versions of the computer program being installed or leftovers from a previous installation. In particular, the clean installation of an operating system is an installation in which the target disk partition is erased before installation. Since the interfering elements are absent, a clean installation may succeed where an unclean installation may fail or may take significantly longer.
1996) 13.4114275, Installation (computer programs) - Wikipedia, the free encyclopedia.txt#17, term: computer, content:During the installation of computer programs it is sometimes necessary to update the installer or package manager itself. To make this possible, a technique called bootstrapping is used. The common pattern for this is to use small executable files which update the installer and starts the real installation after the update. This small executable is called bootstrapper. Sometimes the bootstrapper installs other prerequisites for the software during the bootstrapping process too.
1997) 13.4114275, Instant messaging - Wikipedia, the free encyclopedia.txt#32, term: computer, content:Viruses, computer worms, and trojans usually propagate by sending themselves rapidly through the infected user's contact list. An effective attack using a poisoned URL may reach tens of thousands of users in a short period when each user's contact list receives messages appearing to be from a trusted friend. The recipients click on the web address, and the entire cycle starts again. Infections may range from nuisance to criminal, and are becoming more sophisticated each year.
1998) 13.4114275, Institute of Electrical and Electronics Engineers - Wikipedia, the free encyclopedia.txt#0, term: computer, content:The Institute of Electrical and Electronics Engineers (IEEE, pronounced "I triple E")[2] is a professional association with its corporate office in New York City and its operations center in Piscataway, New Jersey. It was formed in 1963 from the amalgamation of the American Institute of Electrical Engineers and the Institute of Radio Engineers. Today, it is the world's largest association of technical professionals with more than 400,000 members in chapters around the world. Its objectives are the educational and technical advancement of electrical and electronic engineering, telecommunications, computer engineering and allied disciplines.
1999) 13.4114275, Institute of Electrical and Electronics Engineers - Wikipedia, the free encyclopedia.txt#1, term: computer, content:IEEE stands for the "Institute of Electrical and Electronics Engineers". The association is chartered under this full legal name. IEEE's membership has long been composed of engineers and scientists. Allied professionals who are members include computer scientists, software developers, information technology professionals, physicists, and medical doctors, in addition to IEEE's electrical and electronics engineering core. For this reason the organization no longer goes by the full name, except on legal business documents, and is referred to simply as IEEE.
2000) 13.4114275, Institute of Electrical and Electronics Engineers - Wikipedia, the free encyclopedia.txt#7, term: computer, content:IEEE has a dual complementary regional and technical structure  with organizational units based on geography (e.g., the IEEE Philadelphia Section, the IEEE Buenaventura Section, IEEE South Africa Section [1]) and technical focus (e.g., the IEEE Computer Society). It manages a separate organizational unit (IEEE-USA) which recommends policies and implements programs specifically intended to benefit the members, the profession and the public in the United States.
2001) 13.4114275, Instruction set - Wikipedia, the free encyclopedia.txt#0, term: computer, content:An instruction set, or instruction set architecture (ISA), is the part of the computer architecture related to programming, including the native data types, instructions, registers, addressing modes, memory architecture, interrupt and exception handling, and external I/O. An ISA includes a specification of the set of opcodes (machine language), and the native commands implemented by a particular processor.[citation needed]
2002) 13.4114275, Instruction set - Wikipedia, the free encyclopedia.txt#3, term: computer, content:Prior to NPL [System/360], the company's computer designers had been free to honor cost objectives not only by selecting technologies but also by fashioning functional and architectural refinements. The SPREAD compatibility objective, in contrast, postulated a single architecture for a series of five processors spanning a wide range of cost and performance. None of the five engineering design teams could count on being able to bring about adjustments in architectural specifications as a way of easing difficulties in achieving cost and performance objectives.[1]:p.137
2003) 13.4114275, Integral - Wikipedia, the free encyclopedia.txt#90, term: computer, content:Many problems in mathematics, physics, and engineering involve integration where an explicit formula for the integral is desired. Extensive tables of integrals have been compiled and published over the years for this purpose. With the spread of computers, many professionals, educators, and students have turned to computer algebra systems that are specifically designed to perform difficult or tedious tasks, including integration. Symbolic integration has been one of the motivations for the development of the first such systems, like Macsyma.
2004) 13.4114275, Integrated circuit - Wikipedia, the free encyclopedia.txt#12, term: computer, content:Fairchild Semiconductor was also home of the first silicon-gate IC technology with self-aligned gates, the basis of all modern CMOS computer chips. The technology was developed by Italian physicist Federico Faggin in 1968, who later joined Intel in order to develop the very first single-chip Central Processing Unit (CPU) (Intel 4004), for which he received the National Medal of Technology and Innovation in 2010.
2005) 13.4114275, Integrated circuit - Wikipedia, the free encyclopedia.txt#21, term: computer, content:SSI and MSI devices often were manufactured by masks created by hand-cutting Rubylith. An engineer would inspect and verify the completeness of each mask. LSI devices contain so many transistors, interconnecting wires, and other features that it is considered impossible for a human to check the masks or even do the original design entirely by hand. The engineer depends on computer programs and other hardware aids to do most of this work.[25]
2006) 13.4114275, Integrated circuit - Wikipedia, the free encyclopedia.txt#28, term: computer, content:A system-on-a-chip (SoC or SOC) is an integrated circuit in which all the components needed for a computer or other system are included on a single chip. The design of such a device can be complex and costly, and building disparate components on a single piece of silicon may compromise the efficiency of some elements. However, these drawbacks are offset by lower manufacturing and assembly costs and by a greatly reduced power budget: because signals among the components are kept on-die, much less power is required (see Packaging).[30]
2007) 13.4114275, Intel - Wikipedia, the free encyclopedia.txt#23, term: computer, content:In 2010, Intel purchased McAfee, a manufacturer of computer security technology for $7.68billion.[35] As a condition for regulatory approval of the transaction, Intel agreed to provide rival security firms with all necessary information that would allow their products to use Intel's chips and personal computers.[36] After the acquisition, Intel had about 90,000 employees, including about 12,000 software engineers.[37]
2008) 13.4114275, Intel - Wikipedia, the free encyclopedia.txt#43, term: computer, content:In 1983, at the dawn of the personal computer era, Intel's profits came under increased pressure from Japanese memory-chip manufacturers, and then-president Andy Grove focused the company on microprocessors. Grove described this transition in the book Only the Paranoid Survive. A key element of his plan was the notion, then considered radical, of becoming the single source for successors to the popular 8086 microprocessor.
2009) 13.4114275, Intel - Wikipedia, the free encyclopedia.txt#46, term: computer, content:IBM introduced its personal computer in 1981, and it was rapidly successful. In 1982, Intel created the 80286 microprocessor, which, two years later, was used in the IBM PC/AT. Compaq, the first IBM PC "clone" manufacturer, produced a desktop system based on the faster 80286 processor in 1985 and in 1986 quickly followed with the first 80386-based system, beating IBM and establishing a competitive market for PC-compatible systems and setting up Intel as a key component supplier.
2010) 13.4114275, Intel - Wikipedia, the free encyclopedia.txt#54, term: computer, content:The "Pentium flaw" incident, Intel's response to it, and the surrounding media coverage propelled Intel from being a technology supplier generally unknown to most computer users to a household name. Dovetailing with an uptick in the "Intel Inside" campaign, the episode is considered to have been a positive event for Intel, changing some of its business practices to be more end-user focused and generating substantial public awareness, while avoiding a lasting negative impression.[84]
2011) 13.4114275, Intel - Wikipedia, the free encyclopedia.txt#56, term: computer, content:The second program is little-known: Intel's Systems Group began, in the early 1990s, manufacturing PC "motherboards", the main board component of a personal computer, and the one into which the processor (CPU) and memory (RAM) chips are plugged.[86] Shortly after, Intel began manufacturing fully configured "white box" systems for the dozens of PC clone companies that rapidly sprang up.[citation needed] At its peak in the mid-1990s, Intel manufactured over 15% of all PCs, making it the third-largest supplier at the time.[citation needed]
2012) 13.4114275, Intel - Wikipedia, the free encyclopedia.txt#57, term: computer, content:During the 1990s, Intel Architecture Labs (IAL) was responsible for many of the hardware innovations of the personal computer, including the PCI Bus, the PCI Express (PCIe) bus, the Universal Serial Bus (USB). IAL's software efforts met with a more mixed fate; its video and graphics software was important in the development of software digital video,[citation needed] but later its efforts were largely overshadowed by competition from Microsoft. The competition between Intel and Microsoft was revealed in testimony by IAL Vice-President Steven McGeady at the Microsoft antitrust trial.
2013) 13.4114275, Intel - Wikipedia, the free encyclopedia.txt#67, term: computer, content:A case of industrial espionage arose in 1995 that involved both Intel and AMD. Bill Gaede, an Argentine formerly employed both at AMD and at Intel's Arizona plant, was arrested for attempting in 1993 to sell the i486 and P5 Pentium designs to AMD and to certain foreign powers.[154] Gaede videotaped data from his computer screen at Intel and mailed it to AMD, which immediately alerted Intel and authorities, resulting in Gaede's arrest. Gaede was convicted and sentenced to 33 months in prison in June 1996.[155][156]
2014) 13.4114275, Intel - Wikipedia, the free encyclopedia.txt#119, term: computer, content:In October 2006, a Transmeta lawsuit was filed against Intel for patent infringement on computer architecture and power efficiency technologies.[238] The lawsuit was settled in October 2007, with Intel agreeing to pay US$150million initially and US$20million per year for the next five years. Both companies agreed to drop lawsuits against each other, while Intel was granted a perpetual non-exclusive license to use current and future patented Transmeta technologies in its chips for 10 years.[239]
2015) 13.4114275, Intel 4004 - Wikipedia, the free encyclopedia.txt#8, term: computer, content:The 4004 is part of the MCS-4 family of LSI chips that can be used to build digital computers with varying amounts of memory. The other members of the MCS-4 family are memories and input/output circuits, which are necessary to implement a complete computer. The 4001 is a ROM (read-only memory) with four lines of output; the 4002 is a RAM (random access memory) with four lines of input/output. The 4003 is a static shift register to be used for expanding the I/O lines; e.g., for keyboard scanning or controlling a printer.
2016) 13.4114275, Intel 4004 - Wikipedia, the free encyclopedia.txt#15, term: computer, content:A popular myth has it that Pioneer 10, the first spacecraft to leave the solar system, used an Intel 4004 microprocessor. According to Dr. Larry Lasher of Ames Research Center, the Pioneer team did evaluate the 4004, but decided it was too new at the time to include in any of the Pioneer projects.[citation needed] The myth was repeated by Federico Faggin himself in a lecture for the Computer History Museum in 2006.[19]
2017) 13.4114275, Intel 80386 - Wikipedia, the free encyclopedia.txt#3, term: computer, content:In May 2006, Intel announced that 80386 production would stop at the end of September 2007.[9] Although it had long been obsolete as a personal computer CPU, Intel and others had continued making the chip for embedded systems. Such systems using an 80386 or one of many derivatives are common in aerospace technology and electronic musical instruments, among others. Some mobile phones also used (later fully static CMOS variants of) the 80386 processor, such as BlackBerry 950[10] and Nokia 9000 Communicator.
2018) 13.4114275, Intel 80486 - Wikipedia, the free encyclopedia.txt#23, term: computer, content:The introduction of 3D computer graphics spelled the end of the 486's reign, because 3D graphics make heavy use of floating point calculations and require a faster CPU cache and more memory bandwidth. Developers began to target the P5 Pentium processor family almost exclusively with x86 assembly language optimizations (e.g., Quake) which led to the usage of terms like "Pentium compatible processor" for software requirements. Many of these games required the speed of the P5 Pentium processor family's double-pipelined architecture.
2019) 13.4114275, Intel 8080 - Wikipedia, the free encyclopedia.txt#23, term: computer, content:In the Soviet Union, manufacturers cloned the 8080 microprocessor's layout geometry, even using an identical pin arrangement, and started to produce the clone under the name KP580K80 (later marked as KP580BM80). This processor was the base of the Radio86RK ( 86 in Russian), probably the most popular amateur single-board computer in the Soviet Union. Radio86RK's predecessor was the Micro-80 (-80 in Russian), and its successor the Orion-128 (-128 in Russian) which had a graphical display. Both were built on the KP580 processor.
2020) 13.4114275, Intel 8080 - Wikipedia, the free encyclopedia.txt#27, term: computer, content:PCs based upon the 8086 design and its successors evolved into workstations and servers of 16, 32 and 64 bits, with advanced memory protection, segmentation, and multiprocessing features, blurring the difference between small and large computers (the 80286 and 80386's protected mode were important in doing so). The size of chips has grown so that the size and power of large x86 chips is not much different from high end architecture chips, and a common strategy to produce a very large computer is to network many x86 processors.
2021) 13.4114275, Intel 8088 - Wikipedia, the free encyclopedia.txt#12, term: computer, content:IBM chose the 8088 over the 8086 because Intel offered a better price for the former, and could supply more units.[15] Another factor was that the 8088 allowed the computer to be based on a modified 8085 design, as it could easily interface with most nMOS chips with 8-bit databuses, i.e. existing and mature, and therefore economical, components. This included ICs originally intended for support and peripheral functions around the 8085 and similar processors (not exclusively Intel's) which were already well known by many engineers, further reducing cost.[16]
2022) 13.4114275, Interactive fiction - Wikipedia, the free encyclopedia.txt#21, term: computer, content:Adventure International was founded by Scott Adams (not to be confused with the creator of Dilbert). In 1978, Adams wrote Adventureland, which was loosely patterned after (the original) Colossal Cave Adventure. He took out a small ad in a computer magazine in order to promote and sell Adventureland, thus creating the first commercial adventure game. In 1979 he founded Adventure International, the first commercial publisher of interactive fiction.
2023) 13.4114275, Interactive fiction - Wikipedia, the free encyclopedia.txt#52, term: computer, content:Interpreters are the software used to play the works of interactive fiction created with a development system. Since they need to interact with the player, the "story files" created by development systems are programs in their own right. Rather than running directly on any one computer, they are programs run by Interpreters, or virtual machines, which are designed specially for IF. They may be part of the development system, or can be compiled together with the work of fiction as a standalone executable file.
2024) 13.4114275, Interactive fiction - Wikipedia, the free encyclopedia.txt#61, term: computer, content:With the development of web based interpreters also title to run in a web browser the risk has been reduced. The downside of this is that it is not possible to save or skip chapters whilst using these. This is essential for most people, for books you can use bookmarks and for computer games you have levels, which levels can be selected to play, and the gameplay progresses after the level is completed.
2025) 13.4114275, International Organization for Standardization - Wikipedia, the free encyclopedia.txt#30, term: computer, content:Computer security entrepreneur and Ubuntu investor, Mark Shuttleworth, commented on the Standardization of Office Open XML process by saying "I think it de-values the confidence people have in the standards setting process," and Shuttleworth alleged that ISO did not carry out its responsibility. He also noted that Microsoft had intensely lobbied many countries that traditionally had not participated in ISO and stacked technical committees with Microsoft employees, solution providers and resellers sympathetic to Office Open XML.
2026) 13.4114275, Internet - Wikipedia, the free encyclopedia.txt#0, term: computer, content:The Internet is the global system of interconnected computer networks that use the Internet protocol suite (TCP/IP) to link billions of devices worldwide. It is a network of networks that consists of millions of private, public, academic, business, and government networks of local to global scope, linked by a broad array of electronic, wireless, and optical networking technologies. The Internet carries an extensive range of information resources and services, such as the inter-linked hypertext documents and applications of the World Wide Web (WWW), electronic mail, telephony, and peer-to-peer networks for file sharing.
2027) 13.4114275, Internet - Wikipedia, the free encyclopedia.txt#12, term: computer, content:Early international collaborations on the ARPANET were rare. European developers were concerned with developing the X.25 networks.[25] Notable exceptions were the Norwegian Seismic Array (NORSAR) in June 1973, followed in 1973 by Sweden with satellite links to the Tanum Earth Station and Peter T. Kirstein's research group in the United Kingdom, initially at the Institute of Computer Science, University of London and later at University College London.[26][27][28]
2028) 13.4114275, Internet - Wikipedia, the free encyclopedia.txt#13, term: computer, content:In December 1974, RFC 675 (Specification of Internet Transmission Control Program), by Vinton Cerf, Yogen Dalal, and Carl Sunshine, used the term internet as a shorthand for internetworking and later RFCs repeated this use.[29] Access to the ARPANET was expanded in 1981 when the National Science Foundation (NSF) funded the Computer Science Network (CSNET). In 1982, the Internet Protocol Suite (TCP/IP) was standardized, which permitted worldwide proliferation of interconnected networks.
2029) 13.4114275, Internet - Wikipedia, the free encyclopedia.txt#65, term: computer, content:Content management systems allow collaborating teams to work on shared sets of documents simultaneously without accidentally destroying each other's work. Business and project teams can share calendars as well as documents and other information. Such collaboration occurs in a wide variety of areas including scientific research, software development, conference planning, political activism and creative writing. Social and political collaboration is also becoming more widespread as both Internet access and computer literacy spread.
2030) 13.4114275, Internet - Wikipedia, the free encyclopedia.txt#78, term: computer, content:Cyberslacking can become a drain on corporate resources; the average UK employee spent 57 minutes a day surfing the Web while at work, according to a 2003 study by Peninsula Business Services.[95] Internet addiction disorder is excessive computer use that interferes with daily life.[96] Psychologist, Nicolas Carr believe that Internet use has other effects on individuals, for instance improving skills of scan-reading and interfering with the deep thinking that leads to true creativity.[97]
2031) 13.4114275, Internet - Wikipedia, the free encyclopedia.txt#98, term: computer, content:The vast majority of computer surveillance involves the monitoring of data and traffic on the Internet.[116] In the United States for example, under the Communications Assistance For Law Enforcement Act, all phone calls and broadband Internet traffic (emails, web traffic, instant messaging, etc.) are required to be available for unimpeded real-time monitoring by Federal law enforcement agencies.[117][118][119]
2032) 13.4114275, Internet protocol suite - Wikipedia, the free encyclopedia.txt#42, term: computer, content:For the purpose of providing process-specific transmission channels for applications, the layer establishes the concept of the port. This is a numbered logical construct allocated specifically for each of the communication channels an application needs. For many types of services, these port numbers have been standardized so that client computers may address specific services of a server computer without the involvement of service announcements or directory services.
2033) 13.4114275, Interpreter (computing) - Wikipedia, the free encyclopedia.txt#3, term: computer, content:The first interpreted high-level language was Lisp. Lisp was first implemented in 1958 by Steve Russell on an IBM 704 computer. Russell had read John McCarthy's paper, and realized (to McCarthy's surprise) that the Lisp eval function could be implemented in machine code.[2] The result was a working Lisp interpreter which could be used to run Lisp programs, or more properly, "evaluate Lisp expressions".
2034) 13.4114275, Interpreter (computing) - Wikipedia, the free encyclopedia.txt#29, term: computer, content:Defining a computer language is usually done in relation to an abstract machine (so-called operational semantics) or as a mathematical function (denotational semantics). A language may also be defined by an interpreter in which the semantics of the host language is given. The definition of a language by a self-interpreter is not well-founded (it cannot define a language), but a self-interpreter tells a reader about the expressiveness and elegance of a language. It also enables the interpreter to interpret its source code, the first step towards reflective interpreting.
2035) 13.4114275, Interrupt - Wikipedia, the free encyclopedia.txt#37, term: computer, content:For example, a disk interrupt signals the completion of a data transfer from or to the disk peripheral; a process waiting to read or write a file starts up again. As another example, a power-off interrupt predicts or requests a loss of power, allowing the computer equipment to perform an orderly shut-down. Also, interrupts are used in typeahead features for buffering events like keystrokes.
2036) 13.4114275, Jack Dongarra - Wikipedia, the free encyclopedia.txt#1, term: computer, content:Dongarra received a Bachelor of Science degree in Mathematics from Chicago State University in 1972 and a Master of Science in Computer Science from the Illinois Institute of Technology in 1973. He received his Doctor of Philosophy in Applied Mathematics from the University of New Mexico in 1980 under the supervision of Cleve Moler.[2] He worked at the Argonne National Laboratory until 1989, becoming a senior scientist.
2037) 13.4114275, John Mauchly - Wikipedia, the free encyclopedia.txt#13, term: computer, content:The term von Neumann architecture arose from von Neumann's paper, First Draft of a Report on the EDVAC.[8] Dated June 30, 1945, it was an early written account of a general purpose stored-program computing machine (the EDVAC). Goldstine, in a move that was to become controversial, removed any reference to Eckert or Mauchly and distributed the document to a number of von Neumann's associates across the country. The ideas became widely known within the very small world of computer designers.
2038) 13.4114275, John Mauchly - Wikipedia, the free encyclopedia.txt#19, term: computer, content:Mauchlys interest lay in the application of computers, as well as to their architecture and organization. His experience with programming the ENIAC and its successors led him to create Short Code (see "The UNIVAC SHORT CODE"), the first programming language actually used on a computer (predated by Zuses conceptual Plankalkul). It was a pseudocode interpreter for mathematical problems proposed in 1949 and ran on the UNIVAC I and II. Mauchly's belief in the importance of languages led him to hire Grace Murray Hopper to develop a compiler for the UNIVAC.
2039) 13.4114275, John Mauchly - Wikipedia, the free encyclopedia.txt#23, term: computer, content:Mauchly and Eckert's patent on the ENIAC was invalidated by U.S. Federal Court decision in October, 1973 for several reasons. Some had to do with the time between publication (the First Draft) and the patent filing date (1947). The federal judge who presided over the case ruled that "the subject matter was derived" from the earlier AtanasoffBerry Computer (ABC). This statement has become the center of a controversy.
2040) 13.4114275, John Mauchly - Wikipedia, the free encyclopedia.txt#25, term: computer, content:Critics of the court decision also note that there is, at a component level, nothing in common between the two machines. The ABC was binary; the ENIAC was decimal. The ABC used regenerative drum memory; The ENIAC used electronic decade counters. The ABC used its tubes to implement a binary serial adder while the ENIAC used tubes to implement a complete set of decimal operations. The ENIAC's general-purpose instruction set, together with the ability to automatically sequence through them, made it a general-purpose computer.
2041) 13.4114275, Joystick - Wikipedia, the free encyclopedia.txt#1, term: computer, content:Joysticks are often used to control video games, and usually have one or more push-buttons whose state can also be read by the computer. A popular variation of the joystick used on modern video game consoles is the analog stick. Joysticks are also used for controlling machines such as cranes, trucks, underwater unmanned vehicles, wheelchairs, surveillance cameras, and zero turning radius lawn mowers. Miniature finger-operated joysticks have been adopted as input devices for smaller electronic equipment such as mobile phones.
2042) 13.4114275, Joystick - Wikipedia, the free encyclopedia.txt#17, term: computer, content:A hat switch is a control on some joysticks. It is also known as a POV (point of view) switch. It allows one to look around in one's virtual world, browse menus, etc. For example, many flight simulators use it to switch the player's views,[31] while other games sometimes use it as a substitute for the D-pad. Computer gamepads with both an analogue stick and a D-pad usually assign POV switch scancodes to the latter.
2043) 13.4114275, Konrad Zuse - Wikipedia, the free encyclopedia.txt#8, term: computer, content:Zuse completed his work entirely independently of other leading computer scientists and mathematicians of his day. Between 1936 and 1945, he was in near-total intellectual isolation.[14] In 1939, Zuse was called to military service, where he was given the resources to ultimately build the Z2.[11] In September 1940 Zuse presented the Z2, covering several rooms in the parental flat, to experts of the Deutsche Versuchsanstalt fr Luftfahrt (DVL; i.e. German Research Institute for Aviation).[15] The Z2 was a revised version of the Z1 using telephone relays.
2044) 13.4114275, Konrad Zuse - Wikipedia, the free encyclopedia.txt#14, term: computer, content:In 1940, the German government began funding him through the Aerodynamische Versuchsanstalt (AVA, Aerodynamic Research Institute, forerunner of the DLR),[23] which used his work for the production of glide bombs. Zuse built the S1 and S2 computing machines, which were special purpose devices which computed aerodynamic corrections to the wings of radio-controlled flying bombs. The S2 featured an integrated analog-to-digital converter under program control, making it the first process-controlled computer.[20]
2045) 13.4114275, Konrad Zuse - Wikipedia, the free encyclopedia.txt#19, term: computer, content:According to the memoirs of the German computer pioneer Heinz Billing from the Max Planck Institute for Physics, published by Genscher, Dsseldorf, there was a meeting between Alan Turing and Konrad Zuse.[32] It took place in Gttingen in 1947. The encounter had the form of a colloquium. Participants were Womersley, Turing, Porter from England and a few German researchers like Zuse, Walther, and Billing. (For more details see Herbert Bruderer, Konrad Zuse und die Schweiz).
2046) 13.4114275, Laptop - Wikipedia, the free encyclopedia.txt#12, term: computer, content:The latest trend of technological convergence in a portable computer industry spawned a broad range of devices, with a combined features of several previously separate device types. The hybrids, convertibles and 2-in-1s emerged, crossover devices, which share traits of both tablets and laptops. All such devices have a touchscreen display designed to allow users to work in a tablet mode, using either multi-touch gestures or a stylus/digital pen.
2047) 13.4114275, Laptop - Wikipedia, the free encyclopedia.txt#17, term: computer, content:A desktop-replacement laptop is a class of large device, which is not intended primarily for mobile use. They are bulkier and not as portable as other laptops, and are intended for use as compact and transportable alternatives to a desktop computer.[20] Desktop replacements are larger and typically heavier than other classes of laptops. They are capable of containing more powerful components and have a 15-inch or larger display.[20]
2048) 13.4114275, Laptop - Wikipedia, the free encyclopedia.txt#50, term: computer, content:For Internet browsing and typical office applications, where the computer spends the majority of its time waiting for the next user input, even relatively low-end laptops (such as Netbooks) can be fast enough for some users.[52] As of mid-2010, at the lowest end, the cheapest netbooksbetween US$200300remain more expensive than the lowest-end desktop computers (around US$200) only when those are priced without a screen/monitor. Once an inexpensive monitor is added, the prices are comparable.
2049) 13.4114275, Laptop - Wikipedia, the free encyclopedia.txt#58, term: computer, content:A study by State University of New York researchers found that heat generated from laptops can increase the temperature of the lap of male users when balancing the computer on their lap, potentially putting sperm count at risk. The study, which included roughly two dozen men between the ages of 21 and 35, found that the sitting position required to balance a laptop can increase scrotum temperature by as much as 2.1C (3.78F). However, further research is needed to determine whether this directly affects male sterility.[56]
2050) 13.4114275, Laptop - Wikipedia, the free encyclopedia.txt#72, term: computer, content:Many brands, including the major ones, do not design and do not manufacture their laptops. Instead, a small number of Original Design Manufacturers (ODMs) design new models of laptops, and the brands choose the models to be included in their lineup. In 2006, 7 major ODMs manufactured 7 of every 10 laptops in the world, with the largest one (Quanta Computer) having 30% of world market share.[78] Therefore, there often are identical models available both from a major label and from a low-profile ODM in-house brand.
2051) 13.4114275, Library (computing) - Wikipedia, the free encyclopedia.txt#6, term: computer, content:The earliest programming concepts analogous to libraries were intended to separate data definitions from the program implementation. JOVIAL brought the "COMPOOL" (Communication Pool) concept to popular attention in 1959, although it adopted the idea from the large-system SAGE software. Following the computer science principles of separation of concerns and information hiding, "Comm Pool's purpose was to permit the sharing of System Data among many programs by providing a centralized data description."[2]
2052) 13.4114275, Library (computing) - Wikipedia, the free encyclopedia.txt#38, term: computer, content:Another solution to the library issue comes from using completely separate executables (often in some lightweight form) and calling them using a remote procedure call (RPC) over a network to another computer. This approach maximizes operating system re-use: the code needed to support the library is the same code being used to provide application support and security for every other program. Additionally, such systems do not require the library to exist on the same machine, but can forward the requests over the network.
2053) 13.4114275, LINC - Wikipedia, the free encyclopedia.txt#11, term: computer, content:The LINC included a set of eight ten-turn potentiometers (numbered 0-7) that could be each be read by a computer instruction. The knobs were a convenient user input device at a time before general adoption of the mouse. For example, the scaling of a displayed graph could be controlled by turning Knob 0. Or Knob 2 could be used to position a cursor in the graph in order to display the actual data value at that point.
2054) 13.4114275, LINC - Wikipedia, the free encyclopedia.txt#19, term: computer, content:Digital produced a version of the PDP-11/03 called the MINC-11, housed in a portable cart, and equipable with Digital-designed laboratory I/O modules supporting capabilities such as analog input and output. A programming language, MINC BASIC, included integrated support for the laboratory I/O modules. MINC stood for "Modular Instrument Computer." The name undoubtedly was intended to evoke memories of the LINC, but the 16-bit machine had no architectural resemblance to, or compatibility with, the LINC.
2055) 13.4114275, Linux - Wikipedia, the free encyclopedia.txt#4, term: computer, content:The Unix operating system was conceived and implemented in 1969 at AT&T's Bell Laboratories in the United States by Ken Thompson, Dennis Ritchie, Douglas McIlroy, and Joe Ossanna.[29] First released in 1971, Unix was written entirely in assembly language as it was common practice at the time. Later, in a key pioneering approach in 1973, it was rewritten in the C programming language by Dennis Ritchie (with exceptions to the kernel and I/O). The availability of a high-level language implementation of Unix made its porting to different computer platforms easier.
2056) 13.4114275, Linux - Wikipedia, the free encyclopedia.txt#5, term: computer, content:Due to an earlier antitrust case forbidding it from entering the computer business, AT&T was required to license the operating system's source code to anyone who asked.[30] As a result, Unix grew quickly and became widely adopted by academic institutions and businesses. In 1984, AT&T divested itself of Bell Labs; freed of the legal obligation requiring free licensing, Bell Labs began selling Unix as a proprietary product.
2057) 13.4114275, Linux - Wikipedia, the free encyclopedia.txt#9, term: computer, content:MINIX was created by Andrew S. Tanenbaum, a computer science professor, and released in 1987 as a minimal Unix-like operating system targeted at students and others who wanted to learn the operating system principles. Although the complete source code of MINIX was freely available, the licensing terms prevented it from being free software until the licensing changed in April 2000.[35]
2058) 13.4114275, Lisp (programming language) - Wikipedia, the free encyclopedia.txt#0, term: computer, content:Lisp (historically, LISP) is a family of computer programming languages with a long history and a distinctive, fully parenthesized prefix notation.[1] Originally specified in 1958, Lisp is the second-oldest high-level programming language in widespread use today. Only Fortran is older, by one year.[2][3] Lisp has changed since its early days, and many dialects have existed over its history. Today, the best known general-purpose Lisp dialects are Common Lisp and Scheme.
2059) 13.4114275, Lisp (programming language) - Wikipedia, the free encyclopedia.txt#10, term: computer, content:Lisp was a difficult system to implement with the compiler techniques and stock hardware of the 1970s. Garbage collection routines, developed by then-MIT graduate student Daniel Edwards, made it practical to run Lisp on general-purpose computing systems, but efficiency was still a problem.[citation needed] This led to the creation of Lisp machines: dedicated hardware for running Lisp environments and programs. Advances in both computer hardware and compiler technology soon made Lisp machines obsolete.[citation needed]
2060) 13.4114275, List of BSD operating systems - Wikipedia, the free encyclopedia.txt#0, term: computer, content:There are a number of Unix-like operating systems under active development, descended from the Berkeley Software Distribution (BSD) series of UNIX variants developed (originally by Bill Joy) at the University of California, Berkeley Electrical Engineering and Computer Science department. As of 2016[update] there were four major BSD operating systems, and an increasing number of other OSs derived from these, that add or remove certain features but generally remain compatible with their originating OSand so are not really forks of them. This is a list of those that have been active since 2014, and their websites.
2061) 13.4114275, List of BSD operating systems - Wikipedia, the free encyclopedia.txt#3, term: computer, content:OpenBSD is a Unix-like computer operating system descended from Berkeley Software Distribution (BSD), a Unix derivative developed at the University of California, Berkeley. It was forked from NetBSD in 1995. OpenBSD includes a number of security features absent or optional in other operating systems and has a tradition of developers auditing the source code for software bugs and security problems.
2062) 13.4114275, List of programming languages by type - Wikipedia, the free encyclopedia.txt#12, term: computer, content:Declarative languages describe a problem rather than defining a solution. Declarative programming stands in contrast to imperative programming via imperative programming languages, where serial orders (imperatives) are given to a computer. In addition to the examples given just below, all (pure) functional and logic-based programming languages are also declarative. In fact, "functional" and "logical" constitute the usual subcategories of the declarative category.
2063) 13.4114275, Logarithm - Wikipedia, the free encyclopedia.txt#2, term: computer, content:The logarithm to base 10 (that is b = 10) is called the common logarithm and has many applications in science and engineering. The natural logarithm has the number e ( 2.718) as its base; its use is widespread in mathematics and physics, because of its simpler derivative. The binary logarithm uses base 2 (that is b = 2) and is commonly used in computer science.
2064) 13.4114275, Logarithm - Wikipedia, the free encyclopedia.txt#108, term: computer, content:Further logarithm-like inverse functions include the double logarithm ln(ln(x)), the super- or hyper-4-logarithm (a slight variation of which is called iterated logarithm in computer science), the Lambert W function, and the logit. They are the inverse functions of the double exponential function, tetration, of f(w) = wew,[98] and of the logistic function, respectively.[99]
2065) 13.4114275, Logic gate - Wikipedia, the free encyclopedia.txt#2, term: computer, content:Logic circuits include such devices as multiplexers, registers, arithmetic logic units (ALUs), and computer memory, all the way up through complete microprocessors, which may contain more than 100 million gates. In modern practice, most gates are made from field-effect transistors (FETs), particularly MOSFETs (metaloxidesemiconductor field-effect transistors).
2066) 13.4114275, Logic synthesis - Wikipedia, the free encyclopedia.txt#0, term: computer, content:In electronics, logic synthesis is a process by which an abstract form of desired circuit behavior, typically at register transfer level (RTL), is turned into a design implementation in terms of logic gates, typically by a computer program called a synthesis tool. Common examples of this process include synthesis of HDLs, including VHDL and Verilog. Some synthesis tools generate bitstreams for programmable logic devices such as PALs or FPGAs, while others target the creation of ASICs. Logic synthesis is one aspect of electronic design automation.
2067) 13.4114275, Low-level programming language - Wikipedia, the free encyclopedia.txt#3, term: computer, content:Machine code is the only language a computer can process directly without a previous transformation. Currently, programmers almost never write programs directly in machine code, because it requires attention to numerous details that a high-level language handles automatically, requires memorizing or looking up numerical codes for every instruction, and is extremely difficult to modify.
2068) 13.4114275, Mac OS - Wikipedia, the free encyclopedia.txt#31, term: computer, content:Several computer manufacturers over the years have made Macintosh clones capable of running Mac OS. From 1995 to 1997 Apple licensed Macintosh ROMs to several companies, notably Power Computing, UMAX and Motorola. These machines normally ran various versions of classic Mac OS. Steve Jobs ended the clone-licensing program after returning to Apple in 1997.
2069) 13.4114275, Mac OS - Wikipedia, the free encyclopedia.txt#32, term: computer, content:In 2008, Miami-based manufacturing company Psystar Corporation announced a $399 clone called OpenMac that came with a barebones system that can run Mac OS X 10.5 Leopard. Threatened with litigation, Psystar changed the name to Open Computer. Apple filed a lawsuit with the company and asked that Psystar be ordered to stop producing clone systems, recall every system sold, and pay monetary damages.[14] Eventually, Apple prevailed in court, and the Open Computer's production was ceased. Psystar itself appears to be defunct now, as the company's website is gone.
2070) 13.4114275, Machine - Wikipedia, the free encyclopedia.txt#25, term: computer, content:Controllers combine sensors, logic, and actuators to maintain the performance of components of a machine. Perhaps the best known is the flyball governor for a steam engine. Examples of these devices range from a thermostat that as temperature rises opens a valve to cooling water to speed controllers such the cruise control system in an automobile. The programmable logic controller replaced relays and specialized control mechanisms with a programmable computer. Servo motors that accurately position a shaft in response to an electrical command are the actuators that make robotic systems possible.
2071) 13.4114275, Machine code - Wikipedia, the free encyclopedia.txt#1, term: computer, content:Numerical machine code (i.e., not assembly code) may be regarded as the lowest-level representation of a compiled or assembled computer program or as a primitive and hardware-dependent programming language. While it is possible to write programs directly in numerical machine code, it is tedious and error prone to manage individual bits and calculate numerical addresses and constants manually. It is thus rarely done today, except for situations that require extreme optimization or debugging.
2072) 13.4114275, Machine code - Wikipedia, the free encyclopedia.txt#18, term: computer, content:It has been said that machine code is so unreadable that the United States Copyright Office cannot identify whether a particular encoded program is an original work of authorship;[4] however, the US Copyright Office does allow for copyright registration of computer programs.[5] Douglas Hofstadter compares machine code with the genetic code: "Looking at a program written in machine language is vaguely comparable to looking at a DNA molecule atom by atom."[6]
2073) 13.4114275, Machine learning - Wikipedia, the free encyclopedia.txt#18, term: computer, content:The computational analysis of machine learning algorithms and their performance is a branch of theoretical computer science known as computational learning theory. Because training sets are finite and the future is uncertain, learning theory usually does not yield guarantees of the performance of algorithms. Instead, probabilistic bounds on the performance are quite common. The biasvariance decomposition is one way to quantify generalization error.
2074) 13.4114275, Machine learning - Wikipedia, the free encyclopedia.txt#25, term: computer, content:Falling hardware prices and the development of GPUs for personal use in the last few years have contributed to the development of the concept of Deep learning which consists of multiple hidden layers in an artificial neural network. This approach tries to model the way the human brain processes light and sound into vision and hearing. Some successful applications of deep learning are computer vision and speech recognition.[20]
2075) 13.4114275, Magnetic-core memory - Wikipedia, the free encyclopedia.txt#11, term: computer, content:The first use of core was in the Whirlwind computer, but commercialization followed quickly. It was during the early 1950s that Seeburg developed the use of this coincident current ferrite core memory storage in the "Tormat" memory of its new range of jukeboxes, starting with the V200 released in 1955. Development work was completed in 1953. Numerous uses in computing, telephony and industrial control followed.
2076) 13.4114275, Magnetic-core memory - Wikipedia, the free encyclopedia.txt#31, term: computer, content:In 1980, the price of a 16kW (kiloword, equivalent to 32 kB) core memory board that fitted into a DEC Q-bus computer was around US$3,000. At that time, core array and supporting electronics fit on a single printed circuit board about 25 x 20cm in size, the core array was mounted a few mm above the PCB and was protected with a metal or plastic plate.
2077) 13.4114275, Mainframe computer - Wikipedia, the free encyclopedia.txt#1, term: computer, content:The term originally referred to the large cabinets called "main frames" that housed the central processing unit and main memory of early computers.[2][3] Later, the term was used to distinguish high-end commercial machines from less powerful units.[4] Most large-scale computer system architectures were established in the 1960s, but continue to evolve.
2078) 13.4114275, Mainframe computer - Wikipedia, the free encyclopedia.txt#7, term: computer, content:Modern mainframes can run multiple different instances of operating systems at the same time. This technique of virtual machines allows applications to run as if they were on physically distinct computers. In this role, a single mainframe can replace higher-functioning hardware services available to conventional servers. While mainframes pioneered this capability, virtualization is now available on most families of computer systems, though not always to the same degree or level of sophistication.[7]
2079) 13.4114275, Manchester Mark 1 - Wikipedia, the free encyclopedia.txt#2, term: computer, content:The Mark 1 was to provide a computing resource within the university, to allow researchers to gain experience in the practical use of computers, but it very quickly also became a prototype on which the design of Ferranti's commercial version could be based. Development ceased at the end of 1949, and the machine was scrapped towards the end of 1950, replaced in February 1951 by a Ferranti Mark 1, the world's first commercially available general-purpose electronic computer.[2]
2080) 13.4114275, Manchester Mark 1 - Wikipedia, the free encyclopedia.txt#19, term: computer, content:Tootill was temporarily transferred from the University of Manchester to Ferranti in August 1949, to continue work on the Ferranti Mark 1's design, and spent four months working with the company.[24] The Manchester Mark 1 was dismantled and scrapped towards the end of 1950,[25] replaced a few months later by the first Ferranti Mark 1, the world's first commercially available general-purpose computer.[2]
2081) 13.4114275, Manchester Small-Scale Experimental Machine - Wikipedia, the free encyclopedia.txt#4, term: computer, content:Konrad Zuse's Z3 was the world's first working programmable, fully automatic computer, with binary digital arithmetic logic, but it lacked the conditional branching of a Turing machine. On 12 May 1941, it was successfully presented to an audience of scientists of the Deutsche Versuchsanstalt fr Luftfahrt ("German Laboratory for Aviation") in Berlin.[5] The Z3 stored its program on an external tape, but it was electromechanical rather than electronic. The Colossus of 1943 was the first electronic computing device, but it was not a general-purpose machine.[6]
2082) 13.4114275, Manchester Small-Scale Experimental Machine - Wikipedia, the free encyclopedia.txt#5, term: computer, content:The ENIAC (1946) was the first machine that was both electronic and general purpose. It was Turing complete, with conditional branching, and programmable to solve a wide range of problems, but its program was held in the state of switches in patchcords, not in memory, and it could take several days to reprogram.[2] Researchers such as Turing and Zuse investigated the idea of using the computer's memory to hold the program as well as the data it was working on,[7] but it was mathematician John von Neumann who became widely credited with defining that computer architecture, still used in almost all computers.[8]
2083) 13.4114275, Manchester Small-Scale Experimental Machine - Wikipedia, the free encyclopedia.txt#8, term: computer, content:The NPL did not have the expertise to build a machine like ACE, so they contacted Tommy Flowers at the General Post Office's (GPO) Dollis Hill Research Laboratory. Flowers, the designer of Colossus, the world's first programmable electronic computer, was committed elsewhere and was unable to take part in the project, although his team did build some mercury delay lines for ACE.[12] The Telecommunications Research Establishment (TRE) was also approached for assistance, as was Maurice Wilkes at the University of Cambridge Mathematical Laboratory.[12]
2084) 13.4114275, Manchester Small-Scale Experimental Machine - Wikipedia, the free encyclopedia.txt#22, term: computer, content:Three programs were written for the computer. The first, consisting of 17instructions, was written by Kilburn, and so far as can be ascertained first ran on 21 June 1948.[28] It was designed to find the highest proper factor of 218 (262,144) by trying every integer from 2181 downwards. The divisions were implemented by repeated subtractions of the divisor. The SSEM took 3.5million operations and 52minutes to produce the answer (131,072). The program used eight words of working storage in addition to its 17words of instructions, giving a program size of 25words.[29]
2085) 13.4114275, Massachusetts Institute of Technology - Wikipedia, the free encyclopedia.txt#42, term: computer, content:MIT awarded 1,547 master's degrees and 609 doctoral degrees in the academic year 201011.[168] In the 2011 fall term, the School of Engineering was the most popular academic division, enrolling 45.0% of graduate students, followed by the Sloan School of Management (19%), School of Science (16.9%), School of Architecture and Planning (9.2%), Whitaker College of Health Sciences (5.1%),[d] and School of Humanities, Arts, and Social Sciences (4.7%). The largest graduate degree programs were the Sloan MBA, Electrical Engineering and Computer Science, and Mechanical Engineering.[163]
2086) 13.4114275, Massively multiplayer online game - Wikipedia, the free encyclopedia.txt#0, term: computer, content:A massively multiplayer online game (MMOG or MMO) is an online game which is capable of supporting large numbers of players simultaneously in the same instance (or world).[1] MMOs usually feature a huge, persistent open world, however, some games differ. These games can be found for most network-capable platforms, including the personal computer, video game console, or smartphones and other mobile devices.
2087) 13.4114275, Massively multiplayer online game - Wikipedia, the free encyclopedia.txt#2, term: computer, content:The most popular type of MMOG, and the subgenre that pioneered the category, is the massively multiplayer online role-playing game (MMORPG), which descended from university mainframe computer MUD and adventure games such as Rogue and Dungeon on the PDP-10. These games predate the commercial gaming industry and the Internet, but still featured persistent worlds and other elements of MMOGs still used today.
2088) 13.4114275, Massively multiplayer online game - Wikipedia, the free encyclopedia.txt#8, term: computer, content:The popularity of MMOGs was mostly restricted to the computer game market until the sixth-generation consoles, with the launch of Phantasy Star Online on Dreamcast and the emergence and growth of online service Xbox Live. There have been a number of console MMOGs, including EverQuest Online Adventures (PlayStation 2), and the multiconsole Final Fantasy XI. On PCs, the MMOG market has always been dominated by successful fantasy MMORPGs.
2089) 13.4114275, Massively multiplayer online game - Wikipedia, the free encyclopedia.txt#41, term: computer, content:Many types of MMO games can be classified as casual, because they are designed to appeal to all computer users (as opposed to subgroup of frequent game buyers), or to fans of another game genre (such as collectible card games). Such games are easy to learn and require a smaller time commitment than other game types. Other popular casual games include simple management games such as The Sims Online, Monopoly City Streets, Roblox or Kung Fu Panda World.
2090) 13.4114275, Matrix (mathematics) - Wikipedia, the free encyclopedia.txt#3, term: computer, content:Applications of matrices are found in most scientific fields. In every branch of physics, including classical mechanics, optics, electromagnetism, quantum mechanics, and quantum electrodynamics, they are used to study physical phenomena, such as the motion of rigid bodies. In computer graphics, they are used to project a 3D model onto a 2 dimensional screen. In probability theory and statistics, stochastic matrices are used to describe sets of probabilities; for instance, they are used within the PageRank algorithm that ranks the pages in a Google search.[5] Matrix calculus generalizes classical analytical notions such as derivatives and exponentials to higher dimensions.
2091) 13.4114275, Matrix (mathematics) - Wikipedia, the free encyclopedia.txt#8, term: computer, content:Matrices which have a single row are called row vectors, and those which have a single column are called column vectors. A matrix which has the same number of rows and columns is called a square matrix. A matrix with an infinite number of rows or columns (or both) is called an infinite matrix. In some contexts, such as computer algebra programs, it is useful to consider a matrix with no rows or no columns, called an empty matrix.
2092) 13.4114275, Matrix (mathematics) - Wikipedia, the free encyclopedia.txt#97, term: computer, content:Early encryption techniques such as the Hill cipher also used matrices. However, due to the linear nature of matrices, these codes are comparatively easy to break.[74] Computer graphics uses matrices both to represent objects and to calculate transformations of objects using affine rotation matrices to accomplish tasks such as projecting a three-dimensional object onto a two-dimensional screen, corresponding to a theoretical camera observation.[75] Matrices over a polynomial ring are important in the study of control theory.
2093) 13.4114275, Mechanical computer - Wikipedia, the free encyclopedia.txt#0, term: computer, content:A mechanical computer is built from mechanical components such as levers and gears, rather than electronic components. The most common examples are adding machines and mechanical counters, which use the turning of gears to increment output displays. More complex examples could carry out multiplication and divisionFriden used a moving head which paused at each columnand even differential analysis. One model sold in the 1960s calculated square roots.
2094) 13.4114275, Message transfer agent - Wikipedia, the free encyclopedia.txt#0, term: computer, content:Within Internet message handling services (MHS), a message transfer agent[1] or mail transfer agent[2] (MTA) or mail relay is software that transfers electronic mail messages from one computer to another using a clientserver application architecture. An MTA implements both the client (sending) and server (receiving) portions of the Simple Mail Transfer Protocol.[3]
2095) 13.4114275, Microcode - Wikipedia, the free encyclopedia.txt#0, term: computer, content:Microcode is "a technique that imposes an interpreter between the hardware and the architectural level of a computer."[1] As such, the microcode is a layer of hardware-level instructions that implement higher-level machine code instructions or internal state machine sequencing in many digital processing elements. Microcode is used in general-purpose central processing units, in more specialized processors such as microcontrollers, digital signal processors, channel controllers, disk controllers, network interface controllers, network processors, graphics processing units, and in other hardware.
2096) 13.4114275, Microcode - Wikipedia, the free encyclopedia.txt#1, term: computer, content:Microcode typically resides in special high-speed memory and translates machine instructions, state machine data or other input into sequences of detailed circuit-level operations. It separates the machine instructions from the underlying electronics so that instructions can be designed and altered more freely. It also facilitates the building of complex multi-step instructions, while reducing the complexity of computer circuits. Writing microcode is often called microprogramming and the microcode in a particular processor implementation is sometimes called a microprogram.
2097) 13.4114275, Microcode - Wikipedia, the free encyclopedia.txt#21, term: computer, content:Each microinstruction in a microprogram provides the bits that control the functional elements that internally compose a CPU. The advantage over a hard-wired CPU is that internal CPU control becomes a specialized form of a computer program. Microcode thus transforms a complex electronic design challenge (the control of a CPU) into a less complex programming challenge. To take advantage of this, a CPU is divided into several parts:
2098) 13.4114275, Microphone - Wikipedia, the free encyclopedia.txt#61, term: computer, content:Some microphones use other connectors, such as a 5-pin XLR, or mini XLR for connection to portable equipment. Some lavalier (or "lapel", from the days of attaching the microphone to the news reporters suit lapel) microphones use a proprietary connector for connection to a wireless transmitter, such as a radio pack. Since 2005, professional-quality microphones with USB connections have begun to appear, designed for direct recording into computer-based software.
2099) 13.4114275, Microprocessor - Wikipedia, the free encyclopedia.txt#0, term: computer, content:A microprocessor is a computer processor which incorporates the functions of a computer's central processing unit (CPU) on a single integrated circuit (IC),[1] or at most a few integrated circuits.[2] The microprocessor is a multipurpose, clock driven, register based, programmable electronic device which accepts digital or binary data as input, processes it according to instructions stored in its memory, and provides results as output. Microprocessors contain both combinational logic and sequential digital logic. Microprocessors operate on numbers and symbols represented in the binary numeral system.
2100) 13.4114275, Microprocessor - Wikipedia, the free encyclopedia.txt#28, term: computer, content:In 1968, Garrett AiResearch (which employed designers Ray Holt and Steve Geller) was invited to produce a digital computer to compete with electromechanical systems then under development for the main flight control computer in the US Navy's new F-14 Tomcat fighter. The design was complete by 1970, and used a MOS-based chipset as the core CPU. The design was significantly (approximately 20 times) smaller and much more reliable than the mechanical systems it competed against, and was used in all of the early Tomcat models. This system contained "a 20-bit, pipelined, parallel multi-microprocessor". The Navy refused to allow publication of the design until 1997. For this reason the CADC, and the MP944 chipset it used, are fairly unknown.[14] Ray Holt graduated from California Polytechnic University in 1968, and began his computer design career with the CADC. From its inception, it was shrouded in secrecy until 1998 when at Holt's request, the US Navy allowed the documents into the public domain. Since then people[who?] have debated whether this was the first microprocessor. Holt has stated that no one has compared this microprocessor with those that came later.[15] According to Parab et al. (2007), "The scientific papers and literature published around 1971 reveal that the MP944 digital processor used for the F-14 Tomcat aircraft of the US Navy qualifies as the first microprocessor. Although interesting, it was not a single-chip processor, as was not the Intel 4004 they both were more like a set of parallel building blocks you could use to make a general-purpose form. It contains a CPU, RAM, ROM, and two other support chips like the Intel 4004. It was made from the same P-channel technology, operated at military specifications and had larger chips -- an excellent computer engineering design by any standards. Its design indicates a major advance over Intel, and two year earlier. It actually worked and was flying in the F-14 when the Intel 4004 was announced. It indicates that todays industry theme of converging DSP-microcontroller architectures was started in 1971."[16] This convergence of DSP and microcontroller architectures is known as a digital signal controller.[17]
2101) 13.4114275, Microprocessor - Wikipedia, the free encyclopedia.txt#29, term: computer, content:The Four-Phase Systems AL1 was an 8-bit bit slice chip containing eight registers and an ALU.[18] It was designed by Lee Boysel in 1969.[19][20][21] At the time, it formed part of a nine-chip, 24-bit CPU with three AL1s, but it was later called a microprocessor when, in response to 1990s litigation by Texas Instruments, a demonstration system was constructed where a single AL1 formed part of a courtroom demonstration computer system, together with RAM, ROM, and an input-output device.[22]
2102) 13.4114275, Microprocessor - Wikipedia, the free encyclopedia.txt#31, term: computer, content:Pico was a spinout by five GI design engineers whose vision was to create single chip calculator ICs. They had significant previous design experience on multiple calculator chipsets with both GI and Marconi-Elliott.[24] The key team members had originally been tasked by Elliott Automation to create an 8-bit computer in MOS and had helped establish a MOS Research Laboratory in Glenrothes, Scotland in 1967.
2103) 13.4114275, Microprocessor - Wikipedia, the free encyclopedia.txt#79, term: computer, content:Historically, AMD and Intel have switched places as the company with the fastest CPU several times. Intel currently leads on the desktop side of the computer CPU market, with their Sandy Bridge and Ivy Bridge series. In servers, AMD's new Opterons seem to have superior performance for their price point. This means that AMD are currently more competitive in low- to mid-end servers and workstations that more effectively use fewer cores and threads.
2104) 13.4114275, Microprocessor - Wikipedia, the free encyclopedia.txt#82, term: computer, content:In 2002, less than 10% of all the CPUs sold in the world were 32-bit or more. Of all the 32-bit CPUs sold, about 2% are used in desktop or laptop personal computers. Most microprocessors are used in embedded control applications such as household appliances, automobiles, and computer peripherals. Taken as a whole, the average price for a microprocessor, microcontroller, or DSP is just over US$6 (equivalent to $7.89 in 2015).[47]
2105) 13.4114275, Microsoft Windows - Wikipedia, the free encyclopedia.txt#7, term: computer, content:Windows 2.0 was released in December 1987, and was more popular than its predecessor. It features several improvements to the user interface and memory management.[citation needed] Windows 2.03 changed the OS from tiled windows to overlapping windows. The result of this change led to Apple Computer filing a suit against Microsoft alleging infringement on Apple's copyrights.[9][10] Windows 2.0 also introduced more sophisticated keyboard shortcuts and could make use of expanded memory.
2106) 13.4114275, Microsoft Windows - Wikipedia, the free encyclopedia.txt#13, term: computer, content:Windows 3.2, released 1994, is an updated version of the Chinese version of Windows 3.1.[15] The update was limited to this language version, as it fixed only issues related to the complex writing system of the Chinese language.[16] Windows 3.2 was generally sold by computer manufacturers with a ten-disk version of MS-DOS that also had Simplified Chinese characters in basic output and some translated utilities.
2107) 13.4114275, Microsoft Windows - Wikipedia, the free encyclopedia.txt#42, term: computer, content:These design issues combined with programming errors (e.g. buffer overflows) and the popularity of Windows means that it is a frequent target of computer worm and virus writers. In June 2005, Bruce Schneier's Counterpane Internet Security reported that it had seen over 1,000 new viruses and worms in the previous six months.[56] In 2005, Kaspersky Lab found around 11,000 malicious programsviruses, Trojans, back-doors, and exploits written for Windows.[57]
2108) 13.4114275, Minicomputer - Wikipedia, the free encyclopedia.txt#1, term: computer, content:When single-chip CPUs appeared, beginning with the Intel 4004 in 1971, the term "minicomputer" came to mean a machine that lies in the middle range of the computing spectrum, in between the smallest mainframe computers and the microcomputers. The term "minicomputer" is little used today; the contemporary term for this class of system is "midrange computer", such as the higher-end SPARC, Power Architecture and Itanium-based systems from Oracle, IBM and Hewlett-Packard.
2109) 13.4114275, Minicomputer - Wikipedia, the free encyclopedia.txt#4, term: computer, content:Minicomputers were also known as midrange computers. They grew to have relatively high processing power and capacity. They were used in manufacturing process control, telephone switching and to control laboratory equipment. In the 1970s, they were the hardware that was used to launch the computer-aided design (CAD) industry and other similar industries where a smaller dedicated system was needed.
2110) 13.4114275, Minicomputer - Wikipedia, the free encyclopedia.txt#7, term: computer, content:The decline of the minis happened due to the lower cost of microprocessor-based hardware, the emergence of inexpensive and easily deployable local area network systems, the emergence of the 68020, 80286 and the 80386 microprocessors, and the desire of end-users to be less reliant on inflexible minicomputer manufacturers and IT departments or "data centers". The result was that minicomputers and computer terminals were replaced by networked workstations, file servers and PCs in some installations, beginning in the latter half of the 1980s.
2111) 13.4114275, Minicomputer - Wikipedia, the free encyclopedia.txt#10, term: computer, content:Digital Equipment Corporation (DEC) was once the leading minicomputer manufacturer, at one time the second-largest computer company after IBM. But as the minicomputer declined in the face of generic Unix servers and Intel-based PCs, not only DEC, but almost every other minicomputer company including Data General, Prime, Computervision, Honeywell and Wang Laboratories, many based in New England (hence the end of the Massachusetts Miracle), also collapsed or merged. DEC was sold to Compaq in 1998, while Data General was acquired by EMC Corporation.
2112) 13.4114275, Minicomputer - Wikipedia, the free encyclopedia.txt#14, term: computer, content:Several pioneering computer companies first built minicomputers, such as DEC, Data General, and Hewlett-Packard (HP) (who now refers to its HP3000 minicomputers as "servers" rather than "minicomputers"). And although today's PCs and servers are clearly microcomputers physically, architecturally their CPUs and operating systems have developed largely by integrating features from minicomputers.[citation needed]
2113) 13.4114275, MIPS instruction set - Wikipedia, the free encyclopedia.txt#93, term: computer, content:There is a freely available MIPS32 simulator (earlier versions simulated only the R2000/R3000) called SPIM for use in education. EduMIPS64[41] is a GPL graphical cross-platform MIPS64 CPU simulator, written in Java/Swing. It supports a wide subset of the MIPS64 ISA and allows the user to graphically see what happens in the pipeline when an assembly program is run by the CPU. It has educational purposes and is used in some[who?] computer architecture courses in universities around the world.
2114) 13.4114275, MIT Press - Wikipedia, the free encyclopedia.txt#2, term: computer, content:A European marketing office was opened in 1969, and a Journals division was added in 1972. In the late 1970s, responding to changing economic conditions, the publisher narrowed the focus of their catalog to a few key areas, initially architecture, computer science and artificial intelligence, economics, and cognitive science. Other areas, such as technology and design, have been added since. The latest addition is environmental science.[1]
2115) 13.4114275, MOS Technology 6502 - Wikipedia, the free encyclopedia.txt#27, term: computer, content:The 6502 is capable of performing addition and subtraction in binary or binary coded decimal. Placing the CPU into BCD mode with the SED (set D flag) instruction results in decimal arithmetic, in which $99 + $01 would result in $00 and the carry (C) flag being set. In binary mode (CLD, clear D flag), the same operation would result in $9A and the carry flag being cleared. Other than Atari BASIC, BCD mode was seldom used in home computer applications.
2116) 13.4114275, Motorola 6800 - Wikipedia, the free encyclopedia.txt#1, term: computer, content:The 6800 architecture and instruction set were influenced by the then popular Digital Equipment Corporation PDP-11 mini computer.[3][4] The 6800 has a 16-bit address bus that could directly access 64KB of memory and an 8-bit bi-directional data bus. It has 72 instructions with seven addressing modes for a total of 197 opcodes. The original MC6800 could have a clock frequency of up to 1MHz. Later versions had a maximum clock frequency of 2MHz.[5][6]
2117) 13.4114275, Motorola 6800 - Wikipedia, the free encyclopedia.txt#18, term: computer, content:Other divisions in Motorola developed components for the M6800 family. The Components Products Department designed the MC6870 two-phase clock IC, and the Memory Products group provided a full line of ROMs and RAMs. The CMOS group's MC14411 Bit Rate Generator provided a 75 to 9600 baud clock for the MC6850 serial interface. The buffers for address and data buses were standard Motorola products. Motorola could supply every IC, transistor, and diode necessary to build an MC6800-based computer.
2118) 13.4114275, Motorola 6800 - Wikipedia, the free encyclopedia.txt#50, term: computer, content:Motorola's next generation 8-bit microprocessor architecture, the MC6809 (1979), was used in the Radio Shack TRS-80 Color Computer and the compatible Dragon 32/64 which was sold in Europe. SWTPC also released a 6809 based system, the s/09, as did other SS-50 vendors. The 6809 and the 16/32 bit 68000 were incompatible with the instruction set of the 6800, but could use 6800-family peripheral chips.
2119) 13.4114275, Motorola 6809 - Wikipedia, the free encyclopedia.txt#9, term: computer, content:The Hitachi 6309 was an enhanced version of the 6809 with extra registers and additional instructions, including block move, additional multiply instructions and hardware-implemented division. It was used in unofficially-upgraded Tandy Color Computer 3 computers and a version of OS-9 was written to take advantages of the 6309's extra features: NitrOS-9. The most widespread use of the 6309 was likely the Yamaha DX-7 FM Synthesizer.
2120) 13.4114275, MS-DOS - Wikipedia, the free encyclopedia.txt#9, term: computer, content:Microsoft licensed or released versions of MS-DOS under different names like Lifeboat Associates "Software Bus 86"[21] aka SB-DOS,[10] COMPAQ-DOS,[21] NCR-DOS or Z-DOS[10] before it eventually enforced the MS-DOS name for all versions but the IBM one, which was originally called "IBM Personal Computer DOS", later shortened to IBM PC DOS. (Competitors released compatible DOS systems such as DR DOS and PTS-DOS that could also run DOS applications.)
2121) 13.4114275, Multi-core processor - Wikipedia, the free encyclopedia.txt#28, term: computer, content:Given the increasing emphasis on multi-core chip design, stemming from the grave thermal and power consumption problems posed by any further significant increase in processor clock speeds, the extent to which software can be multithreaded to take advantage of these new chips is likely to be the single greatest constraint on computer performance in the future. If developers are unable to design software to fully exploit the resources provided by multiple cores, then they will ultimately reach an insurmountable performance ceiling.
2122) 13.4114275, Multimedia - Wikipedia, the free encyclopedia.txt#11, term: computer, content:In the 1993 first edition of McGraw-Hills Multimedia: Making It Work, Tay Vaughan declared "Multimedia is any combination of text, graphic art, sound, animation, and video that is delivered by computer. When you allow the user  the viewer of the project  to control what and when these elements are delivered, it is interactive multimedia. When you provide a structure of linked elements through which the user can navigate, interactive multimedia becomes hypermedia."[3]
2123) 13.4114275, Multiprocessing - Wikipedia, the free encyclopedia.txt#0, term: computer, content:Multiprocessing is the use of two or more central processing units (CPUs) within a single computer system.[1][2] The term also refers to the ability of a system to support more than one processor and/or the ability to allocate tasks between them.[3] There are many variations on this basic theme, and the definition of multiprocessing can vary with context, mostly as a function of how CPUs are defined (multiple cores on one die, multiple dies in one package, multiple packages in one system unit, etc.).
2124) 13.4114275, Multiprocessing - Wikipedia, the free encyclopedia.txt#1, term: computer, content:According to some on-line dictionaries, a multiprocessor is a computer system having two or more processing units (multiple processors) each sharing main memory and peripherals, in order to simultaneously process programs.[4][5] A 2009 textbook defined multiprocessor system similarly, but noting that the processors may share "some or all of the systems memory and I/O facilities"; it also gave tightly coupled system as a synonymous term.[6]
2125) 13.4114275, Natural language - Wikipedia, the free encyclopedia.txt#1, term: computer, content:Though the exact definition varies between scholars, natural language can broadly be defined in contrast to artificial or constructed languages (such as computer programming languages and international auxiliary languages) and to other communication systems in nature. Such examples include bees' waggle dance[2] and whale song, to which researchers have found and/or applied the linguistic cognates of dialect and even syntax.
2126) 13.4114275, Natural language - Wikipedia, the free encyclopedia.txt#3, term: computer, content:Controlled natural languages are subsets of natural languages whose grammars and dictionaries have been restricted in order to reduce or eliminate both ambiguity and complexity (for instance, by cutting down on rarely used superlative or adverbial forms or irregular verbs). The purpose behind the development and implementation of a controlled natural language typically is to aid non-native speakers of a natural language in understanding it, or to ease computer processing of a natural language. An example of a widely used controlled natural language is Simplified English, which was originally developed for aerospace industry maintenance manuals.
2127) 13.4114275, Negation - Wikipedia, the free encyclopedia.txt#22, term: computer, content:In computer science there is also bitwise negation. This takes the value given and switches all the binary 1s to 0s and 0s to 1s. See bitwise operation. This is often used to create ones' complement or "~" in C or C++ and two's complement (just simplified to "-" or the negative sign since this is equivalent to taking the arithmetic negative value of the number) as it basically creates the opposite (negative value equivalent) or mathematical complement of the value (where both values are added together they create a whole).
2128) 13.4114275, Netbook - Wikipedia, the free encyclopedia.txt#4, term: computer, content:Many major Netbook producing companies stopped producing them by the end of 2012.[15] Many netbook products were replaced on the market by Chromebooks, a variation on the network computer concept in the form of a netbook. With the rise of Chromebooks in 2014, Microsoft began a new effort at creating viable netbooks, with Windows 8.1 with Bing. HP re-entered the non-Chromebook netbook market with the Stream 11 in 2014.[16]
2129) 13.4114275, Netbook - Wikipedia, the free encyclopedia.txt#15, term: computer, content:With the introduction of Chromebooks, major manufacturers produced the new laptops for the same segment of the market that netbooks serviced. Chromebooks, a variation on the network computer concept, in the form of a netbook, require internet connections for full functionality. Chromebooks became top selling laptops in 2014. The threat of Google Chrome OS based Chromebooks prompted Microsoft to revive and revamp netbooks with Windows 8.1 with Bing. HP re-entered the non-Chromebook netbook market with the Stream 11 in 2014.[16]
2130) 13.4114275, Non-uniform memory access - Wikipedia, the free encyclopedia.txt#0, term: computer, content:Non-uniform memory access (NUMA) is a computer memory design used in multiprocessing, where the memory access time depends on the memory location relative to the processor. Under NUMA, a processor can access its own local memory faster than non-local memory (memory local to another processor or memory shared between processors). The benefits of NUMA are limited to particular workloads, notably on servers where the data are often associated strongly with certain tasks or users.[1]
2131) 13.4114275, Oberon (operating system) - Wikipedia, the free encyclopedia.txt#5, term: computer, content:Similar user Interfaces still have to appear in more commonplace operating systems. Rob Pike's Acme system under Plan 9 from Bell Labs was strongly inspired by the Oberon TUI. Whether the worksheet interface of the Macintosh Programmer's Workshop influenced Oberon's TUI or vice versa is difficult to decide: Oberon System was based on Wirth's previous computer design the Lilith, and both the Apple Macintosh (and its precursor Lisa) and the Oberon System (on Ceres and its precursor Lilith) were all inspired by the Alto developed at Xerox PARC.
2132) 13.4114275, Oberon (operating system) - Wikipedia, the free encyclopedia.txt#13, term: computer, content:In 2013 Niklaus Wirth and Paul Reed completed a re-implementation of the original Oberon System for the Digilent Xilinx Spartan 3 FPGA Starter Board. The work included a revision of "Project Oberon",[6] identified as Project Oberon (New Edition 2013). The system has since been ported to a Xilinx Spartan 6 FPGA Development Board by Saanlima Electronics. In 2015 Paul Reed collaborated with Victor Yurkovsky in the creation of OberonStation, a Xilinx Spartan 3-based computer designed specifically to run Oberon.
2133) 13.4114275, Open-source software - Wikipedia, the free encyclopedia.txt#0, term: computer, content:Open-source software (OSS) is computer software with its source code made available with a license in which the copyright holder provides the rights to study, change, and distribute the software to anyone and for any purpose.[1] Open-source software may be developed in a collaborative public manner. Open-source software is the most prominent example of open-source development.[2]
2134) 13.4114275, OpenGL - Wikipedia, the free encyclopedia.txt#21, term: computer, content:Microsoft released Direct3D in 1995, which eventually became the main competitor of OpenGL. On December 17, 1997,[17] Microsoft and SGI initiated the Fahrenheit project, which was a joint effort with the goal of unifying the OpenGL and Direct3D interfaces (and adding a scene-graph API too). In 1998, Hewlett-Packard joined the project.[18] It initially showed some promise of bringing order to the world of interactive 3D computer graphics APIs, but on account of financial constraints at SGI, strategic reasons at Microsoft, and general lack of industry support, it was abandoned in 1999.[19]
2135) 13.4114275, Operating system - Wikipedia, the free encyclopedia.txt#6, term: computer, content:A distributed operating system manages a group of distinct computers and makes them appear to be a single computer. The development of networked computers that could be linked and communicate with each other gave rise to distributed computing. Distributed computations are carried out on more than one machine. When computers in a group work in cooperation, they form a distributed system.[4]
2136) 13.4114275, Operating system - Wikipedia, the free encyclopedia.txt#14, term: computer, content:Later machines came with libraries of programs, which would be linked to a user's program to assist in operations such as input and output and generating computer code from human-readable symbolic code. This was the genesis of the modern-day operating system. However, machines still ran a single job at a time. At Cambridge University in England the job queue was at one time a washing line (clothes line) from which tapes were hung with different colored clothes-pegs to indicate job-priority.[citation needed]
2137) 13.4114275, Operating system - Wikipedia, the free encyclopedia.txt#22, term: computer, content:UNIVAC, the first commercial computer manufacturer, produced a series of EXEC operating systems[citation needed]. Like all early main-frame systems, this batch-oriented system managed magnetic drums, disks, card readers and line printers. In the 1970s, UNIVAC produced the Real-Time Basic (RTB) system to support large-scale time sharing, also patterned after the Dartmouth BC system.
2138) 13.4114275, Operating system - Wikipedia, the free encyclopedia.txt#25, term: computer, content:From the late 1960s through the late 1970s, several hardware capabilities evolved that allowed similar or ported software to run on more than one system. Early systems had utilized microprogramming to implement features on their systems in order to permit different underlying computer architectures to appear to be the same as others in a series. In fact, most 360s after the 360/40 (except the 360/165 and 360/168) were microprogrammed implementations.
2139) 13.4114275, Operating system - Wikipedia, the free encyclopedia.txt#35, term: computer, content:A subgroup of the Unix family is the Berkeley Software Distribution family, which includes FreeBSD, NetBSD, and OpenBSD. These operating systems are most commonly found on webservers, although they can also function as a personal computer OS. The Internet owes much of its existence to BSD, as many of the protocols now commonly used by computers to connect, send and receive data over a network were widely implemented and refined in BSD. The World Wide Web was also first demonstrated on a number of computers running an OS based on BSD called NeXTSTEP.
2140) 13.4114275, Operating system - Wikipedia, the free encyclopedia.txt#54, term: computer, content:When an interrupt is received, the computer's hardware automatically suspends whatever program is currently running, saves its status, and runs computer code previously associated with the interrupt; this is analogous to placing a bookmark in a book in response to a phone call. In modern operating systems, interrupts are handled by the operating system's kernel. Interrupts may come from either the computer's hardware or the running program.
2141) 13.4114275, Operating system - Wikipedia, the free encyclopedia.txt#100, term: computer, content:Many computer operating systems allow the user to install or create any user interface they desire. The XWindow System in conjunction with GNOME or KDE Plasma Desktop is a commonly found setup on most Unix and Unix-like (BSD, Linux, Solaris) systems. A number of Windows shell replacements have been released for Microsoft Windows, which offer alternatives to the included Windows shell, but the shell itself cannot be separated from Windows.
2142) 13.4114275, Operating system - Wikipedia, the free encyclopedia.txt#108, term: computer, content:In some cases, hobby development is in support of a "homebrew" computing device, for example, a simple single-board computer powered by a 6502 microprocessor. Or, development may be for an architecture already in widespread use. Operating system development may come from entirely new concepts, or may commence by modeling an existing operating system. In either case, the hobbyist is his/her own developer, or may interact with a small and sometimes unstructured group of individuals who have like interests.
2143) 13.4114275, Optical disc - Wikipedia, the free encyclopedia.txt#4, term: computer, content:For computer data backup and physical data transfer, optical discs such as CDs and DVDs are gradually being replaced with faster, smaller solid-state devices, especially the USB flash drive.[citation needed] This trend is expected to continue as USB flash drives continue to increase in capacity and drop in price.[citation needed] Additionally, music purchased or shared over the Internet has significantly reduced the number of audio CDs sold annually.
2144) 13.4114275, Optical disc - Wikipedia, the free encyclopedia.txt#13, term: computer, content:Initially, optical discs were used to store music and computer software. The Laserdisc format stored analog video signals for the distribution of home video, but commercially lost to the VHS videocassette format, due mainly to its high cost and non-re-recordability; other first-generation disc formats were designed only to store digital data and were not initially capable of use as a digital video medium.
2145) 13.4114275, Optical disc drive - Wikipedia, the free encyclopedia.txt#24, term: computer, content:Most internal drives for personal computers, servers and workstations are designed to fit in a standard 5.25" drive bay and connect to their host via an ATA or SATA interface. Additionally, there may be digital and analog outputs for audio. The outputs may be connected via a header cable to the sound card or the motherboard. At one time, computer software resembling cd players controlled playback of the CD. Today the information is extracted from the disc as data, to be played back or converted to other file formats.
2146) 13.4114275, Optical disc drive - Wikipedia, the free encyclopedia.txt#27, term: computer, content:When the optical disc drive was first developed, it was not easy to add to computer systems. Some computers such as the IBM PS/2 were standardizing on the 3.5" floppy and 3.5" hard disk, and did not include a place for a large internal device. Also IBM PCs and clones at first only included a single (parallel) ATA drive interface, which by the time the CDROM was introduced, was already being used to support two hard drives. Early laptops simply had no built-in high-speed interface for supporting an external storage device.
2147) 13.4114275, OS X - Wikipedia, the free encyclopedia.txt#75, term: computer, content:Software Updates consist of incremental updates of the Mac OS and its applications, Security Updates, device drivers and firmware updates. All software updates require the user to enter their administrative password, as with all consequential system changes. Some updates require a system restart. Starting with OS X 10.5, updates that require a reboot log out the user prior to installation and automatically restart the computer when complete; in earlier versions, the updates are installed, but critical files are not replaced until the next system startup.
2148) 13.4114275, Pascal (programming language) - Wikipedia, the free encyclopedia.txt#10, term: computer, content:The first successful port of the CDC Pascal compiler to another mainframe was completed by Welsh and Quinn at the Queen's University of Belfast (QUB) in 1972. The target was the ICL 1900 series. This compiler in turn was the parent of the Pascal compiler for the Information Computer Systems (ICS) Multum minicomputer. The Multum port was developed with a view to using Pascal as a systems programming language by Findlay, Cupples, Cavouras and Davis, working at the Department of Computing Science in Glasgow University. It is thought that Multum Pascal, which was completed in the summer of 1973, may have been the first 16-bit implementation.
2149) 13.4114275, PC speaker - Wikipedia, the free encyclopedia.txt#0, term: computer, content:A PC speaker is a loudspeaker built into most IBM PC compatible computers. The first IBM Personal Computer, model 5150, employed a standard 2.25inch magnetic driven speaker.[1] More recent computers use a piezoelectric speaker instead.[2] The speaker allows software and firmware to provide auditory feedback to a user, such as to report a hardware fault. A PC speaker generates waveforms using the programmable interval timer.[3]
2150) 13.4114275, PC speaker - Wikipedia, the free encyclopedia.txt#2, term: computer, content:The PC speaker was often used in very innovative ways to create the impression of polyphonic music or sound effects within computer games of its era, such as the LucasArts series of adventure games from the mid-1990s, using swift arpeggios.[citation needed] Several games such as Space Hulk and Pinball Fantasies were noted for their elaborate sound effects; Space Hulk in particular even had full speech.
2151) 13.4114275, PC speaker - Wikipedia, the free encyclopedia.txt#5, term: computer, content:In some applications, the PC speaker is affixed directly to the computer's motherboard; in others, including the first IBM Personal Computer, the speaker is attached by wire to a connector on the motherboard. Some PC cases come with a PC speaker preinstalled. A wired PC speaker connector may have a two-, three-, or four-pin configuration, and either two or three wires. The female connector of the speaker connects to pin headers on the motherboard, which are sometimes labeled SPEAKER or SPKR.
2152) 13.4114275, PDP-8 - Wikipedia, the free encyclopedia.txt#3, term: computer, content:Later systems (the PDP-8/I and /L, the PDP-8/E, /F, and /M, and the PDP-8/A) returned to a faster, fully parallel implementation but used much less costly transistor-transistor logic (TTL) MSI logic. Most surviving PDP-8s are from this era. The PDP-8/E is common, and well-regarded because so many types of I/O devices were available for it. It was often configured as a general-purpose computer.
2153) 13.4114275, PDP-8 - Wikipedia, the free encyclopedia.txt#10, term: computer, content:Eventually, most machine-language programming came to be generated by compilers and report generators. The reduced instruction set computer returned full-circle to the PDP-8's emphasis on a simple instruction set and achieving multiple actions in a single instruction cycle, in order to maximize execution speed, although the newer computers had much longer instruction words.
2154) 13.4114275, PDP-8 - Wikipedia, the free encyclopedia.txt#21, term: computer, content:Several software simulations of a PDP-8 are available on the Internet, as well as open source hardware re-implementations.[15][16] The best of these correctly execute DEC's operating systems and diagnostic software. The software simulations often simulate late-model PDP-8s with all possible peripherals. Even these use only a tiny fraction of the capacity of a modern personal computer.
2155) 13.4114275, Pentium - Wikipedia, the free encyclopedia.txt#5, term: computer, content:Following Intel's previous series of 8086, 80186, 80286, 80386, and 80486 microprocessors, the company's first P5-based microprocessor was released as the original Intel Pentium on March 22, 1993. Marketing firm Lexicon Branding was hired to coin a name for the new processor. The suffix -ium was chosen as it could connote a fundamental ingredient of a computer, like a chemical element,[4] while the prefix pent- could refer to the fifth generation of x86.[3]
2156) 13.4114275, Pentium FDIV bug - Wikipedia, the free encyclopedia.txt#0, term: computer, content:The Pentium FDIV bug was a computer bug that affected the floating point unit (FPU) of the early Intel Pentium processors. Because of the bug, the processor could return incorrect decimal results during complex mathematical calculations. Discovered in 1994 by Professor Thomas R. Nicely at Lynchburg College,[1] Intel attributed the error to missing entries in the lookup table used by the floating-point division circuitry.[2]
2157) 13.4114275, Personal computer - Wikipedia, the free encyclopedia.txt#3, term: computer, content:Since the early 1990s, Microsoft operating systems and Intel hardware dominated much of the personal computer market, first with MS-DOS and then with Windows. Popular alternatives to Microsoft's Windows operating systems include Apple's OS X and free open-source Unix-like operating systems such as Linux and BSD. AMD provides the major alternative to Intel's processors. ARM architecture processors now outnumber Intel's (and compatibles) in smartphones and tablets, that are also personal computers, outnumbering the traditional kind.
2158) 13.4114275, Personal computer - Wikipedia, the free encyclopedia.txt#5, term: computer, content:NASA bought at least ten Programma 101s and used them for the calculations for the 1969 Apollo 11 Moon landing. The ABC Network used the Programma 101 to predict the presidential election of 1968, and the U.S. military used the machine to plan their operations in the Vietnam War. The Programma 101 was used in schools, hospitals, government offices. This marked the beginning of the era of the personal computer.
2159) 13.4114275, Personal computer - Wikipedia, the free encyclopedia.txt#7, term: computer, content:The Soviet MIR series of computers was developed from 1965 to 1969 in a group headed by Victor Glushkov. It was designed as a relatively small-scale computer for use in engineering and scientific applications and contained a hardware implementation of a high-level programming language. Another innovative feature for that time was the user interface combining a keyboard with a monitor and light pen for correcting texts and drawing on screen.[4]
2160) 13.4114275, Personal computer - Wikipedia, the free encyclopedia.txt#8, term: computer, content:In what was later to be called the Mother of All Demos, SRI researcher Douglas Engelbart in 1968 gave a preview of what would become the staples of daily working life in the 21st century: e-mail, hypertext, word processing, video conferencing and the mouse. The demonstration required technical support staff and a mainframe time-sharing computer that were far too costly for individual business use at the time.
2161) 13.4114275, Personal computer - Wikipedia, the free encyclopedia.txt#10, term: computer, content:Early personal computersgenerally called microcomputerswere often sold in a kit form and in limited volumes, and were of interest mostly to hobbyists and technicians. Minimal programming was done with toggle switches to enter instructions, and output was provided by front panel lamps. Practical use required adding peripherals such as keyboards, computer displays, disk drives, and printers. Micral N was the earliest commercial, non-kit microcomputer based on a microprocessor, the Intel 8008. It was built starting in 1972 and about 90,000 units were sold.
2162) 13.4114275, Personal computer - Wikipedia, the free encyclopedia.txt#14, term: computer, content:The first successfully mass marketed personal computer was the Commodore PET introduced in January 1977, but back-ordered and not available until later in the year.[8] At the same time, the Apple II (usually referred to as the "Apple") was introduced[9] (June 1977), and the TRS-80 from Tandy Corporation / Tandy Radio Shack in summer 1977, delivered in September in a small number. Mass-market ready-assembled computers allowed a wider range of people to use computers, focusing more on software applications and less on development of the processor hardware.
2163) 13.4114275, Personal computer - Wikipedia, the free encyclopedia.txt#17, term: computer, content:Somewhat larger and more expensive systems (for example, running CP/M), or sometimes a home computer with additional interfaces and devices, although still low-cost compared with minicomputers and mainframes, were aimed at office and small business use, typically using "high resolution" monitors capable of at least 80 column text display, and often no graphical or color drawing capability.
2164) 13.4114275, Personal computer - Wikipedia, the free encyclopedia.txt#37, term: computer, content:Single-unit PCs (also known as all-in-one PCs) are a subtype of desktop computers that combine the monitor and case of the computer within a single unit. The monitor often utilizes a touchscreen as an optional method of user input, but separate keyboards and mice are normally still included. The inner components of the PC are often located directly behind the monitor and many of such PCs are built similarly to laptops.
2165) 13.4114275, Personal computer - Wikipedia, the free encyclopedia.txt#43, term: computer, content:A subtype of notebooks, called subnotebook, has most of the features of a standard laptop computer, but with smaller physical dimensions. Subnotebooks are larger than hand-held computers, and usually run full versions of desktop or laptop operating systems. Ultra-Mobile PCs (UMPC) are usually considered subnotebooks, or more specifically, subnotebook tablet PCs, which are described below. Netbooks are sometimes considered to belong to this category, though they are sometimes separated into a category of their own (see below).
2166) 13.4114275, Personal computer - Wikipedia, the free encyclopedia.txt#45, term: computer, content:Netbooks, also called mini notebooks or subnotebooks, are a subgroup of laptops[57] acting as a category of small, lightweight and inexpensive laptop computers suited for general computing tasks and accessing web-based applications. They are often marketed as "companion devices", with an intention to augment other ways in which a user can access computer resources.[57] Walt Mossberg called them a "relatively new category of small, light, minimalist and cheap laptops."[58] By August 2009, CNET called netbooks "nothing more than smaller, cheaper notebooks."[57]
2167) 13.4114275, Personal computer - Wikipedia, the free encyclopedia.txt#61, term: computer, content:The power supply unit (PSU) converts general-purpose mains AC electricity to direct current (DC) for the other components of the computer. The rated output capacity of a PSU should usually be about 40% greater than the calculated system power consumption needs obtained by adding up all the system components. This protects against overloading the supply, and guards against performance degradation.
2168) 13.4114275, Personal computer - Wikipedia, the free encyclopedia.txt#62, term: computer, content:The central processing unit, or CPU, is a part of a computer that executes instructions of a software program. In newer PCs, the CPU contains over a million transistors in one integrated circuit chip called the microprocessor. In most cases, the microprocessor plugs directly into the motherboard. The chip generates so much heat that the PC builder is required to attach a special cooling device to its surface; thus, modern CPUs are equipped with a fan attached via heat sink.
2169) 13.4114275, Personal computer - Wikipedia, the free encyclopedia.txt#78, term: computer, content:A computer mouse is a small handheld device that users hold and slide across a flat surface, pointing at various elements of a graphical user interface with an on-screen cursor, and selecting and moving objects using the mouse buttons. Almost all modern personal computers include a mouse; it may be plugged into a computer's rear mouse socket, or as a USB device, or, more recently, may be connected wirelessly via an USB dongle or Bluetooth link.
2170) 13.4114275, Personal computer - Wikipedia, the free encyclopedia.txt#86, term: computer, content:A USB flash drive performs much of the data transfer and backup functions formerly done with floppy drives, Zip disks and other devices. Mainstream operating systems for personal computers provide built-in support for USB flash drives, allowing interchange even between computers with different processors and operating systems. The compact size and lack of moving parts or dirt-sensitive media, combined with low cost and high capacity, have made USB flash drives a popular and useful accessory for any personal computer user.
2171) 13.4114275, Personal computer - Wikipedia, the free encyclopedia.txt#96, term: computer, content:Linux is a family of Unix-like computer operating systems. Linux is one of the most prominent examples of free software and open source development: typically all underlying source code can be freely modified, used, and redistributed by anyone.[73] The name "Linux" refers to the Linux kernel, started in 1991 by Linus Torvalds. The system's utilities and libraries usually come from the GNU operating system, announced in 1983 by Richard Stallman. The GNU contribution is the basis for the alternative name GNU/Linux.[74]
2172) 13.4114275, Personal digital assistant - Wikipedia, the free encyclopedia.txt#0, term: computer, content:A personal digital assistant (PDA), also known as a handheld PC, or personal data assistant,[1][2] is a mobile device that functions as a personal information manager. The term evolved from Personal Desktop Assistant, a software term for an application that prompts or prods the user of a computer with suggestions or provides quick reference to contacts and other lists. PDAs were largely discontinued in the early 2010s after the widespread adoption of highly capable smartphones, in particular those based on iOS and Android.[3]
2173) 13.4114275, Pierre Jaquet-Droz - Wikipedia, the free encyclopedia.txt#3, term: computer, content:Some consider these devices to be the oldest examples of the computer. The Writer, a mechanical boy who writes with a quill pen upon paper with real ink, has an input device to set tabs, defining individual letters written by the boy, that form a programmable memory. 40 cams that represent the read-only programme. The work of Pierre Jaquet-Droz predates that of Charles Babbage by decades.
2174) 13.4114275, Platform game - Wikipedia, the free encyclopedia.txt#31, term: computer, content:1986 saw the release of the sequel to forward-scrolling platformer Antarctic Adventure called Penguin Adventure, which was designed by Hideo Kojima.[51] It included more action game elements, a greater variety of levels, RPG elements such as upgrading equipment,[52] and multiple endings.[53] Trailblazer, released to various computer systems in 1986, used a simple line scroll effect to create a forward scrolling pseudo-3D play field where players manipulated a bouncing ball to leap over obstacles and pitfalls.
2175) 13.4114275, Portable computer - Wikipedia, the free encyclopedia.txt#13, term: computer, content:Portable computers have been increasing in popularity over the past decade, as they do not restrict the user in terms of mobility as a desktop computer would. Wireless access to the Internet, extended battery life and more comfortable ergonomics have been factors driving this increase in popularity. All-in-One PCs such as the iMac can also be considered portable computers and often have handles built into the case.
2176) 13.4114275, PowerPC - Wikipedia, the free encyclopedia.txt#6, term: computer, content:In 1991, the PowerPC was just one facet of a larger alliance among these three companies. At the time, most of the personal computer industry was shipping systems based on the Intel 80386 and 80486 chips, which had a CISC architecture, and development of the Pentium processor was well underway. The PowerPC chip was one of several joint ventures involving the three, in their efforts to counter the growing Microsoft-Intel dominance of personal computing.
2177) 13.4114275, PowerPC - Wikipedia, the free encyclopedia.txt#11, term: computer, content:When the first PowerPC products reached the market, they were met with enthusiasm. In addition to Apple, both IBM and the Motorola Computer Group offered systems built around the processors. Microsoft released Windows NT 3.51 for the architecture, which was used in Motorola's PowerPC servers, and Sun Microsystems offered a version of its Solaris OS. IBM ported its AIX Unix and planned a release of OS/2. Throughout the mid-1990s, PowerPC processors achieved benchmark test scores that matched or exceeded those of the fastest x86 CPUs.
2178) 13.4114275, PowerPC - Wikipedia, the free encyclopedia.txt#26, term: computer, content:Mercury Computer Systems and Matrox ran the PowerPC in little-endian mode. This was done so that PowerPC devices serving as co-processors on PCI boards could share data structures with host computers based on x86. Both PCI and x86 are little-endian. OS/2 and Windows NT for PowerPC ran the processor in little-endian mode while Solaris, AIX and Linux ran in big endian.[7]
2179) 13.4114275, Presentation program - Wikipedia, the free encyclopedia.txt#1, term: computer, content:Early[when?] presentation graphics software ran on computer workstations, such as those manufactured by Trollman, Genigraphics, Autographix, and Dicomed. It became quite easy[dubious  discuss] to make last-minute changes compared to traditional typesetting and pasteup. It was also a lot easier to produce a large number of slides in a small amount of time. However, these workstations also required skilled operators, and a single workstation represented an investment of $50,000 to $200,000 (in 1979 dollars).
2180) 13.4114275, Printer (computing) - Wikipedia, the free encyclopedia.txt#5, term: computer, content:Personal printers are primarily designed to support individual users, and may be connected to only a single computer. These printers are designed for low-volume, short-turnaround print jobs, requiring minimal setup time to produce a hard copy of a given document. However, they are generally slow devices ranging from 6 to around 25 pages per minute (ppm), and the cost per page is relatively high. However, this is offset by the on-demand convenience. Some printers can print documents stored on memory cards or from digital cameras and scanners.
2181) 13.4114275, Printer (computing) - Wikipedia, the free encyclopedia.txt#8, term: computer, content:A 3D printer is a device for making a three-dimensional object from a 3D model or other electronic data source through additive processes in which successive layers of material ( including plastics, metals, food, cement, wood, and other materials) are laid down under computer control. It is called a printer by analogy with an inkjet printer which produces a two-dimensional document by a similar process of depositing a layer of ink on paper.
2182) 13.4114275, Printer (computing) - Wikipedia, the free encyclopedia.txt#22, term: computer, content:The common teleprinter could easily be interfaced to the computer and became very popular except for those computers manufactured by IBM. Some models used a "typebox" that was positioned, in the X- and Y-axes, by a mechanism and the selected letter form was struck by a hammer. Others used a type cylinder in a similar way as the Selectric typewriters used their type ball. In either case, the letter form then struck a ribbon to print the letterform. Most teleprinters operated at ten characters per second although a few achieved 15 CPS.
2183) 13.4114275, Printer (computing) - Wikipedia, the free encyclopedia.txt#43, term: computer, content:A monochrome printer can only produce an image consisting of one colour, usually black. A monochrome printer may also be able to produce various tones of that color, such as a grey-scale. A colour printer can produce images of multiple colours. A photo printer is a colour printer that can produce images that mimic the colour range (gamut) and resolution of prints made from photographic film. Many can be used on a standalone basis without a computer, using a memory card or USB connector.
2184) 13.4114275, Processor register - Wikipedia, the free encyclopedia.txt#0, term: computer, content:In computer architecture, a processor register is a quickly accessible location available to a digital processor's central processing unit (CPU). Registers usually consist of a small amount of fast storage, although some registers have specific hardware functions, and may be read-only or write-only. Registers are typically addressed by mechanisms other than main memory, but may in some cases be memory mapped.
2185) 13.4114275, Processor register - Wikipedia, the free encyclopedia.txt#3, term: computer, content:A common property of computer programs is locality of reference, which refers to accessing the same values repeatedly and holding frequently used values in registers to improve performance; this makes fast registers and caches meaningful.[1] Allocating frequently used variables to registers can be critical to a program's performance; this register allocation is performed either by a compiler in the code generation phase, or manually by an assembly language programmer.
2186) 13.4114275, Profiling (computer programming) - Wikipedia, the free encyclopedia.txt#3, term: computer, content:Program analysis tools are extremely important for understanding program behavior. Computer architects need such tools to evaluate how well programs will perform on new architectures. Software writers need tools to analyze their programs and identify critical sections of code. Compiler writers often use such tools to find out how well their instruction scheduling or branch prediction algorithm is performing...
2187) 13.4114275, Programmer - Wikipedia, the free encyclopedia.txt#1, term: computer, content:British countess and mathematician Ada Lovelace is often considered the first computer programmer, as she was the first to publish an algorithm intended for implementation on Charles Babbage's analytical engine, in October 1842, intended for the calculation of Bernoulli numbers.[8] Because Babbage's machine was never completed to a functioning standard in her time, she never saw this algorithm run.
2188) 13.4114275, Programmer - Wikipedia, the free encyclopedia.txt#19, term: computer, content:Enrollment in computer-related degrees in US has dropped recently due to lack of general interests in science and mathematics and also out of an apparent fear that programming will be subject to the same pressures as manufacturing and agriculture careers.[citation needed] This situation has resulted in confusion about whether the US economy is entering a "post-information age" and the nature of US comparative advantages.
2189) 13.4114275, Programming language - Wikipedia, the free encyclopedia.txt#7, term: computer, content:The earliest computers were often programmed without the help of a programming language, by writing programs in absolute machine language. The programs, in decimal or binary form, were read in from punched cards or magnetic tape, or toggled in on switches on the front panel of the computer. Absolute machine languages were later termed first-generation programming languages (1GL).
2190) 13.4114275, Programming language - Wikipedia, the free encyclopedia.txt#10, term: computer, content:John Mauchly's Short Code, proposed in 1949, was one of the first high-level languages ever developed for an electronic computer.[27] Unlike machine code, Short Code statements represented mathematical expressions in understandable form. However, the program had to be translated into machine code every time it ran, making the process much slower than running the equivalent machine code.
2191) 13.4114275, Programming language - Wikipedia, the free encyclopedia.txt#68, term: computer, content:Programs for a computer might be executed in a batch process without human interaction, or a user might type commands in an interactive session of an interpreter. In this case the "commands" are simply programs, whose execution is chained together. When a language can run its commands through an interpreter (such as a Unix shell or other command-line interface), without compiling, it is called a scripting language.[64]
2192) 13.4114275, Punched card - Wikipedia, the free encyclopedia.txt#1, term: computer, content:Punched cards were widely used through much of the 20th century in what became known as the data processing industry, where specialized and increasingly complex unit record machines, organized into data processing systems, used punched cards for data input, output, and storage.[2] Many early digital computers used punched cards, often prepared using keypunch machines, as the primary medium for input of both computer programs and data.
2193) 13.4114275, Punched card - Wikipedia, the free encyclopedia.txt#33, term: computer, content:In Arthur C. Clarke's early short story Rescue Party, the alien explorers find a "... wonderful battery of almost human Hollerith analyzers and the five thousand million punched cards holding all that could be recorded on each man, woman and child on the planet". Writing in 1946, Clarke, like almost all sci-fi authors, had not then foreseen the development and eventual ubiquity of the computer.
2194) 13.4114275, Puzzle video game - Wikipedia, the free encyclopedia.txt#17, term: computer, content:Blockbuster by Alan Griesemer and Stephen Bradshaw (Atari 8-bit, 1981), is a computerized version of the Rubik's Cube puzzle.[17] Snark Hunt (Atari 8-bit, 1982) is a single-player game of logical deduction,[18] a clone of the 1970s Black Box board game. Sokoban, also from 1982, introduced a new mechanic which hadn't previously been seen before computer games. Sokoban has been widely cloned and its core mechanic incorporated into other games.
2195) 13.4114275, QNX - Wikipedia, the free encyclopedia.txt#6, term: computer, content:Gordon Bell and Dan Dodge, students at the University of Waterloo in 1980, both took a standard computer science course in operating system design, in which the students constructed a basic real-time kernel. Both were convinced there was a commercial need for such a system, and moved to the high-tech planned community Kanata, Ontario, to start Quantum Software Systems that year. In 1982, the first version of QUNIX was released for the Intel 8088 CPU. Quantum Software Systems renamed QUNIX to be QNX in 1984 so that trademark infringements would not be challenged.
2196) 13.4114275, QNX - Wikipedia, the free encyclopedia.txt#7, term: computer, content:One of first widespread uses of the QNX real-time OS (RTOS) was in the non-embedded world, when it was selected as the operating system for the Ontario education system's own computer design, the Unisys ICON. Over the years QNX was used mostly for "larger" projects, as its 44k kernel was too large to fit inside the single-chip computers of the era. The system garnered a reputation for reliability[citation needed] and found itself in use running machinery in a number of industrial applications.
2197) 13.4114275, Quantum computing - Wikipedia, the free encyclopedia.txt#45, term: computer, content:In September 2012, Australian researchers at the University of New South Wales said the world's first quantum computer was just 5 to 10 years away, after announcing a global breakthrough enabling manufacture of its memory building blocks. A research team led by Australian engineers created the first working qubit based on a single atom in silicon, invoking the same technological platform that forms the building blocks of modern-day computers.[67] [68]
2198) 13.4114275, Random-access machine - Wikipedia, the free encyclopedia.txt#3, term: computer, content:The concept of a random-access machine (RAM) starts with the simplest model of all, the so-called counter machine model. Two additions move it away from the counter machine, however. The first enhances the machine with the convenience of indirect addressing; the second moves the model toward the more conventional accumulator-based computer with the addition of one or more auxiliary (dedicated) registers, the most common of which is called "the accumulator".
2199) 13.4114275, Random-access memory - Wikipedia, the free encyclopedia.txt#12, term: computer, content:The memory cell is the fundamental building block of computer memory. The memory cell is an electronic circuit that stores one bit of binary information and it must be set to store a logic 1 (high voltage level) and reset to store a logic 0 (low voltage level). Its value is maintained/stored until it is changed by the set/reset process. The value in the memory cell can be accessed by reading it.
2200) 13.4114275, Random-access memory - Wikipedia, the free encyclopedia.txt#19, term: computer, content:In many modern personal computers, the RAM comes in an easily upgraded form of modules called memory modules or DRAM modules about the size of a few sticks of chewing gum. These can quickly be replaced should they become damaged or when changing needs demand more storage capacity. As suggested above, smaller amounts of RAM (mostly SRAM) are also integrated in the CPU and other ICs on the motherboard, as well as in hard-drives, CD-ROMs, and several other parts of the computer system.
2201) 13.4114275, Random-access memory - Wikipedia, the free encyclopedia.txt#28, term: computer, content:The "memory wall" is the growing disparity of speed between CPU and memory outside the CPU chip. An important reason for this disparity is the limited communication bandwidth beyond chip boundaries, which is also referred to as bandwidth wall. From 1986 to 2000, CPU speed improved at an annual rate of 55% while memory speed only improved at 10%. Given these trends, it was expected that memory latency would become an overwhelming bottleneck in computer performance.[11]
2202) 13.4114275, Raúl Rojas - Wikipedia, the free encyclopedia.txt#0, term: computer, content:Ral Rojas Gonzlez (born 1955, in Mexico City) is a professor of Computer Science and Mathematics at the Free University of Berlin and a renowned specialist in artificial neural networks. The FU-Fighters, football-playing robots he helped build, were world champions in 2004 and 2005. He is now leading an autonomous car project called Spirit of Berlin.
2203) 13.4114275, Real-time operating system - Wikipedia, the free encyclopedia.txt#2, term: computer, content:A RTOS has an advanced algorithm for scheduling. Scheduler flexibility enables a wider, computer-system orchestration of process priorities, but a real-time OS is more frequently dedicated to a narrow set of applications. Key factors in a real-time OS are minimal interrupt latency and minimal thread switching latency; a real-time OS is valued more for how quickly or how predictably it can respond than for the amount of work it can perform in a given period of time.[3]
2204) 13.4114275, Real-time operating system - Wikipedia, the free encyclopedia.txt#13, term: computer, content:A multitasking operating system like Unix is poor at real-time tasks. The scheduler gives the highest priority to jobs with the lowest demand on the computer, so there is no way to ensure that a time-critical job will have access to enough resources. Multitasking systems must manage sharing data and hardware resources among multiple tasks. It is usually unsafe for two tasks to access the same specific data or hardware resource simultaneously.[6] There are three common approaches to resolve this problem:
2205) 13.4114275, Register machine - Wikipedia, the free encyclopedia.txt#25, term: computer, content:Except there is a problem: If based on the counter machine chassis this computer-like, von Neumann machine will not be Turing equivalent. It cannot compute everything that is computable. Intrinsically the model is bounded by the size of its (very-) finite state machine's instructions. The counter machine based RASP can compute any primitive recursive function (e.g. multiplication) but not all mu recursive functions (e.g. the Ackermann function ).
2206) 13.4114275, Register machine - Wikipedia, the free encyclopedia.txt#43, term: computer, content:All texts and papers excepting the four starred have been witnessed. These four are written in German and appear as references in Shepherdson-Sturgis (1963) and Elgot-Robinson (1964); Shepherdson-Sturgis (1963) offer a brief discussion of their results in Shepherdson-Sturgis' Appendix A. The terminology of at least one paper (Kaphengst (1959) seems to hark back to the Burke-Goldstine-von Neumann (1946-7) analysis of computer architecture.
2207) 13.4114275, Register-transfer level - Wikipedia, the free encyclopedia.txt#4, term: computer, content:When designing digital integrated circuits with a hardware description language, the designs are usually engineered at a higher level of abstraction than transistor level (logic families) or logic gate level. In HDLs the designer declares the registers (which roughly correspond to variables in computer programming languages), and describes the combination logic by using constructs that are familiar from programming languages such as if-then-else and arithmetic operations. This level is called register-transfer level. The term refers to the fact that RTL focuses on describing the flow of signals between registers.
2208) 13.4114275, Rendering (computer graphics) - Wikipedia, the free encyclopedia.txt#19, term: computer, content:Raycasting is primarily used for realtime simulations, such as those used in 3D computer games and cartoon animations, where detail is not important, or where it is more efficient to manually fake the details in order to obtain better performance in the computational stage. This is usually the case when a large number of frames need to be animated. The resulting surfaces have a characteristic 'flat' appearance when no additional tricks are used, as if objects in the scene were all painted with matte finish.
2209) 13.4114275, Robert Noyce - Wikipedia, the free encyclopedia.txt#0, term: computer, content:Robert Norton Noyce (December 12, 1927  June 3, 1990), nicknamed "the Mayor of Silicon Valley," co-founded Fairchild Semiconductor in 1957 and Intel Corporation in 1968. He is also credited (along with Jack Kilby) with the realization of the first integrated circuit or microchip that fueled the personal computer revolution and gave Silicon Valley its name.[1][nb 1]
2210) 13.4114275, Routing - Wikipedia, the free encyclopedia.txt#16, term: computer, content:In computer networking, the metric is computed by a routing algorithm, and can cover information such as bandwidth, network delay, hop count, path cost, load, MTU (maximum transmission unit), reliability, and communication cost (see e.g. this survey for a list of proposed routing metrics). The routing table stores only the best possible routes, while link-state or topological databases may store all other information as well.
2211) 13.4114275, RS-232 - Wikipedia, the free encyclopedia.txt#6, term: computer, content:Later personal computers (and other devices) started to make use of the standard so that they could connect to existing equipment. For many years, an RS-232-compatible port was a standard feature for serial communications, such as modem connections, on many computers. It remained in widespread use into the late 1990s. In personal computer peripherals, it has largely been supplanted by other interface standards, such as USB. RS-232 is still used to connect older designs of peripherals, industrial equipment (such as PLCs), console ports, and special purpose equipment.
2212) 13.4114275, RS-232 - Wikipedia, the free encyclopedia.txt#24, term: computer, content:The standard recommends the D-subminiature 25-pin connector, but does not make it mandatory. Most devices only implement or use a few of the twenty signals specified in the standard, so connectors and cables with fewer pins are sufficient for most connections, more compact, and less expensive. Personal computer manufacturers replaced the DB-25M connector with the smaller DE-9M connector. This connector, with a different pinout (see Serial port  Pinouts), is prevalent for personal computers and associated devices.
2213) 13.4114275, RS-232 - Wikipedia, the free encyclopedia.txt#32, term: computer, content:Ring Indicator (RI), is a signal sent from the DCE to the DTE device. It indicates to the terminal device that the phone line is ringing. In many computer serial ports, a hardware interrupt is generated when the RI signal changes state. Having support for this hardware interrupt means that a program or operating system can be informed of a change in state of the RI pin, without requiring the software to constantly "poll" the state of the pin. RI does not correspond to another signal that carries similar information the opposite way.
2214) 13.4114275, Ruby (programming language) - Wikipedia, the free encyclopedia.txt#29, term: computer, content:Matsumoto has said that Ruby is designed for programmer productivity and fun, following the principles of good user interface design.[58] At a Google Tech Talk in 2008 Matsumoto further stated, "I hope to see Ruby help every programmer in the world to be productive, and to enjoy programming, and to be happy. That is the primary purpose of Ruby language."[59] He stresses that systems design needs to emphasize human, rather than computer, needs:[60]
2215) 13.4114275, Ruby (programming language) - Wikipedia, the free encyclopedia.txt#30, term: computer, content:Often people, especially computer engineers, focus on the machines. They think, "By doing this, the machine will run fast. By doing this, the machine will run more effectively. By doing this, the machine will something something something." They are focusing on machines. But in fact we need to focus on humans, on how humans care about doing programming or operating the application of the machines. We are the masters. They are the slaves.
2216) 13.4114275, Scripting language - Wikipedia, the free encyclopedia.txt#19, term: computer, content:With the advent of graphical user interfaces, a specialized kind of scripting language emerged for controlling a computer. These languages interact with the same graphic windows, menus, buttons, and so on that a human user would. They do this by simulating the actions of a user. These languages are typically used to automate user actions. Such languages are also called "macros" when control is through simulated key presses or mouse clicks, as well as tapping or pressing on a touch-activated screen.
2217) 13.4114275, SCSI - Wikipedia, the free encyclopedia.txt#3, term: computer, content:Until at least February 1982, ANSI developed the specification as "SASI" and "Shugart Associates System Interface;"[5] however, the committee documenting the standard would not allow it to be named after a company. Almost a full day was devoted to agreeing to name the standard "Small Computer System Interface", which Boucher intended to be pronounced "sexy", but ENDL's[6] Dal Allan pronounced the new acronym as "scuzzy" and that stuck.[4]
2218) 13.4114275, Semi-Automatic Ground Environment - Wikipedia, the free encyclopedia.txt#11, term: computer, content:The Committee then had to consider whether or not such a computer was even possible. Valley was introduced to Jerome Wiesner, associate director of the Research Laboratory of Electronics at MIT. Wiesner noted that the Servomechanisms Laboratory had already begun development of a machine that might be fast enough for the task. This was the Whirlwind I, originally developed for the Office of Naval Research as a general purpose flight simulator that could simulate any current or future aircraft simply by changing its software.[12]
2219) 13.4114275, Semi-Automatic Ground Environment - Wikipedia, the free encyclopedia.txt#33, term: computer, content:The Burroughs 416L SAGE System (ESD Project 416L,[54] Semi Automatic Ground Environment System)[45] was the Cold War network of computer sets and centrals that created the display and control environment (SAGE) for operation of the separate radars[54] and to provide command guidance for ground-controlled interception by air defense aircraft in the "SAGE Defense System"[55] ("Air Defense Weapons System").[37] Burroughs Corporation was the prime contractor for SAGE electronic equipment which included 134 Burroughs AN/FST-2 Coordinate Data Transmitting Sets (CDTS) at radar stations and other sites, the AN/FSQ-7 Combat Direction Central at 23 Direction Centers, and the AN/FSQ-8 Combat Control Central at 8 Combat Centers. The 2 computers of each AN/FSQ-7 together weighing 275 short tons-force (2,450kN)[56][this quote needs a citation] used about  of the DC's 2nd floor space[5] and at ~$50 per instruction had approximately 125,000 "computer instructions support[ing] actual operational air-defense mission" processing.[57] The AN/FSQ-7 at Luke AFB had additional memory (32K total) and was used as a "computer center for all other" DCs.[58] Project 416L was the USAF predecessor of NORAD, SAC, and other military organizations' "Big L" computer systems (e.g., 438L Air Force Intelligence Data Handling System & 496L Space Detection and Tracking System).[59]
2220) 13.4114275, Serious game - Wikipedia, the free encyclopedia.txt#7, term: computer, content:In the 1990s, newer games such as The Incredible Machine and the Dr. Brain series were introduced to challenge kids to think in new ways, apply their current skills, and learn new ones, but these games were unpopular among teachers because it was difficult to map these newer games to their curriculum, especially in a high school setting where in-class time is at a premium. The 1990s also saw the Internet being introduced to schools, which with limited computer resources took precedence over playing games.[8]
2221) 13.4114275, Serious game - Wikipedia, the free encyclopedia.txt#13, term: computer, content:The dynamic nature of virtual environments also causes "active participation by the player" (Mouaheb). Active participation leads to "a fertile ground for the generation of real cognitive conflicts ensuring a personal and solid build of knowledge" (Mouaheb). Another researcher confirms this view "computer games are more engaging, motivating and interesting by virtue of their interaction, rich universes, challenges and safety" (Egenfeldt-Nielsen).
2222) 13.4114275, Serious game - Wikipedia, the free encyclopedia.txt#14, term: computer, content:The combination of all these factors leads to significant benefits "retention increases when using computer games compared to other teaching" (Egenfeldt-Nielsen). The use of reward in a behavioral sense is also a powerful tool in serious games "the video game will ask a question and the player will answer. When students link the question and the answer enough times, reinforced by a reward, learning will occur" (Egenfeldt-Nielsen).
2223) 13.4114275, Server (computing) - Wikipedia, the free encyclopedia.txt#1, term: computer, content:Clientserver systems are today most frequently implemented by (and often identified with) the requestresponse model: a client sends a request to the server, which performs some action and sends a response back to the client, typically with a result or acknowledgement. Designating a computer as "server-class hardware" implies that it is specialized for running servers on it. This often implies that it is more powerful and reliable than standard personal computers, but alternatively, large computing clusters may be composed of many relatively simple, replaceable server components.
2224) 13.4114275, SIMD - Wikipedia, the free encyclopedia.txt#3, term: computer, content:The current era of SIMD processors grew out of the desktop-computer market rather than the supercomputer market. As desktop processors became powerful enough to support real-time gaming and audio/video processing during the 1990s, demand grew for this particular type of computing power, and microprocessor vendors turned to SIMD to meet the demand.[3] Sun Microsystems introduced SIMD integer instructions in its "VIS" instruction set extensions in 1995, in its UltraSPARC I microprocessor. MIPS followed suit with their similar MDMX system.
2225) 13.4114275, SIMD - Wikipedia, the free encyclopedia.txt#14, term: computer, content:Adoption of SIMD systems in personal computer software was at first slow, due to a number of problems. One was that many of the early SIMD instruction sets tended to slow overall performance of the system due to the re-use of existing floating point registers. Other systems, like MMX and 3DNow!, offered support for data types that were not interesting to a wide audience and had expensive context switching instructions to switch between using the FPU and MMX registers. Compilers also often lacked support, requiring programmers to resort to assembly language coding.
2226) 13.4114275, SIMD - Wikipedia, the free encyclopedia.txt#27, term: computer, content:Larger scale commercial SIMD processors are available from ClearSpeed Technology, Ltd. and Stream Processors, Inc. ClearSpeed's CSX600 (2004) has 96 cores each with 2 double-precision floating point units while the CSX700 (2008) has 192. Stream Processors is headed by computer architect Bill Dally. Their Storm-1 processor (2007) contains 80 SIMD cores controlled by a MIPS CPU.
2227) 13.4114275, Slide rule - Wikipedia, the free encyclopedia.txt#0, term: computer, content:The slide rule, also known colloquially in the United States as a slipstick,[1] is a mechanical analog computer.[2][3][4][5][6] The slide rule is used primarily for multiplication and division, and also for functions such as roots, logarithms and trigonometry, but is not normally used for addition or subtraction. Though similar in name and appearance to a standard ruler, the slide rule is not ordinarily used for measuring length or drawing straight lines.
2228) 13.4114275, Slide rule - Wikipedia, the free encyclopedia.txt#36, term: computer, content:In 1952, Swiss watch company Breitling introduced a pilot's wristwatch with an integrated circular slide rule specialized for flight calculations: the Breitling Navitimer. The Navitimer circular rule, referred to by Breitling as a "navigation computer", featured airspeed, rate/time of climb/descent, flight time, distance, and fuel consumption functions, as well as kilometernautical mile and gallonliter fuel amount conversion functions.
2229) 13.4114275, Smartphone - Wikipedia, the free encyclopedia.txt#20, term: computer, content:In 2008, Apple introduced the App Store, a centralized storefront for purchasing new software for iPhone devices.[62][63] iOS can also integrate with Apple's desktop music program iTunes to sync media to a personal computer.[64][65] The dependency on a PC was removed with the introduction of iCloud on later versions of iOS, which provides synchronization of user data via internet servers between multiple devices.[66]
2230) 13.4114275, Smartphone - Wikipedia, the free encyclopedia.txt#37, term: computer, content:The introduction of Apple's App Store for the iPhone and iPod Touch in July 2008 popularized manufacturer-hosted online distribution for third-party applications (software, computer programs) focused on a single platform. Up until that point, smartphone application distribution depended on third-party sources providing applications for multiple platforms, such as GetJar, Handango, Handmark, and PocketGear.
2231) 13.4114275, Software - Wikipedia, the free encyclopedia.txt#15, term: computer, content:Software quality is very important, especially for commercial and system software like Microsoft Office, Microsoft Windows and Linux. If software is faulty (buggy), it can delete a person's work, crash the computer and do other unexpected things. Faults and errors are called "bugs" which are often discovered during alpha and beta testing. Software is often also a victim to what is known as software aging, the progressive performance degradation resulting from a combination of unseen bugs.
2232) 13.4114275, Software - Wikipedia, the free encyclopedia.txt#26, term: computer, content:A person who creates software is called a programmer, software engineer or software developer, terms that all have a similar meaning. More informal terms for programmer also exist such as "coder" and "hacker" although use of the latter word may cause confusion, because it is more often used to mean someone who illegally breaks into computer systems.
2233) 13.4114275, Software bug - Wikipedia, the free encyclopedia.txt#37, term: computer, content:A school of thought popularized by Eric S. Raymond as Linus's Law says that popular open-source software has more chance of having few or no bugs than other software, because "given enough eyeballs, all bugs are shallow".[22] This assertion has been disputed, however: computer security specialist Elias Levy wrote that "it is easy to hide vulnerabilities in complex, little understood and undocumented source code," because, "even if people are reviewing the code, that doesn't mean they're qualified to do so."[23]
2234) 13.4114275, Software engineering - Wikipedia, the free encyclopedia.txt#13, term: computer, content:Legal requirements for the licensing or certification of professional software engineers vary around the world. In the UK, the British Computer Society licenses software engineers and members of the society can also become Chartered Engineers (CEng), while in some areas of Canada, such as Alberta, British Columbia, Ontario,[28] and Quebec, software engineers can hold the Professional Engineer (P.Eng) designation and/or the Information Systems Professional (I.S.P.) designation. In Canada, there is a legal requirement to have P.Eng when one wants to use the title "engineer" or practice "software engineering".
2235) 13.4114275, Software engineering - Wikipedia, the free encyclopedia.txt#15, term: computer, content:The IEEE Computer Society and the ACM, the two main US-based professional organizations of software engineering, publish guides to the profession of software engineering. The IEEE's Guide to the Software Engineering Body of Knowledge - 2004 Version, or SWEBOK, defines the field and describes the knowledge the IEEE expects a practicing software engineer to have. The most current SWEBOK v3 is an updated version and was released in 2014.[30] The IEEE also promulgates a "Software Engineering Code of Ethics".[31]
2236) 13.4114275, Software engineering - Wikipedia, the free encyclopedia.txt#18, term: computer, content:Most software engineers and programmers work 40 hours a week, but about 15 percent of software engineers and 11 percent of programmers worked more than 50 hours a week in 2008. Injuries in these occupations are rare. However, like other workers who spend long periods in front of a computer terminal typing at a keyboard, engineers and programmers are susceptible to eyestrain, back discomfort, and hand and wrist problems such as carpal tunnel syndrome.[34]
2237) 13.4114275, Spreadsheet - Wikipedia, the free encyclopedia.txt#0, term: computer, content:A spreadsheet is an interactive computer application for organization, analysis and storage of data in tabular form. Spreadsheets are developed as computerized simulations of paper accounting worksheets. The program operates on data entered in cells of a table. Each cell may contain either numeric or text data, or the results of formulas that automatically calculate and display a value based on the contents of other cells.
2238) 13.4114275, Spreadsheet - Wikipedia, the free encyclopedia.txt#11, term: computer, content:A spreadsheet program is one of the main components of an office productivity suite, which usually also contains a word processor, a presentation program, and a database management system. Programs within a suite use similar commands for similar functions. Usually sharing data between the components is easier than with a non-integrated collection of functionally equivalent programs. This was particularly an advantage at a time when many personal computer systems used text-mode displays and commands, instead of a graphical user interface.
2239) 13.4114275, Spreadsheet - Wikipedia, the free encyclopedia.txt#20, term: computer, content:The LANPAR system was implemented on GE400 and Honeywell 6000 online timesharing systems enabling users to program remotely via computer terminals and modems. Data could be entered dynamically either by paper tape, specific file access, on line, or even external data bases. Sophisticated mathematical expressions including logical comparisons and "if/then" statements could be used in any cell, and cells could be presented in any order.
2240) 13.4114275, Spreadsheet - Wikipedia, the free encyclopedia.txt#28, term: computer, content:VisiCalc went on to become the first killer app,[17][18] an application that was so compelling, people would buy a particular computer just to use it. VisiCalc was in no small part responsible for the Apple II's success. The program was later ported to a number of other early computers, notably CP/M machines, the Atari 8-bit family and various Commodore platforms. Nevertheless, VisiCalc remains best known as an Apple II program.
2241) 13.4114275, Spreadsheet - Wikipedia, the free encyclopedia.txt#30, term: computer, content:The acceptance of the IBM PC following its introduction in August, 1981, began slowly, because most of the programs available for it were translations from other computer models. Things changed dramatically with the introduction of Lotus 1-2-3 in November, 1982, and release for sale in January, 1983. Since it was written especially for the IBM PC, it had good performance and became the killer app for this PC. Lotus 1-2-3 drove sales of the PC due to the improvements in speed and graphics compared to VisiCalc on the Apple II.[19]
2242) 13.4114275, Spreadsheet - Wikipedia, the free encyclopedia.txt#39, term: computer, content:An array of cells is called a sheet or worksheet. It is analogous to an array of variables in a conventional computer program (although certain unchanging values, once entered, could be considered, by the same analogy, constants). In most implementations, many worksheets may be located within a single spreadsheet. A worksheet is simply a subset of the spreadsheet divided for the sake of clarity. Functionally, the spreadsheet operates as a whole and all cells operate as global variables within the spreadsheet (each variable having 'read' access only except its own containing cell).
2243) 13.4114275, Square root - Wikipedia, the free encyclopedia.txt#20, term: computer, content:Most pocket calculators have a square root key. Computer spreadsheets and other software are also frequently used to calculate square roots. Pocket calculators typically implement efficient routines, such as the Newton's method (frequently with an initial guess of 1), to compute the square root of a positive real number.[12][13] When computing square roots with logarithm tables or slide rules, one can exploit the identity
2244) 13.4114275, Stack machine - Wikipedia, the free encyclopedia.txt#26, term: computer, content:On stack machines, temporary values often get spilled into memory, whereas on machines with many registers these temps usually remain in registers. (However, these values often need to be spilled into "activation frames" at the end of a procedure's definition, basic block, or at the very least, into a memory buffer during interrupt processing). Values spilled to memory add more cache cycles. This spilling effect depends on the number of hidden registers used to buffer top-of-stack values, upon the frequency of nested procedure calls, and upon host computer interrupt processing rates.
2245) 13.4114275, Standard Template Library - Wikipedia, the free encyclopedia.txt#17, term: computer, content:The architecture of STL is largely the creation of Alexander Stepanov. In 1979 he began working out his initial ideas of generic programming and exploring their potential for revolutionizing software development. Although David Musser had developed and advocated some aspects of generic programming already by year 1971, it was limited to a rather specialized area of software development (computer algebra).
2246) 13.4114275, Stored-program computer - Wikipedia, the free encyclopedia.txt#2, term: computer, content:Stored-program computer is sometimes used as a synonym for von Neumann architecture,[7][8] however Professor Jack Copeland considers that it is "historically inappropriate, to refer to electronic stored-program digital computers as 'von Neumann machines'".[9] Hennessy and Patterson write that the early Harvard machines were regarded as "reactionary by the advocates of stored-program computers".[10]
2247) 13.4114275, Strategy game - Wikipedia, the free encyclopedia.txt#10, term: computer, content:Strategy video games are categorized based on whether they offer the continuous gameplay of real-time strategy (RTS), or the discrete phases of turn-based strategy (TBS).[4] Often the computer is expected to emulate a strategically thinking "side" similar to that of a human player (such as directing armies and constructing buildings), or emulate the "instinctive" actions of individual units that would be too tedious for a player to administer (such as for a peasant to run away when attacked, as opposed to standing still until otherwise ordered by the player); hence there is an emphasis on artificial intelligence.
2248) 13.4114275, Subroutine - Wikipedia, the free encyclopedia.txt#0, term: computer, content:In computer programming, a subroutine is a sequence of program instructions that perform a specific task, packaged as a unit. This unit can then be used in programs wherever that particular task should be performed. Subprograms may be defined within programs, or separately in libraries that can be used by multiple programs. In different programming languages, a subroutine may be called a procedure, a function, a routine, a method, or a subprogram. The generic term callable unit is sometimes used.[1]
2249) 13.4114275, Supercomputer - Wikipedia, the free encyclopedia.txt#7, term: computer, content:The Atlas was a joint venture between Ferranti and the Manchester University and was designed to operate at processing speeds approaching onemicrosecond per instruction, about onemillion instructions per second.[11] The first Atlas was officially commissioned on 7 December 1962 as one of the world's first supercomputers   considered to be the most powerful computer in the world at that time by a considerable margin, and equivalent to four IBM 7094s.[12]
2250) 13.4114275, Supercomputer - Wikipedia, the free encyclopedia.txt#9, term: computer, content:Cray left CDC in 1972 to form his own company, Cray Research.[15] Four years after leaving CDC, Cray delivered the 80MHz Cray 1 in 1976, and it became one of the most successful supercomputers in history.[18][19] The Cray-2 released in 1985 was an 8 processor liquid cooled computer and Fluorinert was pumped through it as it operated. It performed at 1.9 gigaflops and was the world's second fastest after M-13 supercomputer in Moscow .[20]
2251) 13.4114275, Supercomputer - Wikipedia, the free encyclopedia.txt#12, term: computer, content:While the supercomputers of the 1970s used only a few processors, in the 1990s, machines with thousands of processors began to appear and by the end of the 20th century, massively parallel supercomputers with tens of thousands of "off-the-shelf" processors were the norm. Supercomputers of the 21st century can use over 100,000 processors (some being graphic units) connected by fast connections.[3][4] The Connection Machine CM-5 supercomputer is a massively parallel processing computer capable of many billions of arithmetic operations per second.[27]
2252) 13.4114275, Supercomputer - Wikipedia, the free encyclopedia.txt#22, term: computer, content:The energy efficiency of computer systems is generally measured in terms of "FLOPS per watt". In 2008, IBM's Roadrunner operated at 3.76MFLOPS/W.[59][60] In November 2010, the Blue Gene/Q reached 1,684MFLOPS/W.[61][62] In June 2011 the top 2 spots on the Green 500 list were occupied by Blue Gene machines in New York (one achieving 2097MFLOPS/W) with the DEGIMA cluster in Nagasaki placing third with 1375MFLOPS/W.[63]
2253) 13.4114275, Supercomputer - Wikipedia, the free encyclopedia.txt#31, term: computer, content:Opportunistic Supercomputing is a form of networked grid computing whereby a "super virtual computer" of many loosely coupled volunteer computing machines performs very large computing tasks. Grid computing has been applied to a number of large-scale embarrassingly parallel problems that require supercomputing performance scales. However, basic grid and cloud computing approaches that rely on volunteer computing can not handle traditional supercomputing tasks such as fluid dynamic simulations.
2254) 13.4114275, Supercomputer - Wikipedia, the free encyclopedia.txt#36, term: computer, content:Supercomputers generally aim for the maximum in capability computing rather than capacity computing. Capability computing is typically thought of as using the maximum computing power to solve a single large problem in the shortest amount of time. Often a capability system is able to solve a problem of a size or complexity that no other computer can, e.g. a very complex weather simulation application.[81]
2255) 13.4114275, Supercomputer - Wikipedia, the free encyclopedia.txt#43, term: computer, content:The IBM Blue Gene/P computer has been used to simulate a number of artificial neurons equivalent to approximately one percent of a human cerebral cortex, containing 1.6 billion neurons with approximately 9 trillion connections. The same research group also succeeded in using a supercomputer to simulate a number of artificial neurons equivalent to the entirety of a rat's brain.[91]
2256) 13.4114275, Tablet computer - Wikipedia, the free encyclopedia.txt#3, term: computer, content:The tablet computer and its associated operating system began with the development of pen computing.[8] Electrical devices with data input and output on a flat information display existed as early as 1888 with the telautograph,[9] which used a sheet of paper as display and a pen attached to electromechanical actuators. Throughout the 20th century devices with these characteristics have been imagined and created whether as blueprints, prototypes, or commercial products. In addition to many academic and research systems, several companies released commercial products in the 1980s, with various input/output types tried out:
2257) 13.4114275, Tablet computer - Wikipedia, the free encyclopedia.txt#15, term: computer, content:In 2002, Microsoft attempted to define the Microsoft Tablet PC[35] as a mobile computer for field work in business,[36] though their devices failed, mainly due to pricing and usability decisions that limited them to their original purpose - such as the existing devices being too heavy to be held with one hand for extended periods, and having legacy applications created for desktop interfaces and not well adapted to the slate format.[37]
2258) 13.4114275, Tablet computer - Wikipedia, the free encyclopedia.txt#62, term: computer, content:Previous to the iPad, Axiotron introduced[111] an aftermarket, heavily modified Apple MacBook called Modbook, a Mac OS X-based tablet personal computer. The Modbook uses Apple's Inkwell for handwriting and gesture recognition, and uses digitization hardware from Wacom. To get Mac OS X to talk to the digitizer on the integrated tablet, the Modbook is supplied with a third-party driver called TabletMagic; Wacom does not provide driver support for this device. Another predecessor to the iPad was the Apple MessagePad introduced in 1993.
2259) 13.4114275, Tablet computer - Wikipedia, the free encyclopedia.txt#65, term: computer, content:On June 18, 2012, Microsoft launched the Microsoft Surface tablet (initially named Surface RT upon release), the first computer[123] in the company's history. Surface runs Windows RT and comes with a Tegra 3 SoC, one kickstand position, USB 2.0 port, microSD card slot to expand storage and one-megapixel cameras (front and back). It has been succeeded by Surface 2 in 2013, which was not restocked in the Microsoft online store as of January 22, 2015.[124]
2260) 13.4114275, Tablet computer - Wikipedia, the free encyclopedia.txt#79, term: computer, content:The BlackBerry PlayBook is a tablet computer announced in September 2010 that runs the BlackBerry Tablet OS.[134] The OS is based on the QNX system that Research in Motion acquired in early 2010. Delivery to developers and enterprise customers was expected in October 2010. The BlackBerry PlayBook was officially released to US and Canadian consumers on April 19, 2011. As of 2014, Playbook is not available on sale on any Blackberry websites. The OS though continues on its smartphones.
2261) 13.4114275, Telecommunication - Wikipedia, the free encyclopedia.txt#63, term: computer, content:Despite the growth of the Internet, the characteristics of local area networks (LANs)--computer networks that do not extend beyond a few kilometersremain distinct. This is because networks on this scale do not require all the features associated with larger networks and are often more cost-effective and efficient without them. When they are not connected with the Internet, they also have the advantages of privacy and security. However, purposefully lacking a direct connection to the Internet does not provide assured protection from hackers, military forces, or economic powers. These threats exist if there are any methods for connecting remotely to the LAN.
2262) 13.4114275, Teleprinter - Wikipedia, the free encyclopedia.txt#0, term: computer, content:A teleprinter (teletypewriter, Teletype or TTY) is an electromechanical typewriter that can be used to send and receive typed messages from point to point and point to multipoint over various types of communications channels. They were adapted to provide a user interface to early mainframe computers and minicomputers, sending typed data to the computer and printing the response. Some models could also be used to create punched tape for data storage (either from typed input or from data received from a remote source) and to read back such tape for local printing or transmission.
2263) 13.4114275, Teleprinter - Wikipedia, the free encyclopedia.txt#46, term: computer, content:In the 1980s, packet radio became the most common form of digital communications used in amateur radio. Soon, advanced multimode electronic interfaces such as the AEA PK-232 were developed, which could send and receive not only packet, but various other modulation types including Baudot. This made it possible for a home or laptop computer to replace teleprinters, saving money, complexity, space and the massive amount of paper which mechanical machines used.
2264) 13.4114275, Texas Instruments - Wikipedia, the free encyclopedia.txt#30, term: computer, content:Texas Instruments invented the hand-held calculator (a prototype called "Cal Tech") in 1967 and the single-chip microcomputer in 1971, was assigned the first patent on a single-chip microprocessor (invented by Gary Boone) on September 4, 1973.[29] This was disputed by Gilbert Hyatt, formerly of the Micro Computer Company, in August 1990 when he was awarded a patent superseding TI's. This was over-turned on June 19, 1996 in favor of TI[30] (note: Intel is usually given credit with Texas Instruments for the almost-simultaneous invention of the microprocessor).
2265) 13.4114275, Texas Instruments - Wikipedia, the free encyclopedia.txt#38, term: computer, content:TI sold its software division along with its main product such as the IEF to Sterling Software in 1997. It is now part of Computer Associates. TI still owns small pieces of software though such as the software for calculators like TI Interactive!. TI also creates a significant amount of target software for its digital signal processors, along with host-based tools for creating DSP applications.
2266) 13.4114275, Text editor - Wikipedia, the free encyclopedia.txt#5, term: computer, content:Before text editors existed, computer text was punched into cards with keypunch machines. Physical boxes of these thin cardboard cards were then inserted into a card-reader. Magnetic tape and disk "card-image" files created from such card decks often had no line-separation characters at all, and assumed fixed-length 80-character records. An alternative to cards was punched paper tape. It could be created by some teleprinters (such as the Teletype), which used special characters to indicate ends of records.
2267) 13.4114275, Text-based (computing) - Wikipedia, the free encyclopedia.txt#1, term: computer, content:Before the 1980s, most computers were text-based. The operator used the keyboard as the main input device to type in necessary commands into the terminal that could only display text on a low-resolution monochrome video monitor. The majority of end-user software was also written in text-based mode during this time. During this era, operating a computer was considered to be a challenging task because of the complexity of the text-based environment.
2268) 13.4114275, Text-based (computing) - Wikipedia, the free encyclopedia.txt#9, term: computer, content:Many users may not find an application with a text-based interface very user-friendly. This is especially true for beginning computer users. While the user may learn how to operate the software by simply playing around or navigating through given options, a text-based system usually requires users to have a more detailed understanding of the commands. Many text-based applications have a menu or help system that shows the user some (or all) of the available options of the software.
2269) 13.4114275, Theory of computation - Wikipedia, the free encyclopedia.txt#6, term: computer, content:Computability theory deals primarily with the question of the extent to which a problem is solvable on a computer. The statement that the halting problem cannot be solved by a Turing machine[7] is one of the most important results in computability theory, as it is an example of a concrete problem that is both easy to formulate and impossible to solve using a Turing machine. Much of computability theory builds on the halting problem result.
2270) 13.4114275, Theory of computation - Wikipedia, the free encyclopedia.txt#11, term: computer, content:To simplify this problem, computer scientists have adopted Big O notation, which allows functions to be compared in a way that ensures that particular aspects of a machine's construction do not need to be considered, but rather only the asymptotic behavior as problems become large. So in our previous example we might say that the problem requires     O ( n )   {\displaystyle O(n)}   steps to solve.
2271) 13.4114275, Theory of computation - Wikipedia, the free encyclopedia.txt#12, term: computer, content:Perhaps the most important open problem in all of computer science is the question of whether a certain broad class of problems denoted NP can be solved efficiently. This is discussed further at Complexity classes P and NP, and P versus NP problem is one of the seven Millennium Prize Problems stated by the Clay Mathematics Institute in 2000. The Official Problem Description was given by Turing Award winner Stephen Cook.
2272) 13.4114275, Tide-predicting machine - Wikipedia, the free encyclopedia.txt#0, term: computer, content:A tide-predicting machine was a special-purpose mechanical analog computer of the late 19th and early 20th centuries, constructed and set up to predict the ebb and flow of sea tides and the irregular variations in their heights  which change in mixtures of rhythms, that never (in the aggregate) repeat themselves exactly.[1] Its purpose was to shorten the laborious and error-prone computations of tide-prediction. Such machines usually provided predictions valid from hour to hour and day to day for a year or more ahead.
2273) 13.4114275, Tom Kilburn - Wikipedia, the free encyclopedia.txt#16, term: computer, content:After her death Kilburn lived alone in the modest house they had shared in Manchester. He did not own a personal computer.[4] In 1998 he unveiled a fully functional replica of "The Baby" at the Manchester Museum of Science and Industry.[4] He died at Trafford General Hospital in Davyhulme of pneumonia following abdominal surgery on 17 January 2001.[1]
2274) 13.4114275, Tommy Flowers - Wikipedia, the free encyclopedia.txt#16, term: computer, content:On Thursday 12 December 2013, 70 years after he created Colossus, his legacy was honoured with a memorial commissioned by BT, successor to Post Office Telephones. The life-size bronze bust, designed by James Butler, was unveiled by Trevor Baylis at Adastral Park, BT's research and development centre in Martlesham Heath, near Ipswich, Suffolk. BT also began a computer science scholarship and award in his name.[21]
2275) 13.4114275, TOP500 - Wikipedia, the free encyclopedia.txt#0, term: computer, content:The TOP500 project ranks and details the 500 most powerful (non-distributed) computer systems in the world. The project was started in 1993 and publishes an updated list of the supercomputers twice a year. The first of these updates always coincides with the International Supercomputing Conference in June, and the second is presented in November at the ACM/IEEE Supercomputing Conference. The project aims to provide a reliable basis for tracking and detecting trends in high-performance computing and bases rankings on HPL,[1] a portable implementation of the high-performance LINPACK benchmark written in Fortran for distributed-memory computers.
2276) 13.4114275, TOP500 - Wikipedia, the free encyclopedia.txt#6, term: computer, content:In recent years heterogeneous computing, mostly using Nvidia's graphics processing units (GPU) as coprocessors, has become a popular way to reach a better performance per watt ratio and higher absolute performance; it is almost required for good performance and to make the top (or top 10), with some exceptions, such as the mentioned SPARC computer without any coprocessors. A x86-based coprocessor, Xeon Phi, has also been used.
2277) 13.4114275, Torpedo Data Computer - Wikipedia, the free encyclopedia.txt#1, term: computer, content:Replacing the previously standard hand-held slide rule-type devices (known as the "banjo" & "is/was"),[4] the TDC was designed to provide fire-control solutions for submarine torpedo firing against ships running on the surface (surface warships used a different computer).[5] It had an array of handcranks, dials, and switches for data input and display.[6] To generate a fire control solution, it required inputs on
2278) 13.4114275, Trackball - Wikipedia, the free encyclopedia.txt#4, term: computer, content:The trackball was first built by Tom Cranston, Fred Longstaff and Kenyon Taylor as part of the Royal Canadian Navy's DATAR system in 1952[3] DATAR was similar in concept to Benjamin's display, but used a digital computer to calculate tracks, and sent the resulting data to other ships in a task force using pulse-code modulation radio signals. This trackball used a Canadian five-pin bowling ball.
2279) 13.4114275, Transistor computer - Wikipedia, the free encyclopedia.txt#2, term: computer, content:The design of a full-size Transistor Computer was subsequently adopted by the Manchester firm of Metropolitan-Vickers, who changed all the circuits to more reliable types of junction transistors.[1] The production version was known as the Metrovick 950 and was built from 1956 to the extent of six[1] or seven machines,[3] which were "used commercially within the company"[3] or "mainly for internal use".[1]
2280) 13.4114275, Trigonometry - Wikipedia, the free encyclopedia.txt#21, term: computer, content:Today, scientific calculators have buttons for calculating the main trigonometric functions (sin, cos, tan, and sometimes cis and their inverses). Most allow a choice of angle measurement methods: degrees, radians, and sometimes gradians. Most computer programming languages provide function libraries that include the trigonometric functions. The floating point unit hardware incorporated into the microprocessor chips used in most personal computers has built-in instructions for calculating trigonometric functions.[19]
2281) 13.4114275, Turing completeness - Wikipedia, the free encyclopedia.txt#4, term: computer, content:Real computers constructed so far are essentially similar to a single-tape Turing machine; thus the associated mathematics can apply by abstracting their operation far enough. However, real computers have limited physical resources, so they are only linear bounded automaton complete. In contrast, a universal computer is defined as a device with a Turing complete instruction set, infinite memory, and infinite available time.
2282) 13.4114275, Turing completeness - Wikipedia, the free encyclopedia.txt#6, term: computer, content:Turing completeness is significant in that every real-world design for a computing device can be simulated by a universal Turing machine. The ChurchTuring thesis states that this is a law of mathematics  that a universal Turing machine can, in principle, perform any calculation that any other programmable computer can. This says nothing about the effort needed to write the program, or the time it may take for the machine to perform the calculation, or any abilities the machine may possess that have nothing to do with computation.
2283) 13.4114275, Two's complement - Wikipedia, the free encyclopedia.txt#4, term: computer, content:The method of complements had long been used to perform subtraction in decimal adding machines and mechanical calculators. John von Neumann suggested use of two's complement binary representation in his 1945 First Draft of a Report on the EDVAC proposal for an electronic stored-program digital computer.[3] The 1949 EDSAC, which was inspired by the First Draft, used two's complement representation of binary numbers.
2284) 13.4114275, Two's complement - Wikipedia, the free encyclopedia.txt#24, term: computer, content:In computer circuitry, this method is no faster than the "complement and add one" method; both methods require working sequentially from right to left, propagating logic changes. The method of complementing and adding one can be sped up by a standard carry look-ahead adder circuit; the LSB towards MSB method can be sped up by a similar logic transformation.
2285) 13.4114275, Two's complement - Wikipedia, the free encyclopedia.txt#39, term: computer, content:The system is useful in simplifying the implementation of arithmetic on computer hardware. Adding 0011(3) to 1111(1) at first seems to give the incorrect answer of 10010. However, the hardware can simply ignore the left-most bit to give the correct answer of 0010(2). Overflow checks still must exist to catch operations such as summing 0100 and 0100.
2286) 13.4114275, UNIVAC I - Wikipedia, the free encyclopedia.txt#3, term: computer, content:The first contracts were with government agencies such as the Census Bureau, the U.S. Air Force, and the U.S. Army Map Service.[8] Contracts were also signed by the ACNielsen Company, and the Prudential Insurance Company. Following the sale of EckertMauchly Computer Corporation to Remington Rand, due to the cost overruns on the project, Remington Rand convinced Nielsen and Prudential to cancel their contracts.
2287) 13.4114275, UNIVAC I - Wikipedia, the free encyclopedia.txt#4, term: computer, content:The first sale, to the Census Bureau, was marked with a formal ceremony on March 31, 1951, at the EckertMauchly Division's factory at 3747 Ridge Avenue, Philadelphia. The machine was not actually shipped until the following December, because, as the sole fully set-up model, it was needed for demonstration purposes, and the company was apprehensive about the difficulties of dismantling, transporting, and reassembling the delicate machine.[9] As a result, the first installation was with the second computer, delivered to the Pentagon in June 1952.
2288) 13.4114275, Universal Turing machine - Wikipedia, the free encyclopedia.txt#3, term: computer, content:Davis makes a persuasive argument that Turing's conception of what is now known as "the stored-program computer", of placing the "action table"the instructions for the machinein the same "memory" as the input data, strongly influenced John von Neumann's conception of the first American discrete-symbol (as opposed to analog) computerthe EDVAC. Davis quotes Time magazine to this effect, that "everyone who taps at a keyboard... is working on an incarnation of a Turing machine," and that "John von Neumann [built] on the work of Alan Turing" (Davis 2000:193 quoting Time magazine of 29 March 1999).
2289) 13.4114275, Universal Turing machine - Wikipedia, the free encyclopedia.txt#5, term: computer, content:As the Turing Machine was encouraging the construction of computers, the UTM was encouraging the development of the fledgling computer sciences. An early, if not the very first, assembler was proposed "by a young hot-shot programmer" for the EDVAC (Davis 2000:192). Von Neumann's "first serious program ... [was] to simply sort data efficiently" (Davis 2000:184). Knuth observes that the subroutine return embedded in the program itself rather than in special registers is attributable to von Neumann and Goldstine.[3] Knuth furthermore states that
2290) 13.4114275, Unix - Wikipedia, the free encyclopedia.txt#11, term: computer, content:The name Unics (Uniplexed Information and Computing Service, pronounced as "eunuchs"), a pun on Multics (Multiplexed Information and Computer Services), was initially suggested for the project in 1970: the new operating system was an emasculated Multics. Peter H. Salus credits Peter Neumann with the pun,[16] while Brian Kernighan claims the coining for himself, and adds that "no one can remember" who came up with the final spelling Unix.[17] Dennis Ritchie also credits Kernighan.[15]
2291) 13.4114275, USB - Wikipedia, the free encyclopedia.txt#1, term: computer, content:USB was designed to standardize the connection of computer peripherals (including keyboards, pointing devices, digital cameras, printers, portable media players, disk drives and network adapters) to personal computers, both to communicate and to supply electric power. It has become commonplace on other devices, such as smartphones, PDAs and video game consoles.[5] USB has effectively replaced a variety of earlier interfaces, such as parallel ports, as well as separate power chargers for portable devices.
2292) 13.4114275, USB - Wikipedia, the free encyclopedia.txt#57, term: computer, content:There are several types of USB connector, including some that have been added while the specification progressed. The original USB specification detailed standard-A and standard-B plugs and receptacles; the B connector was necessary so that cabling could be plug ended at both ends and still prevent users from connecting one computer receptacle to another. The first engineering change notice to the USB2.0 specification added Mini-B plugs and receptacles.
2293) 13.4114275, User interface - Wikipedia, the free encyclopedia.txt#14, term: computer, content:Submitting a job to a batch machine involved, first, preparing a deck of punched cards describing a program and a dataset. Punching the program cards wasn't done on the computer itself, but on specialized typewriter-like machines that were notoriously balky, unforgiving, and prone to mechanical failure. The software interface was similarly unforgiving, with very strict syntaxes meant to be parsed by the smallest possible compilers and interpreters.
2294) 13.4114275, User interface - Wikipedia, the free encyclopedia.txt#15, term: computer, content:Once the cards were punched, one would drop them in a job queue and wait. Eventually. operators would feed the deck to the computer, perhaps mounting magnetic tapes to supply another dataset or helper software. The job would generate a printout, containing final results or (all too often) an abort notice with an attached error log. Successful runs might also write a result on magnetic tape or generate some data cards to be used in later computation.
2295) 13.4114275, User interface - Wikipedia, the free encyclopedia.txt#20, term: computer, content:The widespread adoption of video-display terminals (VDTs) in the mid-1970s ushered in the second phase of command-line systems. These cut latency further, because characters could be thrown on the phosphor dots of a screen more quickly than a printer head or carriage can move. They helped quell conservative resistance to interactive programming by cutting ink and paper consumables out of the cost picture, and were to the first TV generation of the late 1950s and 60s even more iconic and comfortable than teleprinters had been to the computer pioneers of the 1940s.
2296) 13.4114275, User interface - Wikipedia, the free encyclopedia.txt#21, term: computer, content:Just as importantly, the existence of an accessible screen  a two-dimensional display of text that could be rapidly and reversibly modified  made it economical for software designers to deploy interfaces that could be described as visual rather than textual. The pioneering applications of this kind were computer games and text editors; close descendants of some of the earliest specimens, such as rogue(6), and vi(1), are still a live part of Unix tradition.
2297) 13.4114275, Video card - Wikipedia, the free encyclopedia.txt#0, term: computer, content:A video card (also called a video graphics adapter or VGA, display card, graphics card, graphics board, display adapter, graphics adapter GPU or frame buffer[1]) is an expansion card which generates a feed of output images to a display (such as a computer monitor). Frequently, these are advertised as discrete or dedicated graphics cards, emphasizing the distinction between these and integrated graphics. Within the industry, video cards are sometimes called graphics add-in-boards, abbreviated as AIBs,[2] with the word "graphics" usually omitted.
2298) 13.4114275, Video card - Wikipedia, the free encyclopedia.txt#14, term: computer, content:A graphics processing unit (GPU), also occasionally called visual processing unit (VPU), is a specialized electronic circuit designed to rapidly manipulate and alter memory to accelerate the building of images in a frame buffer intended for output to a display. Because of the large degree of programmable computational complexity for such a task, a modern video card is also a computer unto itself.
2299) 13.4114275, Video card - Wikipedia, the free encyclopedia.txt#25, term: computer, content:HDMI is a compact audio/video interface for transferring uncompressed video data and compressed/uncompressed digital audio data from an HDMI-compliant device ("the source device") to a compatible digital audio device, computer monitor, video projector, or digital television.[26] HDMI is a digital replacement for existing analog video standards. HDMI supports copy protection through HDCP.
2300) 13.4114275, Video game - Wikipedia, the free encyclopedia.txt#0, term: computer, content:A video game is an electronic game that involves human interaction with a user interface to generate visual feedback on a video device such as a TV screen or computer monitor. The word video in video game traditionally referred to a raster display device, but in the 2000s, it implies any type of display device that can produce two- or three-dimensional images. Video games are sometimes believed to be a form of art, but this designation is controversial.
2301) 13.4114275, Video game - Wikipedia, the free encyclopedia.txt#7, term: computer, content:The term "platform" refers to the specific combination of electronic components or computer hardware which, in conjunction with software, allows a video game to operate.[20] The term "system" is also commonly used. The distinctions below are not always clear and there may be games that bridge one or more platforms. In addition to personal computers, there are other devices which have the ability to play games but are not dedicated video game machines, such as smartphones, PDAs and graphing calculators.
2302) 13.4114275, Video game industry - Wikipedia, the free encyclopedia.txt#7, term: computer, content:Soon after, Space Invaders was licensed for the Atari VCS (later known as Atari 2600), becoming the first "killer app" and quadrupling the console's sales.[13] The success of the Atari 2600 in turn revived the home video game market during the second generation of consoles, up until the North American video game crash of 1983.[14] By the end of the 1970s, the personal computer game industry began forming from a hobby culture.
2303) 13.4114275, Von Neumann architecture - Wikipedia, the free encyclopedia.txt#16, term: computer, content:Both von Neumann's and Turing's papers described stored-program computers, but von Neumann's earlier paper achieved greater circulation and the computer architecture it outlined became known as the "von Neumann architecture". In the 1953 publication Faster than Thought: A Symposium on Digital Computing Machines (edited by B.V. Bowden), a section in the chapter on Computers in America reads as follows:[14]
2304) 13.4114275, Wearable computer - Wikipedia, the free encyclopedia.txt#3, term: computer, content:One common feature of wearable computers is their persistence of activity. There is constant interaction between the wearable and user, so there is no need to turn the device on or off. Another feature is the ability to multi-task. When using a wearable computer, there is no need to stop what one is doing to use the device; its functionality blends seamlessly into all other user actions. These devices can be used by the wearer to act as a prosthetic. It may therefore be an extension of the users mind or body.
2305) 13.4114275, Wearable computer - Wikipedia, the free encyclopedia.txt#20, term: computer, content:The 1970s saw the rise of special purpose hardware timing devices, similar to the ones from the 1960s, such as roulette prediction devices using next-generation technology. In particular, a group known as Eudaemonic Enterprises used a CMOS 6502 microprocessor with 5K RAM to create a shoe computer with inductive radio communications between a data-taker and bettor.[18][19]
2306) 13.4114275, Wearable computer - Wikipedia, the free encyclopedia.txt#50, term: computer, content:Greater response to commercialization has been found in creating devices with designated purposes rather than all-purpose. One example is the WSS1000. The WSS1000 is a wearable computer designed to make the work of inventory employees easier and more efficient. The device allows workers to scan the barcode of items and immediately enter the information into the company system. This removed the need for carrying a clipboard, removed error and confusion from hand written notes, and allowed workers the freedom of both hands while working; the system improves accuracy as well as efficiency.[2]
2307) 13.4114275, Web server - Wikipedia, the free encyclopedia.txt#5, term: computer, content:Web servers are not only used for serving the World Wide Web. They can also be found embedded in devices such as printers, routers, webcams and serving only a local network. The web server may then be used as a part of a system for monitoring and/or administering the device in question. This usually means that no additional software has to be installed on the client computer, since only a web browser is required (which now is included with most operating systems).
2308) 13.4114275, Webcam - Wikipedia, the free encyclopedia.txt#15, term: computer, content:Special software can use the video stream from a webcam to assist or enhance a user's control of applications and games. Video features, including faces, shapes, models and colors can be observed and tracked to produce a corresponding form of control. For example, the position of a single light source can be tracked and used to emulate a mouse pointer, a head-mounted light would enable hands-free computing and would greatly improve computer accessibility. This can be applied to games, providing additional control, improved interactivity and immersiveness.
2309) 13.4114275, Webcam - Wikipedia, the free encyclopedia.txt#20, term: computer, content:First developed in 1991, a webcam was pointed at the Trojan Room coffee pot in the Cambridge University Computer Science Department (initially operating over a local network instead of the web). The camera was finally switched off on August 22, 2001. The final image captured by the camera can still be viewed at its homepage.[9][10] In 2004, the oldest webcam still operating was FogCam at San Francisco State University, which had been running continuously since 1994.[11]
2310) 13.4114275, Webcam - Wikipedia, the free encyclopedia.txt#25, term: computer, content:Around the turn of the 21st century, computer hardware manufacturers began building webcams directly into laptop and desktop screens, thus eliminating the need to use an external USB or FireWire camera. Gradually webcams came to be used more for telecommunications, or videotelephony, between two people, or among several people, than for offering a view on a Web page to an unknown public.
2311) 13.4114275, Webcam - Wikipedia, the free encyclopedia.txt#29, term: computer, content:Support electronics read the image from the sensor and transmit it to the host computer. The camera pictured to the right, for example, uses a Sonix SN9C101 to transmit its image over USB. Typically, each frame is transmitted uncompressed in RGB or YUV or compressed as JPEG. Some cameras, such as mobile-phone cameras, use a CMOS sensor with supporting electronics "on die", i.e. the sensor and the support electronics are built on a single silicon chip to save space and manufacturing costs. Most webcams feature built-in microphones to make video calling and videoconferencing more convenient.
2312) 13.4114275, Webcam - Wikipedia, the free encyclopedia.txt#34, term: computer, content:The fraudulent process of attempting to hack into a person's webcam and activate it without the webcam owner's permission has been called camfecting. The remotely activated webcam can be used to watch anything within the webcam's field of vision, sometimes the webcam owner itself. Camfecting is most often carried out by infecting the victim's computer with a virus that can provide the hacker access to the victim's webcam. This attack is specifically targeted at the victim's webcam, and hence the name camfecting, a portmanteau of the words cam and infecting.
2313) 13.4114275, Williams tube - Wikipedia, the free encyclopedia.txt#8, term: computer, content:Developed at the University of Manchester in England, it provided the medium on which the first electronically stored-memory program was implemented in the Manchester Small-Scale Experimental Machine (SSEM) computer, which first successfully ran a program on 21 June 1948.[6] In fact, rather than the Williams tube memory being designed for the SSEM, the SSEM was a testbed to demonstrate the reliability of the memory.[7][8] Tom Kilburn wrote a 17-line program to calculate the highest factor of 218. Tradition at the university has it that this was the only program Kilburn ever wrote.[9]
2314) 13.4114275, Windows 10 - Wikipedia, the free encyclopedia.txt#0, term: computer, content:Windows 10 is a personal computer operating system developed and released by Microsoft as part of the Windows NT family of operating systems. It was officially unveiled in September 2014 following a brief demo at Build 2014. The first version of the operating system entered a public beta testing process in October 2014, leading up to its consumer release on July 29, 2015.[4]
2315) 13.4114275, Windows 2000 - Wikipedia, the free encyclopedia.txt#38, term: computer, content:The Recovery Console is run from outside the installed copy of Windows to perform maintenance tasks that can neither be run from within it nor feasibly be run from another computer or copy of Windows 2000.[93] It is usually used to recover the system from problems that cause booting to fail, which would render other tools useless, like Safe Mode or Last Known Good Configuration, or chkdsk. It includes commands like 'fixmbr', which are not present in MS-DOS.
2316) 13.4114275, Windows 7 - Wikipedia, the free encyclopedia.txt#0, term: computer, content:Windows 7 (codenamed Vienna, formerly Blackcomb[8]) is a personal computer operating system developed by Microsoft. It is a part of the Windows NT family of operating systems. Windows 7 was released to manufacturing on July 22, 2009, and became generally available on October 22, 2009,[9] less than three years after the release of its predecessor, Windows Vista. Windows 7's server counterpart, Windows Server 2008 R2, was released at the same time.
2317) 13.4114275, Windows 8 - Wikipedia, the free encyclopedia.txt#0, term: computer, content:Windows 8 is a personal computer operating system developed by Microsoft as part of the Windows NT family of operating systems. Development of Windows 8 started before the release of its predecessor, Windows 7, in 2009. It was announced at CES 2011, and followed by the release of three pre-release versions from September 2011 to May 2012. The operating system was released to manufacturing on August 1, 2012, and was released for general availability on October 26, 2012.[6]
2318) 13.4114275, Windows 95 - Wikipedia, the free encyclopedia.txt#40, term: computer, content:Many features that have become key components of the Microsoft Windows series, such as the Start menu and the taskbar, originated in Windows 95. Neil MacDonald, a Gartner analyst, said "If you look at Windows 95, it was a quantum leap in difference in technological capability and stability." Ina Fried of CNET said "By the time Windows 95 was finally ushered off the market in 2001, it had become a fixture on computer desktops around the world."[33]
2319) 13.4114275, Windows 98 - Wikipedia, the free encyclopedia.txt#10, term: computer, content:Windows Driver Model also includes Broadcast Driver Architecture, the backbone for TV technologies support in Windows. WebTV for Windows utilized BDA to allow viewing television on the computer if a compatible TV tuner card is installed. TV listings could be updated from the Internet and WaveTop Data Broadcasting allowed extra data about broadcasts to be received via regular television signals using an antenna or cable, by embedding data streams into the vertical blanking interval (VBI) portion of existing broadcast television signals.
2320) 13.4114275, Windows 98 - Wikipedia, the free encyclopedia.txt#19, term: computer, content:For networked computers that have user profiles enabled, Windows 98 introduces Microsoft Family Logon which lists all users that have been configured for that computer, enabling users to simply select their names from a list rather than having to type it in. The same feature can be added to Windows 95 if Internet Explorer 4.0 is installed.
2321) 13.4114275, Windows ME - Wikipedia, the free encyclopedia.txt#9, term: computer, content:In Windows ME, the CONFIG.SYS and AUTOEXEC.BAT files are used only to set global environment variables. The two files (if present) are scanned for settings relating to the environment variables, and any other commands present are removed into a Windows registry key (see below). The two files thus contain only settings and preferences which configure the "global environment" for the computer during the boot phase or when starting a new virtual DOS machine (VDM).
2322) 13.4114275, Windows ME - Wikipedia, the free encyclopedia.txt#21, term: computer, content:If an installation CD-ROM from the Windows 2000 family is inserted into the drive of a computer running Windows ME, the user is prompted to upgrade to Windows 2000 because Windows ME has an older version number than Windows 2000. While this is not technically so (Windows ME was released several months after Windows 2000), Windows ME is in fact derived from the older, monolithic MS-DOS codebase (Windows 4.x) while Windows 2000 is the first of the NT 5.0 family, making the latter an upgrade.
2323) 13.4114275, Windows ME - Wikipedia, the free encyclopedia.txt#22, term: computer, content:Windows 2000 cannot, however, be upgraded to Windows ME. If an installation CD-ROM from Windows ME is inserted while running Windows 2000, the user will receive an error message that Setup cannot run from within Windows 2000. The user is prompted to shut down Windows 2000, restart the computer using Windows 95, 98, or 98 SE, or start MS-DOS and then run Setup from the MS-DOS command prompt.
2324) 13.4114275, Windows NT - Wikipedia, the free encyclopedia.txt#24, term: computer, content:Microsoft announced on 5 January 2011 that the next major version of the Windows NT family will include support for the ARM architecture. Microsoft demonstrated a preliminary version of Windows (version 6.2.7867) running on an ARM-based computer at the 2011 Consumer Electronics Show.[42] This eventually led to the commercial release of the Windows 8-derived Windows RT on October 26, 2012, and the implementation of NT over CE on Windows Phone 8.
2325) 13.4114275, Word (computer architecture) - Wikipedia, the free encyclopedia.txt#5, term: computer, content:When a computer architecture is designed, the choice of a word size is of substantial importance. There are design considerations which encourage particular bit-group sizes for particular uses (e.g. for addresses), and these considerations point to different sizes for different uses. However, considerations of economy in design strongly push for one size, or a very few sizes related by multiples or fractions (submultiples) to a primary size. That preferred size becomes the word size of the architecture.
2326) 13.4114275, Word (computer architecture) - Wikipedia, the free encyclopedia.txt#14, term: computer, content:Different amounts of memory are used to store data values with different degrees of precision. The commonly used sizes are usually a power of two multiple of the unit of address resolution (byte or word). Converting the index of an item in an array into the address of the item then requires only a shift operation rather than a multiplication. In some cases this relationship can also avoid the use of division operations. As a result, most modern computer designs have word sizes (and other operand sizes) that are a power of two times the size of a byte.
2327) 13.4114275, Word processor - Wikipedia, the free encyclopedia.txt#2, term: computer, content:As the more versatile combination of personal computers and printers became commonplace, and computer software applications for word processing became popular, most business machine companies stopped manufacturing dedicated word processor machines. As of 2009 there were only two U.S. companies, Classic and AlphaSmart, which still made them.[2] Many older machines, however, remain in use. Since 2009, Sentinel has offered a machine described as a "word processor", but it is more accurately a highly specialised microcomputer used for accounting and publishing.[3]
2328) 13.4114275, Word processor - Wikipedia, the free encyclopedia.txt#17, term: computer, content:The IBM Selectric typewriter was a highly successful model line of electric typewriters introduced in 1961. Expensive Typewriter written and improved between 1961 and 1962 by Steve Piner and L. Peter Deutsch, was a text editing program that ran on a DEC PDP-1 computer at MIT. Since it could drive an IBM Selectric typewriter (a letter-quality printer), it may be considered the first word processing program, but the term word processing itself was only introduced, by IBM's Bblingen Laboratory in the late 1960s.[citation needed]
2329) 13.4114275, World Wide Web - Wikipedia, the free encyclopedia.txt#9, term: computer, content:A NeXT Computer was used by Berners-Lee as the world's first web server and also to write the first web browser, WorldWideWeb, in 1990. By Christmas 1990, Berners-Lee had built all the tools necessary for a working Web:[11] the first web browser (which was a web editor as well) and the first web server. The first web site,[12] which described the project itself, was published on 20 December 1990.[13]
2330) 13.4114275, World Wide Web - Wikipedia, the free encyclopedia.txt#21, term: computer, content:The terms Internet and World Wide Web are often used without much distinction. However, the two are not the same. The Internet is a global system of interconnected computer networks. In contrast, the World Wide Web is a global collection of text documents and other resources, linked by hyperlinks and URIs. Web resources are usually accessed using HTTP, which is one of many Internet communication protocols.[32]
2331) 13.4114275, World Wide Web Consortium - Wikipedia, the free encyclopedia.txt#3, term: computer, content:The World Wide Web Consortium (W3C) was founded by Tim Berners-Lee after he left the European Organization for Nuclear Research (CERN) in October, 1994. It was founded at the Massachusetts Institute of Technology Laboratory for Computer Science (MIT/LCS) with support from the European Commission and the Defense Advanced Research Projects Agency (DARPA),[3] which had pioneered the Internet and its predecessor ARPANET.
2332) 13.4114275, World Wide Web Consortium - Wikipedia, the free encyclopedia.txt#20, term: computer, content:The Consortium is jointly administered by the MIT Computer Science and Artificial Intelligence Laboratory (CSAIL, located in Stata Center[6]) in the USA, the European Research Consortium for Informatics and Mathematics (ERCIM) (in Sophia Antipolis, France), Keio University (in Japan) and Beihang University (in China).[7] The W3C also has World Offices in sixteen regions around the world. The W3C Offices work with their regional web communities to promote W3C technologies in local languages, broaden the W3C's geographical base and encourage international participation in W3C Activities.[citation needed]
2333) 13.4114275, Z1 (computer) - Wikipedia, the free encyclopedia.txt#3, term: computer, content:The Z1 contained almost all parts of a modern computer, i.e. control unit, memory, micro sequences, floating-point logic and input-output devices. The Z1 was freely programmable via punched tape and a punched tape reader.[4] There was a clear separation between the punched tape reader, the control unit for supervising the whole machine and the execution of the instructions, the arithmetic unit, and the input and output devices. The input tape unit read perforations in 35-millimeter film.[5]
2334) 13.4114275, Z1 (computer) - Wikipedia, the free encyclopedia.txt#8, term: computer, content:"Z1 was a machine of about 1000 kg weight, which consisted from some 20000 parts. It was a programmable computer, based on binary floating point numbers and a binary switching system. It consisted completely of thin metal sheets, which Kuno and his friends produced using a jigsaw."[6] "The [data] input device was a keyboard...The Z1s programs (Zuse called them Rechenplans) were stored on punch tapes by means of a 8-bit code"[6]
2335) 13.4114275, Z1 (computer) - Wikipedia, the free encyclopedia.txt#13, term: computer, content:'The memory was constructed from thin strips of slotted metal and small pins, and proved faster, smaller, and more reliable, than relays. The Z2 used the mechanical memory of the Z1, but used relay-based arithmetic. The Z3 was experimentally built entirely of relays. The Z4 was the first attempt at a commercial computer, reverting to the faster and more economical mechanical slotted metal strip memory, with relay processing, of the Z2, but the war interrupted the Z4 development.'[8]
2336) 13.4114275, Z22 (computer) - Wikipedia, the free encyclopedia.txt#0, term: computer, content:The Z22 was the seventh computer model Konrad Zuse developed (the first six being the Z1, Z2, Z3, Z4, Z5 and Z11, respectively). One of the early commercial computers, the Z22's design was finished about 1955. The major version jump from Z11 to Z22 was due to the use of vacuum tubes, as opposed to the electromechanical systems used in earlier models. The first machines built were shipped to Berlin and Aachen.
2337) 13.4114275, Z3 (computer) - Wikipedia, the free encyclopedia.txt#11, term: computer, content:The success of Zuse's Z3 is often attributed to its use of the simple binary system.[21] This was invented roughly three centuries earlier by Gottfried Leibniz; Boole later used it to develop his Boolean algebra. In 1937, Claude Shannon introduced the idea of mapping Boolean algebra onto electronic relays in a seminal work on digital circuit design. Zuse however did not know Shannon's work and developed the groundwork independently[22] for his first computer Z1 which he designed and built from 1935 to 1938.
2338) 11.854138, 64-bit computing - Wikipedia, the free encyclopedia.txt#1, term: computer, content:The term 64-bit describes a generation of computers in which 64-bit processors are the norm. 64 bits is a word size that defines certain classes of computer architecture, buses, memory and CPUs, and by extension the software that runs on them. 64-bit CPUs have been used in supercomputers since the 1970s (Cray-1, 1975) and in RISC-based workstations and servers since the early 1990s, notably the MIPS R4000, R8000, and R10000, the DEC Alpha, the Sun UltraSPARC, and the IBM RS64 and POWER3 and later POWER microprocessors. In 2001, NEC released a 64 bit RISC CPU for mobile devices, notably the low cost Casio BE-300.[1][2] In 2003, 64-bit CPUs were introduced to the (previously 32-bit) mainstream personal computer arena in the form of the x86-64 and 64-bit PowerPC processor architectures and in 2012[3] even into the ARM architecture targeting smartphones and tablet computers, first sold on September 20, 2013, in the iPhone 5S powered by the ARMv8-A Apple A7 SoC.
2339) 11.854138, Analog computer - Wikipedia, the free encyclopedia.txt#78, term: computer, content:With the development of very-large-scale integration (VLSI) technology, Yannis Tsividis' group at Columbia University has been revisiting analog/hybrid computers design in standard CMOS process. Two VLSI chips have been developed, an 80th-order analog computer (250nm) by Glenn Cowan[21] in 2005[22] and an 4th-order hybrid computer (65nm) developed by Ning Guo[23] in 2015,[24] both targeting at energy-efficient ODE/PDEs applications. Glenn's chip contains 16 macros, in which there are 25 analog computing blocks, namely integrators, multipliers, fanouts, few nonlinear blocks. Ning's chip contains one macro block, in which there are 26 computing blocks including integrators, multipliers, fanouts, ADCs, SRAMs and DACs. Arbitrary nonlinear function generation is made possible by the ADC+SRAM+DAC chain, where the SRAM block stores the nonlinear function data. The experiments from the related publications revealed that VLSI analog/hybrid computers demonstrated about 1~2 orders magnitude of advantage in both solution time and energy while achieving accuracy within 5%, which points to the promise of using analog/hybrid computing techniques in the area of energy-efficient approximate computing.
2340) 11.854138, ARPANET - Wikipedia, the free encyclopedia.txt#5, term: computer, content:Sutherland and Taylor continued their interest in creating the network, in part, to allow ARPA-sponsored researchers at various corporate and academic locales to utilize computers provided by ARPA, and, in part, to quickly distribute new software and other computer science results.[9] Taylor had three computer terminals in his office, each connected to separate computers, which ARPA was funding: one for the System Development Corporation (SDC) Q-32 in Santa Monica, one for Project Genie at the University of California, Berkeley, and another for Multics at the Massachusetts Institute of Technology. Taylor recalls the circumstance: "For each of these three terminals, I had three different sets of user commands. So, if I was talking online with someone at S.D.C., and I wanted to talk to someone I knew at Berkeley, or M.I.T., about this, I had to get up from the S.D.C. terminal, go over and log into the other terminal and get in touch with them. I said, "Oh Man!", it's obvious what to do: If you have these three terminals, there ought to be one terminal that goes anywhere you want to go. That idea is the ARPANET".[10]
2341) 11.854138, ARPANET - Wikipedia, the free encyclopedia.txt#7, term: computer, content:By mid-1968, Taylor had prepared a complete plan for a computer network, and, after ARPA's approval, a Request for Quotation (RFQ) was issued for 140 potential bidders. Most computer science companies regarded the ARPATaylor proposal as outlandish, and only twelve submitted bids to build a network; of the twelve, ARPA regarded only four as top-rank contractors. At year's end, ARPA considered only two contractors, and awarded the contract to build the network to BBN Technologies on 7 April 1969. The initial, seven-person BBN team were much aided by the technical specificity of their response to the ARPA RFQ, and thus quickly produced the first working system. This team was led by Frank Heart. The BBN-proposed network closely followed Taylor's ARPA plan: a network composed of small computers called Interface Message Processors (or IMPs), similar to the later concept of routers, that functioned as gateways interconnecting local resources. At each site, the IMPs performed store-and-forward packet switching functions, and were interconnected with leased lines via telecommunication data sets (modems), with initial data rates of 56kbit/s. The host computers were connected to the IMPs via custom serial communication interfaces. The system, including the hardware and the packet switching software, was designed and installed in nine months.[22]
2342) 11.854138, Assembly language - Wikipedia, the free encyclopedia.txt#59, term: computer, content:Assembly language is still taught in most computer science and electronic engineering programs. Although few programmers today regularly work with assembly language as a tool, the underlying concepts remain very important. Such fundamental topics as binary arithmetic, memory allocation, stack processing, character set encoding, interrupt processing, and compiler design would be hard to study in detail without a grasp of how a computer operates at the hardware level. Since a computer's behavior is fundamentally defined by its instruction set, the logical way to learn such concepts is to study an assembly language. Most modern computers have similar instruction sets. Therefore, studying a single assembly language is sufficient to learn: I) the basic concepts; II) to recognize situations where the use of assembly language might be appropriate; and III) to see how efficient executable code can be created from high-level languages.[35] This is analogous to children needing to learn the basic arithmetic operations (e.g., long division), although calculators are widely used for all except the most trivial calculations.
2343) 11.854138, Bit - Wikipedia, the free encyclopedia.txt#19, term: computer, content:When the information capacity of a storage system or a communication channel is presented in bits or bits per second, this often refers to binary digits, which is a computer hardware capacity to store binary code (0 or 1, up or down, current or not, etc.). Information capacity of a storage system is only an upper bound to the actual quantity of information stored therein. If the two possible values of onebit of storage are not equally likely, that bit of storage will contain less than onebit of information. Indeed, if the value is completely predictable, then the reading of that value will provide no information at all (zero entropic bits, because no resolution of uncertainty and therefore no information). If a computer file that uses nbits of storage contains only m < nbits of information, then that information can in principle be encoded in about mbits, at least on the average. This principle is the basis of data compression technology. Using an analogy, the hardware binary digits refer to the amount of storage space available (like the number of buckets available to store things), and the information content the filling, which comes in different levels of granularity (fine or coarse, that is, compressed or uncompressed information). When the granularity is finer (when information is more compressed), the same bucket can hold more.
2344) 11.854138, Boolean algebra - Wikipedia, the free encyclopedia.txt#104, term: computer, content:Solid modeling systems for computer aided design offer a variety of methods for building objects from other objects, combination by Boolean operations being one of them. In this method the space in which objects exist is understood as a set S of voxels (the three-dimensional analogue of pixels in two-dimensional graphics) and shapes are defined as subsets of S, allowing objects to be combined as sets via union, intersection, etc. One obvious use is in building a complex shape from simple shapes simply as the union of the latter. Another use is in sculpting understood as removal of material: any grinding, milling, routing, or drilling operation that can be performed with physical machinery on physical materials can be simulated on the computer with the Boolean operation xy or xy, which in set theory is set difference, remove the elements of y from those of x. Thus given two shapes one to be machined and the other the material to be removed, the result of machining the former to remove the latter is described simply as their set difference.
2345) 11.854138, Colossus computer - Wikipedia, the free encyclopedia.txt#39, term: computer, content:In November 2007, to celebrate the project completion and to mark the start of a fundraising initiative for The National Museum of Computing, a Cipher Challenge[64] pitted the rebuilt Colossus against radio amateurs worldwide in being first to receive and decode three messages enciphered using the Lorenz SZ42 and transmitted from radio station DL0HNF in the Heinz Nixdorf MuseumsForum computer museum. The challenge was easily won by radio amateur Joachim Schth, who had carefully prepared[65] for the event and developed his own signal processing and code-breaking code using Ada.[66] The Colossus team were hampered by their wish to use World War II radio equipment,[67] delaying them by a day because of poor reception conditions. Nevertheless, the victor's 1.4GHz laptop, running his own code, took less than a minute to find the settings for all 12 wheels. The German codebreaker said: "My laptop digested ciphertext at a speed of 1.2million characters per second240 times faster than Colossus. If you scale the CPU frequency by that factor, you get an equivalent clock of 5.8MHz for Colossus. That is a remarkable speed for a computer built in 1944."[68]
2346) 11.854138, Computer - Wikipedia, the free encyclopedia.txt#71, term: computer, content:RAM can be read and written to anytime the CPU commands it, but ROM is preloaded with data and software that never changes, therefore the CPU can only read from it. ROM is typically used to store the computer's initial start-up instructions. In general, the contents of RAM are erased when the power to the computer is turned off, but ROM retains its data indefinitely. In a PC, the ROM contains a specialized program called the BIOS that orchestrates loading the computer's operating system from the hard disk drive into RAM whenever the computer is turned on or reset. In embedded computers, which frequently do not have disk drives, all of the required software may be stored in ROM. Software stored in ROM is often called firmware, because it is notionally more like hardware than software. Flash memory blurs the distinction between ROM and RAM, as it retains its data when turned off but is also rewritable. It is typically much slower than conventional ROM and RAM however, so its use is restricted to applications where high speed is unnecessary.[64]
2347) 11.854138, Computer graphics - Wikipedia, the free encyclopedia.txt#14, term: computer, content:Many of the most important early breakthroughs in the transformation of graphics from utilitarian to realistic occurred at the University of Utah in the 1970s, which had hired Ivan Sutherland away from MIT. Sutherland's graphics class would contribute a number of significant pioneers to the field, including a student by the name of Edwin Catmull - a later founder of Pixar. Because of David C. Evans' and Sutherland's presence, UU was gaining quite a reputation as the place to be for computer graphics research so Catmull went there to learn 3D animation. Catmull had just come from The Boeing Company and had been working on his degree in physics. Growing up on Disney, Catmull loved animation yet quickly discovered that he did not have the talent for drawing. Now Catmull (along with many others) saw computers as the natural progression of animation and they wanted to be part of the revolution. The first animation that Catmull saw was his own. He created an animation of his hand opening and closing. It became one of his goals to produce a feature-length motion picture using computer graphics. In the same class, Fred Parke created an animation of his wife's face.
2348) 11.854138, Computer graphics - Wikipedia, the free encyclopedia.txt#21, term: computer, content:The continuing popularity of Star Wars and other science fiction franchises were relevant in cinematic CGI at this time, as Lucasfilm and Industrial Light & Magic became known as the "go-to" house by many other studios for topnotch computer graphics in film. Important advances in chroma keying ("bluescreening", etc.) were made for the later films of the original trilogy. Two other pieces of video would also outlast the era as historically relevant: Dire Straits' iconic, near-fully-CGI video for their song "Money For Nothing" in 1985, which popularized CGI among music fans of that era, and a scene from Young Sherlock Holmes the same year featuring the first fully CGI character in a feature movie (an animated stained-glass knight). In 1988, the first shaders - small programs designed specifically to do shading as a separate algorithm - were developed by Pixar, which had already spun off from Industrial Light & Magic as a separate entity - though the public would not see the results of such technological progress until the next decade. In the late 1980s, SGI computers were used to create some of the first fully computer-generated short films at Pixar, and Silicon Graphics machines were considered a high-water mark for the field during the decade.
2349) 11.854138, Computer monitor - Wikipedia, the free encyclopedia.txt#10, term: computer, content:The first standalone LCDs appeared in the mid-1990s selling for high prices. As prices declined over a period of years they became more popular, and by 1997 were competing with CRT monitors. Among the first desktop LCD computer monitors was the Eizo L66 in the mid-1990s, the Apple Studio Display in 1998, and the Apple Cinema Display in 1999. In 2003, TFT-LCDs outsold CRTs for the first time, becoming the primary technology used for computer monitors.[2] The main advantages of LCDs over CRT displays are that LCDs consume less power, take up much less space, and are considerably lighter. The now common active matrix TFT-LCD technology also has less flickering than CRTs, which reduces eye strain.[4] On the other hand, CRT monitors have superior contrast, have superior response time, are able to use multiple screen resolutions natively, and there is no discernible flicker if the refresh rate is set to a sufficiently high value. LCD monitors have now very high temporal accuracy and can be used for vision research.[5]
2350) 11.854138, Computer program - Wikipedia, the free encyclopedia.txt#8, term: computer, content:The Electronic Numerical Integrator And Computer (Fall 1945) was a Turing complete, general-purpose computer that used 17,468 vacuum tubes to create the circuits. At its core, it was a series of Pascalines wired together.[11] Its 40 units weighed 30 tons, occupied 1,800 square feet, and consumed $650 per hour (in 1940s currency) in electricity when idle.[11] It had 20 base-10 accumulators. Programming the ENIAC took up to two months.[11] Three function tables were on wheels and needed to be rolled to fixed function panels. Function tables were connected to function panels using heavy black cables. Each function table had 728 rotating knobs. Programming the ENIAC also involved setting some of the 3,000 switches. Debugging a program took a week.[11] The ENIAC featured parallel operations. Different sets of accumulators could simultaneously work on different algorithms. It used punched card machines for input and output, and it was controlled with a clock signal. It ran for eight years, calculating hydrogen bomb parameters, predicting weather patterns, and producing firing tables to aim artillery guns.
2351) 11.854138, Computer science - Wikipedia, the free encyclopedia.txt#24, term: computer, content:Artificial intelligence (AI) aims to or is required to synthesise goal-orientated processes such as problem-solving, decision-making, environmental adaptation, learning and communication found in humans and animals. From its origins in cybernetics and in the Dartmouth Conference (1956), artificial intelligence research has been necessarily cross-disciplinary, drawing on areas of expertise such as applied mathematics, symbolic logic, semiotics, electrical engineering, philosophy of mind, neurophysiology, and social intelligence. AI is associated in the popular mind with robotic development, but the main field of practical application has been as an embedded component in areas of software development, which require computational understanding. The starting-point in the late 1940s was Alan Turing's question "Can computers think?", and the question remains effectively unanswered although the Turing test is still used to assess computer output on the scale of human intelligence. But the automation of evaluative and predictive tasks has been increasingly successful as a substitute for human monitoring and intervention in domains of computer application involving complex real-world data.
2352) 11.854138, Computer simulation - Wikipedia, the free encyclopedia.txt#13, term: computer, content:Formerly, the output data from a computer simulation was sometimes presented in a table or a matrix showing how data were affected by numerous changes in the simulation parameters. The use of the matrix format was related to traditional use of the matrix concept in mathematical models. However, psychologists and others noted that humans could quickly perceive trends by looking at graphs or even moving-images or motion-pictures generated from the data, as displayed by computer-generated-imagery (CGI) animation. Although observers could not necessarily read out numbers or quote math formulas, from observing a moving weather chart they might be able to predict events (and "see that rain was headed their way") much faster than by scanning tables of rain-cloud coordinates. Such intense graphical displays, which transcended the world of numbers and formulae, sometimes also led to output that lacked a coordinate grid or omitted timestamps, as if straying too far from numeric data displays. Today, weather forecasting models tend to balance the view of moving rain/snow clouds against a map that uses numeric coordinates and numeric timestamps of events.
2353) 11.854138, Cryptography - Wikipedia, the free encyclopedia.txt#19, term: computer, content:Just as the development of digital computers and electronics helped in cryptanalysis, it made possible much more complex ciphers. Furthermore, computers allowed for the encryption of any kind of data representable in any binary format, unlike classical ciphers which only encrypted written language texts; this was new and significant. Computer use has thus supplanted linguistic cryptography, both for cipher design and cryptanalysis. Many computer ciphers can be characterized by their operation on binary bit sequences (sometimes in groups or blocks), unlike classical and mechanical schemes, which generally manipulate traditional characters (i.e., letters and digits) directly. However, computers have also assisted cryptanalysis, which has compensated to some extent for increased cipher complexity. Nonetheless, good modern ciphers have stayed ahead of cryptanalysis; it is typically the case that use of a quality cipher is very efficient (i.e., fast and requiring few resources, such as memory or CPU capability), while breaking it requires an effort many orders of magnitude larger, and vastly larger than that required for any classical cipher, making cryptanalysis so inefficient and impractical as to be effectively impossible.
2354) 11.854138, Cryptography - Wikipedia, the free encyclopedia.txt#20, term: computer, content:Extensive open academic research into cryptography is relatively recent; it began only in the mid-1970s. In recent times, IBM personnel designed the algorithm that became the Federal (i.e., US) Data Encryption Standard; Whitfield Diffie and Martin Hellman published their key agreement algorithm;[22] and the RSA algorithm was published in Martin Gardner's Scientific American column. Since then, cryptography has become a widely used tool in communications, computer networks, and computer security generally. Some modern cryptographic techniques can only keep their keys secret if certain mathematical problems are intractable, such as the integer factorization or the discrete logarithm problems, so there are deep connections with abstract mathematics. There are very few cryptosystems that are proven to be unconditionally secure. The one-time pad is one. There are a few important ones that are proven secure under certain unproven assumptions. For example, the infeasibility of factoring extremely large integers is the basis for believing that RSA is secure, and some other systems, but even there, the proof is usually lost due to practical considerations. There are systems similar to RSA, such as one by Michael O. Rabin that is provably secure provided factoring n = pq is impossible, but the more practical system RSA has never been proved secure in this sense. The discrete logarithm problem is the basis for believing some other cryptosystems are secure, and again, there are related, less practical systems that are provably secure relative to the discrete log problem.[23]
2355) 11.854138, Digital Equipment Corporation - Wikipedia, the free encyclopedia.txt#50, term: computer, content:Unlike CP/M and DOS microcomputers, every copy of every program for the Professional had to be provided with a unique key for the particular machine and CPU for which it was bought. At that time this was mainstream policy, because most computer software was either bought from the company that built the computer or custom-constructed for one client. However, the emerging third-party software industry disregarded the PDP-11/Professional line and concentrated on other microcomputers where distribution was easier. At DEC itself, creating better programs for the Professional was not a priority, perhaps from fear of cannibalizing the PDP-11 line. As a result, the Professional was a superior machine, running inferior software.[50] In addition, a new user would have to learn an awkward, slow, and inflexible menu-based user interface which appeared to be radically different from PC DOS or CP/M, which were more commonly used on the 8080- and 8088-based microcomputers of the time. A second offering, the DECmate II was the latest version of the PDP-8-based word processors, but not really suited to general computing, nor competitive with Wang Laboratories' popular word processing equipment.
2356) 11.854138, Digital Equipment Corporation - Wikipedia, the free encyclopedia.txt#77, term: computer, content:Market acceptance of Digital Alpha computers and chips had been slower than the company had hoped, even though Alpha's sales for the quarter estimated at $275 million were up significantly from $165 million in the December quarter. Digital had also made a strong push into personal computers and workstations, which had even lower margins than Alpha computers and chips. Also, Digital was playing catchup with its own Unix offerings for client-server networks, as it long emphasized its own VMS software, while corporate computer users based their client-server networks on the industry-standard Unix software (of which Hewlett Packard was one of the market leaders). Digital's problems were similar to that of larger rival IBM, due to the fundamental shift in the computer industry that made it unlikely that Digital could ever again operate profitably at its former size of 120,000 employees, and while its workforce had been reduced to 92,000 people many analysts expected that they would have to cut another 20,000.[71]
2357) 11.854138, Electronic literature - Wikipedia, the free encyclopedia.txt#1, term: computer, content:It is difficult to define electronic literature. The phrase itself consists of two words, each with their own meanings. Arthur Krystal in What Is Literature explains that "lit(t)eratura referred to any writing formed with letters."[2] However, Krystal goes on to explore what literature has transformed into: "a record of one human being's sojourn on earth, proffered in verse or prose that artfully weaves together knowledge of the past with a heightened awareness of the present in ever new verbal configurations." Thus electronic literature can be considered a branch from the main tree of literature. Katherine Hayles discusses the topic in the online article Electronic Literature: What Is It.[3] She argues "electronic literature, generally considered to exclude print literature that has been digitized, is by contrast 'digital born,' and (usually) meant to be read on a computer." A definition offered by the Electronic Literature Organization (ELO) states electronic literature "refers to works with an important literary aspect that takes advantage of the capabilities and contexts provided by the stand-alone or networked computer."
2358) 11.854138, Enigma machine - Wikipedia, the free encyclopedia.txt#83, term: computer, content:In the United States, Enigma machines can be seen at the Computer History Museum in Mountain View, California, and at the National Security Agency's National Cryptologic Museum in Fort Meade, Maryland, where visitors can try their hand at enciphering and deciphering messages. Two machines that were acquired after the capture of U-505 during World War II are on display at the Museum of Science and Industry in Chicago, Illinois. A four rotor device is on display in the ANZUS Corridor of the Pentagon on the second floor, A ring, between corridors 9 and 10. This machine is on loan from Australia. The United States Air Force Academy in Colorado Springs has a machine on display in the Computer Science Department. There's also a machine located at the National World War II Museum in New Orleans. The Museum of World War II in Boston has seven Enigma machines on display, including a U-Boat four-rotor model, one of three surviving examples of an Enigma machine with a printer, one of fewer than ten surviving ten-rotor code machines, an example blown up by a retreating German Army unit, and two three-rotor Enigmas that visitors can operate to encode and decode messages themselves.
2359) 11.854138, Home computer - Wikipedia, the free encyclopedia.txt#7, term: computer, content:While two early home computers (Sinclair ZX80 and Acorn Atom) could be bought either in kit form or assembled, most home computers were only sold pre-assembled. They were enclosed in plastic or metal cases similar in appearance to typewriter or hi-fi equipment enclosures, which were more familiar and attractive to consumers than the industrial metal card-cage enclosures used by the Altair and similar computers. The keyboard - a feature lacking on the Altair - was usually built into the same case as the motherboard. Ports for plug-in peripheral devices such as a video display, cassette tape recorders, joysticks, and (later) disk drives were either built-in or available on expansion cards. Although the Apple II series had internal expansion slots, most other home computer models' expansion arrangements were through externally accessible 'expansion ports' that also served as a place to plug in cartridge-based games. Usually the manufacturer would sell peripheral devices designed to be compatible with their computers as extra cost accessories. Peripherals were not often interchangeable between different brands of home computer, or even between successive models of the same brand.
2360) 11.854138, Home computer - Wikipedia, the free encyclopedia.txt#21, term: computer, content:Sometimes they were equipped with a cheap membrane or chiclet keyboard in the early days, although full-travel keyboards quickly became universal due to overwhelming consumer preference. Most systems could use an RF modulator to display 2040 column text output on a home television. Indeed, the use of a television set as a display almost defines the pre-PC home computer. Although dedicated composite or "green screen" computer displays were available for this market segment and offered a sharper display, a monitor was often a later purchase made only after users had bought a floppy disk drive, printer, modem, and the other pieces of a full system. The reason for this was that while those TV-monitors had difficulty displaying the clear and readable 80-column text that became the industry standard at the time, the only consumers who really needed that were the power users utilizing the machine for business purposes, while the average casual consumer would use the system for games only and was content with the lower resolution for which a TV worked fine.
2361) 11.854138, IBM PC compatible - Wikipedia, the free encyclopedia.txt#20, term: computer, content:One of the strengths of the PC compatible design is its modular hardware design. End-users could readily upgrade peripherals and, to some degree, processor and memory without modifying the computer's motherboard or replacing the whole computer, as was the case with many of the microcomputers of the time. However, as processor speed and memory width increased, the limits of the original XT/AT bus design were soon reached, particularly when driving graphics video cards. IBM did introduce an upgraded bus in the IBM PS/2 computer that overcame many of the technical limits of the XT/AT bus, but this was rarely used as the basis for IBM compatible computers since it required licence payments to IBM both for the PS/2 bus and any prior AT-bus designs produced by the company seeking a license. This was unpopular with hardware manufacturers and several competing bus standards were developed by consortiums, with more agreeable license terms. Various attempts to standardize the interfaces were made, but in practice, many of these attempts were either flawed or ignored. Even so, there were many expansion options, and despite the confusion of its users, the PC compatible design advanced much faster than other competing designs of the time, even if only because of its market dominance.
2362) 11.854138, Image scanner - Wikipedia, the free encyclopedia.txt#19, term: computer, content:"Hand held document scanners are manual devices that are dragged across the surface of the image to be scanned by hand. Scanning documents in this manner requires a steady hand, as an uneven scanning rate produces distorted images; an indicator light on the scanner indicates if motion is too fast. They typically have a "start" button, which is held by the user for the duration of the scan; some switches to set the optical resolution; and a roller, which generates a clock pulse for synchronization with the computer. Older hand scanners were monochrome, and produced light from an array of green LEDs to illuminate the image";[7] later ones scan in monochrome or color, as desired. A hand scanner may have a small window through which the document being scanned could be viewed. In the early 1990s many hand scanners had a proprietary interface module specific to a particular type of computer, such as an Atari ST or Commodore Amiga. Since the introduction of the USB standard, it is the interface most commonly used. As hand scanners are much narrower than most normal document or book sizes, software (or the end user) needed to combine several narrow "strips" of scanned document to produce the finished article.
2363) 11.854138, Intel - Wikipedia, the free encyclopedia.txt#109, term: computer, content:Intel's branding campaign started with "The Computer Inside" tagline in 1990 in US and Europe. The Japan chapter of Intel proposed an "Intel in it" tagline and kicked off the Japanese campaign by hosting EKI-KON (meaning "Station Concert" in Japanese) at the Tokyo railway station dome on Christmas Day, December 25, 1990. Several months later, "The Computer Inside" incorporated the Japan idea to become "Intel Inside" which eventually elevated to the worldwide branding campaign in 1991, by Intel marketing manager Dennis Carter.[219] The case study of the Inside Intel Inside was put together by Harvard Business School.[220] The five-note jingle was introduced in 1994 and by its tenth anniversary was being heard in 130 countries around the world. The initial branding agency for the "Intel Inside" campaign was DahlinSmithWhite Advertising of Salt Lake City. The Intel swirl logo was the work of DahlinSmithWhite art director Steve Grigg under the direction of Intel president and CEO Andy Grove.[citation needed]
2364) 11.854138, Internet - Wikipedia, the free encyclopedia.txt#31, term: computer, content:Common methods of Internet access by users include dial-up with a computer modem via telephone circuits, broadband over coaxial cable, fiber optics or copper wires, Wi-Fi, satellite and cellular telephone technology (3G, 4G). The Internet may often be accessed from computers in libraries and Internet cafes. Internet access points exist in many public places such as airport halls and coffee shops. Various terms are used, such as public Internet kiosk, public access terminal, and Web payphone. Many hotels also have public terminals, though these are usually fee-based. These terminals are widely accessed for various usages, such as ticket booking, bank deposit, or online payment. Wi-Fi provides wireless access to the Internet via local computer networks. Hotspots providing such access include Wi-Fi cafes, where users need to bring their own wireless devices such as a laptop or PDA. These services may be free to all, free to customers only, or fee-based.
2365) 11.854138, Internet - Wikipedia, the free encyclopedia.txt#51, term: computer, content:File sharing is an example of transferring large amounts of data across the Internet. A computer file can be emailed to customers, colleagues and friends as an attachment. It can be uploaded to a website or File Transfer Protocol (FTP) server for easy download by others. It can be put into a "shared location" or onto a file server for instant use by colleagues. The load of bulk downloads to many users can be eased by the use of "mirror" servers or peer-to-peer networks. In any of these cases, access to the file may be controlled by user authentication, the transit of the file over the Internet may be obscured by encryption, and money may change hands for access to the file. The price can be paid by the remote charging of funds from, for example, a credit card whose details are also passed usually fully encrypted across the Internet. The origin and authenticity of the file received may be checked by digital signatures or by MD5 or other message digests. These simple features of the Internet, over a worldwide basis, are changing the production, sale, and distribution of anything that can be reduced to a computer file for transmission. This includes all manner of print publications, software products, news, music, film, video, photography, graphics and the other arts. This in turn has caused seismic shifts in each of the existing industries that previously controlled the production and distribution of these products.
2366) 11.854138, Internet - Wikipedia, the free encyclopedia.txt#52, term: computer, content:Streaming media is the real-time delivery of digital media for the immediate consumption or enjoyment by end users. Many radio and television broadcasters provide Internet feeds of their live audio and video productions. They may also allow time-shift viewing or listening such as Preview, Classic Clips and Listen Again features. These providers have been joined by a range of pure Internet "broadcasters" who never had on-air licenses. This means that an Internet-connected device, such as a computer or something more specific, can be used to access on-line media in much the same way as was previously possible only with a television or radio receiver. The range of available types of content is much wider, from specialized technical webcasts to on-demand popular multimedia services. Podcasting is a variation on this theme, where usually audio material is downloaded and played back on a computer or shifted to a portable media player to be listened to on the move. These techniques using simple equipment allow anybody, with little censorship or licensing control, to broadcast audio-visual material worldwide.
2367) 11.854138, Java (programming language) - Wikipedia, the free encyclopedia.txt#0, term: computer, content:Java is a general-purpose computer programming language that is concurrent, class-based, object-oriented,[14] and specifically designed to have as few implementation dependencies as possible. It is intended to let application developers "write once, run anywhere" (WORA),[15] meaning that compiled Java code can run on all platforms that support Java without the need for recompilation.[16] Java applications are typically compiled to bytecode that can run on any Java virtual machine (JVM) regardless of computer architecture. As of 2016, Java is one of the most popular programming languages in use,[17][18][19][20] particularly for client-server web applications, with a reported 9 million developers.[21] Java was originally developed by James Gosling at Sun Microsystems (which has since been acquired by Oracle Corporation) and released in 1995 as a core component of Sun Microsystems' Java platform. The language derives much of its syntax from C and C++, but it has fewer low-level facilities than either of them.
2368) 11.854138, Massachusetts Institute of Technology - Wikipedia, the free encyclopedia.txt#35, term: computer, content:The four-year, full-time undergraduate program maintains a balance between professional majors and those in the arts and sciences, and has been dubbed "most selective" by U.S. News,[165] admitting few transfer students[156] and 8.0% of its applicants in the 2015 admissions cycle.[166] MIT offers 44 undergraduate degrees across its five schools.[167] In the 20102011 academic year, 1,161 bachelor of science degrees (abbreviated "SB") were granted, the only type of undergraduate degree MIT now awards.[168][169] In the 2011 fall term, among students who had designated a major, the School of Engineering was the most popular division, enrolling 63% of students in its 19 degree programs, followed by the School of Science (29%), School of Humanities, Arts, & Social Sciences (3.7%), Sloan School of Management (3.3%), and School of Architecture and Planning (2%). The largest undergraduate degree programs were in Electrical Engineering and Computer Science (Course 62), Computer Science and Engineering (Course 63), Mechanical Engineering (Course 2), Physics (Course 8), and Mathematics (Course 18).[163]
2369) 11.854138, Microcomputer - Wikipedia, the free encyclopedia.txt#3, term: computer, content:All these improvements in cost and usability resulted in an explosion in their popularity during the late 1970s and early 1980s. A large number of computer makers packaged microcomputers for use in small business applications. By 1979, many companies such as Cromemco, Processor Technology, IMSAI, North Star Computers, Southwest Technical Products Corporation, Ohio Scientific, Altos Computer Systems, Morrow Designs and others produced systems designed either for a resourceful end user or consulting firm to deliver business systems such as accounting, database management, and word processing to small businesses. This allowed businesses unable to afford leasing of a minicomputer or time-sharing service the opportunity to automate business functions, without (usually) hiring a full-time staff to operate the computers. A representative system of this era would have used an S100 bus, an 8-bit processor such as an Intel 8080 or Zilog Z80, and either CP/M or MP/M operating system. The increasing availability and power of desktop computers for personal use attracted the attention of more software developers. In time, and as the industry matured, the market for personal computers standardized around IBM PC compatibles running DOS, and later Windows. Modern desktop computers, video game consoles, laptops, tablet PCs, and many types of handheld devices, including mobile phones, pocket calculators, and industrial embedded systems, may all be considered examples of microcomputers according to the definition given above.
2370) 11.854138, Motorola 6800 - Wikipedia, the free encyclopedia.txt#10, term: computer, content:Jeff LaVell joined Motorola in 1966 and worked in the computer industry marketing organization. Jeff had previously worked for Collins Radio on their C8500 computer that was built with small scale ECL ICs. In 1971 he led a group that examined the needs of their existing customers such as Hewlett Packard, National Cash Register, Control Data Corporation (CDC), and Digital Equipment Corporation. They would study the customer's products and try to identify functions that could be implemented in larger integrated circuits at a lower cost. The result of the survey was a family of 15 building blocks; each could be implemented in an integrated circuit.[11] Some of these blocks were implemented in the initial M6800 release and more were added over the next few years. To evaluate the 6800 architecture while the chip was being designed, Jeff's team built an equivalent circuit using 451 small scale TTL ICs on five 10 by 10 inch (25 by 25cm) circuit boards. Later they reduced this to 114 ICs on one board by using ROMs and MSI (medium scale integration) logic devices.[15]
2371) 11.854138, MS-DOS - Wikipedia, the free encyclopedia.txt#4, term: computer, content:Within a year Microsoft licensed MS-DOS to over 70 other companies.[14] It was designed to be an OS that could run on any 8086-family computer. Each computer would have its own distinct hardware and its own version of MS-DOS, similar to the situation that existed for CP/M, and with MS-DOS emulating the same solution as CP/M to adapt for different hardware platforms. To this end, MS-DOS was designed with a modular structure with internal device drivers, minimally for primary disk drives and the console, integrated with the kernel and loaded by the boot loader, and installable device drivers for other devices loaded and integrated at boot time. The OEM would use a development kit provided by Microsoft to build a version of MS-DOS with their basic I/O drivers and a standard Microsoft kernel, which they would typically supply on disk to end users along with the hardware. Thus, there were many different versions of "MS-DOS" for different hardware, and there is a major distinction between an IBM-compatible (or ISA) machine and an MS-DOS [compatible] machine. Some machines, like the Tandy 2000, were MS-DOS compatible but not IBM-compatible, so they could run software written exclusively for MS-DOS without dependence on the peripheral hardware of the IBM PC architecture.
2372) 11.854138, National Physical Laboratory (United Kingdom) - Wikipedia, the free encyclopedia.txt#9, term: computer, content:Researchers who have worked at NPL include Donald Davies, who was one of two independent inventors of packet switching in the early 1960s;[14] D. W. Dye who did important work in developing the technology of quartz clocks; Louis Essen, who invented a more accurate atomic clock than those first built in America. Others who have spent time at NPL include Harry Huskey, a computer pioneer; Alan Turing, one of the fathers of modern digital computing who was largely responsible for the early ACE computer design; Robert Watson-Watt, generally considered the inventor of radar, Oswald Kubaschewski, the father of computational materials thermodynamics and the numerical analyst James Wilkinson. H.J. Gough one of the pioneers of research into metal fatigue worked at NPL for 19 years from 1914-38. The inventor Sir Barnes Wallis did early development work there on the "Bouncing Bomb" used in the "Dam Busters" wartime raids.[15] Sydney Goldstein and Sir James Lighthill worked in NPL's aerodynamics division during WW2 researching boundary layer theory and supersonic aerodynamics respectively. Dr Clifford Hodge also worked there and was engaged in research on semiconductors
2373) 11.854138, Rendering (computer graphics) - Wikipedia, the free encyclopedia.txt#0, term: computer, content:Rendering is the process of generating an image from a 2D or 3D model (or models in what collectively could be called a scene file) by means of computer programs. Also, the results of such a model can be called a rendering. A scene file contains objects in a strictly defined language or data structure; it would contain geometry, viewpoint, texture, lighting, and shading information as a description of the virtual scene. The data contained in the scene file is then passed to a rendering program to be processed and output to a digital image or raster graphics image file. The term "rendering" may be by analogy with an "artist's rendering" of a scene. Though the technical details of rendering methods vary, the general challenges to overcome in producing a 2D image from a 3D representation stored in a scene file are outlined as the graphics pipeline along a rendering device, such as a GPU. A GPU is a purpose-built device able to assist a CPU in performing complex rendering calculations. If a scene is to look relatively realistic and predictable under virtual lighting, the rendering software should solve the rendering equation. The rendering equation doesn't account for all lighting phenomena, but is a general lighting model for computer-generated imagery. 'Rendering' is also used to describe the process of calculating effects in a video editing program to produce final video output.
2374) 11.854138, SCSI - Wikipedia, the free encyclopedia.txt#6, term: computer, content:Since its standardization in 1986, SCSI has been commonly used in the Amiga, Atari, Apple Macintosh and Sun Microsystems (now part of Oracle Corporation) computer lines and PC server systems. Apple started using Parallel ATA (also known as IDE) for its low-end machines with the Macintosh Quadra 630 in 1994, and added it to its high-end desktops starting with the Power Macintosh G3 in 1997. Apple dropped on-board SCSI completely (in favor of IDE and FireWire) with the (Blue & White) Power Mac G3 in 1999, while still offering a PCI controller card as an option on up to the Power Macintosh G4 (AGP Graphics) models.[8] Sun switched its lower-end range to Serial ATA (SATA). Commodore included a SCSI interface on the Amiga 3000/3000T systems and it was an add-on to previous Amiga 500/2000 models. Starting with the Amiga 600/1200/4000 systems Commodore switched to the IDE interface. Atari included SCSI interface as standard in its Atari MEGA STE, Atari TT and Atari Falcon computer models. SCSI has never been popular in the low-priced IBM PC world, owing to the lower cost and adequate performance of ATA hard disk standard. However, SCSI drives and even SCSI RAIDs became common in PC workstations for video or audio production.
2375) 11.854138, Semi-Automatic Ground Environment - Wikipedia, the free encyclopedia.txt#41, term: computer, content:For airborne command posts, "as early as 1962 the Air Force began exploring possibilites for an Airborne Warning and Control System (AWACS)",[4]:266 and the Strategic Defense Architecture (SDA-2000) planned an integrated air defense and air traffic control network. The USAF declared full operational capability of the 1st 7 Joint Surveillance System ROCCs on December 23, 1980,[45] with Hughes AN/FYQ-93 systems,[80] and many of the SAGE radar stations became Joint Surveillance System (JSS) sites (e.g., San Pedro Hill Z-39 became FAA Ground Equipment Facility J-31.) The North Bay AN/FSQ-7 was dismantled and sent to Boston's Computer Museum.[citation needed] In 1996, AN/FSQ-7 components were moved to Moffett Federal Airfield for storage and later moved[when?] to the Computer History Museum in Mountain View, California. The last AN/FSQ-7 centrals were demolished at McChord AFB (August 1983) and Luke AFB (February 1984).[53] AN/FSQ-7 equipment were used as TV/movie props (e.g., in The Time Tunnel and Voyage to the Bottom of the Sea).
2376) 11.854138, Shor's algorithm - Wikipedia, the free encyclopedia.txt#3, term: computer, content:In 2001, Shor's algorithm was demonstrated by a group at IBM, who factored 15 into 35, using an NMR implementation of a quantum computer with 7 qubits.[4] After IBM's implementation, two independent groups, one at the University of Science and Technology of China, and the other one at the University of Queensland, have implemented Shor's algorithm using photonic qubits, emphasizing that multi-qubit entanglement was observed when running the Shor's algorithm circuits.[5][6] In 2012, the factorization of 15 was repeated.[7] Also in 2012, the factorization of 21 was achieved, setting the record for the largest number factored with a quantum computer.[8] In April 2012, the factorization of 143 was achieved, although this used adiabatic quantum computation rather than Shor's algorithm.[9] In November 2014, it was discovered that this 2012 adiabatic quantum computation had also factored larger numbers, the largest being 56153.[10][11] Since April 2016, the largest integer factored on a quantum device is 200099, using D-Wave 2X quantum processor [1].
2377) 11.854138, Software engineering - Wikipedia, the free encyclopedia.txt#22, term: computer, content:In the U.K. the British Computer Society has developed a legally recognized professional certification called Chartered IT Professional (CITP), available to fully qualified members (MBCS). Software engineers may be eligible for membership of the Institution of Engineering and Technology and so qualify for Chartered Engineer status. In Canada the Canadian Information Processing Society has developed a legally recognized professional certification called Information Systems Professional (ISP).[42] In Ontario, Canada, Software Engineers who graduate from a Canadian Engineering Accreditation Board (CEAB) accredited program, successfully complete PEO's (Professional Engineers Ontario) Professional Practice Examination (PPE) and have at least 48 months of acceptable engineering experience are eligible to be licensed through the Professional Engineers Ontario and can become Professional Engineers P.Eng.[43] The PEO does not recognize any online or distance education however; and does not consider Computer Science programs to be equivalent to software engineering programs despite the tremendous overlap between the two. This has sparked controversy and a certification war. It has also held the number of P.Eng holders for the profession exceptionally low. The vast majority of working professionals in the field hold a degree in CS, not SE. Given the difficult certification path for holders of non-SE degrees, most never bother to pursue the license.
2378) 11.854138, Stan Frankel - Wikipedia, the free encyclopedia.txt#0, term: computer, content:Stanley Phillips "Stan" Frankel (1919  May, 1978) was an American computer scientist. He was born in Los Angeles, attended graduate school at the University of Rochester, received his PhD in physics from the University of California, Berkeley,[1] and began his career as a post-doc student under J. Robert Oppenheimer at University of California, Berkeley in 1942. Frankel helped develop computational techniques used in the nuclear research taking place at the time. He joined the theoretical division of the Manhattan Project at Los Alamos in 1943. While there, he organized teams of persons (known as "computers") using electromechanical calculators to divide the massive calculations required for the project into manageable assembly line groups. Even that proved too slow, and Frankel turned to IBM tabulating machines to help process the numbers. This research led to his interest in the then-dawning field of digital computers. In August 1945, Frankel and Nick Metropolis traveled to the Moore School of Engineering in Pennsylvania to learn how to program the ENIAC computer. That fall they helped design a calculation that would determine the likelihood of being able to develop a fusion weapon. Edward Teller used the ENIAC results to prepare a report in the spring of 1946 that answered this question in the affirmative.
2379) 11.854138, Trackball - Wikipedia, the free encyclopedia.txt#14, term: computer, content:Computer gamers have been able to successfully use trackballs in most modern computer games, including FPS, RPG, and RTS genres, with any slight loss of speed compensated for with an increase in precision. Many trackball gamers are competent at "throwing" their cursor rapidly across the screen, by spinning the trackball, enabling (with practice) much faster motion than can be achieved with a ball-less mouse and arm motion[citation needed]. However, many gamers are deterred by the time it takes to 'get used to' the different style of hand control that a trackball requires. Trackballs have also been regarded as excellent complements to analog joysticks, as pioneered by the Assassin 3D 1996 trackball with joystick pass-through capability. This combination provides for two-hand aiming and a high accuracy and consistency replacement for the traditional mouse and keyboard combo generally used on first-person shooter games. Many such games natively support joysticks and analog player movement, like Valve's Half-Life and id Software's Quake series.
2380) 11.854138, University of Manchester - Wikipedia, the free encyclopedia.txt#72, term: computer, content:Many notable people have worked or studied at one or both of the two former institutions that now form the University of Manchester, including 25 Nobel prize laureates. Some of the best-known are: John Dalton (founder of modern atomic theory), Ludwig Wittgenstein (considered one of the most significant philosophers of the 20th century, who studied for a doctorate in engineering), George E. Davis (founder of the discipline of Chemical Engineering), Marie Stopes (pioneer of birth control and campaigner for women's rights), Bernard Lovell (a pioneer of radio astronomy), Alan Turing (one of the founders of computer science and artificial intelligence), Tom Kilburn and Frederic Calland Williams (who developed Small-Scale Experimental Machine (SSEM) or "Baby", the world's first stored-program computer at Victoria University of Manchester in 1948), Irene Khan (former Secretary General of Amnesty International), physicist and television presenter Brian Cox, the author Anthony Burgess and Robert Bolt (two times Academy Award winner and three times Golden Globe winner for writing the screenplay for Lawrence of Arabia and Doctor Zhivago).
2381) 11.854138, Video game industry - Wikipedia, the free encyclopedia.txt#56, term: computer, content:In addition, the industry is experiencing further significant change driven by convergence, with technology and player comfort being the two primary reasons for this wave of industry convergence. Video games and related content can now be accessed and played on a variety of media, including: cable television, dedicated consoles, handheld devices and smartphones, through social networking sites or through an ISP, through a game developer's website, and online through a game console and/or home or office personal computer. In fact, 12% of U.S. households already make regular use of game consoles for accessing video content provided by online services such as Hulu and Netflix. In 2012, for the first time, entertainment usage passed multiplayer game usage on Xbox, meaning that users spent more time with online video and music services and applications than playing games. This rapid type of industry convergence has caused the distinction between video game console and personal computers to disappear. A game console with high-speed microprocessors attached to a television set is, for all intents and purposes, a computer and monitor.[82]
2382) 11.854138, Wearable computer - Wikipedia, the free encyclopedia.txt#28, term: computer, content:In 1993, the Private Eye was used in Thad Starner's wearable, based on Doug Platt's system and built from a kit from Park Enterprises, a Private Eye display on loan from Devon Sean McCullough, and the Twiddler chording keyboard made by Handykey. Many iterations later this system became the MIT "Tin Lizzy" wearable computer design, and Starner went on to become one of the founders of MIT's wearable computing project. 1993 also saw Columbia University's augmented-reality system known as KARMA (Knowledge-based Augmented Reality for Maintenance Assistance). Users would wear a Private Eye display over one eye, giving an overlay effect when the real world was viewed with both eyes open. KARMA would overlay wireframe schematics and maintenance instructions on top of whatever was being repaired. For example, graphical wireframes on top of a laser printer would explain how to change the paper tray. The system used sensors attached to objects in the physical world to determine their locations, and the entire system ran tethered from a desktop computer.[28][29]
2383) 11.854138, Windows Vista - Wikipedia, the free encyclopedia.txt#27, term: computer, content:User Account Control, or UAC is perhaps the most significant and visible of these changes. UAC is a security technology that makes it possible for users to use their computer with fewer privileges by default, with a view to stopping malware from making unauthorized changes to the system. This was often difficult in previous versions of Windows, as the previous "limited" user accounts proved too restrictive and incompatible with a large proportion of application software, and even prevented some basic operations such as looking at the calendar from the notification tray. In Windows Vista, when an action is performed that requires administrative rights (such as installing/uninstalling software or making system-wide configuration changes), the user is first prompted for an administrator name and password; in cases where the user is already an administrator, the user is still prompted to confirm the pending privileged action. Regular use of the computer such as running programs, printing, or surfing the Internet does not trigger UAC prompts. User Account Control asks for credentials in a Secure Desktop mode, in which the entire screen is dimmed, and only the authorization window is active and highlighted. The intent is to stop a malicious program misleading the user by interfering with the authorization window, and to hint to the user the importance of the prompt.[83]
2384) 11.854138, Word processor - Wikipedia, the free encyclopedia.txt#21, term: computer, content:In the late 1960s IBM provided a program called FORMAT for generating printed documents on any computer capable of running Fortran IV. Written by Gerald M. Berns, FORMAT was described in his paper "Description of FORMAT, a Text-Processing Program" (Communications of the ACM, Volume 12, Number 3, March, 1969) as "a production program which facilitates the editing and printing of 'finished' documents directly on the printer of a relatively small (64k) computer system. It features good performance, totally free-form input, very flexible formatting capabilities including up to eight columns per page, automatic capitalization, aids for index construction, and a minimum of nontext [control elements] items." Input was normally on punched cards or magnetic tape, with up to 80capital letters and non-alphabetic characters per card. The limited typographical controls available were implemented by control sequences; for example, letters were automatically converted to lower case unless they followed a full stop. Output could be printed on a typical line printer in all-capitals  or in upper and lower case using a special ("TN") printer chain  or could be punched as a paper tape which could be printed, in better than line printer quality, on a Flexowriter. A workalike program with some improvements, DORMAT, was developed and used at University College London.[citation needed]
2385) 11.734999, Ada (programming language) - Wikipedia, the free encyclopedia.txt#0, term: computer, content:Ada is a structured, statically typed, imperative, wide-spectrum, and object-oriented high-level computer programming language, extended from Pascal and other languages. It has built-in language support for design-by-contract, extremely strong typing, explicit concurrency, offering tasks, synchronous message passing, protected objects, and non-determinism. Ada improves code safety and maintainability by using the compiler to find errors in favor of runtime errors. Ada is an international standard; the current version (known as Ada 2012[7]) is defined by ISO/IEC 8652:2012.[8]
2386) 11.734999, Ada (programming language) - Wikipedia, the free encyclopedia.txt#14, term: computer, content:In the 1970s, the US Department of Defense (DoD) was concerned by the number of different programming languages being used for its embedded computer system projects, many of which were obsolete or hardware-dependent, and none of which supported safe modular programming. In 1975, a working group, the High Order Language Working Group (HOLWG), was formed with the intent to reduce this number by finding or creating a programming language generally suitable for the department's and UK Ministry of Defence requirements. After many iterations beginning with an original Straw man proposal the eventual programming language was named Ada. The total number of high-level programming languages in use for such projects fell from over 450 in 1983 to 37 by 1996.
2387) 11.734999, Alan Turing - Wikipedia, the free encyclopedia.txt#17, term: computer, content:Although Turing's proof was published shortly after Alonzo Church's equivalent proof[34] using his lambda calculus, Turing's approach is considerably more accessible and intuitive than Church's.[35] It also included a notion of a 'Universal Machine' (now known as a universal Turing machine), with the idea that such a machine could perform the tasks of any other computation machine (as indeed could Church's lambda calculus). According to the ChurchTuring thesis, Turing machines and the lambda calculus are capable of computing anything that is computable. John von Neumann acknowledged that the central concept of the modern computer was due to Turing's paper.[36] To this day, Turing machines are a central object of study in theory of computation.
2388) 11.734999, Algorithm - Wikipedia, the free encyclopedia.txt#34, term: computer, content:Canonical flowchart symbols[42]: The graphical aide called a flowchart offers a way to describe and document an algorithm (and a computer program of one). Like program flow of a Minsky machine, a flowchart always starts at the top of a page and proceeds down. Its primary symbols are only 4: the directed arrow showing program flow, the rectangle (SEQUENCE, GOTO), the diamond (IF-THEN-ELSE), and the dot (OR-tie). The BhmJacopini canonical structures are made of these primitive shapes. Sub-structures can "nest" in rectangles but only if a single exit occurs from the superstructure. The symbols and their use to build the canonical structures are shown in the diagram.
2389) 11.734999, Antikythera mechanism - Wikipedia, the free encyclopedia.txt#64, term: computer, content:Freeth and Jones published their proposal in 2012 after extensive research and work. They came up with a compact and feasible solution to the question of planetary indication. They also propose indicating the solar anomaly (that is, the sun's apparent position in the zodiac dial) on a separate pointer from the date pointer, which indicates the mean position of the sun, as well as the date on the month dial. If the two dials are synchronised correctly, their front panel display is essentially the same as Wright's. Unlike Wright's model however, this model has not been built physically, and is only a 3-D computer model.[6]
2390) 11.734999, ARM architecture - Wikipedia, the free encyclopedia.txt#10, term: computer, content:The first ARM application was as a second processor for the BBC Micro, where it helped in developing simulation software to finish development of the support chips (VIDC, IOC, MEMC), and sped up the CAD software used in ARM2 development. Wilson subsequently rewrote BBC BASIC in ARM assembly language. The in-depth knowledge gained from designing the instruction set enabled the code to be very dense, making ARM BBC BASIC an extremely good test for any ARM emulator. The original aim of a principally ARM-based computer was achieved in 1987 with the release of the Acorn Archimedes.[20] In 1992, Acorn once more won the Queen's Award for Technology for the ARM.
2391) 11.734999, ARPANET - Wikipedia, the free encyclopedia.txt#2, term: computer, content:As the project progressed, protocols for internetworking were developed by which multiple separate networks could be joined into a network of networks. Access to the ARPANET was expanded in 1981 when the National Science Foundation (NSF) funded the Computer Science Network (CSNET). In 1982, the Internet protocol suite (TCP/IP) was introduced as the standard networking protocol on the ARPANET. In the early 1980s the NSF funded the establishment for national supercomputing centers at several universities, and provided interconnectivity in 1986 with the NSFNET project, which also created network access to the supercomputer sites in the United States from research and education organizations. ARPANET was decommissioned in 1990.
2392) 11.734999, Artificial intelligence - Wikipedia, the free encyclopedia.txt#2, term: computer, content:The central problems (or goals) of AI research include reasoning, knowledge, planning, learning, natural language processing (communication), perception and the ability to move and manipulate objects.[5] General intelligence is still among the field's long-term goals.[6] Currently popular approaches include statistical methods, computational intelligence and traditional symbolic AI. There are a large number of tools used in AI, including versions of search and mathematical optimization, logic, methods based on probability and economics, and many others. The AI field is interdisciplinary, in which a number of sciences and professions converge, including computer science, mathematics, psychology, linguistics, philosophy and neuroscience, as well as other specialized fields such as artificial psychology.
2393) 11.734999, Artificial intelligence - Wikipedia, the free encyclopedia.txt#3, term: computer, content:The field was founded on the claim that a central property of humans, human intelligencethe sapience of Homo sapiens sapiens"can be so precisely described that a machine can be made to simulate it."[7] This raises philosophical arguments about the nature of the mind and the ethics of creating artificial beings endowed with human-like intelligence, issues which have been explored by myth, fiction and philosophy since antiquity.[8] Artificial intelligence has been the subject of tremendous optimism[9] but has also suffered stunning setbacks.[10] Today AI techniques have become an essential part of the technology industry, providing the heavy lifting for many of the most challenging problems in computer science.[11]
2394) 11.734999, Artificial intelligence - Wikipedia, the free encyclopedia.txt#8, term: computer, content:In the early 1980s, AI research was revived by the commercial success of expert systems,[30] a form of AI program that simulated the knowledge and analytical skills of one or more human experts. By 1985 the market for AI had reached over a billion dollars. At the same time, Japan's fifth generation computer project inspired the U.S and British governments to restore funding for academic research in the field.[31] However, beginning with the collapse of the Lisp Machine market in 1987, AI once again fell into disrepute, and a second, longer-lasting AI winter began.[32]
2395) 11.734999, Assembly language - Wikipedia, the free encyclopedia.txt#5, term: computer, content:Like early programming languages such as Fortran, Algol, Cobol and Lisp, assemblers have been available since the 1950s and the first generations of text based computer interfaces. However, assemblers came first as they are far simpler to write than compilers for high-level languages. This is because each mnemonic along with the addressing modes and operands of an instruction translates rather directly into the numeric representations of that particular instruction, without much context or analysis. There have also been several classes of translators and semi automatic code generators with properties similar to both assembly and high level languages, with speedcode as perhaps one of the better known examples.
2396) 11.734999, Assembly language - Wikipedia, the free encyclopedia.txt#39, term: computer, content:Macros were used to customize large scale software systems for specific customers in the mainframe era and were also used by customer personnel to satisfy their employers' needs by making specific versions of manufacturer operating systems. This was done, for example, by systems programmers working with IBM's Conversational Monitor System / Virtual Machine (VM/CMS) and with IBM's "real time transaction processing" add-ons, Customer Information Control System CICS, and ACP/TPF, the airline/financial system that began in the 1970s and still runs many large computer reservations systems (CRS) and credit card systems today.
2397) 11.734999, Assembly language - Wikipedia, the free encyclopedia.txt#54, term: computer, content:Typical examples of large assembly language programs from this time are IBM PC DOS operating systems and early applications such as the spreadsheet program Lotus 1-2-3. Even into the 1990s, most console video games were written in assembly, including most games for the Mega Drive/Genesis and the Super Nintendo Entertainment System.[citation needed] According to some[who?] industry insiders, the assembly language was the best computer language to use to get the best performance out of the Sega Saturn, a console that was notoriously challenging to develop and program games for.[21] The arcade game NBA Jam (1993) is another example.
2398) 11.734999, Asynchronous Transfer Mode - Wikipedia, the free encyclopedia.txt#38, term: computer, content:ATM became popular with telephone companies and many computer makers in the 1990s. However, even by the end of the decade, the better price/performance of Internet Protocol-based products was competing with ATM technology for integrating real-time and bursty network traffic.[13] Companies such as FORE Systems focused on ATM products, while other large vendors such as Cisco Systems provided ATM as an option.[14] After the burst of the dot-com bubble, some still predicted that "ATM is going to dominate".[15] However, in 2005 the ATM Forum, which had been the trade organization promoting the technology, merged with groups promoting other technologies, and eventually became the Broadband Forum.[16]
2399) 11.734999, Atanasoff–Berry computer - Wikipedia, the free encyclopedia.txt#1, term: computer, content:Conceived in 1937, the machine was not programmable, being designed only to solve systems of linear equations. It was successfully tested in 1942. However, its intermediate result storage mechanism, a paper card writer/reader, was unreliable, and when John Vincent Atanasoff left Iowa State College for World War II assignments, work on the machine was discontinued.[4] The ABC pioneered important elements of modern computing, including binary arithmetic and electronic switching elements,[5] but its special-purpose nature and lack of a changeable, stored program distinguish it from modern computers. The computer was designated an IEEE Milestone in 1990.[6]
2400) 11.734999, Atanasoff–Berry computer - Wikipedia, the free encyclopedia.txt#3, term: computer, content:According to Atanasoff's account, several key principles of the AtanasoffBerry Computer were conceived in a sudden insight after a long nighttime drive to Rock Island, Illinois, during the winter of 193738. The ABC innovations included electronic computation, binary arithmetic, parallel processing, regenerative capacitor memory, and a separation of memory and computing functions. The mechanical and logic design was worked out by Atanasoff over the next year. A grant application to build a proof of concept prototype was submitted in March 1939 to the Agronomy department which was also interested in speeding up computation for economic and research analysis. $5,000 of further funding (equivalent to $85,000 in 2015) to complete the machine came from the nonprofit Research Corporation of New York City.
2401) 11.734999, Atanasoff–Berry computer - Wikipedia, the free encyclopedia.txt#7, term: computer, content:The memory of the AtanasoffBerry Computer was a system called regenerative capacitor memory, which consisted of a pair of drums, each containing 1600 capacitors that rotated on a common shaft once per second. The capacitors on each drum were organized into 32 "bands" of 50 (30 active bands and two spares in case a capacitor failed), giving the machine a speed of 30 additions/subtractions per second. Data was represented as 50-bit binary fixed-point numbers. The electronics of the memory and arithmetic units could store and operate on 60 such numbers at a time (3000 bits).
2402) 11.734999, BASIC - Wikipedia, the free encyclopedia.txt#8, term: computer, content:Knowledge of the relatively simple BASIC became widespread for a computer language, and it was implemented by a number of manufacturers, becoming fairly popular on newer minicomputers such as the DEC PDP series, where BASIC-PLUS was an extended dialect for use on the RSTS/E time-sharing operating system. The BASIC language was available for the Data General Nova, and also central to the HP Time-Shared BASIC system in the late 1960s and early 1970s, where the language was implemented as an interpreter. A version was a core part of the Pick operating system from 1973 onward, where a compiler renders it into bytecode, able to be interpreted by a virtual machine.
2403) 11.734999, BASIC - Wikipedia, the free encyclopedia.txt#10, term: computer, content:The introduction of the first microcomputers in the mid-1970s was the start of explosive growth for BASIC. It had the advantage that it was fairly well known to the young designers and computer hobbyists who took an interest in microcomputers. Despite Dijkstra's famous judgement in 1975, "It is practically impossible to teach good programming to students that have had a prior exposure to BASIC: as potential programmers they are mentally mutilated beyond hope of regeneration",[10] BASIC was one of the few languages that was both high-level enough to be usable by those without training and small enough to fit into the microcomputers of the day, making it the de facto standard programming language on early microcomputers.
2404) 11.734999, Berkeley Software Distribution - Wikipedia, the free encyclopedia.txt#7, term: computer, content:A VAX computer was installed at Berkeley in 1978, but the port of Unix to the VAX architecture, UNIX/32V, did not take advantage of the VAX's virtual memory capabilities. The kernel of 32V was largely rewritten by Berkeley students to include a virtual memory implementation, and a complete operating system including the new kernel, ports of the 2BSD utilities to the VAX, and the utilities from 32V was released as 3BSD at the end of 1979. 3BSD was also alternatively called Virtual VAX/UNIX or VMUNIX (for Virtual Memory Unix), and BSD kernel images were normally called /vmunix until 4.4BSD.
2405) 11.734999, BIOS - Wikipedia, the free encyclopedia.txt#9, term: computer, content:A modern Wintel-compatible computer provides a setup routine essentially unchanged in nature from the ROM-resident BIOS setup utilities of the late 1990s; the user can configure hardware options using the keyboard and video display. Also, when errors occur at boot time, a modern BIOS usually displays user-friendly error messages, often presented as pop-up boxes in a TUI style, and offers to enter the BIOS setup utility or to ignore the error and proceed if possible. Instead of battery-backed RAM, the modern Wintel machine may store the BIOS configuration settings in flash ROM, perhaps the same flash ROM that holds the BIOS itself.
2406) 11.734999, BIOS - Wikipedia, the free encyclopedia.txt#37, term: computer, content:At this point, the extension ROM code takes over, typically testing and initializing the hardware it controls and registering interrupt vectors for use by post-boot applications. It may use BIOS services (including those provided by previously initialized option ROMs) to provide a user configuration interface, to display diagnostic information, or to do anything else that it requires. While the actions mentioned are typical behaviors of BIOS extension ROMs, each option ROM receives total control of the computer and may do anything at all, as noted with more detail in the Extensions section below; it is possible that an option ROM will not return to BIOS, pre-empting the BIOS's boot sequence altogether.
2407) 11.734999, Bit - Wikipedia, the free encyclopedia.txt#6, term: computer, content:The encoding of data by discrete bits was used in the punched cards invented by Basile Bouchon and Jean-Baptiste Falcon (1732), developed by Joseph Marie Jacquard (1804), and later adopted by Semen Korsakov, Charles Babbage, Hermann Hollerith, and early computer manufacturers like IBM. Another variant of that idea was the perforated paper tape. In all those systems, the medium (card or tape) conceptually carried an array of hole positions; each position could be either punched through or not, thus carrying onebit of information. The encoding of text by bits was also used in Morse code (1844) and early digital communications machines such as teletypes and stock ticker machines (1870).
2408) 11.734999, Bit - Wikipedia, the free encyclopedia.txt#7, term: computer, content:Ralph Hartley suggested the use of a logarithmic measure of information in 1928.[6] Claude E. Shannon first used the word bit in his seminal 1948 paper A Mathematical Theory of Communication.[7] He attributed its origin to John W. Tukey, who had written a Bell Labs memo on 9 January 1947 in which he contracted "binary digit" to simply "bit". Interestingly, Vannevar Bush had written in 1936 of "bits of information" that could be stored on the punched cards used in the mechanical computers of that time.[8] The first programmable computer built by Konrad Zuse used binary notation for numbers.
2409) 11.734999, Booting - Wikipedia, the free encyclopedia.txt#8, term: computer, content:The IBM 701 computer (19521956) had a "Load" button that initiated reading of the first 36-bit word into main memory from a punched card in a card reader, a magnetic tape in a tape drive, or a magnetic drum unit, depending on the position of the Load Selector switch. The left 18-bit half-word was then executed as an instruction, which usually read additional words into memory.[7][8] The loaded boot program was then executed, which, in turn, loaded a larger program from that medium into memory without further help from the human operator. The term "boot" has been used in this sense since at least 1958.[9]
2410) 11.734999, Booting - Wikipedia, the free encyclopedia.txt#36, term: computer, content:Smaller computers often use less flexible but more automatic boot loader mechanisms to ensure that the computer starts quickly and with a predetermined software configuration. In many desktop computers, for example, the bootstrapping process begins with the CPU executing software contained in ROM (for example, the BIOS of an IBM PC) at a predefined address (some CPUs, including the Intel x86 series are designed to execute this software after reset without outside help). This software contains rudimentary functionality to search for devices eligible to participate in booting, and load a small program from a special section (most commonly the boot sector) of the most promising device, typically starting at a fixed entry point such as the start of the sector.
2411) 11.734999, Booting - Wikipedia, the free encyclopedia.txt#41, term: computer, content:The boot process can be considered complete when the computer is ready to interact with the user, or the operating system is capable of running system programs or application programs. Typical modern personal computers boot in about one minute, of which about 15 seconds are taken by a power-on self-test (POST) and a preliminary boot loader, and the rest by loading the operating system and other software. Time spent after the operating system loading can be considerably shortened to as little as 3 seconds[28] by bringing the system up with all cores at once, as with coreboot.[29] Large servers may take several minutes to boot and start all their services.
2412) 11.734999, Brian Randell - Wikipedia, the free encyclopedia.txt#2, term: computer, content:Randell was employed at English Electric from 1957 to 1964 where he was working on compilers. His work on Algol 60 is particularly known, including the development of a compiler for the English Electric KDF9, an early stack machine.[1] In 1964 he joined IBM, where he worked at the Thomas J. Watson Research Center on high performance computer architectures and also on operating system design methodology. In May 1969 he became a Professor of Computing Science at the University of Newcastle upon Tyne, where he has been working ever since in the area of software fault tolerance and dependability.
2413) 11.734999, Bus (computing) - Wikipedia, the free encyclopedia.txt#15, term: computer, content:One of the first complications was the use of interrupts. Early computer programs performed I/O by waiting in a loop for the peripheral to become ready. This was a waste of time for programs that had other tasks to do. Also, if the program attempted to perform those other tasks, it might take too long for the program to check again, resulting in loss of data. Engineers thus arranged for the peripherals to interrupt the CPU. The interrupts had to be prioritized, because the CPU can only execute code for one peripheral at a time, and some devices are more time-critical than others.
2414) 11.734999, Calculator - Wikipedia, the free encyclopedia.txt#19, term: computer, content:For instance, instead of a hardware multiplier, a calculator might implement floating point mathematics with code in ROM, and compute trigonometric functions with the CORDIC algorithm because CORDIC does not require hardware floating-point. Bit serial logic designs are more common in calculators whereas bit parallel designs dominate general-purpose computers, because a bit serial design minimizes chip complexity, but takes many more clock cycles. (Again, the line blurs with high-end calculators, which use processor chips associated with computer and embedded systems design, particularly the Z80, MC68000, and ARM architectures, as well as some custom designs specifically made for the calculator market.)
2415) 11.734999, Calculator - Wikipedia, the free encyclopedia.txt#30, term: computer, content:The Olivetti Programma 101 was introduced in late 1965; it was a stored program machine which could read and write magnetic cards and displayed results on its built-in printer. Memory, implemented by an acoustic delay line, could be partitioned between program steps, constants, and data registers. Programming allowed conditional testing and programs could also be overlaid by reading from magnetic cards. It is regarded as the first personal computer produced by a company (that is, a desktop electronic calculating machine programmable by non-specialists for personal use). The Olivetti Programma 101 won many industrial design awards.
2416) 11.734999, Cellular automaton - Wikipedia, the free encyclopedia.txt#2, term: computer, content:The concept was originally discovered in the 1940s by Stanislaw Ulam and John von Neumann while they were contemporaries at Los Alamos National Laboratory. While studied by some throughout the 1950s and 1960s, it was not until the 1970s and Conway's Game of Life, a two-dimensional cellular automaton, that interest in the subject expanded beyond academia. In the 1980s, Stephen Wolfram engaged in a systematic study of one-dimensional cellular automata, or what he calls elementary cellular automata; his research assistant Matthew Cook showed that one of these rules is Turing-complete. Wolfram published A New Kind of Science in 2002, claiming that cellular automata have applications in many fields of science. These include computer processors and cryptography.
2417) 11.734999, Cellular automaton - Wikipedia, the free encyclopedia.txt#43, term: computer, content:The BelousovZhabotinsky reaction is a spatio-temporal chemical oscillator that can be simulated by means of a cellular automaton. In the 1950s A. M. Zhabotinsky (extending the work of B. P. Belousov) discovered that when a thin, homogenous layer of a mixture of malonic acid, acidified bromate, and a ceric salt were mixed together and left undisturbed, fascinating geometric patterns such as concentric circles and spirals propagate across the medium. In the "Computer Recreations" section of the August 1988 issue of Scientific American,[68] A. K. Dewdney discussed a cellular automaton[69] developed by Martin Gerhardt and Heike Schuster of the University of Bielefeld (Germany). This automaton produces wave patterns that resemble those in the Belousov-Zhabotinsky reaction.
2418) 11.734999, Central processing unit - Wikipedia, the free encyclopedia.txt#2, term: computer, content:Most modern CPUs are microprocessors, meaning they are contained on a single integrated circuit (IC) chip. An IC that contains a CPU may also contain memory, peripheral interfaces, and other components of a computer; such integrated devices are variously called microcontrollers or systems on a chip (SoC). Some computers employ a multi-core processor, which is a single chip containing two or more CPUs called "cores"; in that context, single chips are sometimes referred to as "sockets".[3] Array processors or vector processors have multiple processors that operate in parallel, with no unit considered central.
2419) 11.734999, Central processing unit - Wikipedia, the free encyclopedia.txt#11, term: computer, content:During this period, a method of manufacturing many interconnected transistors in a compact space was developed. The integrated circuit (IC) allowed a large number of transistors to be manufactured on a single semiconductor-based die, or "chip". At first only very basic non-specialized digital circuits such as NOR gates were miniaturized into ICs. CPUs based upon these "building block" ICs are generally referred to as "small-scale integration" (SSI) devices. SSI ICs, such as the ones used in the Apollo guidance computer, usually contained up to a few score transistors. To build an entire CPU out of SSI ICs required thousands of individual chips, but still consumed much less space and power than earlier discrete transistor designs.
2420) 11.734999, Central processing unit - Wikipedia, the free encyclopedia.txt#13, term: computer, content:Lee Boysel published influential articles, including a 1967 "manifesto", which described how to build the equivalent of a 32-bit mainframe computer from a relatively small number of large-scale integration circuits (LSI).[33][34] At the time, the only way to build LSI chips, which are chips with a hundred or more gates, was to build them using a MOS process (i.e., PMOS logic, NMOS logic, or CMOS logic). However, some companies continued to build processors out of bipolar chips because bipolar junction transistors were so much faster than MOS chips; for example, Datapoint built processors out of TTL chips until the early 1980s.[34]
2421) 11.734999, Charles Babbage - Wikipedia, the free encyclopedia.txt#73, term: computer, content:Ada Lovelace corresponded with him during his development of the Analytical Engine. She is credited with developing an algorithm for the Analytical Engine to calculate a sequence of Bernoulli numbers. Although there is disagreement over how much of the ideas were Lovelace's own, she is often described as the first computer programmer.[154] She also translated and wrote literature supporting the project. With respect to the engine's programming by punch cards, she once wrote: "We may say most aptly that the Analytical Engine weaves algebraical patterns just as the Jacquard-loom weaves flowers and leaves."[153]
2422) 11.734999, Chemical computer - Wikipedia, the free encyclopedia.txt#2, term: computer, content:The wave properties of the BZ reaction means it can move information in the same way as all other waves. This still leaves the need for computation, performed by conventional microchips using the binary code transmitting and changing ones and zeros through a complicated system of logic gates. To perform any conceivable computation it is sufficient to have NAND gates. (A NAND gate has two bits input. Its output is 0 if both bits are 1, otherwise it's 1). In the chemical computer version logic gates are implemented by concentration waves blocking or amplifying each other in different ways.
2423) 11.734999, COBOL - Wikipedia, the free encyclopedia.txt#0, term: computer, content:COBOL (/kobl/, an acronym for common business-oriented language) is a compiled English-like computer programming language designed for business use. It is imperative, procedural and, since 2002, object-oriented. COBOL is primarily used in business, finance, and administrative systems for companies and governments. COBOL is still widely used in legacy applications deployed on mainframe computers, such as large-scale batch and transaction processing jobs. But due to its declining popularity and the retirement of experienced COBOL programmers, programs are being migrated to new platforms, rewritten in modern languages or replaced with software packages.[6] Most programming in COBOL is now purely to maintain existing applications.[7]
2424) 11.734999, COBOL - Wikipedia, the free encyclopedia.txt#1, term: computer, content:COBOL was designed in 1959, by CODASYL and was partly based on previous programming language design work by Grace Hopper, commonly referred to as "the (grand)mother of COBOL".[8][9][10] It was created as part of a US Department of Defense effort to create a portable programming language for data processing. Intended as a stopgap, the Department of Defense promptly forced computer manufacturers to provide it, resulting in its widespread adoption.[11] It was standardized in 1968 and has since been revised four times. Expansions include support for structured and object-oriented programming. The current standard is ISO/IEC 1989:2014.[12]
2425) 11.734999, Compiler - Wikipedia, the free encyclopedia.txt#6, term: computer, content:The first high-level programming language (Plankalkl) was proposed by Konrad Zuse in 1943. The first compiler was written by Grace Hopper, in 1952, for the A-0 programming language; the A-0 functioned more as a loader or linker than the modern notion of a compiler. The first autocode and its compiler were developed by Alick Glennie in 1952 for the Mark 1 computer at the University of Manchester and is considered by some to be the first compiled programming language.[2] The FORTRAN team led by John Backus at IBM is generally credited as having introduced the first complete compiler in 1957. COBOL was an early language to be compiled on multiple architectures, in 1960.[3]
2426) 11.734999, Computational science - Wikipedia, the free encyclopedia.txt#8, term: computer, content:Both historically and today, Fortran remains popular for most applications of scientific computing.[2][3] Other programming languages and computer algebra systems commonly used for the more mathematical aspects of scientific computing applications include GNU Octave, Haskell,[2] Julia,[2] Maple,[3] Mathematica,[4] MATLAB, Python (with third-party SciPy library), Perl (with third-party PDL library),[citation needed] R, SciLab, and TK Solver. The more computationally intensive aspects of scientific computing will often use some variation of C or Fortran and optimized algebra libraries such as BLAS or LAPACK.
2427) 11.734999, Computer - Simple English Wikipedia, the free encyclopedia.txt#10, term: computer, content:Some people did not want a machine that would do the same thing over and over again. For example, a music box is a machine that plays the same music over and over again. Some people wanted to be able to tell their machine to do different things. For example, they wanted to tell the music box to play different music every time. They wanted to be able to program the music box- to order the music box to play different music. This part of computer history is called the "history of programmable machines" which is a fancy phrase for "The history of machines that I can order to do different things if I know how to speak their language."
2428) 11.734999, Computer - Simple English Wikipedia, the free encyclopedia.txt#23, term: computer, content:In the 1950's computers were built out of mostly vacuum tubes. Transistors replaced vacuum tubes in the 1960's because they were smaller and cheaper. They also need less power and do not break down as much as vacuum tubes. In the 1970s, technologies were based on integrated circuits. Microprocessors, such as the Intel 4004 made computers smaller and cheaper. They also made computers faster and more reliable. By the 1980s, computers became small and cheap enough to replace mechanical controls in things like washing machines. The 1980s also saw home computers and personal computer. With the evolution of the Internet, personal computers are becoming as common as the television and the telephone in the household.
2429) 11.734999, Computer - Wikipedia, the free encyclopedia.txt#9, term: computer, content:The slide rule was invented around 16201630, shortly after the publication of the concept of the logarithm. It is a hand-operated analog computer for doing multiplication and division. As slide rule development progressed, added scales provided reciprocals, squares and square roots, cubes and cube roots, as well as transcendental functions such as logarithms and exponentials, circular and hyperbolic trigonometry and other functions. Aviation is one of the few fields where slide rules are still in widespread use, particularly for solving timedistance problems in light aircraft. To save space and for ease of reading, these are typically circular devices rather than the classic linear slide rule shape. A popular example is the E6B.
2430) 11.734999, Computer - Wikipedia, the free encyclopedia.txt#14, term: computer, content:The machine was about a century ahead of its time. All the parts for his machine had to be made by hand this was a major problem for a device with thousands of parts. Eventually, the project was dissolved with the decision of the British Government to cease funding. Babbage's failure to complete the analytical engine can be chiefly attributed to difficulties not only of politics and financing, but also to his desire to develop an increasingly sophisticated computer and to move ahead faster than anyone else could follow. Nevertheless, his son, Henry Babbage, completed a simplified version of the analytical engine's computing unit (the mill) in 1888. He gave a successful demonstration of its use in computing tables in 1906.
2431) 11.734999, Computer - Wikipedia, the free encyclopedia.txt#24, term: computer, content:Colossus was the world's first electronic digital programmable computer.[17] It used a large number of valves (vacuum tubes). It had paper-tape input and was capable of being configured to perform a variety of boolean logical operations on its data, but it was not Turing-complete. Nine Mk II Colossi were built (The Mk I was converted to a Mk II making ten machines in total). Colossus Mark I contained 1500 thermionic valves (tubes), but Mark II with 2400 valves, was both 5 times faster and simpler to operate than Mark 1, greatly speeding the decoding process.[30][31]
2432) 11.734999, Computer - Wikipedia, the free encyclopedia.txt#68, term: computer, content:In almost all modern computers, each memory cell is set up to store binary numbers in groups of eight bits (called a byte). Each byte is able to represent 256 different numbers (28 = 256); either from 0 to 255 or 128 to +127. To store larger numbers, several consecutive bytes may be used (typically, two, four or eight). When negative numbers are required, they are usually stored in two's complement notation. Other arrangements are possible, but are usually not seen outside of specialized applications or historical contexts. A computer can store any kind of information in memory if it can be represented numerically. Modern computers have billions or even trillions of bytes of memory.
2433) 11.734999, Computer animation - Wikipedia, the free encyclopedia.txt#3, term: computer, content:For 3D animations, all frames must be rendered after the modeling is complete. For 2D vector animations, the rendering process is the key frame illustration process, while tweened frames are rendered as needed. For pre-recorded presentations, the rendered frames are transferred to a different format or medium, like digital video. The frames may also be rendered in real time as they are presented to the end-user audience. Low bandwidth animations transmitted via the internet (e.g. Adobe Flash, X3D) often use software on the end-users computer to render in real time as an alternative to streaming or pre-loaded high bandwidth animations.
2434) 11.734999, Computer animation - Wikipedia, the free encyclopedia.txt#22, term: computer, content:One of the greatest challenges in computer animation has been creating human characters that look and move with the highest degree of realism. Part of the difficulty in making pleasing, realistic human characters is the uncanny valley, the concept where the human audience (up to a point) tends to have an increasingly negative, emotional response as a human replica looks and acts more and more human. Films that have attempted photorealistic human characters, such as The Polar Express,[28][29][30] Beowulf,[31] and A Christmas Carol[32][33] have been criticized as "creepy" and "disconcerting".
2435) 11.734999, Computer architecture - Wikipedia, the free encyclopedia.txt#25, term: computer, content:In a typical home computer, the simplest, most reliable way to speed performance is usually to add random access memory (RAM). More RAM increases the likelihood that needed data or a program is stored in the RAMso the system is less likely to need to move memory data from the disk. The reason why RAM is important is because in a HDD (Hard disk drive) you have physical moving parts that you would need to move to access certain parts of a memory. SSD (Solid state drive) are faster than HDD but they still are much slower than the read/write speed of RAM.
2436) 11.734999, Computer cluster - Wikipedia, the free encyclopedia.txt#7, term: computer, content:A computer cluster may be a simple two-node system which just connects two personal computers, or may be a very fast supercomputer. A basic approach to building a cluster is that of a Beowulf cluster which may be built with a few personal computers to produce a cost-effective alternative to traditional high performance computing. An early project that showed the viability of the concept was the 133-node Stone Soupercomputer.[7] The developers used Linux, the Parallel Virtual Machine toolkit and the Message Passing Interface library to achieve high performance at a relatively low cost.[8]
2437) 11.734999, Computer cluster - Wikipedia, the free encyclopedia.txt#20, term: computer, content:In a Beowulf system, the application programs never see the computational nodes (also called slave computers) but only interact with the "Master" which is a specific computer handling the scheduling and management of the slaves.[14] In a typical implementation the Master has two network interfaces, one that communicates with the private Beowulf network for the slaves, the other for the general purpose network of the organization.[14] The slave computers typically have their own version of the same operating system, and local memory and disk space. However, the private slave network may also have a large and shared file server that stores global persistent data, accessed by the slaves as needed.[14]
2438) 11.734999, Computer data storage - Wikipedia, the free encyclopedia.txt#3, term: computer, content:Without a significant amount of memory, a computer would merely be able to perform fixed operations and immediately output the result. It would have to be reconfigured to change its behavior. This is acceptable for devices such as desk calculators, digital signal processors, and other specialized devices. Von Neumann machines differ in having a memory in which they store their operating instructions and data. Such computers are more versatile in that they do not need to have their hardware reconfigured for each new program, but can simply be reprogrammed with new in-memory instructions; they also tend to be simpler to design, in that a relatively simple processor may keep state between successive computations to build up complex procedural results. Most modern computers are von Neumann machines.
2439) 11.734999, Computer data storage - Wikipedia, the free encyclopedia.txt#45, term: computer, content:Paper data storage, typically in the form of paper tape or punched cards, has long been used to store information for automatic processing, particularly before general-purpose computers existed. Information was recorded by punching holes into the paper or cardboard medium and was read mechanically (or later optically) to determine whether a particular location on the medium was solid or contained a hole. A few technologies allow people to make marks on paper that are easily read by machinethese are widely used for tabulating votes and grading standardized tests. Barcodes made it possible for any object that was to be sold or transported to have some computer readable information securely attached to it.
2440) 11.734999, Computer hardware - Wikipedia, the free encyclopedia.txt#6, term: computer, content:The computer case is a plastic or metal enclosure that houses most of the components. Those found on desktop computers are usually small enough to fit under a desk; however, in recent years more compact designs have become more commonplace, such as the all-in-one style designs from Apple, namely the iMac. A case can be either big or small, but the form factor of motherboard for which it is designed matters more.[6] Laptops are computers that usually come in a clamshell form factor; however, in more recent years, deviations from this form factor, such as laptops that have a detachable screen that become tablet computers in their own right, have started to emerge.
2441) 11.734999, Computer keyboard - Wikipedia, the free encyclopedia.txt#15, term: computer, content:Flexible keyboards are a junction between normal type and laptop type keyboards, normal from the full arrangement of keys, and laptop from the sort key distance, additionally the flexibility it allows the user to fold/roll the keyboard for better storage / transfer, however for typing, the keyboard must be resting on a hard surface. The vast majority of flexible keyboards in market are made from silicone, this material makes it water and dust proof, a very pleasant feature especially in hospitals where keyboards are subjected to frequent washing. For connection with the computer, the keyboards having USB cable and the support of operating systems reach far back as the Windows 2000.
2442) 11.734999, Computer keyboard - Wikipedia, the free encyclopedia.txt#25, term: computer, content:Software keyboards or on-screen keyboards often take the form of computer programs that display an image of a keyboard on the screen. Another input device such as a mouse or a touchscreen can be used to operate each virtual key to enter text. Software keyboards have become very popular in touchscreen enabled cell phones, due to the additional cost and space requirements of other types of hardware keyboards. Microsoft Windows, Mac OS X, and some varieties of Linux include on-screen keyboards that can be controlled with the mouse. In software keyboards, the mouse has to be maneuvered onto the on-screen letters given by the software. On the click of a letter, the software writes the respective letter on the respective spot.
2443) 11.734999, Computer keyboard - Wikipedia, the free encyclopedia.txt#29, term: computer, content:There are a number of different arrangements of alphabetic, numeric, and punctuation symbols on keys. These different keyboard layouts arise mainly because different people need easy access to different symbols, either because they are inputting text in different languages, or because they need a specialized layout for mathematics, accounting, computer programming, or other purposes. The United States keyboard layout is used as default in the currently most popular operating systems: Windows,[4] Mac OS X[5] and Linux.[6][7] The common QWERTY-based layout was designed early in the era of mechanical typewriters, so its ergonomics were compromised to allow for the mechanical limitations of the typewriter.
2444) 11.734999, Computer keyboard - Wikipedia, the free encyclopedia.txt#39, term: computer, content:Many, but not all,computer keyboards have a numeric keypad to the right of the alphabetic keyboard which contains numbers, basic mathematical symbols (e.g., addition, subtraction, etc.), and a few function keys. On Japanese/Korean keyboards, there may be Language input keys for changing the language to use. Some keyboards have power management keys (e.g., power key, sleep key and wake key); Internet keys to access a web browser or E-mail; and/or multimedia keys, such as volume controls or keys that can be programmed by the user to launch a specified software or command like launching a game or minimize all windows.
2445) 11.734999, Computer keyboard - Wikipedia, the free encyclopedia.txt#69, term: computer, content:Keystroke logging can be achieved by both hardware and software means. Hardware key loggers are attached to the keyboard cable or installed inside standard keyboards. Software keyloggers work on the target computer's operating system and gain unauthorized access to the hardware, hook into the keyboard with functions provided by the OS, or use remote access software to transmit recorded data out of the target computer to a remote location. Some hackers also use wireless keylogger sniffers to collect packets of data being transferred from a wireless keyboard and its receiver, and then they crack the encryption key being used to secure wireless communications between the two devices.
2446) 11.734999, Computer keyboard - Wikipedia, the free encyclopedia.txt#71, term: computer, content:Keyboards are also known to emit electromagnetic signatures that can be detected using special spying equipment to reconstruct the keys pressed on the keyboard. Neal O'Farrell, executive director of the Identity Theft Council, revealed to InformationWeek that "More than 25 years ago, a couple of former spooks showed me how they could capture a user's ATM PIN, from a van parked across the street, simply by capturing and decoding the electromagnetic signals generated by every keystroke," O'Farrell said. "They could even capture keystrokes from computers in nearby offices, but the technology wasn't sophisticated enough to focus in on any specific computer."[16]
2447) 11.734999, Computer memory - Wikipedia, the free encyclopedia.txt#6, term: computer, content:Two alternatives to the delay line, the Williams tube and Selectron tube, originated in 1946, both using electron beams in glass tubes as means of storage. Using cathode ray tubes, Fred Williams would invent the Williams tube, which would be the first random access computer memory. The Williams tube would prove more capacious than the Selectron tube (the Selectron was limited to 256 bits, while the Williams tube could store thousands) and less expensive. The Williams tube would nevertheless prove to be frustratingly sensitive to environmental disturbances.
2448) 11.734999, Computer memory - Wikipedia, the free encyclopedia.txt#10, term: computer, content:Volatile memory is computer memory that requires power to maintain the stored information. Most modern semiconductor volatile memory is either static RAM (SRAM) or dynamic RAM (DRAM). SRAM retains its contents as long as the power is connected and is easy for interfacing, but uses six transistors per bit. Dynamic RAM is more complicated for interfacing and control, needing regular refresh cycles to prevent losing its contents, but uses only one transistor and one capacitor per bit, allowing it to reach much higher densities and much cheaper per-bit costs.
2449) 11.734999, Computer monitor - Wikipedia, the free encyclopedia.txt#5, term: computer, content:The first computer monitors used cathode ray tubes (CRTs). Prior to the advent of home computers in the late 1970s, it was common for a video display terminal (VDT) using a CRT to be physically integrated with a keyboard and other components of the system in a single large chassis. The display was monochrome and far less sharp and detailed than on a modern flat-panel monitor, necessitating the use of relatively large text and severely limiting the amount of information that could be displayed at one time. High-resolution CRT displays were developed for specialized military, industrial and scientific applications but they were far too costly for general use.
2450) 11.734999, Computer monitor - Wikipedia, the free encyclopedia.txt#8, term: computer, content:There are multiple technologies that have been used to implement liquid crystal displays (LCD). Throughout the 1990s, the primary use of LCD technology as computer monitors was in laptops where the lower power consumption, lighter weight, and smaller physical size of LCDs justified the higher price versus a CRT. Commonly, the same laptop would be offered with an assortment of display options at increasing price points: (active or passive) monochrome, passive color, or active matrix color (TFT). As volume and manufacturing capability have improved, the monochrome and passive color technologies were dropped from most product lines.
2451) 11.734999, Computer monitor - Wikipedia, the free encyclopedia.txt#40, term: computer, content:A panel mount computer monitor is intended for mounting into a flat surface with the front of the display unit protruding just slightly. They may also be mounted to the rear of the panel. A flange is provided around the LCD, sides, top and bottom, to allow mounting. This contrasts with a rack mount display where the flanges are only on the sides. The flanges will be provided with holes for thru-bolts or may have studs welded to the rear surface to secure the unit in the hole in the panel. Often a gasket is provided to provide a water-tight seal to the panel and the front of the LCD will be sealed to the back of the front panel to prevent water and dirt contamination.
2452) 11.734999, Computer mouse - Wikipedia, the free encyclopedia.txt#7, term: computer, content:DATAR was similar in concept to Benjamin's display. The trackball used four disks to pick up motion, two each for the X and Y directions. Several rollers provided mechanical support. When the ball was rolled, the pickup discs spun and contacts on their outer rim made periodic contact with wires, producing pulses of output with each movement of the ball. By counting the pulses, the physical movement of the ball could be determined. A digital computer calculated the tracks, and sent the resulting data to other ships in a task force using pulse-code modulation radio signals. This trackball used a standard Canadian five-pin bowling ball. It was not patented, as it was a secret military project as well.[8][9]
2453) 11.734999, Computer mouse - Wikipedia, the free encyclopedia.txt#8, term: computer, content:On 2 October 1968, a mouse device named Rollkugel (German for "rolling ball") was released that had been developed and published by the German company Telefunken. As the name suggests and unlike Engelbart's mouse, the Telefunken model already had a ball. It was based on an earlier trackball-like device (also named Rollkugel) that was embedded into radar flight control desks. This had been developed around 1965 by a team led by Rainer Mallebrein at Telefunken Konstanz for the German Bundesanstalt fr Flugsicherung as part of their TR86 process computer system with its SIG100-86[10] vector graphics terminal.
2454) 11.734999, Computer mouse - Wikipedia, the free encyclopedia.txt#23, term: computer, content:The ball mouse replaced the external wheels with a single ball that could rotate in any direction. It came as part of the hardware package of the Xerox Alto computer. Perpendicular chopper wheels housed inside the mouse's body chopped beams of light on the way to light sensors, thus detecting in their turn the motion of the ball. This variant of the mouse resembled an inverted trackball and became the predominant form used with personal computers throughout the 1980s and 1990s. The Xerox PARC group also settled on the modern technique of using both hands to type on a full-size keyboard and grabbing the mouse when required.
2455) 11.734999, Computer mouse - Wikipedia, the free encyclopedia.txt#27, term: computer, content:Modern computer mice took form at the cole Polytechnique Fdrale de Lausanne (EPFL) under the inspiration of Professor Jean-Daniel Nicoud and at the hands of engineer and watchmaker Andr Guignard.[33] This new design incorporated a single hard rubber mouseball and three buttons, and remained a common design until the mainstream adoption of the scroll-wheel mouse during the 1990s.[34] In 1985, Ren Sommer added a microprocessor to Nicoud's and Guignard's design.[35] Through this innovation, Sommer is credited with inventing a significant component of the mouse, which made it more "intelligent;"[35] though optical mice from Mouse Systems had incorporated microprocessors by 1984.[36]
2456) 11.734999, Computer mouse - Wikipedia, the free encyclopedia.txt#48, term: computer, content:With the arrival of the IBM PS/2 personal-computer series in 1987, IBM introduced the eponymous PS/2 interface for mice and keyboards, which other manufacturers rapidly adopted. The most visible change was the use of a round 6-pin mini-DIN, in lieu of the former 5-pin connector. In default mode (called stream mode) a PS/2 mouse communicates motion, and the state of each button, by means of 3-byte packets.[60] For any motion, button press or button release event, a PS/2 mouse sends, over a bi-directional serial port, a sequence of three bytes, with the following format:
2457) 11.734999, Computer mouse - Wikipedia, the free encyclopedia.txt#54, term: computer, content:Cordless or wireless mice transmit data via infrared radiation (see IrDA) or radio (including Bluetooth and Wi-Fi). The receiver is connected to the computer through a serial or USB port, or can be built in (as is sometimes the case with Bluetooth and WiFi[64]). Modern non-Bluetooth and non-WiFi wireless mice use USB receivers. Some of these can be stored inside the mouse for safe transport while not in use, while other, newer mice use newer "nano" receivers, designed to be small enough to remain plugged into a laptop during transport, while still being large enough to easily remove.[65]
2458) 11.734999, Computer multitasking - Wikipedia, the free encyclopedia.txt#2, term: computer, content:In the case of a computer with a single CPU, only one task is said to be running at any point in time, meaning that the CPU is actively executing instructions for that task. Multitasking solves the problem by scheduling which task may be the one running at any given time, and when another waiting task gets a turn. The act of reassigning a CPU from one task to another one is called a context switch; the illusion of parallelism is achieved when context switches occur frequently enough. Operating systems may adopt one of many different scheduling strategies, which generally fall into the following categories:[citation needed]
2459) 11.734999, Computer multitasking - Wikipedia, the free encyclopedia.txt#11, term: computer, content:Preemptive multitasking allows the computer system to guarantee more reliably each process a regular "slice" of operating time. It also allows the system to deal rapidly with important external events like incoming data, which might require the immediate attention of one or another process. Operating systems were developed to take advantage of these hardware capabilities and run multiple processes preemptively. Preemptive multitasking was supported on DEC's PDP-8 computers, and implemented in OS/360 MFT in 1967, in MULTICS (1964), and Unix (1969); it is a core feature of all Unix-like operating systems, such as Linux, Solaris and BSD with its derivatives.[3]
2460) 11.734999, Computer multitasking - Wikipedia, the free encyclopedia.txt#13, term: computer, content:The earliest preemptive multitasking OS available to home users was Sinclair QDOS on the Sinclair QL, released in 1984, but very few people bought the machine. Commodore's powerful Amiga, released the following year, was the first commercially successful home computer to use the technology, and its multimedia abilities make it a clear ancestor of contemporary multitasking personal computers. Microsoft made preemptive multitasking a core feature of their flagship operating system in the early 1990s when developing Windows NT 3.1 and then Windows 95. It was later adopted on the Apple Macintosh by Mac OS X that, as a Unix-like operating system, uses preemptive multitasking for all native applications.
2461) 11.734999, Computer music - Wikipedia, the free encyclopedia.txt#24, term: computer, content:Machine improvisation uses computer algorithms to create improvisation on existing music materials. This is usually done by sophisticated recombination of musical phrases extracted from existing music, either live or pre-recorded. In order to achieve credible improvisation in particular style, machine improvisation uses machine learning and pattern matching algorithms to analyze existing musical examples. The resulting patterns are then used to create new variations "in the style" of the original music, developing a notion of stylistic reinjection. This is different from other improvisation methods with computers that use algorithmic composition to generate new music without performing analysis of existing music examples.[citation needed]
2462) 11.734999, Computer program - Wikipedia, the free encyclopedia.txt#34, term: computer, content:In the 1950s, the programmer, who was also the operator, would write a program and run it.[13] After the program finished executing, the output may have been printed, or it may have been punched onto paper tape or cards for later processing.[13] More often than not the program did not work. [24] The programmer then looked at the console lights and fiddled with the console switches. If less fortunate, a memory printout was made for further study.[24] In the 1960s, programmers reduced the amount of wasted time by automating the operator's job.[24] A program called an operating system was kept in the computer at all times.[24]
2463) 11.734999, Computer security - Wikipedia, the free encyclopedia.txt#32, term: computer, content:Serious financial damage has been caused by security breaches, but because there is no standard model for estimating the cost of an incident, the only data available is that which is made public by the organizations involved. "Several computer security consulting firms produce estimates of total worldwide losses attributable to virus and worm attacks and to hostile digital acts in general. The 2003 loss estimates by these firms range from $13 billion (worms and viruses only) to $226 billion (for all forms of covert attacks). The reliability of these estimates is often challenged; the underlying methodology is basically anecdotal."[50]
2464) 11.734999, Computer security - Wikipedia, the free encyclopedia.txt#39, term: computer, content:Today, computer security comprises mainly "preventive" measures, like firewalls or an exit procedure. A firewall can be defined as a way of filtering network data between a host or a network and another network, such as the Internet, and can be implemented as software running on the machine, hooking into the network stack (or, in the case of most UNIX-based operating systems such as Linux, built into the operating system kernel) to provide real time filtering and blocking. Another implementation is a so-called physical firewall which consists of a separate machine filtering network traffic. Firewalls are common amongst machines that are permanently connected to the Internet.
2465) 11.734999, Computer security - Wikipedia, the free encyclopedia.txt#51, term: computer, content:One use of the term "computer security" refers to technology that is used to implement secure operating systems. In the 1980s the United States Department of Defense (DoD) used the "Orange Book"[75] standards, but the current international standard ISO/IEC 15408, "Common Criteria" defines a number of progressively more stringent Evaluation Assurance Levels. Many common operating systems meet the EAL4 standard of being "Methodically Designed, Tested and Reviewed", but the formal verification required for the highest levels means that they are uncommon. An example of an EAL6 ("Semiformally Verified Design and Tested") system is Integrity-178B, which is used in the Airbus A380[76] and several military jets.[77]
2466) 11.734999, Computer simulation - Wikipedia, the free encyclopedia.txt#8, term: computer, content:Systems that accept data from external sources must be very careful in knowing what they are receiving. While it is easy for computers to read in values from text or binary files, what is much harder is knowing what the accuracy (compared to measurement resolution and precision) of the values are. Often they are expressed as "error bars", a minimum and maximum deviation from the value range within which the true value (is expected to) lie. Because digital computer mathematics is not perfect, rounding and truncation errors multiply this error, so it is useful to perform an "error analysis"[7] to confirm that values output by the simulation will still be usefully accurate.
2467) 11.734999, Computer simulation - Wikipedia, the free encyclopedia.txt#24, term: computer, content:Although sometimes ignored in computer simulations, it is very important to perform a sensitivity analysis to ensure that the accuracy of the results is properly understood. For example, the probabilistic risk analysis of factors determining the success of an oilfield exploration program involves combining samples from a variety of statistical distributions using the Monte Carlo method. If, for instance, one of the key parameters (e.g., the net ratio of oil-bearing strata) is known to only one significant figure, then the result of the simulation might not be more precise than one significant figure, although it might (misleadingly) be presented as having four significant figures.
2468) 11.734999, Control flow - Wikipedia, the free encyclopedia.txt#46, term: computer, content:The notion of multi-level breaks is of some interest in theoretical computer science, because it gives rise to what is nowadays called the Kosaraju hierarchy.[11] In 1973 S. Rao Kosaraju refined the structured program theorem by proving that it's possible to avoid adding additional variables in structured programming, as long as arbitrary-depth, multi-level breaks from loops are allowed.[12] Furthermore, Kosaraju proved that a strict hierarchy of programs exists: for every integer n, there exists a program containing a multi-level break of depth n that cannot be rewritten as a program with multi-level breaks of depth less than n without introducing additional variables.[11]
2469) 11.734999, Control unit - Wikipedia, the free encyclopedia.txt#2, term: computer, content:The Control Unit (CU) is digital circuitry contained within the processor that coordinates the sequence of data movements into, out of, and between a processor's many sub-units. The result of these routed data movements through various digital circuits (sub-units) within the processor produces the manipulated data expected by a software instruction (loaded earlier, likely from memory).[citation needed] In a way, the CU is the "brain within the brain", as it controls (conducts) data flow inside the processor and additionally provides several external control signals to the rest of the computer to further direct data and instructions to/from processor external destinations (i.e. memory).[citation needed]
2470) 11.734999, Conventional PCI - Wikipedia, the free encyclopedia.txt#3, term: computer, content:Conventional PCI and PCI-X are sometimes called Parallel PCI in order to distinguish them technologically from their more recent successor PCI Express, which adopted a serial, lane-based architecture.[7][8] Conventional PCI's heyday in the desktop computer market was approximately the decade 19952005.[7] PCI and PCI-X have become obsolete for most purposes; however, they are still common on modern desktops for the purposes of backwards compatibility and the low relative cost to produce. Many kinds of devices previously available on PCI expansion cards are now commonly integrated onto motherboards or available in universal serial bus and PCI Express versions.
2471) 11.734999, Conventional PCI - Wikipedia, the free encyclopedia.txt#6, term: computer, content:PCI was immediately put to use in servers, replacing MCA and EISA as the server expansion bus of choice. In mainstream PCs, PCI was slower to replace VESA Local Bus (VLB), and did not gain significant market penetration until late 1994 in second-generation Pentium PCs. By 1996, VLB was all but extinct, and manufacturers had adopted PCI even for 486 computers.[9] EISA continued to be used alongside PCI through 2000. Apple Computer adopted PCI for professional Power Macintosh computers (replacing NuBus) in mid-1995, and the consumer Performa product line (replacing LC PDS) in mid-1996.
2472) 11.734999, Cryptography - Wikipedia, the free encyclopedia.txt#2, term: computer, content:Modern cryptography is heavily based on mathematical theory and computer science practice; cryptographic algorithms are designed around computational hardness assumptions, making such algorithms hard to break in practice by any adversary. It is theoretically possible to break such a system, but it is infeasible to do so by any known practical means. These schemes are therefore termed computationally secure; theoretical advances, e.g., improvements in integer factorization algorithms, and faster computing technology require these solutions to be continually adapted. There exist information-theoretically secure schemes that provably cannot be broken even with unlimited computing poweran example is the one-time padbut these schemes are more difficult to implement than the best theoretically breakable but computationally secure mechanisms.
2473) 11.734999, Cryptography - Wikipedia, the free encyclopedia.txt#21, term: computer, content:As well as being aware of cryptographic history, cryptographic algorithm and system designers must also sensibly consider probable future developments while working on their designs. For instance, continuous improvements in computer processing power have increased the scope of brute-force attacks, so when specifying key lengths, the required key lengths are similarly advancing.[24] The potential effects of quantum computing are already being considered by some cryptographic system designers; the announced imminence of small implementations of these machines may be making the need for this preemptive caution rather more than merely speculative.[4]
2474) 11.734999, Cryptography - Wikipedia, the free encyclopedia.txt#22, term: computer, content:Essentially, prior to the early 20th century, cryptography was chiefly concerned with linguistic and lexicographic patterns. Since then the emphasis has shifted, and cryptography now makes extensive use of mathematics, including aspects of information theory, computational complexity, statistics, combinatorics, abstract algebra, number theory, and finite mathematics generally. Cryptography is also a branch of engineering, but an unusual one since it deals with active, intelligent, and malevolent opposition (see cryptographic engineering and security engineering); other kinds of engineering (e.g., civil or chemical engineering) need deal only with neutral natural forces. There is also active research examining the relationship between cryptographic problems and quantum physics (see quantum cryptography and quantum computer).
2475) 11.734999, Database - Wikipedia, the free encyclopedia.txt#74, term: computer, content:Database access control deals with controlling who (a person or a certain computer program) is allowed to access what information in the database. The information may comprise specific database objects (e.g., record types, specific records, data structures), certain computations over certain objects (e.g., query types, or specific queries), or utilizing specific access paths to the former (e.g., using specific indexes or other data structures to access information). Database access controls are set by special authorized (by the database owner) personnel that uses dedicated protected security DBMS interfaces.
2476) 11.734999, DEC Alpha - Wikipedia, the free encyclopedia.txt#0, term: computer, content:Alpha, originally known as Alpha AXP, is a 64-bit reduced instruction set computing (RISC) instruction set developed by Digital Equipment Corporation (DEC), designed to replace their 32-bit VAX complex instruction set computer (CISC) instruction set. Alpha was implemented in microprocessors originally developed and fabricated by DEC. These microprocessors were most prominently used in a variety of DEC workstations and servers, which eventually formed the basis for almost all of their mid-to-upper-scale lineup. Several third-party vendors also produced Alpha systems, including PC form factor motherboards.
2477) 11.734999, Desktop publishing - Wikipedia, the free encyclopedia.txt#0, term: computer, content:Desktop publishing (abbreviated DTP) is the creation of documents using page layout skills on a personal computer. Desktop publishing software can generate layouts and produce typographic quality text and images comparable to traditional typography and printing. This technology allows individuals, businesses, and other organizations to self-publish a wide range of printed matter. Desktop publishing is also the main reference for digital typography. When used skillfully, desktop publishing allows the user to produce a wide variety of materials, from menus to magazines and books, without the expense of commercial printing.
2478) 11.734999, Desktop publishing - Wikipedia, the free encyclopedia.txt#5, term: computer, content:Before the advent of desktop publishing, the only option available to most people for producing typed documents (as opposed to handwritten documents) was a typewriter, which offered only a handful of typefaces (usually fixed-width) and one or two font sizes. Indeed, one popular desktop publishing book was entitled The Mac is not a typewriter.[4] The ability was revolutionary to create WYSIWYG page layouts on screen and then print pages containing text and graphical elements at crisp 300 dpi resolution for both the typesetting industry and the personal computer industry. Newspapers and other print publications made the move to DTP-based programs from older layout systems such as Atex and other programs in the early 1980s.
2479) 11.734999, Differential equation - Wikipedia, the free encyclopedia.txt#17, term: computer, content:Linear differential equations, which have solutions that can be added and multiplied by coefficients, are well-defined and understood, and exact closed-form solutions are obtained. By contrast, ODEs that lack additive solutions are nonlinear, and solving them is far more intricate, as one can rarely represent them by elementary functions in closed form: Instead, exact and analytic solutions of ODEs are in series or integral form. Graphical and numerical methods, applied by hand or by computer, may approximate solutions of ODEs and perhaps yield useful information, often sufficing in the absence of exact, analytic solutions.
2480) 11.734999, Digital data - Wikipedia, the free encyclopedia.txt#6, term: computer, content:Devices with many switches (such as a computer keyboard) usually arrange these switches in a scan matrix, with the individual switches on the intersections of x and y lines. When a switch is pressed, it connects the corresponding x and y lines together. Polling (often called scanning in this case) is done by activating each x line in sequence and detecting which y lines then have a signal, thus which keys are pressed. When the keyboard processor detects that a key has changed state, it sends a signal to the CPU indicating the scan code of the key and its new state. The symbol is then encoded, or converted into a number, based on the status of modifier keys and the desired character encoding.
2481) 11.734999, Digital electronics - Wikipedia, the free encyclopedia.txt#52, term: computer, content:Almost all computers are synchronous. However, true asynchronous computers have also been designed. One example is the Aspida DLX core.[10] Another was offered by ARM Holdings. Speed advantages have not materialized, because modern computer designs already run at the speed of their slowest componment, usually memory. These do use somewhat less power because a clock distribution network is not needed. An unexpected advantage is that asynchronous computers do not produce spectrally-pure radio noise, so they are used in some mobile-phone base-station controllers. They may be more secure in cryptographic applications because their electrical and radio emissions can be more difficult to decode.[11]
2482) 11.734999, Digital Equipment Corporation - Wikipedia, the free encyclopedia.txt#5, term: computer, content:The company was acquired in June 1998 by Compaq, in what was at that time the largest merger in the history of the computer industry. At the time, Compaq was focused on the enterprise market and had recently purchased several other large vendors. DEC was a major player overseas where Compaq had less presence. However, Compaq had little idea what to do with its acquisitions, and soon found itself in financial difficulty of its own. The company subsequently merged with Hewlett-Packard in May 2002. As of 2007[update] some of DEC's product lines were still produced under the HP name.
2483) 11.734999, Digital Equipment Corporation - Wikipedia, the free encyclopedia.txt#12, term: computer, content:The Laboratory Modules were packaged in an extruded aluminum housing,[10] intended to sit on an engineer's workbench, although a rack-mount bay was sold that held nine laboratory modules.[11] They were then connected together using banana plug patch cords inserted at the front of the modules. Three versions were offered, running at 5MHz (1957), 500kHz (1959), or 10MHz (1960).[12] The Modules proved to be in high demand in other computer companies, who used them to build equipment to test their own systems. Despite the recession of the late 1950s, the company sold $94,000 worth of these modules during 1958 alone, turning a profit at the end of its first year.[7]
2484) 11.734999, Digital Equipment Corporation - Wikipedia, the free encyclopedia.txt#46, term: computer, content:Supporting the VAX's success was the VT52, one of the most successful smart terminals. Building on earlier less successful models (the VT05 and VT50), the VT52 was the first terminal that did everything one might want in a single chassis. The VT52 was followed by the even more successful VT100 and its follow-ons, making DEC one of the largest terminal vendors in the industry. With the VT series, DEC could now offer a complete top-to-bottom system from computer to all peripherals, which formerly required collecting the required devices from different suppliers.
2485) 11.734999, Digital photography - Wikipedia, the free encyclopedia.txt#14, term: computer, content:The number of pixels n for a given maximum resolution (w horizontal pixels by h vertical pixels) is the product n = w  h. This yields e. g. 1.92 megapixels (1,920,000 pixels) for an image of 1600  1200. The majority of compact as well as some DSLR cameras have a 4:3 aspect ratio, i.e. w/h = 4/3.[9] According to Digital Photography Review, the 4:3 ratio is because "computer monitors are 4:3 ratio, old CCDs always had a 4:3 ratio, and thus digital cameras inherited this aspect ratio."[9]
2486) 11.734999, Digital photography - Wikipedia, the free encyclopedia.txt#37, term: computer, content:For many consumers, the advantages of digital cameras outweigh the disadvantages. Some professional photographers still prefer film. Concerns that have been raised by professional photographers include: editing and post-processing of RAW files can take longer than 35mm film, downloading a large number of images to a computer can be time-consuming, shooting in remote sites requires the photographer to carry a number of batteries, equipment failurewhile all cameras may fail, some film camera problems (e.g., meter or rangefinder problems, failure of only some shutter speeds) can be worked around. As time passes, it is expected that more professional photographers will switch to digital.[22]
2487) 11.734999, Digital signal processing - Wikipedia, the free encyclopedia.txt#19, term: computer, content:The main applications of DSP are audio signal processing, audio compression, digital image processing, video compression, speech processing, speech recognition, digital communications, radar, sonar, financial signal processing, seismology and biomedicine. Specific examples are speech compression and transmission in digital mobile phones, room correction of sound in hi-fi and sound reinforcement applications, weather forecasting, economic forecasting, seismic data processing, analysis and control of industrial processes, medical imaging such as CAT scans and MRI, MP3 compression, computer graphics, image manipulation, hi-fi loudspeaker crossovers and equalization, and audio effects for use with electric guitar amplifiers.
2488) 11.734999, DNA computing - Wikipedia, the free encyclopedia.txt#6, term: computer, content:The organisation and complexity of all living beings is based on a coding system functioning with four key components of the DNA-molecule. Because of this, the DNA is very suited as a medium for data processing.[11] According to different calculations a DNA-computer with one liter of fluid containing six grams of DNA could potentially have a memory capacity of 3072 exabytes. The theoretical maximum data transfer speed would also be enormous due to the massive parallelism of the calculations. Therefore, about 1000 petaFLOPS could be reached, while today's most powerful computers do not go above a few dozen (33.86 petaFLOPS by Tianhe-2 being the current record holder).[citation needed]
2489) 11.734999, DNA computing - Wikipedia, the free encyclopedia.txt#11, term: computer, content:In 2002, J. Macdonald, D. Stefanovic and Mr. Stojanovic created a DNA computer able to play Tic-tac-toe against a human player.[13] The calculator consists of nine bins corresponding to the nine squares of the game. Each bin contains a substrate and various combinations of DNA enzymes. The substrate itself is composed of a DNA strand onto which was grafted a fluorescent chemical group at one end, and the other end, a repressor group. Fluorescence is only active if the molecules of the substrate are halved. The DNA enzyme simulate logical functions. For example, such a DNA will unfold if we introduce two specific types of DNA strand, reproducing the logic function AND.
2490) 11.734999, DOS - Wikipedia, the free encyclopedia.txt#2, term: computer, content:In spite of the common usage, none of these systems were simply named "DOS" (a name given only to an unrelated IBM mainframe operating system in the 1960s). A number of unrelated, non-x86 microcomputer disk operating systems had "DOS" in their names, and are often referred to simply as "DOS" when discussing machines that use them (e.g. AmigaDOS, AMSDOS, ANDOS, Apple DOS, Atari DOS, Commodore DOS, CSI-DOS, ProDOS, and TRSDOS). While providing many of the same operating system functions for their respective computer systems, programs running under any one of these operating systems would not run under others.
2491) 11.734999, DOS - Wikipedia, the free encyclopedia.txt#5, term: computer, content:IBM again approached Bill Gates. Gates in turn approached Seattle Computer Products. There, programmer Tim Paterson had developed a variant of CP/M-80, intended as an internal product for testing SCP's new 16-bit Intel 8086 CPU card for the S-100 bus. The system was initially named QDOS (Quick and Dirty Operating System), before being made commercially available as 86-DOS. Microsoft purchased 86-DOS, allegedly for $50,000. This became Microsoft Disk Operating System, MS-DOS, introduced in 1981.[4]
2492) 11.734999, E6B - Wikipedia, the free encyclopedia.txt#1, term: computer, content:They are mostly used in flight training, because these flight computers have been replaced with electronic planning tools or software and websites that make these calculations for the pilots. These flight computers are used during flight planning (on the ground before takeoff) to aid in calculating fuel burn, wind correction, time en route, and other items. In the air, the flight computer can be used to calculate ground speed, estimated fuel burn and updated estimated time of arrival. The back is designed for wind vector solutions, i.e., determining how much the wind is affecting one's speed and course. See wind triangle and dead reckoning.
2493) 11.734999, E6B - Wikipedia, the free encyclopedia.txt#4, term: computer, content:In flight training for a private pilot or instrument rating mechanical flight computers are still often used to teach the fundamental computations. This is in part also due to the complex nature of some trigonometric calculations which would be comparably difficult to perform on a conventional scientific calculator. The graphic nature of the flight computer also helps catching many errors which in part explains their continued popularity. The ease of use of electronic calculators means typical flight training literature[1] does not cover the use of calculators or computers at all. In the ground exams for numerous pilot ratings, programmable calculators or calculators containing flight planning software are permitted to be used.[2]
2494) 11.734999, E6B - Wikipedia, the free encyclopedia.txt#8, term: computer, content:The front side of the flight computer is a logarithmic slide rule that performs multiplication and division. Throughout the wheel, unit names are marked (such as gallons, miles, kilometers, pounds, minutes, seconds, etc.) at locations that correspond to the constants that are used when going from one unit to another in various calculations. Once the wheel is positioned to represent a certain fixed ratio (for example, pounds of fuel per hour), the rest of the wheel can be consulted to utilize that same ratio in a problem (for example, how many pounds of fuel for a 2.5-hour cruise?) This is one area where the E6B and CRP-1 are different. Since the CRP-1s are made for the UK market, they can be used to perform the added conversions of Imperial to Metric units.
2495) 11.734999, E6B - Wikipedia, the free encyclopedia.txt#10, term: computer, content:To solve this problem with a flight computer, first the wheel is turned so the wind direction (C) is at the top of the wheel. Then a pencil mark is made just above the hole, at a distance representing the wind speed (D) away from the hole. After the mark is made, the wheel is turned so that the course (A) is now selected at the top of the wheel. The ruler then is slid so that the pencil mark is aligned with the true airspeed (B) seen through the transparent part of the wheel. The wind correction angle is determined by matching how far right or left the pencil mark is from the hole, to the wind correction angle portion of the slide's grid. The true ground speed is determined by matching the center hole to the speed portion of the grid.
2496) 11.734999, E6B - Wikipedia, the free encyclopedia.txt#26, term: computer, content:After Dalton's death, Weems[5] updated the E-6B and tried calling it the E-6C, E-10, and so forth, but finally fell back on the original name, which was so well known by 50,000 World War II Army Air Force navigator veterans. After the patent ran out, many manufacturers made copies, sometimes using a marketing name of "E6-B" (note the moved hyphen). An aluminium version was made by the London Name Plate Mfg. Co. Ltd. of London and Brighton and was marked "Computer Dead Reckoning Mk. 4A Ref. No. 6B/2645" followed by the arrowhead of UK military stores.
2497) 11.734999, Electronic delay storage automatic calculator - Wikipedia, the free encyclopedia.txt#14, term: computer, content:David Wheeler, who earned the world's first Computer Science PhD working on the project, is credited with inventing the concept of a subroutine. A user wrote a program that called a routine by jumping to the start of the subroutine with the address of the program counter plus one in the single register (a Wheeler jump). By convention the subroutine expected this and the first thing it did was to overwrite its final jump instruction with that address so that it returned. Multiple and nested subroutines could be called so long as the user knew the length of each one in order to calculate the location to jump to. The user then copied the code for the subroutine from a master tape onto their own tape following the end of their own program.
2498) 11.734999, Electronic engineering - Wikipedia, the free encyclopedia.txt#37, term: computer, content:Fundamental to the discipline are the sciences of physics and mathematics as these help to obtain both a qualitative and quantitative description of how such systems will work. Today most engineering work involves the use of computers and it is commonplace to use computer-aided design and simulation software programs when designing electronic systems. Although most electronic engineers will understand basic circuit theory, the theories employed by engineers generally depend upon the work they do. For example, quantum mechanics and solid state physics might be relevant to an engineer working on VLSI but are largely irrelevant to engineers working with macroscopic electrical systems.
2499) 11.734999, ENIAC - Wikipedia, the free encyclopedia.txt#5, term: computer, content:ENIAC was a modular computer, composed of individual panels to perform different functions. Twenty of these modules were accumulators, which could not only add and subtract but hold a ten-digit decimal number in memory. Numbers were passed between these units across several general-purpose buses (or trays, as they were called). In order to achieve its high speed, the panels had to send and receive numbers, compute, save the answer and trigger the next operation, all without any moving parts. Key to its versatility was the ability to branch; it could trigger different operations, depending on the sign of a computed result.
2500) 11.734999, Ethernet - Wikipedia, the free encyclopedia.txt#13, term: computer, content:An EtherType field in each frame is used by the operating system on the receiving station to select the appropriate protocol module (e.g., an Internet Protocol version such as IPv4). Ethernet frames are said to be self-identifying, because of the frame type. Self-identifying frames make it possible to intermix multiple protocols on the same physical network and allow a single computer to use multiple protocols together.[27] Despite the evolution of Ethernet technology, all generations of Ethernet (excluding early experimental versions) use the same frame formats.[28] Mixed-speed networks can be built using Ethernet switches and repeaters supporting the desired Ethernet variants.[29]
2501) 11.734999, Fairchild Semiconductor - Wikipedia, the free encyclopedia.txt#13, term: computer, content:Fairchild had not initially done well in the digital integrated circuit market. Their first line of ICs was the "micrologic" RTL (Resistor-Transistor-Logic) line which was used in the Apollo Guidance Computer. It had the advantage of being extremely simple each inverter consisted of just one transistor and two resistors. The logic family had many drawbacks that had made it marginal for commercial purposes, and not well suited for military applications: The logic could only tolerate about 100 millivolts of noise far too low for comfort. It was a while before Fairchild relied on more robust designs, such as DTL (diode-transistor-logic) which had much better noise margins.
2502) 11.734999, Federico Faggin - Wikipedia, the free encyclopedia.txt#3, term: computer, content:Born in Vicenza, Faggin received a laurea degree in physics, summa cum laude, at the University of Padua, Italy.[3] At age 19, after his graduation from I.T.I.S. Alessandro Rossi, a technical high school in Vicenza, he took a job at Olivetti, in Italy, where he co-designed and led the implementation of a small digital transistor computer with 4 Ki  12 bit of magnetic memory (1960). Faggin's father was a scholar who translated the Roman poet Virgil's works from the original Latin into modern Italian and the Enneads of the Greek pholosopher Plotinus from original Greek.
2503) 11.734999, Federico Faggin - Wikipedia, the free encyclopedia.txt#4, term: computer, content:After obtaining his university degree he worked at SGS Fairchild in Italy, where he developed SGS's first MOS process technology and designed its first integrated circuits. In 1968 he moved to Palo Alto and worked at Fairchild Semiconductor, where he created the MOS Silicon Gate technology with self-aligned gate, the basis of all modern CMOS computer chips. At Fairchild he produced the world's first commercial integrated circuit using Silicon Gate Technology with self aligned MOSFET transistors: the Fairchild 3708.[4]
2504) 11.734999, Federico Faggin - Wikipedia, the free encyclopedia.txt#15, term: computer, content:The Intel 8080 microprocessor (1974) was the first high-performance 8-bit microprocessor, using the faster n-channel silicon gate technology. The 8080 was conceived by F. Faggin and designed by M. Shima under Faggins supervision. The 8080 was a major improvement over the 8008 architecture, yet it retained software compatibility with it. It was much faster and easier to interface to external memory and I/O devices than the 8008. The high performance and low cost of the 8080 allowed for the first time the use of microprocessors for many new applications, including the forerunners of the personal computer.[23] US patent 4,010,440[24]
2505) 11.734999, Ferranti - Wikipedia, the free encyclopedia.txt#11, term: computer, content:From the 1960s through to the late 1980s the Bristol Ferranti Bloodhound SAM, for which Ferranti developed radar systems, was a key money earner. In 1970 Ferranti became involved in the sonar field through its involvement with Plessey in a new series of sonars, for which it designed and built the computer subsystems. This work later expanded when it won a contract for the complete Sonar 2050. The work was originally carried out at the Wythenshawe factory and then at Cheadle Heath. Takeovers of other companies gave it expertise in sonar arrays. This business later became Ferranti Thomson Sonar Systems.[17]
2506) 11.734999, Ferranti - Wikipedia, the free encyclopedia.txt#19, term: computer, content:Meanwhile, in Bracknell the Digital Systems Division was developing a range of mainframe computers for naval applications. Early computers using discrete transistors were the Hermes and Poseidon and these were followed by the F1600 in the mid 1960s.[23] Some of these machines remained in active service on naval vessels for many years. The FM1600B was the first of the range to use integrated circuits and was used in many naval and commercial applications. The FM1600D was a single-rack version of the computer for smaller systems. An airborne version of this was also made and used aboard the RAF Nimrod. The FM1600E was a redesigned and updated version of the FM1600B, and the last in the series was the F2420, an upgraded FM1600E with 60% more memory and 3.5 times the processing speed, still in service at sea in 2010.[17]
2507) 11.734999, File format - Wikipedia, the free encyclopedia.txt#1, term: computer, content:Some file formats are designed for very particular types of data: PNG files, for example, store bitmapped images using lossless data compression. Other file formats, however, are designed for storage of several different types of data: the Ogg format can act as a container for different types of multimedia, including any combination of audio and video, with or without text (such as subtitles), and metadata. A text file can contain any stream of characters, including possible control characters, and is encoded in one of various character encoding schemes. Some file formats, such as HTML, scalable vector graphics, and the source code of computer software are text files with defined syntaxes that allow them to be used for specific purposes.
2508) 11.734999, Finite-state machine - Wikipedia, the free encyclopedia.txt#0, term: computer, content:A finite-state machine (FSM) or finite-state automaton (FSA, plural: automata), or simply a state machine, is a mathematical model of computation used to design both computer programs and sequential logic circuits. It is conceived as an abstract machine that can be in one of a finite number of states. The machine is in only one state at a time; the state it is in at any given time is called the current state. It can change from one state to another when initiated by a triggering event or condition; this is called a transition. A particular FSM is defined by a list of its states, and the triggering condition for each transition.
2509) 11.734999, First-person shooter - Wikipedia, the free encyclopedia.txt#18, term: computer, content:In 1994, Exact released Geograph Seal for the Japanese Sharp X68000 home computer. An obscure import title as far as the Western market was concerned, it was nonetheless "a fully 3D polygonal first-person shooter" with innovative platform game mechanics and "free-roaming" outdoor environments. The following year, Exact released its successor for the PlayStation console, Jumping Flash!, which placed more emphasis on its platform elements.[47] Descent (released by Parallax Software in 1995), a game in which the player pilots a spacecraft around caves and factory ducts, was a truly three-dimensional first-person shooter. It abandoned sprites and ray casting in favour of polygons and six degrees of freedom.[1][14]
2510) 11.734999, Floating point - Wikipedia, the free encyclopedia.txt#20, term: computer, content:In 1914, Leonardo Torres y Quevedo designed an electro-mechanical version of Charles Babbage's Analytical Engine, and included floating-point arithmetic.[2] In 1938, Konrad Zuse of Berlin completed the Z1, the first binary, programmable mechanical computer;[3] it uses a 24-bit binary floating-point number representation with a 7-bit signed exponent, a 16-bit significand (including one implicit bit), and a sign bit. The more reliable relay-based Z3, completed in 1941, has representations for both positive and negative infinities; in particular, it implements defined operations with infinity, such as        1     /      = 0   {\displaystyle ^{1}/_{\infty }=0}  , and it stops on undefined operations, such as     0     {\displaystyle 0\times \infty }  .
2511) 11.734999, Fortran - Wikipedia, the free encyclopedia.txt#25, term: computer, content:FORTRAN was provided for the IBM 1401 computer by an innovative 63-phase compiler that ran entirely in its core memory of only 8000 (6-bit) characters. The compiler could be run from tape, or from a 2200-card deck; it used no further tape or disk storage. It kept the program in memory and loaded overlays that gradually transformed it, in place, into executable form, as described by Haines.[14] and in IBM document C24-1455. The executable form was not entirely machine language; rather, floating-point arithmetic, subscripting, input/output, and function references were interpreted, anticipating UCSD Pascal P-code by two decades.
2512) 11.734999, GNOME - Wikipedia, the free encyclopedia.txt#8, term: computer, content:GNOME Shell is the official user interface of the GNOME desktop environment. It features a top bar holding (from left to right) an Activities button, an application menu, a clock and an integrated system status menu.[14][15] The application menu displays the name of the application in focus and provides access to functions such as accessing the application's preferences, closing the application, or creating a new application window. The status menu holds various system status indicators, shortcuts to system settings, and session actions including logging out, switching users, locking the screen, and suspending the computer.
2513) 11.734999, GNU - Wikipedia, the free encyclopedia.txt#7, term: computer, content:Richard Stallman's experience with the Incompatible Timesharing System (ITS),[28] an early operating system written in assembly language that became obsolete due to discontinuation of PDP-10, the computer architecture for which ITS was written, led to a decision that a portable system was necessary.[4](00:40:52)[29] It was thus decided that the development would be started using C and Lisp as system programming languages,[30] and that GNU would be compatible with Unix.[31] At the time, Unix was already a popular proprietary operating system. The design of Unix was modular, so it could be reimplemented piece by piece.[29]
2514) 11.734999, Grace Hopper - Wikipedia, the free encyclopedia.txt#10, term: computer, content:In the spring of 1959, a two-day conference known as the Conference on Data Systems Languages (CODASYL) brought together computer experts from industry and government. Hopper served as a technical consultant to the committee, and many of her former employees served on the short-term committee that defined the new language COBOL (an acronym for COmmon Business-Oriented Language). The new language extended Hopper's FLOW-MATIC language with some ideas from the IBM equivalent, COMTRAN. Hopper's belief that programs should be written in a language that was close to English (rather than in machine code or in languages close to machine code, such as assembly languages) was captured in the new business language, and COBOL went on to be the most ubiquitous business language to date.[22]
2515) 11.734999, Graphical Environment Manager - Wikipedia, the free encyclopedia.txt#28, term: computer, content:The GEM Desktop was an application program that used AES to provide a file manager and launcher, the traditional "desktop" environment that users had come to expect from the Macintosh. Unlike the Macintosh, the GEM Desktop was based on top of DOS (MS-DOS, DOS Plus or DR DOS on the PC, GEMDOS on the Atari), and as a result the actual display was cluttered with computer-like items including path names and wildcards. In general GEM was much more "geeky" than the Mac, but simply running a usable shell on DOS was a huge achievement on its own. Otherwise, GEM has its own advantages over Mac OS such as proportional sliders.
2516) 11.734999, Graphical user interface - Wikipedia, the free encyclopedia.txt#1, term: computer, content:The actions in a GUI are usually performed through direct manipulation of the graphical elements.[4] Beyond computers, GUIs are used in many handheld mobile devices such as MP3 players, portable media players, gaming devices, smartphones and smaller household, office and industrial equipment. The term GUI tends not to be applied to other lower-display resolution types of interfaces, such as video games (where head-up display (HUD)[5] is preferred), or not restricted to flat screens, like volumetric displays[6] because the term is restricted to the scope of two-dimensional display screens able to describe generic information, in the tradition of the computer science research at the Xerox Palo Alto Research Center (PARC).
2517) 11.734999, Graphics tablet - Wikipedia, the free encyclopedia.txt#0, term: computer, content:A graphics tablet (also digitizer, digital drawing tablet, pen tablet, digital art board) is a computer input device that enables a user to hand-draw images, animations and graphics, with a special pen-like stylus, similar to the way a person draws images with a pencil and paper. These tablets may also be used to capture data or handwritten signatures. It can also be used to trace an image from a piece of paper which is taped or otherwise secured to the tablet surface. Capturing data in this way, by tracing or entering the corners of linear poly-lines or shapes, is called digitizing.
2518) 11.734999, Graphics tablet - Wikipedia, the free encyclopedia.txt#15, term: computer, content:After styluses, pucks are the most commonly used tablet accessory. A puck is a mouse-like device that can detect its absolute position and rotation. This is opposed to mouse pointing device, which can only sense their relative velocity on a surface (most tablet drivers are capable of allowing a puck to emulate a mouse in operation, and many pucks are marketed as a "mouse".) Pucks range in size and shape, some are externally indistinguishable from a mouse, while others are a fairly large device with dozens of buttons and controls. Professional pucks often have a reticle or loupe which allows the user to see the exact point on the tablet's surface targeted by the puck, for detailed tracing and computer aided design (CAD) work.
2519) 11.734999, Hard disk drive - Wikipedia, the free encyclopedia.txt#2, term: computer, content:The primary characteristics of an HDD are its capacity and performance. Capacity is specified in unit prefixes corresponding to powers of 1000: a 1-terabyte (TB) drive has a capacity of 1,000 gigabytes (GB; where 1 gigabyte = 1 billion bytes). Typically, some of an HDD's capacity is unavailable to the user because it is used by the file system and the computer operating system, and possibly inbuilt redundancy for error correction and recovery. Performance is specified by the time required to move the heads to a track or cylinder (average access time) plus the time it takes for the desired sector to move under the head (average latency, which is a function of the physical rotational speed in revolutions per minute), and finally the speed at which the data is transmitted (data rate).
2520) 11.734999, Hard disk drive - Wikipedia, the free encyclopedia.txt#45, term: computer, content:The total capacity of HDDs is given by manufacturers in SI-based units[i] such as gigabytes (1GB= 1,000,000,000bytes) and terabytes (1TB= 1,000,000,000,000bytes).[80][82][83][84][85][86] The practice of using SI-based prefixes (denoting powers of 1,000) in the hard disk drive and computer industries dates back to the early days of computing;[87] by the 1970s, "million", "mega" and "M" were consistently used in the decimal sense for drive capacity.[88][89][90] However, capacities of memory (RAM, ROM) and CDs are traditionally quoted using a binary interpretation of the prefixes, i.e. using powers of 1024 instead of 1000.
2521) 11.734999, Hard disk drive - Wikipedia, the free encyclopedia.txt#67, term: computer, content:Modern HDDs present a consistent interface to the rest of the computer, no matter what data encoding scheme is used internally. Typically a DSP in the electronics inside the HDD takes the raw analog voltages from the read head and uses PRML and ReedSolomon error correction[137] to decode the sector boundaries and sector data, then sends that data out the standard interface. That DSP also watches the error rate detected by error detection and correction, and performs bad sector remapping, data collection for Self-Monitoring, Analysis, and Reporting Technology, and other internal tasks.
2522) 11.734999, Hellenistic period - Wikipedia, the free encyclopedia.txt#114, term: computer, content:The level of Hellenistic achievement in astronomy and engineering is impressively shown by the Antikythera mechanism (150100 BC). It is a 37-gear mechanical computer which computed the motions of the Sun and Moon, including lunar and solar eclipses predicted on the basis of astronomical periods believed to have been learned from the Babylonians.[124] Devices of this sort are not found again until the 10th century, when a simpler eight-geared luni-solar calculator incorporated into an astrolabe was described by the Persian scholar, Al-Biruni.[125][not in citation given] Similarly complex devices were also developed by other Muslim engineers and astronomers during the Middle Ages.[124][not in citation given]
2523) 11.734999, High-level programming language - Wikipedia, the free encyclopedia.txt#3, term: computer, content:"High-level language" refers to the higher level of abstraction from machine language. Rather than dealing with registers, memory addresses and call stacks, high-level languages deal with variables, arrays, objects, complex arithmetic or boolean expressions, subroutines and functions, loops, threads, locks, and other abstract computer science concepts, with a focus on usability over optimal program efficiency. Unlike low-level assembly languages, high-level languages have few, if any, language elements that translate directly into a machine's native opcodes. Other features, such as string handling routines, object-oriented language features, and file input/output, may also be present.
2524) 11.734999, History of computing hardware - Wikipedia, the free encyclopedia.txt#34, term: computer, content:He also introduced the notion of a 'Universal Machine' (now known as a Universal Turing machine), with the idea that such a machine could perform the tasks of any other machine, or in other words, it is provably capable of computing anything that is computable by executing a program stored on tape, allowing the machine to be programmable. Von Neumann acknowledged that the central concept of the modern computer was due to this paper.[48] Turing machines are to this day a central object of study in theory of computation. Except for the limitations imposed by their finite memory stores, modern computers are said to be Turing-complete, which is to say, they have algorithm execution capability equivalent to a universal Turing machine.
2525) 11.734999, History of computing hardware - Wikipedia, the free encyclopedia.txt#55, term: computer, content:Early computing machines had fixed programs. For example, a desk calculator is a fixed program computer. It can do basic mathematics, but it cannot be used as a word processor or a gaming console. Changing the program of a fixed-program machine requires re-wiring, re-structuring, or re-designing the machine. The earliest computers were not so much "programmed" as they were "designed". "Reprogramming", when it was possible at all, was a laborious process, starting with flowcharts and paper notes, followed by detailed engineering designs, and then the often-arduous process of physically re-wiring and re-building the machine.[76]
2526) 11.734999, History of computing hardware - Wikipedia, the free encyclopedia.txt#63, term: computer, content:The SSEM had a 32-bit word length and a memory of 32words. As it was designed to be the simplest possible stored-program computer, the only arithmetic operations implemented in hardware were subtraction and negation; other arithmetic operations were implemented in software. The first of three programs written for the machine found the highest proper divisor of 218 (262,144), a calculation that was known would take a long time to runand so prove the computer's reliabilityby testing every integer from 218-1 downwards, as division was implemented by repeated subtraction of the divisor. The program consisted of 17instructions and ran for 52minutes before reaching the correct answer of 131,072, after the SSEM had performed 3.5million operations (for an effective CPU speed of 1.1 kIPS).
2527) 11.734999, History of computing hardware - Wikipedia, the free encyclopedia.txt#65, term: computer, content:The computer is especially historically significant because of its pioneering inclusion of index registers, an innovation which made it easier for a program to read sequentially through an array of words in memory. Thirty-four patents resulted from the machine's development, and many of the ideas behind its design were incorporated in subsequent commercial products such as the IBM 701 and 702 as well as the Ferranti Mark 1. The chief designers, Frederic C. Williams and Tom Kilburn, concluded from their experiences with the Mark1 that computers would be used more in scientific roles than in pure mathematics. In 1951 they started development work on Meg, the Mark1's successor, which would include a floating point unit.
2528) 11.734999, History of computing hardware - Wikipedia, the free encyclopedia.txt#98, term: computer, content:In April 1975 at the Hannover Fair, Olivetti presented the P6060, the world's first complete, pre-assembled personal computer system. The central processing unit consisted of two cards, code named PUCE1 and PUCE2, and unlike most other personal computers was built with TTL components rather than a microprocessor. It had one or two 8" floppy disk drives, a 32-character plasma display, 80-column graphical thermal printer, 48 Kbytes of RAM, and BASIC language. It weighed 40kg (88lb). As a complete system, this was a significant step from the Altair, though it never achieved the same success. It was in competition with a similar product by IBM that had an external floppy disk drive.
2529) 11.734999, History of computing hardware - Wikipedia, the free encyclopedia.txt#103, term: computer, content:This has allowed computing to become a commodity which is now ubiquitous, embedded in many forms, from greeting cards and telephones to satellites. The thermal design power which is dissipated during operation has become as essential as computing speed of operation. In 2006 servers consumed 1.5% of the total energy budget of the U.S.[141] The energy consumption of computer data centers was expected to double to 3% of world consumption by 2011. The SoC (system on a chip) has compressed even more of the integrated circuitry into a single chip; SoCs are enabling phones and PCs to converge into single hand-held wireless mobile devices.[142] Computing hardware and its software have even become a metaphor for the operation of the universe.[143]
2530) 11.734999, Home computer - Wikipedia, the free encyclopedia.txt#0, term: computer, content:Home computers were a class of microcomputers entering the market in 1977, and becoming common during the 1980s. They were marketed to consumers as affordable and accessible computers that, for the first time, were intended for the use of a single nontechnical user. These computers were a distinct market segment that typically cost much less than business, scientific or engineering-oriented computers of the time such as the IBM PC,[1] and were generally less powerful in terms of memory and expandability. However, a home computer often had better graphics and sound than contemporary business computers. Their most common uses were playing video games, but they were also regularly used for word processing, doing homework, and programming.
2531) 11.734999, Home computer - Wikipedia, the free encyclopedia.txt#10, term: computer, content:Almost universally, home computers had a BASIC interpreter combined with a line editor in permanent read-only memory which one could use to type in BASIC programs and execute them immediately or save them to tape or disk. In direct mode, the BASIC interpreter was also used as the user interface, and given tasks such as loading, saving, managing, and running files.[18] One exception was the Jupiter Ace, which had a Forth interpreter instead of BASIC. A built-in programming language was seen as a requirement for any computer of the era, and was the main feature setting home computers apart from video game consoles.
2532) 11.734999, HP 2100 - Wikipedia, the free encyclopedia.txt#25, term: computer, content:The new L and A series models had HP-IB interface ability, but as with all HP systems at that time, the blinking LED lights were removed from the front panel. Despite customer demands for a real-time ability and HP R&D's efforts using an installable real-time card, the RTE-A OS was not as good at real-time operations as RTE on a 21MX. This was an important reason this computer was hard to kill. Many companies use real-time operations to take a measurements and control processes turn on or off a pump, heater, a valve, speed up or slow down a motor, etc.
2533) 11.734999, Human computer - Wikipedia, the free encyclopedia.txt#4, term: computer, content:For some men, being a computer was a temporary position until they moved on to greater advancements. For women the occupation was generally closed, with some exceptions such as Mary Edwards who worked from the 1780s to 1815 as one of thirty five computers for the British Nautical Almanac used for navigation at sea. This changed in the late nineteenth century with Edward Charles Pickering.[3] His group was at times termed "Pickering's Harem". Many of the women astronomers from this era were computers with possibly the best known being Henrietta Swan Leavitt, who worked with Pickering from 1893.
2534) 11.734999, Human–computer interaction - Wikipedia, the free encyclopedia.txt#31, term: computer, content:11. Replace memory with visual information: knowledge in the world. A user should not need to retain important information solely in working memory or retrieve it from long-term memory. A menu, checklist, or another display can aid the user by easing the use of their memory. However, the use of memory may sometimes benefit the user by eliminating the need to reference some type of knowledge in the world (e.g., an expert computer operator would rather use direct commands from memory than refer to a manual). The use of knowledge in a user's head and knowledge in the world must be balanced for an effective design.
2535) 11.734999, Hybrid computer - Wikipedia, the free encyclopedia.txt#2, term: computer, content:Digital computers can be built to take the solution of equations to almost unlimited precision, but quite slowly compared to analog computers. Generally, complex mathematical equations are approximated using iterative methods which take huge numbers of iterations, depending on how good the initial "guess" at the final value is and how much precision is desired. (This initial guess is known as the numerical "seed".) For many real-time operations in the 20th century, such digital calculations were too slow to be of much use (e.g., for very high frequency phased array radars or for weather calculations), but the precision of an analog computer is insufficient.
2536) 11.734999, Hybrid computer - Wikipedia, the free encyclopedia.txt#4, term: computer, content:Consider that the nervous system in animals is a form of hybrid computer. Signals pass across the synapses from one nerve cell to the next as discrete (digital) packets of chemicals, which are then summed within the nerve cell in an analog fashion by building an electro-chemical potential until its threshold is reached, whereupon it discharges and sends out a series of digital packets to the next nerve cell. The advantages are at least threefold: noise within the system is minimized (and tends not to be additive), no common grounding system is required, and there is minimal degradation of the signal even if there are substantial differences in activity of the cells along a path (only the signal delays tend to vary). The individual nerve cells are analogous to analog computers; the synapses are analogous to digital computers.
2537) 11.734999, IBM PC compatible - Wikipedia, the free encyclopedia.txt#0, term: computer, content:IBM PC compatible computers are those similar to the original IBM PC, XT, and AT, able to run the same software and support the same expansion cards as those. Such computers used to be referred to as PC clones, or IBM clones. They duplicate almost exactly all the significant features of the PC architecture, facilitated by IBM's choice of commodity hardware components and various manufacturers' ability to reverse engineer the BIOS firmware using a "clean room design" technique. Columbia Data Products built the first clone of the IBM personal computer by a clean room implementation of its BIOS.[citation needed]
2538) 11.734999, IBM PC compatible - Wikipedia, the free encyclopedia.txt#3, term: computer, content:IBM decided in 1980 to market a low-cost single-user computer as quickly as possible in response[citation needed] to Apple Computer's success in the burgeoning microcomputer market. On 12 August 1981, the first IBM PC went on sale. There were three operating systems (OS) available for it. The least expensive and most popular was PC DOS made by Microsoft. In a crucial concession, IBM's agreement allowed Microsoft to sell its own version, MS-DOS, for non-IBM computers. The only component of the original PC architecture exclusive to IBM was the BIOS (Basic Input/Output System).
2539) 11.734999, IBM PC compatible - Wikipedia, the free encyclopedia.txt#12, term: computer, content:During development, Compaq engineers found that Microsoft Flight Simulator would not run because of what subLOGIC's Bruce Artwick described as "a bug in one of Intel's chips", forcing them to make their new computer bug compatible with the IBM PC.[9] At first, few clones other than Compaq's offered truly full compatibility;[10] Columbia University reported in January 1984, for example, that Kermit ran without modification on Compaq and Columbia clones, but not on those from Eagle or Seequa. Other MS-DOS computers also required custom code.[11]
2540) 11.734999, IBM PC compatible - Wikipedia, the free encyclopedia.txt#13, term: computer, content:When PC Magazine requested samples from computer manufacturers that claimed to produce PC compatibles for an April 1984 review, 14 of 31 declined.[12][13] Corona Data Systems specified that "Our systems run all software that conforms to IBM PC programming standards. And the most popular software does."[14] When a BYTE journalist asked to test Peachtext at the Spring 1983 COMDEX, Corona representatives "hemmed and hawed a bit, but they finally led me ... off in the corner where no one would see it should it fail". The magazine reported that "Their hesitancy was unnecessary. The disk booted up without a problem".[15]
2541) 11.734999, IBM PC compatible - Wikipedia, the free encyclopedia.txt#17, term: computer, content:After 1987, IBM PC compatibles dominated both the home and business markets of commodity computers,[29] with other notable alternative architectures being used in niche markets, like the Macintosh computers offered by Apple Inc. and used mainly for desktop publishing at the time, the aging 8-bit Commodore 64 which was selling for $150 by this time and became the world's best-selling computer, the 32-bit Commodore Amiga line used for television and video production and the 32-bit Atari ST used by the music industry. However, IBM itself lost the main role in the market for IBM PC compatibles by 1990. A few events in retrospect are important:
2542) 11.734999, IBM PC compatible - Wikipedia, the free encyclopedia.txt#19, term: computer, content:As of October 2007, Hewlett-Packard and Dell have the largest shares of the PC market in North America. They are also successful overseas, with Acer, Lenovo, and Toshiba also notable. Worldwide, a huge number of PCs are "white box" systems assembled by myriad local systems builders. Despite advances of computer technology, all current IBM PC compatibles remain very much compatible with the original IBM PC computers, although most of the components implement the compatibility in special backward compatibility modes used only during a system boot. It is often more practical to run old software on a modern system using an emulator rather than relying on these features.
2543) 11.734999, Image scanner - Wikipedia, the free encyclopedia.txt#49, term: computer, content:Some scanners, especially those designed for scanning printed documents, only work in black and white but most modern scanners work in color. For the latter, the scanned result is a non-compressed RGB image, which can be transferred to a computer's memory. The color output of different scanners is not the same due to the spectral response of their sensing elements, the nature of their light source and the correction applied by the scanning software. While most image sensors have a linear response, the output values are usually gamma compressed. Some scanners compress and clean up the image using embedded firmware. Once on the computer, the image can be processed with a raster graphics program (such as Photoshop or the GIMP) and saved on a storage device (such as a hard disk).
2544) 11.734999, Information technology - Wikipedia, the free encyclopedia.txt#2, term: computer, content:Humans have been storing, retrieving, manipulating and communicating information since the Sumerians in Mesopotamia developed writing in about 3000BC,[6] but the term information technology in its modern sense first appeared in a 1958 article published in the Harvard Business Review; authors Harold J. Leavitt and Thomas L. Whisler commented that "the new technology does not yet have a single established name. We shall call it information technology (IT)." Their definition consists of three categories: techniques for processing, the application of statistical and mathematical methods to decision-making, and the simulation of higher-order thinking through computer programs.[7]
2545) 11.734999, Installation (computer programs) - Wikipedia, the free encyclopedia.txt#8, term: computer, content:On Windows systems, this is the most common form of installation. An installation process usually needs a user who attends it to make choices, such as accepting or declining an end-user license agreement (EULA), specifying preferences such as the installation location, supplying passwords or assisting in product activation. In graphical environments, installers that offer a wizard-based interface are common. Attended installers may ask users to help mitigate the errors. For instance, if the disk in which the computer program is being installed was full, the installer may ask the user to specify another target path or clear enough space in the disk.
2546) 11.734999, Intel - Wikipedia, the free encyclopedia.txt#16, term: computer, content:While Intel created the first commercially available microprocessor (Intel 4004) in 1971[15] and one of the first microcomputers in 1972,[24][26] by the early 1980s its business was dominated by dynamic random-access memory chips. However, increased competition from Japanese semiconductor manufacturers had, by 1983, dramatically reduced the profitability of this market. The growing success of the IBM personal computer, based on an Intel microprocessor, was among factors that convinced Gordon Moore (CEO since 1975) to shift the company's focus to microprocessors, and to change fundamental aspects of that business model. Moore's decision to sole-source Intel's 386 chip played into the company's continuing success.
2547) 11.734999, Intel - Wikipedia, the free encyclopedia.txt#17, term: computer, content:By the end of the 1980s, buoyed by its fortuitous position as microprocessor supplier to IBM and IBM's competitors within the rapidly growing personal computer market, Intel embarked on a 10-year period of unprecedented growth as the primary (and most profitable) hardware supplier to the PC industry, part of the winning 'Wintel' combination. Moore handed over to Andy Grove in 1987. By launching its Intel Inside marketing campaign in 1991, Intel was able to associate brand loyalty with consumer selection, so that by the end of the 1990s, its line of Pentium processors had become a household name.
2548) 11.734999, Intel 4004 - Wikipedia, the free encyclopedia.txt#1, term: computer, content:The first public mention of 4004 was an advertisement in the November 15, 1971 edition of Electronic News,[7] though unconfirmed reports put the date of first delivery as early as March 1971. Packaged in a 16-pin ceramic dual in-line package, the 4004 was the first commercially available computer processor designed and manufactured by chip maker Intel, which had previously made semiconductor memory chips. The chief designers of the chip were Federico Faggin who created the design methodology and the silicon-based chip design, Ted Hoff who formulated the architecture, both of Intel, and Masatoshi Shima of Busicom who assisted in the development.
2549) 11.734999, Intel 8008 - Wikipedia, the free encyclopedia.txt#0, term: computer, content:The Intel 8008 ("eight-thousand-eight" or "eighty-oh-eight") is an early byte-oriented microprocessor designed and manufactured by Intel and introduced in April 1972. It was an 8-bit CPU with an external 14-bit address bus that could address 16KB of memory. Originally known as the 1201, the chip was commissioned by Computer Terminal Corporation (CTC) to implement an instruction set of their design for their Datapoint 2200 programmable terminal. As the chip was delayed and did not meet CTC's performance goals, the 2200 ended up using CTC's own TTL based CPU instead. An agreement permitted Intel to market the chip to other customers after Seiko expressed an interest in using it for a calculator.
2550) 11.734999, Intel 8008 - Wikipedia, the free encyclopedia.txt#1, term: computer, content:CTC formed in San Antonio in 1968 under the direction of Austin O. "Gus" Roche and Phil Ray, both NASA engineers. Roche, in particular, was primarily interested in producing a desktop computer. However, given the immaturity of the market, the company's business plan mentioned only a Teletype Model 33 ASR replacement, which shipped as the Datapoint 3300. The case was deliberately designed to fit in the same space as an IBM Selectric typewriter, and used a video screen shaped to be the same aspect ratio as an IBM punched card.[3] Although commercially successful, the 3300 had ongoing heat problems due to the amount of circuitry packed into such a small space.
2551) 11.734999, Intel 8080 - Wikipedia, the free encyclopedia.txt#28, term: computer, content:The basic architecture of the 8080 and its successors has replaced many proprietary mid-range and mainframe computers, and withstood challenges of technologies such as RISC. Most computer manufacturers have abandoned producing their own processors below the highest performance points. Though x86 may not be the most elegant, or theoretically most efficient design, the sheer market force of so many dollars going into refining a design has made the x86 family today, and will remain for some time, the dominant processor architecture, even bypassing Intel's attempts to replace it with incompatible architectures such as the iAPX 432 and Itanium.
2552) 11.734999, Interactive fiction - Wikipedia, the free encyclopedia.txt#79, term: computer, content:Randy smith has written extensively on the design of computer games. He comments that if the puzzle requires the use of an object then it should be clear to the player if they use the wrong object, or the right object in the wrong way. Furthermore, when an item is used it is often unclear if the item will be needed again or not. If the puzzle requires a set of steps or actions which need to be performed, the player should receive feedback if they are trying the wrong actions or if they are tying the right actions in the wrong sequence. It must also be clear if the puzzle has be completed, if an action is tried then the game should give some form of feedback.
2553) 11.734999, Internet - Wikipedia, the free encyclopedia.txt#1, term: computer, content:The origins of the Internet date back to research commissioned by the United States federal government in the 1960s to build robust, fault-tolerant communication via computer networks.[1] The primary precursor network, the ARPANET, initially served as a backbone for interconnection of regional academic and military networks in the 1980s. The funding of the National Science Foundation Network as a new backbone in the 1980s, as well as private funding for other commercial extensions, led to worldwide participation in the development of new networking technologies, and the merger of many networks.[2] The linking of commercial networks and enterprises by the early 1990s marks the beginning of the transition to the modern Internet,[3] and generated a sustained exponential growth as generations of institutional, personal, and mobile computers were connected to the network.
2554) 11.734999, Internet - Wikipedia, the free encyclopedia.txt#11, term: computer, content:ARPANET development began with two network nodes which were interconnected between the Network Measurement Center at the University of California, Los Angeles (UCLA) Henry Samueli School of Engineering and Applied Science directed by Leonard Kleinrock, and the NLS system at SRI International (SRI) by Douglas Engelbart in Menlo Park, California, on 29 October 1969.[22] The third site was the Culler-Fried Interactive Mathematics Center at the University of California, Santa Barbara, followed by the University of Utah Graphics Department. In an early sign of future growth, fifteen sites were connected to the young ARPANET by the end of 1971.[23][24] These early years were documented in the 1972 film Computer Networks: The Heralds of Resource Sharing.
2555) 11.734999, Internet - Wikipedia, the free encyclopedia.txt#40, term: computer, content:IPv6 is not directly interoperable by design with IPv4. In essence, it establishes a parallel version of the Internet not directly accessible with IPv4 software. Thus, translation facilities must exist for internetworking or nodes must have duplicate networking software for both networks. Essentially all modern computer operating systems support both versions of the Internet Protocol. Network infrastructure, however, is still lagging in this development. Aside from the complex array of physical connections that make up its infrastructure, the Internet is facilitated by bi- or multi-lateral commercial contracts, e.g., peering agreements, and by technical specifications or protocols that describe the exchange of data over the network. Indeed, the Internet is defined by its interconnections and routing policies.
2556) 11.734999, Internet protocol suite - Wikipedia, the free encyclopedia.txt#64, term: computer, content:The Internet protocol suite does not presume any specific hardware or software environment. It only requires that hardware and a software layer exists that is capable of sending and receiving packets on a computer network. As a result, the suite has been implemented on essentially every computing platform. A minimal implementation of TCP/IP includes the following: Internet Protocol (IP), Address Resolution Protocol (ARP), Internet Control Message Protocol (ICMP), Transmission Control Protocol (TCP), User Datagram Protocol (UDP), and IGMP. In addition to IP, ICMP, TCP, UDP, Internet Protocol version 6 requires Neighbor Discovery Protocol (NDP), ICMPv6, and IGMPv6 and is often accompanied by an integrated IPSec security layer.
2557) 11.734999, Interpreter (computing) - Wikipedia, the free encyclopedia.txt#5, term: computer, content:While compilers (and assemblers) generally produce machine code directly executable by computer hardware, they can often (optionally) produce an intermediate form called object code. This is basically the same machine specific code but augmented with a symbol table with names and tags to make executable blocks (or modules) identifiable and relocatable. Compiled programs will typically use building blocks (functions) kept in a library of such object code modules. A linker is used to combine (pre-made) library files with the object file(s) of the application to form a single executable file. The object files that are used to generate an executable file are thus often produced at different times, and sometimes even by different languages (capable of generating the same object format).
2558) 11.734999, Interrupt - Wikipedia, the free encyclopedia.txt#1, term: computer, content:Hardware interrupts are used by devices to communicate that they require attention from the operating system.[2] Internally, hardware interrupts are implemented using electronic alerting signals that are sent to the processor from an external device, which is either a part of the computer itself, such as a disk controller, or an external peripheral. For example, pressing a key on the keyboard or moving the mouse triggers hardware interrupts that cause the processor to read the keystroke or mouse position. Unlike the software type (described below), hardware interrupts are asynchronous and can occur in the middle of instruction execution, requiring additional care in programming. The act of initiating a hardware interrupt is referred to as an interrupt request (IRQ).
2559) 11.734999, J. Presper Eckert - Wikipedia, the free encyclopedia.txt#10, term: computer, content:Eckert believed that the widely-adopted term "von Neumann architecture" should properly be known as the "Eckert Architecture," since the stored-program concept central to the von Neumann architecture had already been developed at the Moore School by the time von Neumann arrived on the scene in 1944-1945.[5] Eckert's contention that von Neumann improperly took credit for devising the stored program computer architecture was supported by Jean Bartik, one of the original ENIAC programmers.[6][7] Many others[who?] in the field, however, believe that the concept of a stored program predates both of these men, going as far back as Charles Babbage and others.
2560) 11.734999, JavaScript - Wikipedia, the free encyclopedia.txt#42, term: computer, content:JavaScript and the DOM provide the potential for malicious authors to deliver scripts to run on a client computer via the Web. Browser authors contain this risk using two restrictions. First, scripts run in a sandbox in which they can only perform Web-related actions, not general-purpose programming tasks like creating files. Second, scripts are constrained by the same origin policy: scripts from one Web site do not have access to information such as usernames, passwords, or cookies sent to another site. Most JavaScript-related security bugs are breaches of either the same origin policy or the sandbox.
2561) 11.734999, JavaScript - Wikipedia, the free encyclopedia.txt#63, term: computer, content:In addition to the native computer software, there are online JavaScript IDEs, debugging aids are themselves written in JavaScript and built to run on the Web. An example is the program JSLint, developed by Douglas Crockford who has written extensively on the language. JSLint scans JavaScript code for conformance to a set of standards and guidelines. Many libraries for JavaScript, such as three.js, provide links to demonstration code that can be edited by users. They are also used as a pedagogical tool by institutions such as Khan Academy[101] to allow students to experience writing code in an environment where they can see the output of their programs, without needing any setup beyond a Web browser.
2562) 11.734999, John Mauchly - Wikipedia, the free encyclopedia.txt#4, term: computer, content:In the summer of 1941, Mauchly took a Defense Training Course for Electronics at the University of Pennsylvania Moore School of Electrical Engineering. There he met the lab instructor, J. Presper Eckert (1919-1995), with whom he would form a long-standing working partnership. Following the course, Mauchly was hired as an instructor of electrical engineering and in 1943, he was promoted to assistant professor of electrical engineering. Following the outbreak of World War II, the United States Army Ordnance Department contracted the Moore School to build an electronic computer which, as proposed by Mauchly and Eckert, would accelerate the recomputation of artillery firing tables.[3]
2563) 11.734999, John Mauchly - Wikipedia, the free encyclopedia.txt#18, term: computer, content:Very early in the history of EMCC, John Mauchly assumed responsibility for programming, coding, and applications for the planned computer systems. His early interaction with representatives of the Census Bureau in 1944 and 1945, and discussion with people interested in statistics, weather prediction, and various business problems in 1945 and 1946 focused his attention on the need to provide new users with the software to accomplish their objectives. He knew it would be difficult to sell computers without application materials, and without training in how to use the systems. And so, EMCC began to assemble a staff of mathematicians interested in coding in early 1947. (from Norberg)
2564) 11.734999, John von Neumann - Wikipedia, the free encyclopedia.txt#0, term: computer, content:John von Neumann (/vn nmn/; Hungarian: Neumann Jnos Lajos, pronounced[njmn jano ljo]; December 28, 1903 February 8, 1957) was a Hungarian-American pure and applied mathematician, physicist, inventor, computer scientist, and polymath. He made major contributions to a number of fields, including mathematics (foundations of mathematics, functional analysis, ergodic theory, geometry, topology, and numerical analysis), physics (quantum mechanics, hydrodynamics and quantum statistical mechanics), economics (game theory), computing (Von Neumann architecture, linear programming, self-replicating machines, stochastic computing), and statistics.
2565) 11.734999, John von Neumann - Wikipedia, the free encyclopedia.txt#73, term: computer, content:Von Neumann's team performed the world's first numerical weather forecasts on the ENIAC computer; von Neumann published the paper Numerical Integration of the Barotropic Vorticity Equation in 1950.[138] Von Neumann's interest in weather systems and meteorological prediction led him to propose manipulating the environment by spreading colorants on the polar ice caps to enhance absorption of solar radiation (by reducing the albedo).[139][140] thereby inducing global warming.[139][140] Noting that the Earth was only 6F (3.3C) colder during the last glacial period, he said that the burning of coal and oil would result in "a general warming of the Earth by about one degree Fahrenheit."[141]
2566) 11.734999, Joystick - Wikipedia, the free encyclopedia.txt#7, term: computer, content:In the 1960s the use of joysticks became widespread in radio-controlled model aircraft systems such as the Kwik Fly produced by Phill Kraft (1964). The now-defunct Kraft Systems firm eventually became an important OEM supplier of joysticks to the computer industry and other users. The first use of joysticks outside the radio-controlled aircraft industry may have been in the control of powered wheelchairs, such as the Permobil (1963). During this time period NASA used joysticks as control devices as part of the Apollo missions. For example, the lunar lander test models were controlled with a joystick.
2567) 11.734999, Joystick - Wikipedia, the free encyclopedia.txt#23, term: computer, content:Specialist joysticks, classed as an assistive technology pointing device, are used to replace the computer mouse for people with fairly severe physical disabilities. Rather than controlling games, these joysticks control the pointer. They are often useful to people with athetoid conditions, such as cerebral palsy, who find them easier to grasp than a standard mouse.[33] Miniature joysticks are available for people with conditions involving muscular weakness such as muscular dystrophy or motor neurone disease as well. They are also used on electric powered wheelchairs for control since they are simple and effective to use as a control method.[34]
2568) 11.734999, KDE - Wikipedia, the free encyclopedia.txt#3, term: computer, content:The name KDE was intended as a wordplay on the existing Common Desktop Environment, available for Unix systems. CDE is an X11-based user environment jointly developed by HP, IBM, and Sun through the X/Open consortium, with an interface and productivity tools based on the Motif graphical widget toolkit. It was supposed to be an intuitively easy-to-use desktop computer environment.[4] The K was originally suggested to stand for "Kool", but it was quickly decided that the K should stand for nothing in particular. Therefore, the KDE initialism expanded to "K Desktop Environment" before it was dropped altogether in favor of KDE = Community due to the rebranding effort.
2569) 11.734999, Konrad Zuse - Wikipedia, the free encyclopedia.txt#10, term: computer, content:Improving on the basic Z2 machine, he built the Z3 in 1941. On 12 May 1941 Zuse presented the Z3, built in his workshop, to the public.[17][18] The Z3 was a binary 22-bit floating point calculator featuring programmability with loops but without conditional jumps, with memory and a calculation unit based on telephone relays. The telephone relays used in his machines were largely collected from discarded stock. Despite the absence of conditional jumps, the Z3 was a Turing complete computer. However, Turing-completeness was never considered by Zuse (who had practical applications in mind) and only demonstrated in 1998 (see History of computing hardware).
2570) 11.734999, Laptop - Wikipedia, the free encyclopedia.txt#5, term: computer, content:As 8-bit CPU machines became widely accepted, the number of portables increased rapidly. The Osborne 1, released in 1981, used the Zilog Z80 and weighed 23.6 pounds (10.7kg). It had no battery, a 5in (13cm) CRT screen, and dual 5.25in (13.3cm) single-density floppy drives. In the same year the first laptop-sized portable computer, the Epson HX-20, was announced.[9] The Epson had a LCD screen, a rechargeable battery, and a calculator-size printer in a 1.6kg (3.5lb) chassis. Both Tandy/RadioShack and HP also produced portable computers of varying designs during this period.[10][11]
2571) 11.734999, Laptop - Wikipedia, the free encyclopedia.txt#9, term: computer, content:The form of a traditional laptop computer is a clamshell, with a screen on one of its inner sides and a keyboard on the opposite. It can be easily folded to conserve space while traveling. The screen and keyboard are inaccessible while closed. Devices of this form are commonly called a 'traditional laptop' or notebook, particularly if they have a screen size of 11 to 17 inches measured diagonally and run a full-featured operating system like Windows 10, OS X or Linux. Traditional laptops are the most common form of laptops, although Chromebooks, Ultrabooks, convertibles and 2-in-1s (described below) are becoming more common, with similar performance being achieved in their more portable or affordable forms.
2572) 11.734999, LEO (computer) - Wikipedia, the free encyclopedia.txt#4, term: computer, content:On their return to the UK, Standingford and Thompson visited Hartree and Wilkes in Cambridge, and were favourably impressed with their technical expertise and vision. Hartree and Wilkes estimated that EDSAC was twelve to eighteen months from completion, but said that this timeline could be shortened if additional funding were available. Standingford and Thompson wrote a report to the Lyons' Board recommending that Lyons should acquire or build a computer to meet their business needs. The board agreed that, as a first step, Lyons would provide Hartree and Wilkes with 3,000 funding for the EDSAC project, and would also provide them with the services of a Lyons electrical engineer, Ernest Lenaerts. EDSAC was completed and ran its first program in May 1949.[3]
2573) 11.734999, Library (computing) - Wikipedia, the free encyclopedia.txt#1, term: computer, content:In computer science, a library is a collection of implementations of behavior, written in terms of a language, that has a well-defined interface by which the behavior is invoked. This means that as long as a higher level program uses a library to make system calls, it does not need to be re-written to implement those system calls over and over again. In addition, the behavior is provided for reuse by multiple independent programs. A program invokes the library-provided behavior via a mechanism of the language. For example, in a simple imperative language such as C, the behavior in a library is invoked by using C's normal function-call. What distinguishes the call as being to a library, versus being to another function in the same program, is the way that the code is organized in the system.
2574) 11.734999, LINC - Wikipedia, the free encyclopedia.txt#5, term: computer, content:The LINC control panel was used for single-stepping through programs and for program debugging. Execution could be stopped when the program counter matched a set of switches. Another function allowed execution to be stopped when a particular address was accessed. The single-step and the resume functions could be automatically repeated. The repetition rate could be varied over four orders of magnitude by means of an analog knob and a four-position decade switch, from about one step per second to about half of the full speed. Running a program at one step per second and gradually accelerating it to full speed provided an extremely dramatic way to experience and appreciate the speed of the computer.
2575) 11.734999, Linux - Wikipedia, the free encyclopedia.txt#0, term: computer, content:Linux (pronounced i/lnks/ LIN-ks[9][10] or, less frequently, /lanks/ LYN-ks[10][11]) is a Unix-like and mostly POSIX-compliant[12] computer operating system (OS) assembled under the model of free and open-source software development and distribution. The defining component of Linux is the Linux kernel,[13] an operating system kernel first released on October 5, 1991 by Linus Torvalds.[14][15] The Free Software Foundation uses the name GNU/Linux to describe the operating system, which has led to some controversy.[16][17]
2576) 11.734999, Linux - Wikipedia, the free encyclopedia.txt#46, term: computer, content:Beside the Linux distributions designed for general-purpose use on desktops and servers, distributions may be specialized for different purposes including: computer architecture support, embedded systems, stability, security, localization to a specific region or language, targeting of specific user groups, support for real-time applications, or commitment to a given desktop environment. Furthermore, some distributions deliberately include only free software. As of 2015[update], over four hundred Linux distributions are actively developed, with about a dozen distributions being most popular for general-purpose use.[75]
2577) 11.734999, Lisp (programming language) - Wikipedia, the free encyclopedia.txt#23, term: computer, content:Common Lisp is a successor to MacLisp. The primary influences were Lisp Machine Lisp, MacLisp, NIL, S-1 Lisp, Spice Lisp, and Scheme.[31] It has many of the features of Lisp Machine Lisp (a large Lisp dialect used to program Lisp Machines), but was designed to be efficiently implementable on any personal computer or workstation. Common Lisp has a large language standard including many built-in data types, functions, macros and other language elements, as well as an object system (Common Lisp Object System). Common Lisp also borrowed certain features from Scheme such as lexical scoping and lexical closures.
2578) 11.734999, Lisp (programming language) - Wikipedia, the free encyclopedia.txt#36, term: computer, content:The use of parentheses is Lisp's most immediately obvious difference from other programming language families. As a result, students have long given Lisp nicknames such as Lost In Stupid Parentheses, or Lots of Irritating Superfluous Parentheses.[38] However, the S-expression syntax is also responsible for much of Lisp's power: the syntax is extremely regular, which facilitates manipulation by computer. However, the syntax of Lisp is not limited to traditional parentheses notation. It can be extended to include alternative notations. For example, XMLisp is a Common Lisp extension that employs the metaobject protocol to integrate S-expressions with the Extensible Markup Language (XML).
2579) 11.734999, Logic gate - Wikipedia, the free encyclopedia.txt#27, term: computer, content:In an 1886 letter, Charles Sanders Peirce described how logical operations could be carried out by electrical switching circuits.[8] Eventually, vacuum tubes replaced relays for logic operations. Lee De Forest's modification, in 1907, of the Fleming valve can be used as an AND logic gate. Ludwig Wittgenstein introduced a version of the 16-row truth table as proposition 5.101 of Tractatus Logico-Philosophicus (1921). Walther Bothe, inventor of the coincidence circuit, got part of the 1954 Nobel Prize in physics, for the first modern electronic AND gate in 1924. Konrad Zuse designed and built electromechanical logic gates for his computer Z1 (from 193538).
2580) 11.734999, Low-level programming language - Wikipedia, the free encyclopedia.txt#0, term: computer, content:In computer science, a low-level programming language is a programming language that provides little or no abstraction from a computer's instruction set architecturecommands or functions in the language map closely to processor instructions. Generally this refers to either machine code or assembly language. The word "low" refers to the small or nonexistent amount of abstraction between the language and machine language; because of this, low-level languages are sometimes described as being "close to the hardware." Because of the close relationship between the language and the hardware architecture programs written in low-level languages tend to be relatively non-portable.
2581) 11.734999, Low-level programming language - Wikipedia, the free encyclopedia.txt#1, term: computer, content:Low-level languages can convert to machine code without a compiler or interpreter second-generation programming languages use a simpler processor called an assembler and the resulting code runs directly on the processor. A program written in a low-level language can be made to run very quickly, with a small memory footprint. An equivalent program in a high-level language can be less efficient and use more memory. Low-level languages are simple, but considered difficult to use, due to numerous technical details that the programmer must remember. By comparison, a high-level programming language isolates execution semantics of a computer architecture from the specification of the program, which simplifies development.
2582) 11.734999, Machine learning - Wikipedia, the free encyclopedia.txt#0, term: computer, content:Machine learning is a subfield of computer science[1] that evolved from the study of pattern recognition and computational learning theory in artificial intelligence.[1] In 1959, Arthur Samuel defined machine learning as a "Field of study that gives computers the ability to learn without being explicitly programmed".[2] Machine learning explores the study and construction of algorithms that can learn from and make predictions on data.[3] Such algorithms operate by building a model from an example training set of input observations in order to make data-driven predictions or decisions expressed as outputs,[4]:2 rather than following strictly static program instructions.
2583) 11.734999, Machine learning - Wikipedia, the free encyclopedia.txt#1, term: computer, content:Machine learning is closely related to (and often overlaps with) computational statistics; a discipline which also focuses in prediction-making through the use of computers. It has strong ties to mathematical optimization, which delivers methods, theory and application domains to the field. Machine learning is employed in a range of computing tasks where designing and programming explicit algorithms is unfeasible. Example applications include spam filtering, optical character recognition (OCR),[5] search engines and computer vision. Machine learning is sometimes conflated with data mining,[6] where the latter sub-field focuses more on exploratory data analysis and is known as unsupervised learning.[4]:vii[7]
2584) 11.734999, Machine learning - Wikipedia, the free encyclopedia.txt#3, term: computer, content:Tom M. Mitchell provided a widely quoted, more formal definition: "A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P if its performance at tasks in T, as measured by P, improves with experience E."[9] This definition is notable for its defining machine learning in fundamentally operational rather than cognitive terms, thus following Alan Turing's proposal in his paper "Computing Machinery and Intelligence" that the question "Can machines think?" be replaced with the question "Can machines do what we (as thinking entities) can do?"[10]
2585) 11.734999, Magnetic-core memory - Wikipedia, the free encyclopedia.txt#10, term: computer, content:Two key inventions led to the development of magnetic core memory in 1951. The first, An Wang's, was the write-after-read cycle, which solved the problem of how to use a storage medium in which the act of reading erased the data read enabling the construction of a serial, one-dimensional shift register of o(50) bits, using two cores to store a bit. A Wang core shift register is in the Revolution exhibit at the Computer History Museum. The second, Jay Forrester's, was the coincident-current system, which enabled a small number of wires to control a large number of cores enabling 3D memory arrays of several million bits e.g. 8K x 8K x 64 bits.[citation needed]
2586) 11.734999, Manchester Mark 1 - Wikipedia, the free encyclopedia.txt#3, term: computer, content:The computer is especially historically significant because of its pioneering inclusion of index registers, an innovation which made it easier for a program to read sequentially through an array of words in memory. Thirty-four patents resulted from the machine's development, and many of the ideas behind its design were incorporated in subsequent commercial products such as the IBM 701 and 702 as well as the Ferranti Mark 1. The chief designers, Frederic C. Williams and Tom Kilburn, concluded from their experiences with the Mark1 that computers would be used more in scientific roles than in pure mathematics. In 1951, they started development work on Meg, the Mark1's successor, which would include a floating point unit.
2587) 11.734999, Manchester Small-Scale Experimental Machine - Wikipedia, the free encyclopedia.txt#2, term: computer, content:The SSEM had a 32-bit word length and a memory of 32words. As it was designed to be the simplest possible stored-program computer, the only arithmetic operations implemented in hardware were subtraction and negation; other arithmetic operations were implemented in software. The first of three programs written for the machine found the highest proper divisor of 218 (262,144), a calculation that was known would take a long time to runand so prove the computer's reliabilityby testing every integer from 2181 downwards, as division was implemented by repeated subtraction of the divisor. The program consisted of 17instructions and ran for 52minutes before reaching the correct answer of 131,072, after the SSEM had performed 3.5million operations (for an effective CPU speed of 1.1 kIPS).
2588) 11.734999, Manchester Small-Scale Experimental Machine - Wikipedia, the free encyclopedia.txt#11, term: computer, content:For use in a binary digital computer, the tube had to be capable of storing either one of two states at each of its memory locations, corresponding to the binary digits (bits) 0 and1. It exploited the positive or negative electrostatic charge generated by displaying either a dash or a dot at any position on the CRT screen, a phenomenon known as secondary emission. A dash generated a positive charge, and a dot a negative charge, either of which could be picked up by a detector plate in front of the screen; a negative charge represented 0, and a positive charge1. The charge dissipated in about 0.2seconds, but it could be automatically refreshed from the data picked up by the detector.[18]
2589) 11.734999, Marcian Hoff - Wikipedia, the free encyclopedia.txt#5, term: computer, content:In 1954, he was one of the Westinghouse Science Talent Search (now Intel STS) finalists.[11] He was awarded the Stuart Ballantine Medal in 1979, the IEEE Cledo Brunetti Award in 1980, and the Franklin Institute Certificate of Merit in 1996. He was inducted into the National Inventors Hall of Fame in 1996[12] and received the National Medal of Technology and Innovation in 2009 from President Barack Obama. He was made a Fellow of the Computer History Museum in 2009 "for his work as part of the team that developed the Intel 4004, the world's first commercial microprocessor."[13] He received the 2011 IEEE/RSE Wolfson James Clerk Maxwell Award.[14]
2590) 11.734999, Massachusetts Institute of Technology - Wikipedia, the free encyclopedia.txt#23, term: computer, content:MIT's on-campus nuclear reactor[105] is one of the most powerful university-based nuclear reactors in the United States. The prominence of the reactor's containment building in a densely populated area has been controversial,[106] but MIT maintains that it is well-secured.[107] In 1999 Bill Gates donated US$20 million to MIT for the construction of a computer laboratory named the "William H. Gates Building", and designed by architect Frank O. Gehry. While Microsoft had previously given financial support to the institution, this was the first personal donation received from Gates.[108]
2591) 11.734999, Massachusetts Institute of Technology - Wikipedia, the free encyclopedia.txt#34, term: computer, content:MIT students refer to both their majors and classes using numbers or acronyms alone.[160] Departments and their corresponding majors are numbered in the approximate order of their foundation; for example, Civil and Environmental Engineering is Course 1, while Linguistics and Philosophy is Course 24.[161] Students majoring in Electrical Engineering and Computer Science (EECS), the most popular department, collectively identify themselves as "Course 6". MIT students use a combination of the department's course number and the number assigned to the class to identify their subjects; the introductory calculus-based classical mechanics course is simply "8.01" at MIT.[162][c]
2592) 11.734999, Massachusetts Institute of Technology - Wikipedia, the free encyclopedia.txt#43, term: computer, content:MIT places among the top ten in many overall rankings of universities (see right) and rankings based on students' revealed preferences.[196][197][198] For several years, U.S. News & World Report, the QS World University Rankings, and the Academic Ranking of World Universities have ranked MIT's School of Engineering first, as did the 1995 National Research Council report.[199] In the same lists, MIT's strongest showings apart from in engineering are in computer science, the natural sciences, business, economics, linguistics, mathematics, and, to a lesser extent, political science and philosophy.[10][11][12][13][200]
2593) 11.734999, Max Newman - Wikipedia, the free encyclopedia.txt#21, term: computer, content:Newman lost no time in establishing the renowned Royal Society Computing Machine Laboratory at the University.[23] In February 1946, he wrote to John von Neumann, expressing his desire to build a computing machine.[18] The Royal Society approved Newman's grant application in July 1946.[18] Frederic Calland Williams and Thomas Kilburn, experts in electronic circuit design, were recruited from the Telecommunications Research Establishment.[18][23] Kilburn and Williams built Baby, the world's first electronic stored-program digital computer based on Alan Turing's ideas.[18][23]
2594) 11.734999, Maynard, Massachusetts - Wikipedia, the free encyclopedia.txt#6, term: computer, content:After the woolen mill finally shut down in 1950, local businessmen bought the property and began leasing it as office or manufacturing space. Digital Equipment Corporation (DEC) moved into the complex in 1957, initially renting only 8,600 square feet. The company grew and grew until it bought the entire complex in 1974, which led to Maynard's nickname "Mini Computer Capital of the World". DEC remained in Maynard until 1998 when it was purchased by Compaq, which was itself later bought out by Hewlett Packard in 2002.[7] The mill complex was nearly empty for almost ten years.
2595) 11.734999, Mechanical computer - Wikipedia, the free encyclopedia.txt#1, term: computer, content:Mechanical computers reached their zenith during World War II, when they formed the basis of complex bombsights including the Norden, as well as the similar devices for ship computations such as the US Torpedo Data Computer or British Admiralty Fire Control Table. Noteworthy are mechanical flight instruments for early spacecraft, which provided their computed output not in the form of digits, but through the displacements of indicator surfaces. From Yuri Gagarin's first manned spaceflight until 2002, every manned Soviet and Russian spacecraft Vostok, Voskhod and Soyuz was equipped with a Globus instrument showing the apparent movement of the Earth under the spacecraft through the displacement of a miniature terrestrial globe, plus latitude and longitude indicators.
2596) 11.734999, Microcode - Wikipedia, the free encyclopedia.txt#9, term: computer, content:Microcode was originally developed as a simpler method of developing the control logic for a computer. Initially, CPU instruction sets were "hardwired". Each step needed to fetch, decode, and execute the machine instructions (including any operand address calculations, reads, and writes) was controlled directly by combinational logic and rather minimal sequential state machine circuitry. While very efficient, the need for powerful instruction sets with multi-step addressing and complex operations (see below) made such hard-wired processors difficult to design and debug; highly encoded and varied-length instructions can contribute to this as well, especially when very irregular encodings are used.
2597) 11.734999, Microcode - Wikipedia, the free encyclopedia.txt#18, term: computer, content:In 1947, the design of the MIT Whirlwind introduced the concept of a control store as a way to simplify computer design and move beyond ad hoc methods. The control store was a diode matrix: a two-dimensional lattice, where one dimension accepts "control time pulses" from the CPU's internal clock, and the other connects to control signals on gates and other circuits. A "pulse distributor" takes the pulses generated by the CPU clock and break them up into eight separate time pulses, each of which would activate a different row of the lattice. When the row is activated, it activates the control signals connected to it.[7]
2598) 11.734999, Microcode - Wikipedia, the free encyclopedia.txt#20, term: computer, content:In 1951, Maurice Wilkes enhanced this concept by adding conditional execution, a concept akin to a conditional in computer software. His initial implementation consisted of a pair of matrices: the first one generated signals in the manner of the Whirlwind control store, while the second matrix selected which row of signals (the microprogram instruction word, so to speak) to invoke on the next cycle. Conditionals were implemented by providing a way that a single line in the control store could choose from alternatives in the second matrix. This made the control signals conditional on the detected internal signal. Wilkes coined the term microprogramming to describe this feature and distinguish it from a simple control store.
2599) 11.734999, Microcode - Wikipedia, the free encyclopedia.txt#22, term: computer, content:There may also be a memory address register and a memory data register, used to access the main computer storage. Together, these elements form an "execution unit". Most modern CPUs have several execution units. Even simple computers usually have one unit to read and write memory, and another to execute user code. These elements could often be brought together as a single chip. This chip comes in a fixed width that would form a "slice" through the execution unit. These are known as "bit slice" chips. The AMD Am2900 family is one of the best known examples of bit slice elements. The parts of the execution units and the execution units themselves are interconnected by a bundle of wires called a bus.
2600) 11.734999, Microcomputer - Wikipedia, the free encyclopedia.txt#0, term: computer, content:A microcomputer is a small, relatively inexpensive computer with a microprocessor as its central processing unit (CPU).[2] It includes a microprocessor, memory, and minimal input/output (I/O) circuitry mounted on a single printed circuit board.[3] Microcomputers became popular in the 1970s and 1980s with the advent of increasingly powerful microprocessors. The predecessors to these computers, mainframes and minicomputers, were comparatively much larger and more expensive (though indeed present-day mainframes such as the IBM System z machines use one or more custom microprocessors as their CPUs). Many microcomputers (when equipped with a keyboard and screen for input and output) are also personal computers (in the generic sense).[4]
2601) 11.734999, Microcomputer - Wikipedia, the free encyclopedia.txt#13, term: computer, content:In 1972, a Sacramento State University team led by Bill Pentz built the Sac State 8008 computer,[14] able to handle thousands of patients' medical records. The Sac State 8008 was designed with the Intel 8008. It had a full set of hardware and software components: a disk operating system included in a series of programmable read-only memory chips (PROMs); 8 Kilobytes of RAM; IBM's Basic Assembly Language (BAL); a hard drive; a color display; a printer output; a 150 bit/s serial interface for connecting to a mainframe; and even the world's first microcomputer front panel.[15]
2602) 11.734999, Microprocessor - Wikipedia, the free encyclopedia.txt#23, term: computer, content:Intel introduced its first 4-bit microprocessor 4004 in 1971 and its 8-bit microprocessor 8008 in 1972. During the 1960s, computer processors were constructed out of small and medium-scale ICseach containing from tens of transistors to a few hundred. These were placed and soldered onto printed circuit boards, and often multiple boards were interconnected in a chassis. The large number of discrete logic gates used more electrical powerand therefore produced more heatthan a more integrated design with fewer ICs. The distance that signals had to travel between ICs on the boards limited a computer's operating speed.
2603) 11.734999, Minicomputer - Wikipedia, the free encyclopedia.txt#6, term: computer, content:At the launch of the MITS Altair 8800 in 1975, Radio Electronics magazine referred to the system as a "minicomputer", although the term microcomputer soon became usual for personal computers based on single-chip microprocessors. At the time, microcomputers were 8-bit single-user, relatively simple machines running simple program-launcher operating systems like CP/M or MS-DOS, while minis were much more powerful systems that ran full multi-user, multitasking operating systems, such as VMS and Unix, and although the classical mini was a 16-bit computer, the emerging higher performance superminis were 32-bit.
2604) 11.734999, MIPS instruction set - Wikipedia, the free encyclopedia.txt#3, term: computer, content:MIPS implementations are primarily used in embedded systems such as Windows CE devices, routers, residential gateways, and video game consoles such as the Nintendo 64, Sony PlayStation, PlayStation 2 and PlayStation Portable. Until late 2006, they were also used in many of SGI's computer products. MIPS implementations were also used by Digital Equipment Corporation, NEC, Pyramid Technology, Siemens Nixdorf, Tandem Computers and others during the late 1980s and 1990s. In the mid to late 1990s, it was estimated that one in three RISC microprocessors produced was a MIPS implementation.[7]
2605) 11.734999, MIPS instruction set - Wikipedia, the free encyclopedia.txt#54, term: computer, content:In 1999 MIPS formalized their licensing system around two basic designs, the 32-bit MIPS32 (based on MIPS II with some additional features from MIPS III, MIPS IV, and MIPS V) and the 64-bit MIPS64 (based on MIPS V). NEC, Toshiba and SiByte (later acquired by Broadcom) each obtained licenses for the MIPS64 as soon as it was announced. Philips, LSI Logic and IDT have since joined them. Today, the MIPS cores are one of the most-used "heavyweight"[clarification needed] cores in the marketplace for computer-like devices (hand-held computers, set-top boxes, etc.).
2606) 11.734999, MOS Technology 6502 - Wikipedia, the free encyclopedia.txt#0, term: computer, content:The MOS Technology 6502 (typically "sixty-five-oh-two" or "six-five-oh-two")[2] is an 8-bit microprocessor that was designed by a small team led by Chuck Peddle for MOS Technology. When it was introduced in 1975, the 6502 was, by a considerable margin, the least expensive full-featured microprocessor on the market. It initially sold for less than one-sixth the cost of competing designs from larger companies, such as Motorola and Intel, and caused rapid decreases in pricing across the entire processor market. Along with the Zilog Z80 it sparked a series of projects that resulted in the home computer revolution of the early 1980s.
2607) 11.734999, MOS Technology 6502 - Wikipedia, the free encyclopedia.txt#17, term: computer, content:One of the first "public" uses for the design was the Apple I microcomputer, introduced in 1976. The 6502 was next used in the Commodore PET and the Apple II,[48] both released in 1977. It was later used in the Atari home computers, the BBC Micro[48] family, the Commodore VIC-20 and a large number of other designs both for home computers and business, such as Ohio Scientific and Oric. The 6510, a direct successor of the 6502 with a digital I/O port and a tri-state address bus, was the CPU utilized in the best-selling[49][50] Commodore 64 home computer. Commodore's floppy disk drive, the 1541, had a processor of its ownit too was a 6502.
2608) 11.734999, Motorola 6809 - Wikipedia, the free encyclopedia.txt#16, term: computer, content:Series II of the Fairlight CMI (computer musical instrument) used dual 6809 CPUs and OS9, and also used one 6809 CPU per voice card. The 6809 was often employed in music synthesizers from other manufacturers such as Oberheim (Xpander, Matrix 6/12/1000), PPG (Wave 2/2.2/2.3, Waveterm A), and Ensoniq (Mirage sampler, SDP-1, ESQ1, SQ80). The latter used the 6809E as their main CPU. The (E) version was used in order to synchronize the microprocessor's clock to the sound chip (Ensoniq 5503 DOC) in those machines; in the ESQ1 and SQ80 the 68B09E was used, requiring a dedicated arbiter logic in order to ensure 1MHz bus timing when accessing the DOC chip.
2609) 11.734999, MS-DOS - Wikipedia, the free encyclopedia.txt#1, term: computer, content:MS-DOS resulted from a request in 1981 by IBM for an operating system to use in its IBM PC range of personal computers.[7][8] Microsoft quickly bought the rights to 86-DOS from Seattle Computer Products,[9] and began work on modifying it to meet IBM's specification. IBM licensed and released it in August 1981 as PC DOS 1.0 for use in their PCs. Although MS-DOS and PC DOS were initially developed in parallel by Microsoft and IBM, in subsequent years the two products diverged, with recognizable differences in compatibility, syntax, and capabilities.
2610) 11.734999, Non-uniform memory access - Wikipedia, the free encyclopedia.txt#4, term: computer, content:Limiting the number of memory accesses provided the key to extracting high performance from a modern computer. For commodity processors, this meant installing an ever-increasing amount of high-speed cache memory and using increasingly sophisticated algorithms to avoid cache misses. But the dramatic increase in size of the operating systems and of the applications run on them has generally overwhelmed these cache-processing improvements. Multi-processor systems without NUMA make the problem considerably worse. Now a system can starve several processors at the same time, notably because only one processor can access the computer's memory at a time.[2]
2611) 11.734999, Non-volatile memory - Wikipedia, the free encyclopedia.txt#1, term: computer, content:Non-volatile memory is typically used for the task of secondary storage, or long-term persistent storage.[1] The most widely used form of primary storage today is a volatile form of random access memory (RAM), meaning that when the computer is shut down, anything contained in RAM is lost. However, most forms of non-volatile memory have limitations that make them unsuitable for use as primary storage. Typically, non-volatile memory costs more, provides lower performance, or has worse write endurance than volatile random access memory.
2612) 11.734999, Oberon (operating system) - Wikipedia, the free encyclopedia.txt#9, term: computer, content:The computer science department at ETHZ has in recent years begun exploring active objects and concurrency for operating systems, and has released an early version of a new language Active Oberon and a new operating system for it, first called AOS and  due to copyright issues  now called A2, and/or Bluebottle. It is available from ETHZ with most source via the Internet. Native versions (A2), i.e. running on the bare hardware, are currently possible for Intel IA-32 and X86-64 single- and multi-processor systems and for the StrongARM CPU family, versions running on top of another operating system are available on Windows (WinAos), Unix (UnixAos), Linux (LinuxAos), and OS-X (DarwinAos).
2613) 11.734999, Operating system - Wikipedia, the free encyclopedia.txt#36, term: computer, content:In 1974, University of California, Berkeley installed its first Unix system. Over time, students and staff in the computer science department there began adding new programs to make things easier, such as text editors. When Berkeley received new VAX computers in 1978 with Unix installed, the school's undergraduates modified Unix even more in order to take advantage of the computer's hardware possibilities. The Defense Advanced Research Projects Agency of the US Department of Defense took interest, and decided to fund the project. Many schools, corporations, and government organizations took notice and started to use Berkeley's version of Unix instead of the official one distributed by AT&T.
2614) 11.734999, Operating system - Wikipedia, the free encyclopedia.txt#52, term: computer, content:The operating system provides an interface between an application program and the computer hardware, so that an application program can interact with the hardware only by obeying rules and procedures programmed into the operating system. The operating system is also a set of services which simplify development and execution of application programs. Executing an application program involves the creation of a process by the operating system kernel which assigns memory space and other resources, establishes a priority for the process in multi-tasking systems, loads program binary code into memory, and initiates execution of the application program which then interacts with the user and with hardware devices.
2615) 11.734999, Operating system - Wikipedia, the free encyclopedia.txt#53, term: computer, content:Interrupts are central to operating systems, as they provide an efficient way for the operating system to interact with and react to its environment. The alternative  having the operating system "watch" the various sources of input for events (polling) that require action  can be found in older systems with very small stacks (50 or 60 bytes) but is unusual in modern systems with large stacks. Interrupt-based programming is directly supported by most modern CPUs. Interrupts provide a computer with a way of automatically saving local register contexts, and running specific code in response to events. Even very basic computers support hardware interrupts, and allow the programmer to specify code which may be run when that event takes place.
2616) 11.734999, Optical engineering - Wikipedia, the free encyclopedia.txt#1, term: computer, content:Because optical engineers want to design and build devices that make light do something useful, they must understand and apply the science of optics in substantial detail, in order to know what is physically possible to achieve (physics and chemistry). However, they also must know what is practical in terms of available technology, materials, costs, design methods, etc. As with other fields of engineering, computers are important to many (perhaps most) optical engineers. They are used with instruments, for simulation, in design, and for many other applications. Engineers often use general computer tools such as spreadsheets and programming languages, and they make frequent use of specialized optical software designed specifically for their field.
2617) 11.734999, Package manager - Wikipedia, the free encyclopedia.txt#0, term: computer, content:A package manager or package management system is a collection of software tools that automates the process of installing, upgrading, configuring, and removing computer programs for a computer's operating system in a consistent manner. A package manager deals with packages, distributions of software and data in archive files. Packages contain metadata, such as the software's name, description of its purpose, version number, vendor, checksum, and a list of dependencies necessary for the software to run properly. Upon installation, metadata is stored in a local package database. Package managers typically maintain a database of software dependencies and version information to prevent software mismatches and missing prerequisites. They work closely with software repositories, binary repository managers, and app stores.
2618) 11.734999, Package manager - Wikipedia, the free encyclopedia.txt#4, term: computer, content:Computer systems which rely on dynamic library linking, instead of static library linking, share executable libraries of machine instructions across packages and applications. In these systems, complex relationships between different packages requiring different versions of libraries results in a challenge colloquially known as "dependency hell". On Microsoft Windows systems, this is also called "DLL hell" when working with dynamically linked libraries. Good package management is vital on these systems.[3] The Framework system from OPENSTEP was an attempt at solving this issue, by allowing multiple versions of libraries to be installed simultaneously, and for software packages to specify which version they were linked against.
2619) 11.734999, Pascal (programming language) - Wikipedia, the free encyclopedia.txt#5, term: computer, content:Pascal was the primary high-level language used for development in the Apple Lisa, and in the early years of the Macintosh. Parts of the original Macintosh operating system were hand-translated into Motorola 68000 assembly language from the Pascal sources.[6] The typesetting system TeX by Donald E. Knuth was written in WEB, the original literate programming system, based on DEC PDP-10 Pascal, while applications like Total Commander, Skype and Macromedia Captivate were written in Delphi (Object Pascal). Apollo Computer used Pascal as the systems programming language for its operating systems beginning in 1980.
2620) 11.734999, Pascal (programming language) - Wikipedia, the free encyclopedia.txt#9, term: computer, content:The first Pascal compiler was designed in Zrich for the CDC 6000 series mainframe computer family. Niklaus Wirth reports that a first attempt to implement it in Fortran in 1969, was unsuccessful due to Fortran's inadequacy to express complex data structures. The second attempt was formulated in the Pascal language itself and was operational by mid-1970. Many Pascal compilers since have been similarly self-hosting, that is, the compiler is itself written in Pascal, and the compiler is usually capable of recompiling itself when new features are added to the language, or when the compiler is to be ported to a new environment. The GNU Pascal compiler is one notable exception, being written in C.
2621) 11.734999, PDP-11 - Wikipedia, the free encyclopedia.txt#21, term: computer, content:In the 1980s, the IBM PC and its clones largely took over the small computer market; BYTE in 1984 reported that Venix on the PC's Intel 8088 microprocessor outperformed the same operating system on the PDP-11/23.[16] Newer microprocessors such as the Motorola 68000 (1979) and Intel 80386 (1985) also included 32-bit logical addressing. The mass-production of those chips eliminated any cost advantage for the 16-bit PDP-11. A line of personal computers based on the PDP-11, the DEC Professional series, failed commercially, along with other non-PDP-11 PC offerings from DEC.
2622) 11.734999, Personal computer - Wikipedia, the free encyclopedia.txt#15, term: computer, content:During the early 1980s, home computers were further developed for household use, with software for personal productivity, programming and games. They typically could be used with a television already in the home as the computer display, with low-detail blocky graphics and a limited color range, and text about 40 characters wide by 25 characters tall. Sinclair Research,[10] a UK company, produced the ZX Series  the ZX80 (1980), ZX81 (1981), and the ZX Spectrum; the latter was introduced in 1982, and totaled 8 million unit sold. Following came the Commodore 64, totaled 17 million units sold.[11][12]
2623) 11.734999, Personal computer - Wikipedia, the free encyclopedia.txt#19, term: computer, content:Eventually, due to the influence of the IBM PC on the personal computer market, personal computers and home computers lost any technical distinction. Business computers acquired color graphics capability and sound, and home computers and game systems users used the same processors and operating systems as office workers. Mass-market computers had graphics capabilities and memory comparable to dedicated workstations of a few years before. Even local area networking, originally a way to allow business computers to share expensive mass storage and peripherals, became a standard feature of personal computers used at home.
2624) 11.734999, Personal computer - Wikipedia, the free encyclopedia.txt#26, term: computer, content:In the developed world, there has been a vendor tradition to keep adding functions to maintain high prices of personal computers. However, since the introduction of the One Laptop per Child foundation and its low-cost XO-1 laptop, the computing industry started to pursue the price too. Although introduced only one year earlier, there were 14 million netbooks sold in 2008.[33] Besides the regular computer manufacturers, companies making especially rugged versions of computers have sprung up, offering alternatives for people operating their machines in extreme weather or environments.[34]
2625) 11.734999, Personal computer - Wikipedia, the free encyclopedia.txt#97, term: computer, content:Known for its use in servers, with the LAMP application stack as one of prominent examples, Linux is supported by corporations such as Dell, Hewlett-Packard, IBM, Novell, Oracle Corporation, Red Hat, Canonical Ltd. and Sun Microsystems. It is used as an operating system for a wide variety of computer hardware, including desktop computers, netbooks, supercomputers,[75] video game systems such as the Steam Machine or PlayStation3 (until this option was removed remotely by Sony in 2010[76]), several arcade games, and embedded devices such as mobile phones, portable media players, routers, and stage lighting systems.
2626) 11.734999, Personal computer - Wikipedia, the free encyclopedia.txt#98, term: computer, content:Generally, a computer user uses application software to carry out a specific task. System software supports applications and provides common services such as memory management, network connectivity and device drivers, all of which may be used by applications but are not directly of interest to the end user. A simplified analogy in the world of hardware would be the relationship of an electric light bulb (an application) to an electric power generation plant (a system): the power plant merely generates electricity, not itself of any real use until harnessed to an application like the electric light that performs a service that benefits the user.
2627) 11.734999, Plan 9 from Bell Labs - Wikipedia, the free encyclopedia.txt#49, term: computer, content:Several projects work to extend Plan 9, including 9atom and 9front. These forks augment Plan 9 with additional hardware drivers and software, including an improved version of the Upas e-mail system, the go compiler, Mercurial version control system support, and other programs.[6][52] Plan 9 was ported to the Raspberry Pi single-board computer.[53][54] The Harvey project attempts to replace the custom Plan 9 C compiler with GCC, to leverage modern development tools such as GitHub and Coverity and speed up development.[55]
2628) 11.734999, Platform game - Wikipedia, the free encyclopedia.txt#24, term: computer, content:Although there had long been important platform games on home computers, a second generation of platform games for computers appeared alongside the new wave of consoles. In the late 1980s and early 1990s, the Amiga was known as a stronger gaming platform than IBM-compatible PCs, thanks to its more powerful stock video hardware and sound hardware.[41] The Atari ST was solidly supported as well. Games like Shadow of the Beast and Turrican showed that computer platform games could rival the graphics and sound of their console contemporaries, and Prince of Persia featured an unprecedented level of animation.
2629) 11.734999, Platform game - Wikipedia, the free encyclopedia.txt#39, term: computer, content:In 1994, a small developer called Exact released a game for the X68000 computer called Geograph Seal. The game was a fully 3D polygonal first-person shooter hybrid with a pronounced platform jumping component. Players piloted a frog-like mech that could jump and then double-jump or triple-jump high into the air, as the camera panned down to help players line up their landings. In addition to shooting, jumping on enemies was a primary means of attack.[67] This was the first true 3D platform-action game with free-roaming environments, but it was never ported to another platform nor released outside Japan, so it remains relatively unknown in the West.[68]
2630) 11.734999, Platform game - Wikipedia, the free encyclopedia.txt#68, term: computer, content:Early examples of free-roaming, side-scrolling, 2D platform-adventures in the vein of "Metroidvania" include Nintendo's original Metroid in 1986 and Konami's Castlevania games: Vampire Killer in 1986[100][101] and Simon's Quest in 1987,[102][103] as well as Enix's sci-fi Sharp X1 computer game Brain Breaker in 1985,[29][104] Pony Canyon's Super Pitfall in 1986,[30] System Sacom's Euphory in 1987,[29] Bothtec's The Scheme in 1988,[29] and several Dragon Slayer action RPGs by Nihon Falcom such as the 1985 release Xanadu[105][106] and 1987 releases Faxanadu[105] and Legacy of the Wizard.[107]
2631) 11.734999, Portable computer - Wikipedia, the free encyclopedia.txt#1, term: computer, content:These are distinct from desktop replacement computers in that they are usually constructed from full-specification desktop components, and do not incorporate features associated with laptops or mobile devices. The principal advantage of a portable computer versus a laptop or other mobile computing device is the use of standard motherboards or backplanes providing plug-in slots for add-in cards. This allows mission specific cards such as test, A/D, or communication protocol (IEEE-488, 1553) to be installed. Portable computers also provide for more disk storage by using standard 3-1/2" drives and providing for multiple drives.
2632) 11.734999, PowerPC - Wikipedia, the free encyclopedia.txt#31, term: computer, content:The second generation was "pure" and included the "low end" PowerPC 603 and "high end" PowerPC 604. The 603 is notable due to its very low cost and power consumption. This was a deliberate design goal on Motorola's part, who used the 603 project to build the basic core for all future generations of PPC chips. Apple tried to use the 603 in a new laptop design but was unable due to the small 8KiB level 1 cache. The 68000 emulator in the Mac OS could not fit in 8KiB and thus slowed the computer drastically. The 603e solved this problem by having a 16KiB L1 cache which allowed the emulator to run efficiently.
2633) 11.734999, Program counter - Wikipedia, the free encyclopedia.txt#7, term: computer, content:Use of a PC that normally increments assumes that what a computer does is execute a usually linear sequence of instructions. Such a PC is central to the von Neumann architecture. Thus programmers write a sequential control flow even for algorithms that do not have to be sequential. The resulting von Neumann bottleneck led to research into parallel computing,[8] including non-von Neumann or dataflow models that did not use a PC; for example, rather than specifying sequential steps, the high-level programmer might specify desired function and the low-level programmer might specify this using combinatory logic.
2634) 11.734999, Programmer - Wikipedia, the free encyclopedia.txt#12, term: computer, content:Programmers in software development companies may work directly with experts from various fields to create software either programs designed for specific clients or packaged software for general use ranging from video games to educational software to programs for desktop publishing and financial planning. Programming of packaged software constitutes one of the most rapidly growing segments of the computer services industry. Some companies or organizations  even small ones  have set up their own IT team to ensure the design and development of in-house software to answer to very specific needs from their internal end-users, especially when existing software are not suitable or too expensive. This is for example the case in research laboratories.[citation needed]
2635) 11.734999, Punched card - Wikipedia, the free encyclopedia.txt#26, term: computer, content:In the early 1970s, IBM introduced a new, smaller, round-hole, 96-column card format along with the IBM System/3 computer. These cards have tiny (1mm), circular holes, smaller than those in paper tape. Data is stored in six-bit binary-coded decimal code, with three rows of 32 characters each, or 8-bit EBCDIC. In this format, each column of the top tiers are combined with two punch rows from the bottom tier to form an 8-bit byte, and the middle tier is combined with two more punch rows, so that each card contains 64 bytes of 8-bit-per-byte binary coded data.[41]
2636) 11.734999, Python (programming language) - Wikipedia, the free encyclopedia.txt#6, term: computer, content:Over six years ago, in December 1989, I was looking for a "hobby" programming project that would keep me occupied during the week around Christmas. My office ... would be closed, but I had a home computer, and not much else on my hands. I decided to write an interpreter for the new scripting language I had been thinking about lately: a descendant of ABC that would appeal to Unix/C hackers. I chose Python as a working title for the project, being in a slightly irreverent mood (and a big fan of Monty Python's Flying Circus).
2637) 11.734999, Quantum computing - Wikipedia, the free encyclopedia.txt#4, term: computer, content:An example of an implementation of qubits of a quantum computer could start with the use of particles with two spin states: "down" and "up" (typically written      |        {\displaystyle |{\downarrow }\rangle }   and      |        {\displaystyle |{\uparrow }\rangle }  , or      |  0      {\displaystyle |0{\rangle }}   and      |  1      {\displaystyle |1{\rangle }}  ). But in fact any system possessing an observable quantity A, which is conserved under time evolution such that A has at least two discrete and sufficiently spaced consecutive eigenvalues, is a suitable candidate for implementing a qubit. This is true because any such system can be mapped onto an effective spin-1/2 system.
2638) 11.734999, Quantum computing - Wikipedia, the free encyclopedia.txt#7, term: computer, content:The state of a three-qubit quantum computer is similarly described by an eight-dimensional vector (a,b,c,d,e,f,g,h), called a ket. Here, however, the coefficients can have complex values, and it is the sum of the squares of the coefficients' magnitudes,      |  a   |   2   +  |  b   |   2   +  +  |  h   |   2     {\displaystyle |a|^{2}+|b|^{2}+\cdots +|h|^{2}}  , that must equal 1. These squared magnitudes represent the probability of each of the given states. However, because a complex number encodes not just a magnitude but also a direction in the complex plane, the phase difference between any two coefficients (states) represents a meaningful parameter. This is a fundamental difference between quantum computing and probabilistic classical computing.[12]
2639) 11.734999, Quantum computing - Wikipedia, the free encyclopedia.txt#57, term: computer, content:The class of problems that can be efficiently solved by quantum computers is called BQP, for "bounded error, quantum, polynomial time". Quantum computers only run probabilistic algorithms, so BQP on quantum computers is the counterpart of BPP ("bounded error, probabilistic, polynomial time") on classical computers. It is defined as the set of problems solvable with a polynomial-time algorithm, whose probability of error is bounded away from one half.[84] A quantum computer is said to "solve" a problem if, for every instance, its answer will be right with high probability. If that solution runs in polynomial time, then that problem is in BQP.
2640) 11.734999, Random-access machine - Wikipedia, the free encyclopedia.txt#23, term: computer, content:So how do we address a register beyond the bounds of the finite state machine? One approach would be to modify the program-instructions (the ones stored in the registers) so that they contain more than one command. But this too can be exhausted unless an instruction is of (potentially) unbounded size. So why not use just one "ber-instruction" one really really big number that contains all the program instructions encoded into it! This is how Minsky solves the problem, but the Gdel numbering he uses represents a great inconvenience to the model, and the result is nothing at all like our intuitive notion of a "stored program computer".
2641) 11.734999, Random-access memory - Wikipedia, the free encyclopedia.txt#0, term: computer, content:Random-access memory (RAM /rm/) is a form of computer data storage. A random-access memory device allows data items to be accessed (read or written) in almost the same amount of time irrespective of the physical location of data inside the memory. In contrast, with other direct-access data storage media such as hard disks, CD-RWs, DVD-RWs and the older drum memory, the time required to read and write data items varies significantly depending on their physical locations on the recording medium, due to mechanical limitations such as media rotation speeds and arm movement.
2642) 11.734999, Real number - Wikipedia, the free encyclopedia.txt#49, term: computer, content:The precision is limited by the number of bits allocated to store a number, whether as floating-point numbers or arbitrary precision numbers. However, computer algebra systems can operate on irrational quantities exactly by manipulating formulas for them (such as        2      {\displaystyle \textstyle {\sqrt {2}}}  ,      arcsin   (    2   23    )     {\displaystyle \textstyle \arcsin \left({{2} \over {23}}\right)}  , or        0   1     x  x     d x    {\displaystyle \textstyle \int _{0}^{1}{x^{x}}\;dx}  ) rather than their rational or decimal approximation;[11] however, it is not in general possible to determine whether two such expressions are equal (the constant problem).
2643) 11.734999, Register machine - Wikipedia, the free encyclopedia.txt#40, term: computer, content:For a good treatment of the counter machine see Minsky (1967) Chapter 11 "Models similar to Digital Computers"he calls the counter machine a "program computer". A recent overview is found at van Emde Boas (1990). A recent treatment of the Minsky (1961)/Lambek (1961) model can be found Boolos-Burgess-Jeffrey (2002); they reincarnate Lambek's "abacus model" to demonstrate equivalence of Turing machines and partial recursive functions, and they provide a graduate-level introduction to both abstract machine models (counter- and Turing-) and the mathematics of recursion theory. Beginning with the first edition Boolos-Burgess (1970) this model appeared with virtually the same treatment.
2644) 11.734999, Rendering (computer graphics) - Wikipedia, the free encyclopedia.txt#46, term: computer, content:Though it receives less attention, an understanding of human visual perception is valuable to rendering. This is mainly because image displays and human perception have restricted ranges. A renderer can simulate an almost infinite range of light brightness and color, but current displays  movie screen, computer monitor, etc.  cannot handle so much, and something must be discarded or compressed. Human perception also has limits, and so does not need to be given large-range images to create realism. This can help solve the problem of fitting images into displays, and, furthermore, suggest what short-cuts could be used in the rendering simulation, since certain subtleties won't be noticeable. This related subject is tone mapping result.
2645) 11.734999, SCSI - Wikipedia, the free encyclopedia.txt#0, term: computer, content:Small Computer System Interface (SCSI, /skzi/ SKUZ-ee)[1] is a set of standards for physically connecting and transferring data between computers and peripheral devices. The SCSI standards define commands, protocols, electrical and optical interfaces. SCSI is most commonly used for hard disk drives and tape drives, but it can connect a wide range of other devices, including scanners and CD drives, although not all controllers can handle all devices. The SCSI standard defines command sets for specific peripheral device types; the presence of "unknown" as one of these types means that in theory it can be used as an interface to almost any device, but the standard is highly pragmatic and addressed toward commercial requirements.
2646) 11.734999, SCSI - Wikipedia, the free encyclopedia.txt#32, term: computer, content:In modern SCSI transport protocols, there is an automated process for the "discovery" of the IDs. The SSA initiator (normally the host computer through the 'host adaptor') "walk the loop" to determine what devices are connected and then assigns each one a 7-bit "hop-count" value. Fibre Channel  Arbitrated Loop (FC-AL) initiators use the LIP (Loop Initialization Protocol) to interrogate each device port for its WWN (World Wide Name). For iSCSI, because of the unlimited scope of the (IP) network, the process is quite complicated. These discovery processes occur at power-on/initialization time and also if the bus topology changes later, for example if an extra device is added.
2647) 11.734999, Semi-Automatic Ground Environment - Wikipedia, the free encyclopedia.txt#10, term: computer, content:The only solution to this problem was to build a huge number of stations with overlapping coverage. At that point the problem became one of managing the information. Manual plotting was immediately ruled out as too slow, and a computerized solution was the only possibility. In order to be able to handle this task, the computer would need to be fed information directly, eliminating any manual translation by phone operators, and it would have to be able to analyze that information and automatically develop tracks.[12] A system tasked with defending cities against the predicted future Soviet bomber fleet would have to be dramatically more powerful that the models used in the NTDS or DATAR.[13][14]
2648) 11.734999, Semi-Automatic Ground Environment - Wikipedia, the free encyclopedia.txt#14, term: computer, content:Project Charles was placed under the direction of Francis Wheeler Loomis and included 28 scientists, about half of whom were already associated with MIT. Their study ran from February to August 1951, and in their final report they stated that "We endorse the concept of a centralized system as proposed by the Air Defense Systems Engineering Committee, and we agree that the central coordinating apparatus of this system should be a high-speed electronic digital computer."[15] The report went on to describe a new lab that would be used for generic technology development for the Air Force, Army and Navy, and would be known as Project Lincoln.[15]
2649) 11.734999, Semi-Automatic Ground Environment - Wikipedia, the free encyclopedia.txt#24, term: computer, content:On October 28, 1953, the Air Force Council recommended 1955 funding for "ADC to convert to the Lincoln automated system"[4]:193 ("redesignated the SAGE System in 1954").[4]:201 The "experimental SAGE subsector, located in Lexington, Mass., was completed in 1955with a prototype AN/FSQ-7known as XD-1"[9] (single computer system[30] in Building F).[21] In 1955, Air Force personnel began IBM training at the Kingston, New York, prototype facility,[5] and the "4620th Air Defense Wing (experimental SAGE) was established at Lincoln Laboratory"
2650) 11.734999, Serious game - Wikipedia, the free encyclopedia.txt#8, term: computer, content:The early 2000s saw a surge in different types of educational games, especially those designed for the younger learner. Many of these games were not computer-based but took on the model of other traditional gaming system both in the console and hand-held format. In 1999, LeapFrog Enterprises introduced the LeapPad, which combined an interactive book with a cartridge and allowed kids to play games and interact with a paper-based book. Based on the popularity of traditional hand-held gaming systems like Nintendo's Game Boy, they also introduced their hand-held gaming system called the Leapster in 2003. This system was cartridge-based and integrated arcadestyle games with educational content.[9]
2651) 11.734999, Slide rule - Wikipedia, the free encyclopedia.txt#66, term: computer, content:Computers also changed the nature of calculation. With slide rules a great emphasis was put on working the algebra to get expressions into the most computable form. Users would simply approximate or drop small terms to simplify a calculation. FORTRAN allowed complicated formulas to be typed in from textbooks without the effort of reformulation. Numerical integration was often easier than trying to find closed-form solutions for difficult problems. The young engineer asking for computer time to solve a problem that could have been done by a few swipes on the slide rule became a humorous clich.
2652) 11.734999, Smartphone - Wikipedia, the free encyclopedia.txt#34, term: computer, content:In late 2001, Handspring launched the Springboard GSM phone module with limited success. In May 2002, Handspring released the Palm OS Treo 270 smartphone, that did not support Springboard, with both a touchscreen and a full keyboard. The Treo had wireless web browsing, email, calendar, a contact organizer and mobile third-party applications that could be downloaded or synced with a computer.[109] Handspring was purchased by Palm, Inc which released the Treo 600 and continued releasing Treo devices with a few Treo devices using Windows Mobile. After buying Palm in 2011, Hewlett-Packard (HP) discontinued its webOS smartphone and tablet production.[110]
2653) 11.734999, Software bug - Wikipedia, the free encyclopedia.txt#0, term: computer, content:A software bug is an error, flaw, failure or fault in a computer program or system that causes it to produce an incorrect or unexpected result, or to behave in unintended ways. Most bugs arise from mistakes and errors made by people in either a program's source code or its design, or in frameworks and operating systems used by such programs, and a few are caused by compilers producing incorrect code. A program that contains a large number of bugs, and/or bugs that seriously interfere with its functionality, is said to be buggy or defective. Reports detailing bugs in a program are commonly known as bug reports, defect reports, fault reports, problem reports, trouble reports, change requests and so forth.
2654) 11.734999, Software synthesizer - Wikipedia, the free encyclopedia.txt#1, term: computer, content:Dedicated hardware synthesizers can have software as complex as a soft synth. The distinction is that softsynths run on a general purpose computer with a sound card, and the hardware (dedicated) synthesizers have the custom software built-in. The advantage to dedicated hardware is that it can be more stable, and also that it often has a user interface that is physical (knobs and sliders) and therefore easier to manipulate during performances. Many softsynths use mathematical algorithms that directly emulate the electronic components and circuitry of the original hardware synthesizer. This produces an exceptionally authentic sound, even capturing "flaws" in the original hardware, such as oscillator drift caused by thermal sensitivity of the components.
2655) 11.734999, Spreadsheet - Wikipedia, the free encyclopedia.txt#4, term: computer, content:LANPAR was the first electronic spreadsheet on mainframe and time sharing computers. VisiCalc was the first electronic spreadsheet on a microcomputer,[1] and it helped turn the Apple II computer into a popular and widely used system. Lotus 1-2-3 was the leading spreadsheet when DOS was the dominant operating system.[2] Excel now has the largest market share on the Windows and Macintosh platforms.[3][4][5] A spreadsheet program is a standard feature of an office productivity suite; since the advent of web apps, office suites now also exist in web app form.
2656) 11.734999, Spreadsheet - Wikipedia, the free encyclopedia.txt#18, term: computer, content:The actual software was called LANPAR  LANguage for Programming Arrays at Random.[10] This was conceived and entirely developed in the summer of 1969 following Pardo and Landau's recent graduation from Harvard University. Co-inventor Rene Pardo recalls that he felt that one manager at Bell Canada should not have to depend on programmers to program and modify budgeting forms, and he thought of letting users type out forms in any order and having computer calculating results in the right order ("Forward Referencing/Natural Order Calculation"). Pardo and Landau developed and implemented the software in 1969.[11]
2657) 11.734999, Spreadsheet - Wikipedia, the free encyclopedia.txt#42, term: computer, content:Computer scientist Alan Kay used the term value rule to summarize a spreadsheet's operation: a cell's value relies solely on the formula the user has typed into the cell.[26]The formula may rely on the value of other cells, but those cells are likewise restricted to user-entered data or formulas. There are no 'side effects' to calculating a formula: the only output is to display the calculated result inside its occupying cell. There is no natural mechanism for permanently modifying the contents of a cell unless the user manually modifies the cell's contents. In the context of programming languages, this yields a limited form of first-order functional programming.[27]
2658) 11.734999, Stack machine - Wikipedia, the free encyclopedia.txt#64, term: computer, content:Some Burroughs stack machines do support up-level refs directly in the hardware, with specialized address modes and a special 'display' register file holding the frame addresses of all outer scopes. No subsequent computer lines have done this in hardware. When Niklaus Wirth developed the first Pascal compiler for the CDC 6000, he found that it was faster overall to pass in the frame pointers as a chain, rather than constantly updating complete arrays of frame pointers. This software method also adds no overhead for common languages like C which lack up-level refs.
2659) 11.734999, Subroutine - Wikipedia, the free encyclopedia.txt#1, term: computer, content:As the name subprogram suggests, a subroutine behaves in much the same way as a computer program that is used as one step in a larger program or another subprogram. A subroutine is often coded so that it can be started (called) several times and from several places during one execution of the program, including from other subroutines, and then branch back (return) to the next instruction after the call once the subroutine's task is done. Maurice Wilkes, David Wheeler, and Stanley Gill are credited with the invention of this concept, which they termed a closed subroutine,[2][3] contrasted with an open subroutine or macro.[4]
2660) 11.734999, Subroutine - Wikipedia, the free encyclopedia.txt#7, term: computer, content:A subroutine call may also have side effects such as modifying data structures in a computer memory, reading from or writing to a peripheral device, creating a file, halting the program or the machine, or even delaying the program's execution for a specified time. A subprogram with side effects may return different results each time it is called, even if it is called with the same arguments. An example is a random number function, available in many languages, that returns a different pseudo-random number each time it is called. The widespread use of subroutines with side effects is a characteristic of imperative programming languages.
2661) 11.734999, Subroutine - Wikipedia, the free encyclopedia.txt#54, term: computer, content:Modern languages after ALGOL such as PL/1 and C almost invariably use a stack, usually supported by most modern computer instruction sets to provide a fresh activation record for every execution of a subprogram. That way, the nested execution is free to modify its local variables without concern for the effect on other suspended executions in progress. As nested calls accumulate, a call stack structure is formed, consisting of one activation record for each suspended subprogram. In fact, this stack structure is virtually ubiquitous, and so activation records are commonly termed stack frames.
2662) 11.734999, Tablet computer - Wikipedia, the free encyclopedia.txt#52, term: computer, content:The multiple licensees ensured that multiple fabricators could supply near-identical products, while encouraging price competition. This forced unit prices down to a fraction of their x86 equivalents. The architecture has historically had limited support from Microsoft, with only Windows CE available, but with the 2012 release of Windows 8, Microsoft announced additional support for the architecture, shipping their own ARM-based tablet computer, branded the Microsoft Surface, as well as an x86-64 Intel Core i5 variant branded as Microsoft Surface Pro.[92][93][94][95]
2663) 11.734999, Telecommunication - Wikipedia, the free encyclopedia.txt#13, term: computer, content:On 11 September 1940, George Stibitz was able to transmit problems using teletype to his Complex Number Calculator in New York and receive the computed results back at Dartmouth College in New Hampshire.[28] This configuration of a centralized computer or mainframe with remote dumb terminals remained popular throughout the 1950s. However, it was not until the 1960s that researchers started to investigate packet switching  a technology that would allow chunks of data to be sent to different computers without first passing through a centralized mainframe. A four-node network emerged on 5 December 1969; this network would become ARPANET, which by 1981 would consist of 213 nodes.[29]
2664) 11.734999, Telecommunications engineering - Wikipedia, the free encyclopedia.txt#0, term: computer, content:'Telecommunications engineering, or telecoms engineering', is an engineering discipline that brings together all electrical engineering disciplines including computer engineering with systems engineering to enhance telecommunication systems.[1][2] The work ranges from basic circuit design to strategic mass developments. A telecommunication engineer is responsible for designing and overseeing the installation of telecommunications equipment and facilities, such as complex electronic switching systems, copper wire telephone facilities,fiber optics cabling,IP data systems,Terrestrial radio link systems for conventional communications and process information. Telecommunication engineering also overlaps heavily with broadcast engineering.
2665) 11.734999, Telecommunications engineering - Wikipedia, the free encyclopedia.txt#43, term: computer, content:As civil engineers, OSP engineers are responsible for drafting plans, either by hand or using Computer-aided design (CAD) software, for how telecom plant facilities will be placed. Often when working with municipalities trenching or boring permits are required and drawings must be made for these. Often these drawings include about 70% or so of the detailed information required to pave a road or add a turn lane to an existing street. Structural calculations are required when boring under heavy traffic areas such as highways or when attaching to other structures such as bridges. As civil engineers, telecom engineers provide the modern communications backbone for all technological communications distributed throughout civilizations today.
2666) 11.734999, Tom Kilburn - Wikipedia, the free encyclopedia.txt#1, term: computer, content:A graduate of Sidney Sussex College, Cambridge, Kilburn worked on radar at the Telecommunications Research Establishment (TRE) in Malvern under Frederic Calland Williams during the Second World War. After the war ended, he was recruited by Williams to work on the development of computers at Victoria University of Manchester. He led the development of a succession of innovative Manchester computers that incorporated a host of ground-breaking innovations and developments, including the Ferranti Mark 1, the world's first commercial computer, and the Atlas, one of the first time-sharing multiprocessing computers that incorporated job scheduling, spooling, interrupts, pipelining and paging.
2667) 11.734999, Tom Kilburn - Wikipedia, the free encyclopedia.txt#5, term: computer, content:Kilburn's wartime work inspired his enthusiasm for some form of electronic computer. The principal technical barrier to such a development at that time was the lack of any practical means of storage for data and instructions. In July 1946, Kilburn and Williams collaboratively developed a storage device based on a cathode ray tube (CRT) called the Williams-Kilburn tube. A patent was filed in 1946.[3] Initially they used it to store a single bit. The CRT image soon faded, so they devised a scheme by which it was read and refreshed continually, effectively making the data storage permanent. By December 1947, they were able to store 2,048 bits on one 6-inch (150mm) diameter CRT.[4][5]
2668) 11.734999, Tom Kilburn - Wikipedia, the free encyclopedia.txt#6, term: computer, content:In December 1946, Williams took up the Edward Stocks Massey Chair of Electrotechnics at Victoria University of Manchester, and recruited Kilburn on secondment from Malvern.[2] The two developed their storage technology and, in 1948, Kilburn put it to a practical test in constructing the Small-Scale Experimental Machine (SSEM) which became the first stored-program computer to run a program, on 21 June 1948.[1] He received the degree of Ph.D. in 1948 for his work at Manchester, writing his thesis on "A storage system for use with binary digital computing machines" under Williams' supervision.[6]
2669) 11.734999, Tom Kilburn - Wikipedia, the free encyclopedia.txt#7, term: computer, content:Kilburn anticipated a return to Malvern but Williams persuaded him to stay to work on the university's collaborative project developing the Ferranti Mark 1, the world's first commercial computer.[7][8][9] Max Newman withdrew from the project, believing that the development of computers required engineers and not mathematicians at this point, but Williams preferred to return to electrotechnics, leaving Kilburn in charge.[2] He was assisted by Alan Turing, who arrived at Manchester in 1948.[4][10] The Mark I incorporated innovations such as index registers, and combined CRTs with magnetic drum storage.[1][11] Nine Mark I computers were sold by between 1951 and 1957.[2]
2670) 11.734999, Touchscreen - Wikipedia, the free encyclopedia.txt#14, term: computer, content:Sears et al. (1990)[22] gave a review of academic research on single and multi-touch humancomputer interaction of the time, describing gestures such as rotating knobs, adjusting sliders, and swiping the screen to activate a switch (or a U-shaped gesture for a toggle switch). The University of Maryland Human  Computer Interaction Lab team developed and studied small touchscreen keyboards (including a study that showed that users could type at 25 wpm for a touchscreen keyboard compared with 58 wpm for a standard keyboard), thereby paving the way for the touchscreen keyboards on mobile devices. They also designed and implemented multitouch gestures such as selecting a range of a line, connecting objects, and a "tap-click" gesture to select while maintaining location with another finger.
2671) 11.734999, Trackball - Wikipedia, the free encyclopedia.txt#12, term: computer, content:Large trackballs are sometimes seen on computerized special-purpose workstations, such as the radar consoles in an air-traffic control room or sonar equipment on a ship or submarine. Modern installations of such equipment may use mice instead, since most people now already know how to use one. However, military mobile anti-aircraft radars, commercial airliners (such as Airbus A380) and submarine sonars tend to continue using trackballs, since they can be made more durable and more fit for fast emergency use. Large and well made ones allow easier high precision work, for which reason they may still be used in these applications (where they are often called "tracker balls") and in computer-aided design.
2672) 11.734999, Trackball - Wikipedia, the free encyclopedia.txt#21, term: computer, content:Some computer users prefer a trackball over the more common mouse for ergonomic reasons. There seems to be no conclusive evidence from studies performed to determine which type of pointing device works best for most applications. Application users are encouraged to test different devices, and to maintain proper posture and scheduled breaks for comfort. Some disabled users find trackballs easier since they only have to move their thumb relative to their hand, instead of moving the whole hand, while others incur unacceptable fatigue of the thumb. Elderly people sometimes have difficulty holding a mouse still while double-clicking; the trackball allows them to let go of the ball while using the button.
2673) 11.734999, Trigonometry - Wikipedia, the free encyclopedia.txt#23, term: computer, content:Fields that use trigonometry or trigonometric functions include astronomy (especially for locating apparent positions of celestial objects, in which spherical trigonometry is essential) and hence navigation (on the oceans, in aircraft, and in space), music theory, audio synthesis, acoustics, optics, electronics, biology, medical imaging (CAT scans and ultrasound), pharmacy, chemistry, number theory (and hence cryptology), seismology, meteorology, oceanography, many physical sciences, land surveying and geodesy, architecture, image compression, phonetics, economics, electrical engineering, mechanical engineering, civil engineering, computer graphics, cartography, crystallography and game development.
2674) 11.734999, Universal Turing machine - Wikipedia, the free encyclopedia.txt#0, term: computer, content:In computer science, a universal Turing machine (UTM) is a Turing machine that can simulate an arbitrary Turing machine on arbitrary input. The universal machine essentially achieves this by reading both the description of the machine to be simulated as well as the input thereof from its own tape. Alan Turing introduced this machine in 19361937. This model is considered by some (for example, Martin Davis (2000)) to be the origin of the stored program computerused by John von Neumann (1946) for the "Electronic Computing Instrument" that now bears von Neumann's name: the von Neumann architecture. It is also known as universal computing machine, universal machine (UM), machine U, U.
2675) 11.734999, Universal Turing machine - Wikipedia, the free encyclopedia.txt#2, term: computer, content:Every Turing machine computes a certain fixed partial computable function from the input strings over its alphabet. In that sense it behaves like a computer with a fixed program. However, we can encode the action table of any Turing machine in a string. Thus we can construct a Turing machine that expects on its tape a string describing an action table followed by a string describing the input tape, and computes the tape that the encoded Turing machine would have computed. Turing described such a construction in complete detail in his 1936 paper:
2676) 11.734999, University of Pennsylvania - Wikipedia, the free encyclopedia.txt#3, term: computer, content:All of Penn's schools exhibit very high research activity. Penn is consistently ranked among the top research universities in the world, for both quality and quantity of research.[10] In fiscal year 2015, Penn's academic research budget was $851 million, involving more than 4,300 faculty, 1,100 postdoctoral fellows and 5,500 support staff/graduate assistants.[2] As one of the most active and prolific research institutions, Penn is associated with several important innovations and discoveries in many fields of science and the humanities. Among them are the first general purpose electronic computer (ENIAC), the rubella and hepatitis B vaccines, Retin-A, cognitive therapy, conjoint analysis and others.
2677) 11.734999, University of Pennsylvania - Wikipedia, the free encyclopedia.txt#12, term: computer, content:Penn's educational innovations include: the nation's first medical school in 1765; the first university teaching hospital in 1874; the Wharton School, the world's first collegiate school of business, in 1881; the first American student union building, Houston Hall, in 1896;[25] the country's second school of veterinary medicine; and the home of ENIAC, the world's first electronic, large-scale, general-purpose digital computer in 1946. Penn is also home to the oldest continuously functioning psychology department in North America and is where the American Medical Association was founded.[26][27] Penn was also the first university to award a PhD to an African-American woman, Sadie Tanner Mossell Alexander, in 1921 (in economics).[28]
2678) 11.734999, Unix - Wikipedia, the free encyclopedia.txt#12, term: computer, content:In 1972, Unix was rewritten in the C programming language.[18] The migration from assembly to the higher-level language C resulted in much more portable software,[19] requiring only a relatively small amount of machine-dependent code to be replaced when porting Unix to other computing platforms. Bell Labs produced several versions of Unix that are collectively referred to as Research Unix. In 1975, the first source license for UNIX was sold to faculty at the University of Illinois Department of Computer Science.[20] UIUC graduate student Greg Chesson (who had worked on the UNIX kernel at Bell Labs) was instrumental in negotiating the terms of this license.[21]
2679) 11.734999, UNIX System V - Wikipedia, the free encyclopedia.txt#20, term: computer, content:Many companies licensed SVR4 and bundled it with computer systems such as workstations and network servers. SVR4 systems vendors included Atari (Atari System V), Commodore (Amiga Unix), Data General (DG/UX), Fujitsu (UXP/DS), Hitachi (HI-UX), Hewlett-Packard (HP-UX), NCR (Unix/NS), NEC (EWS-UX, UP-UX, UX/4800), OKI (OKI System V), Pyramid Technology (DC/OSx), SGI (IRIX), Siemens (SINIX), Sony (NEWS-OS), Sumitomo Electric Industries (SEIUX), and Sun Microsystems (Solaris).
2680) 11.734999, USB - Wikipedia, the free encyclopedia.txt#36, term: computer, content:Though most computers since mid-2004 can boot from USB mass storage devices, USB is not intended as a primary bus for a computer's internal storage. Buses such as Parallel ATA (PATA or IDE), Serial ATA (SATA), or SCSI fulfill that role in PC class computers. However, USB has one important advantage, in that it is possible to install and remove devices without rebooting the computer (hot-swapping), making it useful for mobile peripherals, including drives of various kinds (given SATA or SCSI devices may or may not support hot-swapping).
2681) 11.734999, USB - Wikipedia, the free encyclopedia.txt#91, term: computer, content:USB Battery Charging defines a new port type, the charging port, as opposed to the standard downstream port (SDP) of the base specification. Charging ports are divided into 2 further types: the charging downstream port (CDP), which has data signals, and the dedicated charging port (DCP), which does not. Dedicated charging ports can be found on USB power adapters that convert utility power or another power source (e.g., a car's electrical system) to run attached devices and battery packs. On a host (such as a laptop computer) with both standard and charging USB ports, the charging ports should be labeled as such.[95]
2682) 11.734999, User interface - Wikipedia, the free encyclopedia.txt#0, term: computer, content:The user interface (UI), in the industrial design field of humanmachine interaction, is the space where interactions between humans and machines occur. The goal of this interaction is to allow effective operation and control of the machine from the human end, whilst the machine simultaneously feeds back information that aids the operators' decision-making process. Examples of this broad concept of user interfaces include the interactive aspects of computer operating systems, hand tools, heavy machinery operator controls, and process controls. The design considerations applicable when creating user interfaces are related to or involve such disciplines as ergonomics and psychology.
2683) 11.734999, Vacuum tube - Wikipedia, the free encyclopedia.txt#62, term: computer, content:A system widely used in Europe known as the Mullard-Philips tube designation, also extended to transistors, uses a letter, followed by one or more further letters, and a number. The type designator specifies the heater voltage or current (one letter), the functions of all sections of the tube (one letter per section), the socket type (first digit), and the particular tube (remaining digits). For example, the ECC83 (equivalent to the 12AX7) is a 6.3V (E) double triode (CC) with a miniature base (8). In this system special-quality tubes (e.g., for long-life computer use) are indicated by moving the number immediately after the first letter: the E83CC is a special-quality equivalent of the ECC83, the E55L a power pentode with no consumer equivalent.
2684) 11.734999, Vannevar Bush - Wikipedia, the free encyclopedia.txt#1, term: computer, content:For his master's thesis, Bush invented and patented a "profile tracer", a mapping device for assisting surveyors. It was the first of a string of inventions. He joined the Department of Electrical Engineering at Massachusetts Institute of Technology (MIT) in 1919, and founded the company now known as Raytheon in 1922. Starting in 1927, Bush constructed a differential analyzer, an analog computer with some digital components that could solve differential equations with as many as 18 independent variables. An offshoot of the work at MIT by Bush and others was the beginning of digital circuit design theory. Bush became vice president of MIT and dean of the MIT School of Engineering in 1932, and president of the Carnegie Institution of Washington in 1938.
2685) 11.734999, VAX - Wikipedia, the free encyclopedia.txt#3, term: computer, content:The name "VAX" originated as an acronym for virtual address extension, both because the VAX was seen as a 32-bit extension of the older 16-bit PDP-11 and because it was (after Prime Computer) an early adopter of virtual memory to manage this larger address space. Early versions of the VAX processor implemented a "compatibility mode" that emulated many of the PDP-11's instructions, and were in fact called VAX-11 to highlight this compatibility and the fact that VAX-11 was an outgrowth of the PDP-11 family. Later versions offloaded the compatibility mode and some of the less used CISC instructions to emulation in the operating system software.
2686) 11.734999, Version control - Wikipedia, the free encyclopedia.txt#0, term: computer, content:A component of software configuration management, version control, also known as revision control or source control,[1] is the management of changes to documents, computer programs, large web sites, and other collections of information. Changes are usually identified by a number or letter code, termed the "revision number", "revision level", or simply "revision". For example, an initial set of files is "revision 1". When the first change is made, the resulting set is "revision 2", and so on. Each revision is associated with a timestamp and the person making the change. Revisions can be compared, restored, and with some types of files, merged.
2687) 11.734999, Video card - Wikipedia, the free encyclopedia.txt#11, term: computer, content:As of 2016, the primary suppliers of the GPUs (video chips or chipsets) used in video cards are AMD and Nvidia. In the third quarter of 2013, AMD had a 35.5% market share while Nvidia had a 64.5% market share,[16] according to Jon Peddie Research. In economics, this industry structure is termed a duopoly. AMD and Nvidia also build and sell video cards, which are termed graphics add-in-board (AIBs) in the industry. (See Comparison of Nvidia graphics processing units and Comparison of AMD graphics processing units.) In addition to marketing their own video cards, AMD and Nvidia sell their GPUs to authorized AIB suppliers, which AMD and Nvidia refer to as "partners".[2] The fact that Nvidia and AMD compete directly with their customer/partners complicates relationships in the industry. The fact that AMD and Intel are direct competitors in the CPU industry is also noteworthy, since AMD-based video cards may be used in computers with Intel CPUs. Intel's move to APUs may weaken AMD, which until now has derived a significant portion of its revenue from graphics components. As of the second quarter of 2013, there were 52 AIB suppliers.[2] These AIB suppliers may market video cards under their own brands, and/or produce video cards for private label brands and/or produce video cards for computer manufacturers. Some AIB suppliers such as MSI build both AMD-based and Nvidia-based video cards. Others, such as EVGA, build only Nvidia-based video cards, while XFX, now builds only AMD-based video cards. Several AIB suppliers are also motherboard suppliers. The largest AIB suppliers, based on global retail market share for graphics cards, include Taiwan-based Palit Microsystems, Hong Kong-based PC Partner (which markets AMD-based video cards under its Sapphire brand and Nvidia-based video cards under its Zotac brand), Taiwan-based computer-maker Asustek Computer (Asus), Taiwan-based Micro-Star International (MSI), Taiwan-based Gigabyte Technology,[17] Brea, California, USA-based EVGA (which also sells computer components such as power supplies) and Ontario, California USA-based XFX. (The parent corporation of XFX is based in Hong Kong.)
2688) 11.734999, Video card - Wikipedia, the free encyclopedia.txt#23, term: computer, content:Digital-based standard designed for displays such as flat-panel displays (LCDs, plasma screens, wide high-definition television displays) and video projectors. In some rare cases high end CRT monitors also use DVI. It avoids image distortion and electrical noise, corresponding each pixel from the computer to a display pixel, using its native resolution. It is worth to note that most manufacturers include DVI-I connector, allowing (via simple adapter) standard RGB signal output to an old CRT or LCD monitor with VGA input.
2689) 11.734999, Video card - Wikipedia, the free encyclopedia.txt#26, term: computer, content:DisplayPort is a digital display interface developed by the Video Electronics Standards Association (VESA). The interface is primarily used to connect a video source to a display device such as a computer monitor, though it can also be used to transmit audio, USB, and other forms of data.[27] The VESA specification is royalty-free. VESA designed it to replace VGA, DVI, and LVDS. Backward compatibility to VGA and DVI by using adapter dongles enables consumers to use DisplayPort fitted video sources without replacing existing display devices. Although DisplayPort has a greater throughput of the same functionality as HDMI, it is expected to complement the interface, not replace it.[28][29]
2690) 11.734999, Video game - Wikipedia, the free encyclopedia.txt#23, term: computer, content:Cheating in computer games may involve cheat codes and hidden spots implemented by the game developers,[43][44] modification of game code by third parties,[45][46] or players exploiting a software glitch. Modifications are facilitated by either cheat cartridge hardware or a software trainer.[45] Cheats usually make the game easier by providing an unlimited amount of some resource; for example weapons, health, or ammunition; or perhaps the ability to walk through walls.[44][45] Other cheats might give access to otherwise unplayable levels or provide unusual or amusing features, like altered game colors or other graphical appearances.
2691) 11.734999, Video game - Wikipedia, the free encyclopedia.txt#24, term: computer, content:Software errors not detected by software testers during development can find their way into released versions of computer and video games. This may happen because the glitch only occurs under unusual circumstances in the game, was deemed too minor to correct, or because the game development was hurried to meet a publication deadline. Glitches can range from minor graphical errors to serious bugs that can delete saved data or cause the game to malfunction. In some cases publishers will release updates (referred to as patches) to repair glitches. Sometimes a glitch may be beneficial to the player; these are often referred to as exploits.
2692) 11.734999, Video game console - Wikipedia, the free encyclopedia.txt#12, term: computer, content:The VES continued to be sold at a profit after 1977, and both Bally (with their Home Library Computer in 1977) and Magnavox (with the Odyssey in 1978) brought their own programmable cartridge-based consoles to the market. However, it was not until Atari released a conversion of the golden age arcade hit Space Invaders in 1980 for the Atari 2600 that the home console industry took off. Many consumers bought an Atari console so they could play Space Invaders at home. The unprecedented success of Space Invaders started the trend of console manufacturers trying to get exclusive rights to arcade titles, and the trend of advertisements for game consoles claiming to bring the arcade experience home.
2693) 11.734999, Video game console - Wikipedia, the free encyclopedia.txt#15, term: computer, content:In 1983, Nintendo released the Family Computer (or Famicom) in Japan. The Famicom supported high-resolution sprites, larger color palettes, and tiled backgrounds. This allowed Famicom games to be longer and have more detailed graphics. Nintendo began attempts to bring their Famicom to the U.S. after the video game market had crashed. In the U.S., video games were seen as a fad that had already passed. To distinguish its product from older game consoles, Nintendo released their Famicom as the Nintendo Entertainment System (NES) which used a front-loading cartridge port similar to a VCR, included a plastic "robot" (R.O.B.), and was initially advertised as a toy.
2694) 11.734999, Video game console - Wikipedia, the free encyclopedia.txt#24, term: computer, content:The fourth generation graphics chips allowed these consoles to reproduce the art styles that were becoming popular in arcades and on home computers. These games often featured lavish background scenery, huge characters, broader color pallettes, and increased emphasis on dithering and texture. Games written specifically for the NES, like Megaman, Shatterhand, and Super Mario Bros 3 were able to work cleverly within its limitations. Ports of the increasingly detailed arcade and home computer games came up with various solutions. For example, when Capcom released Strider in the arcade they created an entirely separate Strider game for the NES that only incorporated themes and characters from the arcade.
2695) 11.734999, Video game console - Wikipedia, the free encyclopedia.txt#47, term: computer, content:With more and more PDAs arriving during the previous generation, the difference between consumer electronics and traditional computing began to blur and cheap console technology grew as a result. It was said of PDAs that they were "the computers of handheld gaming" because of their multi-purpose capabilities and the increasingly powerful computer hardware that resided within them. This capability existed to move gaming beyond the last generation's 16-bit limitations; however, PDAs were still geared towards the typical businessman, and lacked new, affordable software franchises to compete with dedicated handheld gaming consoles.
2696) 11.734999, Video game industry - Wikipedia, the free encyclopedia.txt#5, term: computer, content:The game industry employs those experienced in other traditional businesses, but some have experience tailored to the game industry. Some of the disciplines specific to the game industry include: game programmer, game designer, level designer, game producer, game artist and game tester. Most of these professionals are employed by video game developers or video game publishers. However, many hobbyists also produce computer games and sell them commercially.[citation needed] Recently,[when?] game developers have begun to employ those with extensive or long-term experience within the modding communities.
2697) 11.734999, Video game industry - Wikipedia, the free encyclopedia.txt#48, term: computer, content:Valve Corporation operates Steam, the largest computer gaming platform, with an active user base (~125 million) that rivals the combined user bases of the current console generation (~150 million).[74] While not specifically focused on games, the largest mobile gaming platforms are operated by Google (Google Play), and Apple Inc. (App Store).[75] Microsoft operates Xbox, the only major game console hardware franchise not controlled by a Japanese company. Sony established Sony Interactive Entertainment LLC in Silicon Valley to run PlayStation, the world's largest and longest-running video game console franchise.[76]
2698) 11.734999, Von Neumann architecture - Wikipedia, the free encyclopedia.txt#2, term: computer, content:A stored-program digital computer is one that keeps its program instructions, as well as its data, in read-write, random-access memory (RAM). Stored-program computers were an advancement over the program-controlled computers of the 1940s, such as the Colossus and the ENIAC, which were programmed by setting switches and inserting patch leads to route data and to control signals between various functional units. In the vast majority of modern computers, the same memory is used for both data and program instructions, and the von Neumann vs. Harvard distinction applies to the cache architecture, not the main memory (Modified Harvard architecture#split cache architecture).
2699) 11.734999, Von Neumann architecture - Wikipedia, the free encyclopedia.txt#18, term: computer, content:In 1945, Professor J. von Neumann, who was then working at the Moore School of Engineering in Philadelphia, where the E.N.I.A.C. had been built, issued on behalf of a group of his co-workers a report on the logical design of digital computers. The report contained a fairly detailed proposal for the design of the machine which has since become known as the E.D.V.A.C. (electronic discrete variable automatic computer). This machine has only recently been completed in America, but the von Neumann report inspired the construction of the E.D.S.A.C. (electronic delay-storage automatic calculator) in Cambridge (see page 130).
2700) 11.734999, Wearable computer - Wikipedia, the free encyclopedia.txt#1, term: computer, content:Different stakeholders have defined wearable computing differently. For example, consumers often refer to wearable computers as computers that can be easily carried on the body or systems with a heads-up display or speech activated. This contrasts with academics that define wearables as a system that can perform a set of functions without being constrained by the physical hardware of the system. Finally, people who are trying to sell these products and the idea of these products will use the broadest definition; any computing device worn on the body. Because of the broad context in which a wearable computer can be defined, this article page will use the broadest definition, any computing device worn on the body.[2][3]
2701) 11.734999, Wearable computer - Wikipedia, the free encyclopedia.txt#41, term: computer, content:Google Glass launched their optical head-mounted display (OHMD) to a test group of users in 2013, before it became available to the public on May 15, 2014.[37] Google's mission was to produce a mass-market ubiquitous computer that displays information in a smartphone-like hands-free format[38] that can interact with the Internet via natural language voice commands.[39][40] Google Glass received criticism over privacy and safety concerns. On January 15, 2015, Google announced that it would stop producing the Google Glass prototype but would continue to develop the product. According to Google, Project Glass was ready to "graduate" from Google X, the experimental phase of the project.[41]
2702) 11.734999, Web browser - Wikipedia, the free encyclopedia.txt#14, term: computer, content:Internet Explorer, on the other hand, was bundled free with the Windows operating system (and was also downloadable free), and therefore it was funded partly by the sales of Windows to computer manufacturers and direct to users. Internet Explorer also used to be available for the Mac. It is likely that releasing IE for the Mac was part of Microsoft's overall strategy to fight threats to its quasi-monopoly platform dominance - threats such as web standards and Java - by making some web developers, or at least their managers, assume that there was "no need" to develop for anything other than Internet Explorer. In this respect, IE may have contributed to Windows and Microsoft applications sales in another way, through "lock-in" to Microsoft's browser.
2703) 11.734999, Williams tube - Wikipedia, the free encyclopedia.txt#4, term: computer, content:Reading the memory took place via a secondary effect caused by the writing operation. During the short period when the write takes place, the redistribution of charges in the phosphor creates an electrical current that induces voltage in any nearby conductors. This is read by placing a thin metal sheet just in front of the display side of the CRT. During a read operation, the beam writes to the selected bit locations on the display. Those locations that were previously written to are already depleted of electrons, so no current flows, and no voltage appears on the plate. This allows the computer to determine there was a 1 in that location. If the location had not been written to previously, the write process will create a well and a pulse will be read on the plate, indicating a 0.
2704) 11.734999, Windows 10 - Wikipedia, the free encyclopedia.txt#27, term: computer, content:Microsoft Family Safety is replaced by Microsoft Family, a parental controls system that applies across Windows platforms and Microsoft online services. Users can create a designated family, and monitor and restrict the actions of users designated as children, such as access to websites, enforcing age ratings on Windows Store purchases, and other restrictions. The service can also send weekly e-mail reports to parents detailing a child's computer usage. Unlike previous versions of Windows, Child accounts in a family must be associated with a Microsoft accountwhich allows these settings to apply across all Windows 10 devices that a particular child is using.[79][80]
2705) 11.734999, Windows 10 - Wikipedia, the free encyclopedia.txt#45, term: computer, content:Windows 10 is available in four main editions for personal computer devices, of which the Home and Pro versions are sold at retail in most countries, and as pre-loaded software on new computers. Home is aimed at home users, while Pro is aimed at small businesses and enthusiasts. Each edition of Windows10 includes all of the capabilities and features of the edition below it, and add additional features oriented towards their market segments; for example, Pro adds additional networking and security features such as BitLocker, Device Guard, Windows Update for Business, and the ability to join a domain. The remaining editions, Enterprise and Education, contain additional features aimed towards business environments, and are only available through volume licensing.[122][123]
2706) 11.734999, Windows 2000 - Wikipedia, the free encyclopedia.txt#19, term: computer, content:Windows Explorer has been enhanced in several ways in Windows 2000. It is the first Windows NT release to include Active Desktop, first introduced as a part of Internet Explorer 4.0 (specifically Windows Desktop Update), and only pre-installed in Windows 98 by that time.[64] It allowed users to customize the way folders look and behave by using HTML templates, having the file extension HTT. This feature was abused by computer viruses that employed malicious scripts, Java applets, or ActiveX controls in folder template files as their infection vector. Two such viruses are VBS/Roor-C[65] and VBS.Redlof.a.[66]
2707) 11.734999, Windows 2000 - Wikipedia, the free encyclopedia.txt#46, term: computer, content:There can be two ways of implementing a DFS namespace on Windows 2000: either through a standalone DFS root or a domain-based DFS root. Standalone DFS allows for only DFS roots on the local computer, and thus does not use Active Directory. Domain-based DFS roots exist within Active Directory and can have their information distributed to other domain controllers within the domain  this provides fault tolerance to DFS. DFS roots that exist on a domain must be hosted on a domain controller or on a domain member server. The file and root information is replicated via the Microsoft File Replication Service (FRS).[99]
2708) 11.734999, Windows 2000 - Wikipedia, the free encyclopedia.txt#62, term: computer, content:Microsoft had originally intended to release a fifth service pack for Windows 2000, but Microsoft cancelled this project early in its development, and instead released Update Rollup 1 for SP4, a collection of all the security-related hotfixes and some other significant issues.[109] The Update Rollup does not include all non-security related hotfixes and is not subjected to the same extensive regression testing as a full service pack. Microsoft states that this update will meet customers' needs better than a whole new service pack, and will still help Windows 2000 customers secure their PCs, reduce support costs, and support existing computer hardware.[110]
2709) 11.734999, Windows 8 - Wikipedia, the free encyclopedia.txt#34, term: computer, content:Backup and Restore, the backup component of Windows, is deprecated. It still ships with Windows 8 and continues to work on preset schedules, but is pushed to the background and can only be accessed through a Control Panel applet called "Windows 7 File Recovery".[114]:76 Shadow Copy, a component of Windows Explorer that once saved previous versions of changed files, no longer protects local files and folders. It can only access previous versions of shared files stored on a Windows Server computer.[114]:74 The subsystem on which these components worked, however, is still available for other software to use.[114]:74
2710) 11.734999, Windows 95 - Wikipedia, the free encyclopedia.txt#29, term: computer, content:Windows 95 OEM Service Release 1 was the first release of Windows to include Internet Explorer (version 2.0) with the OS. While there was no uninstaller, it could be deleted easily if the user so desired. OEM Service Release 2 included Internet Explorer 3. The installation of Internet Explorer 4 on Windows 95 (or the OSR2.5 version preinstalled on a computer) gave Windows 95 active desktop and browser integration into Windows Explorer, known as the Windows Desktop Update. The CD version of the last release of Windows 95, OEM Service Release 2.5 (Version 4.00.950C), includes Internet Explorer 4, and installs it after Windows 95's initial setup and first boot is complete.
2711) 11.734999, Word (computer architecture) - Wikipedia, the free encyclopedia.txt#15, term: computer, content:As computer designs have grown more complex, the central importance of a single word size to an architecture has decreased. Although more capable hardware can use a wider variety of sizes of data, market forces exert pressure to maintain backward compatibility while extending processor capability. As a result, what might have been the central word size in a fresh design has to coexist as an alternative size to the original word size in a backward compatible design. The original word size remains available in future designs, forming the basis of a size family.
2712) 11.734999, Word processor - Wikipedia, the free encyclopedia.txt#1, term: computer, content:The word processor was a stand-alone office machine in the 1960s, combining the keyboard text-entry and printing functions of an electric typewriter, with a recording unit, either tape or floppy disk (as used by the Wang machine) with a simple dedicated computer processor for the editing of text.[1] Although features and designs varied among manufacturers and models, and new features were added as technology advanced, word processors typically featured a monochrome display and the ability to save documents on memory cards or diskettes. Later models introduced innovations such as spell-checking programs, and improved formatting options.
2713) 11.734999, Word processor - Wikipedia, the free encyclopedia.txt#18, term: computer, content:In 1969, two software based text editing products (Astrotype and Astrocomp) were developed and marketed by Information Control Systems (Ann Arbor Michigan).[9][10][11] Both products used the Digital Equipment Corporation PDP-8 mini computer, DECtape (4 reel) randomly accessible tape drives, and a modified version of the IBM Selectric typewriter (the IBM 2741 Terminal). These 1969 products preceded CRT display-based word processors. Text editing was done using a line numbering system viewed on a paper copy inserted in the Selectric typewriter.
2714) 11.734999, World Wide Web Consortium - Wikipedia, the free encyclopedia.txt#5, term: computer, content:It was originally intended that CERN host the European branch of W3C; however, CERN wished to focus on particle physics, not information technology. In April 1995, the French Institute for Research in Computer Science and Automation (INRIA) became the European host of W3C, with Keio University becoming the Japanese branch in September 1996. Starting in 1997, W3C created regional offices around the world. As of September 2009, it had eighteen World Offices covering Australia, the Benelux countries (Netherlands, Luxembourg, and Belgium), Brazil, China, Finland, Germany, Austria, Greece, Hong Kong, Hungary, India, Israel, Italy, South Korea, Morocco, South Africa, Spain, Sweden, and the United Kingdom and Ireland.[4]
2715) 11.734999, Zilog Z80 - Wikipedia, the free encyclopedia.txt#36, term: computer, content:Memory, especially EPROM, but also Flash, were generally slow as compared to the state machine sub-cycles (clock cycles) used in contemporary microprocessors. The shortest machine cycle that could safely be used in embedded designs has therefore often been limited by memory access times, not by the maximum CPU frequency (especially so during the home computer era). However, this relation has slowly changed during the last decades, particularly regarding SRAM; cacheless, single-cycle designs such as the eZ80 have therefore become much more meaningful recently.
2716) 11.614636, Computer programming - Wikipedia, the free encyclopedia.txt#10, term: computer, content:The synthesis of numerical calculation, predetermined operation and output, along with a way to organize and input instructions in a manner relatively easy for humans to conceive and produce, led to the modern development of computer programming. In 1954, FORTRAN was invented; it was the first widely used high level programming language to have a functional implementation, as opposed to just a design on paper.[12][13] (A high-level language is, in very general terms, any programming language that allows the programmer to write programs in terms that are more abstract than assembly language instructions, i.e. at a level of abstraction "higher" than that of an assembly language.) It allowed programmers to specify calculations by entering a formula directly (e.g. Y = X*2 + 5*X + 9). The program text, or source, is converted into machine instructions using a special program called a compiler, which translates the FORTRAN program into machine language. In fact, the name FORTRAN stands for "Formula Translation". Many other languages were developed, including some for commercial programming, such as COBOL. Programs were mostly still entered using punched cards or paper tape. (See computer programming in the punch card era). By the late 1960s, data storage devices and computer terminals became inexpensive enough that programs could be created by typing directly into the computers. Text editors were developed that allowed changes and corrections to be made much more easily than with punched cards. (Usually, an error in punching a card meant that the card had to be discarded and a new one punched to replace it.)
2717) 11.614636, Computer science - Wikipedia, the free encyclopedia.txt#3, term: computer, content:Blaise Pascal designed and constructed the first working mechanical calculator, Pascal's calculator, in 1642.[2] In 1673, Gottfried Leibniz demonstrated a digital mechanical calculator, called the Stepped Reckoner.[3] He may be considered the first computer scientist and information theorist, for, among other reasons, documenting the binary number system. In 1820, Thomas de Colmar launched the mechanical calculator industry[note 1] when he released his simplified arithmometer, which was the first calculating machine strong enough and reliable enough to be used daily in an office environment. Charles Babbage started the design of the first automatic mechanical calculator, his Difference Engine, in 1822, which eventually gave him the idea of the first programmable mechanical calculator, his Analytical Engine.[4] He started developing this machine in 1834 and "in less than two years he had sketched out many of the salient features of the modern computer".[5] "A crucial step was the adoption of a punched card system derived from the Jacquard loom"[5] making it infinitely programmable.[note 2] In 1843, during the translation of a French article on the Analytical Engine, Ada Lovelace wrote, in one of the many notes she included, an algorithm to compute the Bernoulli numbers, which is considered to be the first computer program.[6] Around 1885, Herman Hollerith invented the tabulator, which used punched cards to process statistical information; eventually his company became part of IBM. In 1937, one hundred years after Babbage's impossible dream, Howard Aiken convinced IBM, which was making all kinds of punched card equipment and was also in the calculator business[7] to develop his giant programmable calculator, the ASCC/Harvard Mark I, based on Babbage's Analytical Engine, which itself used cards and a central computing unit. When the machine was finished, some hailed it as "Babbage's dream come true".[8]
2718) 11.614636, Intel - Wikipedia, the free encyclopedia.txt#107, term: computer, content:In the late 1980s, Intel's market share was being seriously eroded by upstart competitors such as American Micro Devices (now AMD), Zilog, and others who had started to sell their less expensive microprocessors to computer manufacturers. This was because, by using cheaper processors, manufacturers could make cheaper computers and gain more market share in an increasingly price-sensitive market. In 1989, Intel's Dennis Carter visited MicroAge's headquarters in Tempe, Arizona, to meet with MicroAge's VP of Marketing, Ron Mion. MicroAge had become one of the largest distributors of Compaq, IBM, HP, and others and thus was a primary  although indirect  driver of demand for microprocessors. Intel wanted MicroAge to petition its computer suppliers to favor Intel chips. However, Mion felt that the marketplace should decide which processors they wanted. Intel's counterargument was that it would be too difficult to educate PC buyers on why Intel microprocessors were worth paying more for ... and they were right.[218] But Mion felt that the public didn't really need to fully understand why Intel chips were better, they just needed to feel they were better. So Mion proposed a market test. Intel would pay for a MicroAge billboard somewhere saying, "If you're buying a personal computer, make sure it has Intel inside." In turn, MicroAge would put "Intel Inside" stickers on the Intel-based computers in their stores in that area. To make the test easier to monitor, Mion decided to do the test in Boulder, Colorado, where it had a single store. Virtually overnight, the sales of personal computers in that store dramatically shifted to Intel-based PCs. Intel very quickly adopted "Intel Inside" as its primary branding and rolled it out worldwide.[218]
2719) 11.614636, Intel 8080 - Wikipedia, the free encyclopedia.txt#21, term: computer, content:The 8080 was used in many early microcomputers, such as the MITS Altair 8800 Computer, Processor Technology SOL-20 Terminal Computer and IMSAI 8080 Microcomputer, forming the basis for machines running the CP/M operating system (the later, almost fully compatible and more capable, Zilog Z80 processor would capitalize on this, with Z80 & CP/M becoming the dominant CPU & OS combination of the period circa 1976 to 1983 much as did the x86 & MS-DOS for the PC a decade later). Even in 1979 after introduction of the Z80 and 8085 processors, five manufacturers of the 8080 were selling an estimated 500,000 units per month at a price around $3 to $4 per unit.[9] The first single-board microcomputers, such as MYCRO-1 and the dyna-micro were based on the Intel 8080. One of the early uses of the 8080 was made in the late 1970s by Cubic-Western Data of San Diego, CA in its Automated Fare Collection Systems custom designed for mass transit systems around the world. An early industrial use of the 8080 was as the "brain" of the DatagraphiX Auto-COM (Computer Output Microfiche) line of products which took large amounts of user data from reel-to-reel tape and imaged it onto microfiche. The Auto-COM instruments also included an entire automated film cutting, processing, washing, and drying sub-system  quite a feat, both then and in the 21st century, to all be accomplished successfully with only an 8-bit microprocessor running at a clock speed of less than 1MHz with a 64KB memory limit. In addition, several early arcade video games were built around the 8080 microprocessor, including Space Invaders, one of the most popular arcade games ever made.
2720) 11.614636, VAX - Wikipedia, the free encyclopedia.txt#8, term: computer, content:For a while the VAX-11/780 was used as a standard in CPU benchmarks. It was initially described as a one-MIPS machine, because its performance was equivalent to an IBM System/360 that ran at one MIPS, and the System/360 implementations had previously been de facto performance standards. The actual number of instructions executed in 1 second was about 500,000, which led to complaints of marketing exaggeration. The result was the definition of a "VAX MIPS," the speed of a VAX-11/780; a computer performing at 27 VAX MIPS would run the same program roughly 27 times faster than the VAX-11/780. Within the Digital community the term VUP (VAX Unit of Performance) was the more common term, because MIPS do not compare well across different architectures. The related term cluster VUPs was informally used to describe the aggregate performance of a VAXcluster. (The performance of the VAX-11/780 still serves as the baseline metric in the BRL-CAD Benchmark, a performance analysis suite included in the BRL-CAD solid modeling software distribution.) The VAX-11/780 included a subordinate stand-alone LSI-11 computer that performed microcode load, booting, and diagnostic functions for the parent computer. This was dropped from subsequent VAX models. Enterprising VAX-11/780 users could therefore run three different Digital Equipment Corporation operating systems: VMS on the VAX processor, and either RSX-11M or RT-11 on the LSI-11.
2721) 10.058571, 16-bit - Wikipedia, the free encyclopedia.txt#2, term: computer, content:The MIT Whirlwind (c. 1951)[1][2] was quite possibly the first-ever 16-bit computer. Other early (c. 196570) 16-bit computers include the IBM 1130,[3] the HP 2100,[4] the Data General Nova,[5] and the DEC PDP-11.[6] Early (c. 197375) multi-chip 16-bit microprocessors include the National Semiconductor IMP-16 and the Western Digital MCP-1600. Early (c. 197576) single-chip 16-bit microprocessors include the National Semiconductor PACE, the HP BPC, and the TI TMS9900. Other notable 16-bit processors include the Intel 8086, the Intel 80286, the WDC 65C816, and the Zilog Z8000. The Intel 8088 was binary compatible with the Intel 8086, and was 16-bit in that its registers were 16 bits wide, and arithmetic instructions could operate on 16-bit quantities, even though its external bus was 8 bits wide.
2722) 10.058571, 86-DOS - Wikipedia, the free encyclopedia.txt#5, term: computer, content:In October 1980, IBM was developing what would become the original IBM Personal Computer. CP/M was by far the most popular operating system in use at the time, and IBM felt it needed CP/M in order to compete. IBM's representatives visited Digital Research and discussed licensing with Digital Research's licensing representative, Dorothy Kildall (ne McEwen), who hesitated to sign IBM's non-disclosure agreement. Although the NDA was later accepted, Digital Research would not accept IBM's proposal of $250,000 in exchange for as many copies as IBM could sell, insisting on the usual royalty-based plan.[6] In later discussions between IBM and Bill Gates, Gates mentioned the existence of 86-DOS and IBM representative Jack Sams told him to get a license for it.[citation needed]
2723) 10.058571, Abacus - Wikipedia, the free encyclopedia.txt#24, term: computer, content:Altogether, there were 13 rows with 7 beads in each one, which made up 91 beads in each Nephualtzintzin. This was a basic number to understand, 7 times 13, a close relation conceived between natural phenomena, the underworld and the cycles of the heavens. One Nephualtzintzin (91) represented the number of days that a season of the year lasts, two Nephualtzitzin (182) is the number of days of the corn's cycle, from its sowing to its harvest, three Nephualtzintzin (273) is the number of days of a baby's gestation, and four Nephualtzintzin (364) completed a cycle and approximate a year (11/4 days short). When translated into modern computer arithmetic, the Nephualtzintzin amounted to the rank from 10 to the 18 in floating point, which calculated stellar as well as infinitesimal amounts with absolute precision, meant that no round off was allowed.
2724) 10.058571, Algorithm - Wikipedia, the free encyclopedia.txt#57, term: computer, content:The analysis and study of algorithms is a discipline of computer science, and is often practiced abstractly without the use of a specific programming language or implementation. In this sense, algorithm analysis resembles other mathematical disciplines in that it focuses on the underlying properties of the algorithm and not on the specifics of any particular implementation. Usually pseudocode is used for analysis as it is the simplest and most general representation. However, ultimately, most algorithms are usually implemented on particular hardware / software platforms and their algorithmic efficiency is eventually put to the test using real code. For the solution of a "one off" problem, the efficiency of a particular algorithm may not have significant consequences (unless n is extremely large) but for algorithms designed for fast interactive, commercial or long life scientific usage it may be critical. Scaling from small n to large n frequently exposes inefficient algorithms that are otherwise benign.
2725) 10.058571, Analog computer - Wikipedia, the free encyclopedia.txt#77, term: computer, content:In 1950s to 1970s, digital computers based on first vacuum tubes, transistors, integrated circuits and then micro-processors became more economical and precise. This led digital computers to largely replace analog computers. Even so, some research in analog computation is still being done. A few universities still use analog computers to teach control system theory. The American company Comdyna manufactures small analog computers.[20] At Indiana University Bloomington, Jonathan Mills has developed the Extended Analog Computer based on sampling voltages in a foam sheet. At the Harvard Robotics Laboratory, analog computation is a research topic. [ Lyric Semiconductor]'s error correction circuits use analog probabilistic signals. Slide rules are still popular among aircraft personnel.[citation needed]
2726) 10.058571, Analytical Engine - Wikipedia, the free encyclopedia.txt#5, term: computer, content:There was to be a store (that is, a memory) capable of holding 1,000 numbers of 40 decimal digits[14] each (ca. 16.2 kB). An arithmetical unit (the "mill") would be able to perform all four arithmetic operations, plus comparisons and optionally square roots.[15] Initially (1838) it was conceived as a difference engine curved back upon itself, in a generally circular layout, with the long store exiting off to one side.[16] Later drawings (1858) depict a regularized grid layout.[17] Like the central processing unit (CPU) in a modern computer, the mill would rely upon its own internal procedures, to be stored in the form of pegs inserted into rotating drums called "barrels", to carry out some of the more complex instructions the user's program might specify.[7]
2727) 10.058571, Application software - Wikipedia, the free encyclopedia.txt#9, term: computer, content:Since the development and near-universal adoption of the web, an important distinction that has emerged has been between web applications  written with HTML, JavaScript and other web-native technologies and typically requiring one to be online and running a web browser, and the more traditional native applications written in whatever languages are available for one's particular type of computer. There has been contentious debate in the computing community regarding web applications replacing native applications for many purposes, especially on mobile devices such as smart phones and tablets. Web apps have indeed greatly increased in popularity for some uses, but the advantages of applications make them unlikely to disappear soon, if ever. Furthermore, the two can be complementary, and even integrated.[7][8][9]
2728) 10.058571, Arcade game - Wikipedia, the free encyclopedia.txt#26, term: computer, content:Virtually all modern arcade games (other than the very traditional midway-type games at county fairs) make extensive use of solid state electronics and integrated circuits. In the past, coin-operated arcade video games generally used custom per-game hardware often with multiple CPUs, highly specialized sound and graphics chips, and the latest in expensive computer graphics display technology. This allowed arcade system boards to produce more complex graphics and sound than what was then possible on video game consoles or personal computers, which is no longer the case today. Recent arcade game hardware is often based on modified video game console hardware or high-end PC components.
2729) 10.058571, ARPANET - Wikipedia, the free encyclopedia.txt#32, term: computer, content:The starting point for host-to-host communication on the ARPANET in 1969 was the 1822 protocol, which defined the transmission of messages to an IMP.[48] The message format was designed to work unambiguously with a broad range of computer architectures. An 1822 message essentially consisted of a message type, a numeric host address, and a data field. To send a data message to another host, the transmitting host formatted a data message containing the destination host's address and the data message being sent, and then transmitted the message through the 1822 hardware interface. The IMP then delivered the message to its destination address, either by delivering it to a locally connected host, or by delivering it to another IMP. When the message was ultimately delivered to the destination host, the receiving IMP would transmit a Ready for Next Message (RFNM) acknowledgement to the sending, host IMP.
2730) 10.058571, Artificial intelligence - Wikipedia, the free encyclopedia.txt#0, term: computer, content:Artificial intelligence (AI) is the intelligence exhibited by machines. In computer science, an ideal "intelligent" machine is a flexible rational agent that perceives its environment and takes actions that maximize its chance of success at an arbitrary goal.[1] Colloquially, the term "artificial intelligence" is likely to be applied when a machine uses cutting-edge techniques to competently perform or mimic "cognitive" functions that we intuitively associate with human minds, such as "learning" and "problem solving".[2] The colloquial connotation, especially among the public, associates artificial intelligence with machines that are "cutting-edge" (or even "mysterious"). This subjective borderline around what constitutes "artificial intelligence" tends to shrink over time; for example, optical character recognition is no longer perceived as an exemplar of "artificial intelligence" as it is nowadays a mundane routine technology.[3] Modern examples of AI include computers that can beat professional players at Chess and Go, and self-driving cars that navigate crowded city streets.
2731) 10.058571, Artificial intelligence - Wikipedia, the free encyclopedia.txt#9, term: computer, content:In the late 1990s and early 21st century, AI achieved its greatest successes, and began to be used for logistics, data mining, medical diagnosis and many other areas throughout the technology industry.[11] The success was due to several factors: the increasing computational power of computers (see Moore's law), a greater emphasis on solving specific subproblems, the creation of new ties between AI and other fields working on similar problems, and a new commitment by researchers to solid mathematical methods and rigorous scientific standards.[33] Faster computers were the principal reason that Deep Blue became the first computer chess-playing system to beat a reigning world chess champion, Garry Kasparov on 11 May 1997.[34]
2732) 10.058571, Artificial intelligence - Wikipedia, the free encyclopedia.txt#21, term: computer, content:Unsupervised learning is the ability to find patterns in a stream of input. Supervised learning includes both classification and numerical regression. Classification is used to determine what category something belongs in, after seeing a number of examples of things from several categories. Regression is the attempt to produce a function that describes the relationship between inputs and outputs and predicts how the outputs should change as the inputs change. In reinforcement learning[66] the agent is rewarded for good responses and punished for bad ones. The agent uses this sequence of rewards and punishments to form a strategy for operating in its problem space. These three types of learning can be analyzed in terms of decision theory, using concepts like utility. The mathematical analysis of machine learning algorithms and their performance is a branch of theoretical computer science known as computational learning theory.[67]
2733) 10.058571, Asynchronous Transfer Mode - Wikipedia, the free encyclopedia.txt#0, term: computer, content:Asynchronous Transfer Mode (ATM) is, according to the ATM Forum, "a telecommunications concept defined by ANSI and ITU (formerly CCITT) standards for carriage of a complete range of user traffic, including voice, data, and video signals".[1] ATM was developed to meet the needs of the Broadband Integrated Services Digital Network, as defined in the late 1980s,[2] and designed to unify telecommunication and computer networks. It was designed for a network that must handle both traditional high-throughput data traffic (e.g., file transfers), and real-time, low-latency content such as voice and video. The reference model for ATM approximately maps to the three lowest layers of the ISO-OSI reference model: network layer, data link layer, and physical layer.[3] ATM is a core protocol used over the SONET/SDH backbone of the public switched telephone network (PSTN) and Integrated Services Digital Network (ISDN), but its use is declining in favour of all IP.
2734) 10.058571, Asynchronous Transfer Mode - Wikipedia, the free encyclopedia.txt#39, term: computer, content:Wireless ATM,[17] or Mobile ATM, consists of an ATM core network with a wireless access network. ATM cells are transmitted from base stations to mobile terminals. Mobility functions are performed at an ATM switch in the core network, known as "crossover switch",[18] which is similar to the MSC (mobile switching center) of GSM Networks. The advantage of Wireless ATM is its high bandwidth and high speed handoffs done at Layer 2. In the early 1990s, Bell Labs and NEC[19] Research Labs worked actively in this field. Andy Hopper from Cambridge University Computer Laboratory also worked in this area.[20] There was a Wireless ATM Forum formed to standardize the technology behind Wireless ATM Networks. The forum was supported by several telecommunication companies, including NEC, Fujitsu, AT&T, etc. Mobile ATM aimed to provide high speed multimedia communications technology, capable of delivering broadband mobile communications beyond that of GSM and WLANs.
2735) 10.058571, Atanasoff–Berry computer - Wikipedia, the free encyclopedia.txt#4, term: computer, content:The ABC was built by Atanasoff and Berry in the basement of the physics building at Iowa State College during 193942. The initial funds were released in September, and the 11-tube prototype was first demonstrated in October 1939. A December demonstration prompted a grant for construction of the full-scale machine.[7] The ABC was built and tested over the next two years. A January 15, 1941 story in the Des Moines Register announced the ABC as "an electrical computing machine" with more than 300 vacuum tubes that would "compute complicated algebraic equations" (but gave no precise technical description of the computer). The system weighed more than seven hundred pounds (320kg). It contained approximately 1 mile (1.6km) of wire, 280 dual-triode vacuum tubes, 31 thyratrons, and was about the size of a desk.
2736) 10.058571, Berkeley Software Distribution - Wikipedia, the free encyclopedia.txt#3, term: computer, content:The earliest distributions of Unix from Bell Labs in the 1970s included the source code to the operating system, allowing researchers at universities to modify and extend Unix. The operating system arrived at Berkeley in 1974, at the request of computer science professor Bob Fabry who had been on the program committee for the Symposium on Operating Systems Principles where Unix was first presented. A PDP-11/45 was bought to run the system, but for budgetary reasons, this machine was shared with the mathematics and statistics groups at Berkeley, who used RSTS, so that Unix only ran on the machine eight hours per day (sometimes during the day, sometimes during the night). A larger PDP-11/70 was installed at Berkeley the following year, using money from the Ingres database project.[2]
2737) 10.058571, BIOS - Wikipedia, the free encyclopedia.txt#26, term: computer, content:The behavior if the BIOS does not find a bootable device has varied as personal computers developed. The original IBM PC and XT had Microsoft Cassette BASIC in ROM, and if no bootable device was found, ROM BASIC was started by calling INT 18h. Therefore, barring a hardware failure, an original IBM PC or XT would never fail to boot, either into BASIC or from disk (or through an option ROM). One model of the original IBM PC was available with no disk drive; a cassette recorder could be attached via the cassette port on the rear, for loading and saving BASIC programs to tape. Since few programs used BASIC in ROM, clone PC makers left it out; then a computer that failed to boot from a disk would display "No ROM BASIC" and halt (in response to INT 18h).
2738) 10.058571, BIOS - Wikipedia, the free encyclopedia.txt#47, term: computer, content:Computer manufacturers that distribute OEM versions of Microsoft Windows and Microsoft application software can use the SLIC to authenticate licensing to the OEM Windows Installation disk and system recovery disc containing Windows software. Systems with an SLIC can be preactivated with an OEM product key, and they verify an XML formatted OEM certificate against the SLIC in the BIOS as a means of self-activating (see System Locked Preinstallation, SLP). If a user performs a fresh install of Windows, they will need to have possession of both the OEM key (either SLP or COA) and the digital certificate for their SLIC in order to bypass activation.[17] This can be achieved if the user performs a restore using a pre-customised image provided by the OEM. Power users can copy the necessary certificate files from the OEM image, decode the SLP product key, then perform SLP activation manually. Cracks for non-genuine Windows distributions usually edit the SLIC or emulate it in order to bypass Windows activation.[citation needed]
2739) 10.058571, BIOS - Wikipedia, the free encyclopedia.txt#67, term: computer, content:EEPROM chips are advantageous because they can be easily updated by the user; hardware manufacturers frequently issue BIOS updates to upgrade their products, improve compatibility and remove bugs. However, this advantage had the risk that an improperly executed or aborted BIOS update could render the computer or device unusable. To avoid these situations, more recent BIOSes use a "boot block"; a portion of the BIOS which runs first and must be updated separately. This code verifies if the rest of the BIOS is intact (using hash checksums or other methods) before transferring control to it. If the boot block detects any corruption in the main BIOS, it will typically warn the user that a recovery process must be initiated by booting from removable media (floppy, CD or USB flash drive) so the user can try flashing the BIOS again. Some motherboards have a backup BIOS (sometimes referred to as DualBIOS boards) to recover from BIOS corruptions.
2740) 10.058571, Boolean algebra - Wikipedia, the free encyclopedia.txt#5, term: computer, content:Logic sentences that can be expressed in classical propositional calculus have an equivalent expression in Boolean algebra. Thus, Boolean logic is sometimes used to denote propositional calculus performed in this way.[9][10][11] Boolean algebra is not sufficient to capture logic formulas using quantifiers, like those from first order logic. Although the development of mathematical logic did not follow Boole's program, the connection between his algebra and logic was later put on firm ground in the setting of algebraic logic, which also studies the algebraic systems of many other logics.[4] The problem of determining whether the variables of a given Boolean (propositional) formula can be assigned in such a way as to make the formula evaluate to true is called the Boolean satisfiability problem (SAT), and is of importance to theoretical computer science, being the first problem shown to be NP-complete. The closely related model of computation known as a Boolean circuit relates time complexity (of an algorithm) to circuit complexity.
2741) 10.058571, Booting - Wikipedia, the free encyclopedia.txt#9, term: computer, content:Other IBM computers of that era had similar features. For example, the IBM 1401 system (c. 1958) used a card reader to load a program from a punched card. The 80 characters stored in the punched card were read into memory locations 001 to 080, then the computer would branch to memory location 001 to read its first stored instruction. This instruction was always the same: move the information in these first 80 memory locations to an assembly area where the information in punched cards 2, 3, 4, and so on, could be combined to form the stored program. Once this information was moved to the assembly area, the machine would branch to an instruction in location 080 (read a card) and the next card would be read and its information processed.
2742) 10.058571, Booting - Wikipedia, the free encyclopedia.txt#24, term: computer, content:The earliest microcomputers, such as the Altair 8800 (released first in 1975) and an even earlier, similar machine (based on the Intel 8008 CPU) had no bootstrapping hardware as such.[16] When started, the CPU would see memory that would contain executable code containing only binary zerosmemory was cleared by resetting when powering up. The front panels of these machines carried toggle switches, one switch per bit of the computer memory word. Simple additions to the hardware permitted one memory location at a time to be loaded from those switches to store bootstrap code. Meanwhile, the CPU was kept from attempting to execute memory content. Once correctly loaded, the CPU was enabled to execute the bootstrapping code. This process was tedious and had to be error-free.[17]
2743) 10.058571, Booting - Wikipedia, the free encyclopedia.txt#25, term: computer, content:The boot process was revolutionized by the introduction of integrated circuit read-only memory (ROM), with its many variants, including mask-programmed ROMs, programmable ROMs (PROM), erasable programmable ROMs (EPROM), and flash memory. These allowed firmware boot programs to be shipped installed on the computer. The introduction of an (external) ROM was in an Italian telephone switching elaborator, called "Gruppi Speciali", patented in 1975 by Alberto Ciaramella, a researcher at CSELT.[18] Gruppi Speciali was, starting from 1975, a fully single-button machine booting into the operating system from a ROM memory composed from semiconductors, not from ferrite cores. Although the ROM was not natively integrated, due to the design of the machine, it also allowed the single-button ROM booting in machines not designed for that (therefore, it was architecture-independent), e.g. the PDP-11. Storing the state of the machine after the switch-off was also in place, which was another critical feature in the telephone switching contest.[19]
2744) 10.058571, Booting - Wikipedia, the free encyclopedia.txt#35, term: computer, content:Some computer systems, upon receiving a boot signal from a human operator or a peripheral device, may load a very small number of fixed instructions into memory at a specific location, initialize at least one CPU, and then point the CPU to the instructions and start their execution. These instructions typically start an input operation from some peripheral device (which may be switch-selectable by the operator). Other systems may send hardware commands directly to peripheral devices or I/O controllers that cause an extremely simple input operation (such as "read sector zero of the system device into memory starting at location 1000") to be carried out, effectively loading a small number of boot loader instructions into memory; a completion signal from the I/O device may then be used to start execution of the instructions by the CPU.
2745) 10.058571, Bus (computing) - Wikipedia, the free encyclopedia.txt#20, term: computer, content:Early microcomputer bus systems were essentially a passive backplane connected directly or through buffer amplifiers to the pins of the CPU. Memory and other devices would be added to the bus using the same address and data pins as the CPU itself used, connected in parallel. Communication was controlled by the CPU, which had read and written data from the devices as if they are blocks of memory, using the same instructions, all timed by a central clock controlling the speed of the CPU. Still, devices interrupted the CPU by signaling on separate CPU pins. For instance, a disk drive controller would signal the CPU that new data was ready to be read, at which point the CPU would move the data by reading the "memory location" that corresponded to the disk drive. Almost all early microcomputers were built in this fashion, starting with the S-100 bus in the Altair 8800 computer system.
2746) 10.058571, Bus (computing) - Wikipedia, the free encyclopedia.txt#25, term: computer, content:"Second generation" bus systems like NuBus addressed some of these problems. They typically separated the computer into two "worlds", the CPU and memory on one side, and the various devices on the other. A bus controller accepted data from the CPU side to be moved to the peripherals side, thus shifting the communications protocol burden from the CPU itself. This allowed the CPU and memory side to evolve separately from the device bus, or just "bus". Devices on the bus could talk to each other with no CPU intervention. This led to much better "real world" performance, but also required the cards to be much more complex. These buses also often addressed speed issues by being "bigger" in terms of the size of the data path, moving from 8-bit parallel buses in the first generation, to 16 or 32-bit in the second, as well as adding software setup (now standardised as Plug-n-play) to supplant or replace the jumpers.
2747) 10.058571, C++ - Wikipedia, the free encyclopedia.txt#4, term: computer, content:In 1979, Bjarne Stroustrup, a Danish computer scientist, began work on the predecessor to C++, "C with Classes".[8] The motivation for creating a new language originated from Stroustrup's experience in programming for his Ph.D. thesis. Stroustrup found that Simula had features that were very helpful for large software development, but the language was too slow for practical use, while BCPL was fast but too low-level to be suitable for large software development. When Stroustrup started working in AT&T Bell Labs, he had the problem of analyzing the UNIX kernel with respect to distributed computing. Remembering his Ph.D. experience, Stroustrup set out to enhance the C language with Simula-like features.[9] C was chosen because it was general-purpose, fast, portable and widely used. As well as C and Simula's influences, other languages also influenced C++, including ALGOL 68, Ada, CLU and ML.
2748) 10.058571, Cellular automaton - Wikipedia, the free encyclopedia.txt#49, term: computer, content:While a complete theory along this line has not been developed, entertaining and developing this hypothesis led scholars to interesting speculation and fruitful intuitions on how can we make sense of our world within a discrete framework. Marvin Minsky, the AI pioneer, investigated how to understand particle interaction with a four-dimensional CA lattice;[76] Konrad Zusethe inventor of the first working computer, the Z3developed an irregularly organized lattice to address the question of the information content of particles.[77] More recently, Edward Fredkin exposed what he terms the "finite nature hypothesis", i.e., the idea that "ultimately every quantity of physics, including space and time, will turn out to be discrete and finite."[78] Fredkin and Wolfram are strong proponents of a CA-based physics.
2749) 10.058571, Central processing unit - Wikipedia, the free encyclopedia.txt#5, term: computer, content:Early CPUs were custom designs used as part of a larger and sometimes distinctive computer.[10] However, this method of designing custom CPUs for a particular application has largely given way to the development of multi-purpose processors produced in large quantities. This standardization began in the era of discrete transistor mainframes and minicomputers and has rapidly accelerated with the popularization of the integrated circuit(IC). The IC has allowed increasingly complex CPUs to be designed and manufactured to tolerances on the order of nanometers.[11] Both the miniaturization and standardization of CPUs have increased the presence of digital devices in modern life far beyond the limited application of dedicated computing machines. Modern microprocessors appear in electronic devices ranging from automobiles[12] to cellphones,[13] and sometimes even in toys.[14]
2750) 10.058571, Central processing unit - Wikipedia, the free encyclopedia.txt#6, term: computer, content:While von Neumann is most often credited with the design of the stored-program computer because of his design of EDVAC, and the design became known as the von Neumann architecture, others before him, such as Konrad Zuse, had suggested and implemented similar ideas.[15] The so-called Harvard architecture of the Harvard Mark I, which was completed before EDVAC,[16][17] also utilized a stored-program design using punched paper tape rather than electronic memory.[18] The key difference between the von Neumann and Harvard architectures is that the latter separates the storage and treatment of CPU instructions and data, while the former uses the same memory space for both.[19] Most modern CPUs are primarily von Neumann in design, but CPUs with the Harvard architecture are seen as well, especially in embedded applications; for instance, the Atmel AVR microcontrollers are Harvard architecture processors.[20]
2751) 10.058571, Central processing unit - Wikipedia, the free encyclopedia.txt#7, term: computer, content:Relays and vacuum tubes (thermionic tubes) were commonly used as switching elements;[21][22] a useful computer requires thousands or tens of thousands of switching devices. The overall speed of a system is dependent on the speed of the switches. Tube computers like EDVAC tended to average eight hours between failures, whereas relay computers like the (slower, but earlier) Harvard Mark I failed very rarely.[1] In the end, tube-based CPUs became dominant because the significant speed advantages afforded generally outweighed the reliability problems. Most of these early synchronous CPUs ran at low clock rates compared to modern microelectronic designs. Clock signal frequencies ranging from 100 kHz to 4MHz were very common at this time, limited largely by the speed of the switching devices they were built with.[23]
2752) 10.058571, Central processing unit - Wikipedia, the free encyclopedia.txt#18, term: computer, content:While the complexity, size, construction, and general form of CPUs have changed enormously since 1950,[45] it is notable that the basic design and function has not changed much at all. Almost all common CPUs today can be very accurately described as von Neumann stored-program machines.[b] As the aforementioned Moore's law continues to hold true,[44] concerns have arisen about the limits of integrated circuit transistor technology. Extreme miniaturization of electronic gates is causing the effects of phenomena like electromigration and subthreshold leakage to become much more significant. These newer concerns are among the many factors causing researchers to investigate new methods of computing such as the quantum computer, as well as to expand the usage of parallelism and other methods that extend the usefulness of the classical von Neumann model.
2753) 10.058571, Computational science - Wikipedia, the free encyclopedia.txt#9, term: computer, content:Computational science application programs often model real-world changing conditions, such as weather, air flow around a plane, automobile body distortions in a crash, the motion of stars in a galaxy, an explosive device, etc. Such programs might create a 'logical mesh' in computer memory where each item corresponds to an area in space and contains information about that space relevant to the model. For example, in weather models, each item might be a square kilometer; with land elevation, current wind direction, humidity, temperature, pressure, etc. The program would calculate the likely next state based on the current state, in simulated time steps, solving equations that describe how the system operates; and then repeat the process to calculate the next state.
2754) 10.058571, Computer - Simple English Wikipedia, the free encyclopedia.txt#8, term: computer, content:Humans have a problem with maths. To show this, try doing 584 x 3,220 in your head. It is hard to remember all the steps! People made tools to help them remember where they were in a maths problem. The other problem people have is that they have to do the same problem over and over and over again. A cashier had to make change every day in her head or with a piece of paper. That took a lot of time and made mistakes. So, people made calculators that did those same things over and over. This part of computer history is called the "history of automated calculation," which is a fancy phrase for "the history of machines that make it easy for me to do this same maths problem over and over without making mistakes."
2755) 10.058571, Computer - Wikipedia, the free encyclopedia.txt#21, term: computer, content:In 1941, Zuse followed his earlier machine up with the Z3, the world's first working electromechanical programmable, fully automatic digital computer.[19][20] The Z3 was built with 2000 relays, implementing a 22bit word length that operated at a clock frequency of about 510Hz.[21] Program code was supplied on punched film while data could be stored in 64 words of memory or supplied from the keyboard. It was quite similar to modern machines in some respects, pioneering numerous advances such as floating point numbers. Replacement of the hard-to-implement decimal system (used in Charles Babbage's earlier design) by the simpler binary system meant that Zuse's machines were easier to build and potentially more reliable, given the technologies available at that time.[22] The Z3 was Turing complete.[23][24]
2756) 10.058571, Computer animation - Wikipedia, the free encyclopedia.txt#4, term: computer, content:To trick the eye and the brain into thinking they are seeing a smoothly moving object, the pictures should be drawn at around 12 frames per second or faster.[1] (A frame is one complete image.) With rates above 75-120 frames per second, no improvement in realism or smoothness is perceivable due to the way the eye and the brain both process images. At rates below 12 frames per second, most people can detect jerkiness associated with the drawing of new images that detracts from the illusion of realistic movement.[2] Conventional hand-drawn cartoon animation often uses 15 frames per second in order to save on the number of drawings needed, but this is usually accepted because of the stylized nature of cartoons. To produce more realistic imagery, computer animation demands higher frame rates.
2757) 10.058571, Computer animation - Wikipedia, the free encyclopedia.txt#15, term: computer, content:3D computer animation combines 3D models of objects and programmed or hand "keyframed" movement. These models are constructed out of geometrical vertices, faces, and edges in a 3D coordinate system. Objects are sculpted much like real clay or plaster, working from general forms to specific details with various sculpting tools. Unless a 3D model is intended to be a solid color, it must be painted with "textures" for realism. A bone/joint animation system is set up to deform the CGI model (e.g., to make a humanoid model walk). In a process known as rigging, the virtual marionette is given various controllers and handles for controlling movement.[17] Animation data can be created using motion capture, or keyframing by a human animator, or a combination of the two.[18]
2758) 10.058571, Computer cluster - Wikipedia, the free encyclopedia.txt#12, term: computer, content:The first commercial loosely coupled clustering product was Datapoint Corporation's "Attached Resource Computer" (ARC) system, developed in 1977, and using ARCnet as the cluster interface. Clustering per se did not really take off until Digital Equipment Corporation released their VAXcluster product in 1984 for the VAX/VMS operating system (now named as OpenVMS). The ARC and VAXcluster products not only supported parallel computing, but also shared file systems and peripheral devices. The idea was to provide the advantages of parallel processing, while maintaining data reliability and uniqueness. Two other noteworthy early commercial clusters were the Tandem Himalayan (a circa 1994 high-availability product) and the IBM S/390 Parallel Sysplex (also circa 1994, primarily for business use).
2759) 10.058571, Computer graphics - Wikipedia, the free encyclopedia.txt#6, term: computer, content:Early projects like the Whirlwind and SAGE Projects introduced the CRT as a viable display and interaction interface and introduced the light pen as an input device. Douglas T. Ross of the Whirlwind SAGE system performed a personal experiment in 1954 in which a small program he wrote captured the movement of his finger and displayed its vector (his traced name) on a display scope. One of the first interactive video games to feature recognizable, interactive graphics  Tennis for Two  was created for an oscilloscope by William Higinbotham to entertain visitors in 1958 at Brookhaven National Laboratory and simulated a tennis match. In 1959, Douglas T. Ross innovated again while working at MIT on transforming mathematic statements into computer generated machine tool vectors, and took the opportunity to create a display scope image of a Disney cartoon character.[4]
2760) 10.058571, Computer graphics - Wikipedia, the free encyclopedia.txt#9, term: computer, content:The phrase computer graphics itself was coined in 1960 by William Fetter, a graphic designer for Boeing.[5] This old quote in many secondary sources comes complete with the following sentence: (Fetter has said that the terms were actually given to him by Verne Hudson of the Wichita Division of Boeing.)[6] In 1961 another student at MIT, Steve Russell, created the second video game, Spacewar. Written for the DEC PDP-1, Spacewar was an instant success and copies started flowing to other PDP-1 owners and eventually DEC got a copy.[citation needed] The engineers at DEC used it as a diagnostic program on every new PDP-1 before shipping it. The sales force picked up on this quickly enough and when installing new units, would run the "world's first video game" for their new customers. (Higginbotham's Tennis For Two had beaten Spacewar by almost three years; but it was almost unknown outside of a research or academic setting.)
2761) 10.058571, Computer graphics - Wikipedia, the free encyclopedia.txt#22, term: computer, content:The 1980s is also called the golden era of videogames; millions-selling systems from Atari, Nintendo and Sega, among other companies, exposed computer graphics for the first time to a new, young, and impressionable audience - as did MS-DOS-based personal computers, Apple IIs and Macs, and Amigas, which also allowed users to program their own games if skilled enough. Demoscenes and shareware games proliferated; John Carmack, a later 3D innovator, would start out in this period developing sprite-based games. In the arcades, advances were made in commercial, real-time 3D graphics. In 1988, the first dedicated real-time 3D graphics boards were introduced in arcades, with the Namco System 21[10] and Taito Air System.[11] This innovation would be the precursor of the later home graphics processing unit or GPU, a technology where a separate and very powerful chip is used in parallel processing with a CPU to optimize graphics.
2762) 10.058571, Computer graphics - Wikipedia, the free encyclopedia.txt#27, term: computer, content:Technology and algorithms for rendering continued to improve greatly. In 1996, Krishnamurty and Levoy invented normal mapping - an improvement on Jim Blinn's bump mapping. 1999 saw Nvidia release the seminal GeForce 256, the first home video card billed as a graphics processing unit or GPU, which in its own words contained "integrated transform, lighting, triangle setup/clipping, and rendering engines". By the end of the decade, computers adopted common frameworks for graphics processing such as DirectX and OpenGL. Since then, computer graphics have only become more detailed and realistic, due to more powerful graphics hardware and 3D modeling software. AMD also became a leading developer of graphics boards in this decade, creating a "duopoly" in the field which exists to this day.
2763) 10.058571, Computer keyboard - Wikipedia, the free encyclopedia.txt#16, term: computer, content:Handheld ergonomic keyboards are designed to be held like a game controller, and can be used as such, instead of laid out flat on top of a table surface. Typically handheld keyboards hold all the alphanumeric keys and symbols that a standard keyboard would have, yet only be accessed by pressing two sets of keys at once; one acting as a function key similar to a 'Shift' key that would allow for capital letters on a standard keyboard.[2] Handheld keyboards allow the user the ability to move around a room or to lean back on a chair while also being able to type in front or away from the computer.[3] Some variations of handheld ergonomic keyboards also include a trackball mouse that allow mouse movement and typing included in one handheld device.
2764) 10.058571, Computer monitor - Wikipedia, the free encyclopedia.txt#6, term: computer, content:Some of the earliest home computers (such as the TRS-80 and Commodore PET) were limited to monochrome CRT displays, but color display capability was already a standard feature of the pioneering Apple II, introduced in 1977, and the specialty of the more graphically sophisticated Atari 800, introduced in 1979. Either computer could be connected to the antenna terminals of an ordinary color TV set or used with a purpose-made CRT color monitor for optimum resolution and color quality. Lagging several years behind, in 1981 IBM introduced the Color Graphics Adapter, which could display four colors with a resolution of 320 x 200 pixels, or it could produce 640 x 200 pixels with two colors. In 1984 IBM introduced the Enhanced Graphics Adapter which was capable of producing 16 colors and had a resolution of 640 x 350.[1]
2765) 10.058571, Computer mouse - Wikipedia, the free encyclopedia.txt#13, term: computer, content:By 1982 the Xerox 8010 was probably the best-known computer with a mouse, and the forthcoming Apple Lisa was rumored to use one, but the peripheral remained obscure; Jack Hawley of The Mouse House reported that one buyer for a large organization believed at first that his company sold lab mice. Hawley, who manufactured mice for Xerox, stated that "Practically, I have the market all to myself right now"; a Hawley mouse cost $415.[22] That year Microsoft made the decision to make the MS-DOS program Microsoft Word mouse-compatible, and developed the first PC-compatible mouse. Microsoft's mouse shipped in 1983, thus beginning Microsoft hardware.[23] However, the mouse remained relatively obscure until the 1984 appearance of the Macintosh 128K, which included an updated version of the Lisa Mouse[24] and the Atari ST in 1985.
2766) 10.058571, Computer mouse - Wikipedia, the free encyclopedia.txt#52, term: computer, content:In 1986 Apple first implemented the Apple Desktop Bus allowing the daisy-chaining together of up to 16 devices, including arbitrarily many mice and other devices on the same bus with no configuration whatsoever. Featuring only a single data pin, the bus used a purely polled approach to computer/mouse communications and survived as the standard on mainstream models (including a number of non-Apple workstations) until 1998 when iMac joined the industry-wide switch to using USB. Beginning with the Bronze Keyboard PowerBook G3 in May 1999, Apple dropped the external ADB port in favor of USB, but retained an internal ADB connection in the PowerBook G4 for communication with its built-in keyboard and trackpad until early 2005.
2767) 10.058571, Computer multitasking - Wikipedia, the free encyclopedia.txt#9, term: computer, content:Early multitasking systems used applications that voluntarily ceded time to one another. This approach, which was eventually supported by many computer operating systems, is known today as cooperative multitasking. Although it is now rarely used in larger systems except for specific applications such as CICS or the JES2 subsystem, cooperative multitasking was once the scheduling scheme employed by Microsoft Windows (prior to Windows 95 and Windows NT) and Mac OS (prior to OS X) in order to enable multiple applications to be run simultaneously. Windows 9x also used cooperative multitasking, but only for 16-bit legacy applications, much the same way as pre-Leopard PowerPC versions of Mac OS X used it for Classic applications. The network operating system NetWare used cooperative multitasking up to NetWare 6.5. Cooperative multitasking is still used today on RISC OS systems.[2]
2768) 10.058571, Computer music - Wikipedia, the free encyclopedia.txt#12, term: computer, content:Later, composers such as Gottfried Michael Koenig had computers generate the sounds of the composition as well as the score. Koenig produced algorithmic composition programs which were a generalisation of his own serial composition practice. This is not exactly similar to Xenakis' work as he used mathematical abstractions and examined how far he could explore these musically. Koenig's software translated the calculation of mathematical equations into codes which represented musical notation. This could be converted into musical notation by hand and then performed by human players. His programs Project 1 and Project 2 are examples of this kind of software. Later, he extended the same kind of principles into the realm of synthesis, enabling the computer to produce the sound directly. SSP is an example of a program which performs this kind of function. All of these programs were produced by Koenig at the Institute of Sonology in Utrecht in the 1970s.[citation needed]
2769) 10.058571, Computer music - Wikipedia, the free encyclopedia.txt#31, term: computer, content:Generally, this practice stages a more general approach: one of interactive programming, of writing (parts of) programs while they are interpreted. Traditionally most computer music programs have tended toward the old write/compile/run model which evolved when computers were much less powerful. This approach has locked out code-level innovation by people whose programming skills are more modest. Some programs have gradually integrated real-time controllers and gesturing (for example, MIDI-driven software synthesis and parameter control). Until recently, however, the musician/composer rarely had the capability of real-time modification of program code itself. This legacy distinction is somewhat erased by languages such as ChucK, SuperCollider, and Impromptu.[citation needed]
2770) 10.058571, Computer network - Wikipedia, the free encyclopedia.txt#40, term: computer, content:Whilst the use of protocol layering is today ubiquitous across the field of computer networking, it has been historically criticized by many researchers[13] for two principal reasons. Firstly, abstracting the protocol stack in this way may cause a higher layer to duplicate functionality of a lower layer, a prime example being error recovery on both a per-link basis and an end-to-end basis.[14] Secondly, it is common that a protocol implementation at one layer may require data, state or addressing information that is only present at another layer, thus defeating the point of separating the layers in the first place. For example, TCP uses the ECN field in the IPv4 header as an indication of congestion; IP is a network layer protocol whereas TCP is a transport layer protocol.
2771) 10.058571, Computer security - Wikipedia, the free encyclopedia.txt#19, term: computer, content:Computers control functions at many utilities, including coordination of telecommunications, the power grid, nuclear power plants, and valve opening and closing in water and gas networks. The Internet is a potential attack vector for such machines if connected, but the Stuxnet worm demonstrated that even equipment controlled by computers not connected to the Internet can be vulnerable to physical damage caused by malicious commands sent to industrial equipment (in that case uranium enrichment centrifuges) which are infected via removable media. In 2014, the Computer Emergency Readiness Team, a division of the Department of Homeland Security, investigated 79 hacking incidents at energy companies.[12] Vulnerabilities in smart meters (many of which use local radio or cellular communications) can cause problems with billing fraud.[13]
2772) 10.058571, Computer security - Wikipedia, the free encyclopedia.txt#30, term: computer, content:While the IoT creates opportunities for more direct integration of the physical world into computer-based systems,[43][44] It also provides also opportunities misuse. In particular, as the Internet of Things spreads widely, cyber attacks are likely to become an increasingly physical (rather than simply virtual) threat.[45] If a front door's lock is connected to the Internet, and can be locked/unlocked from a phone, then a criminal could enter the home at the press of a button from a stolen or hacked phone. People could stand to lose much more than their credit card numbers in a world controlled by IoT-enabled devices. Thieves have also used electronic means to circumvent non-Internet-connected hotel door locks.[46]
2773) 10.058571, Computer simulation - Wikipedia, the free encyclopedia.txt#25, term: computer, content:The following three steps should be used to produce accurate simulation models: calibration, verification, and validation. Computer simulations are good at portraying and comparing theoretical scenarios, but in order to accurately model actual case studies they have to match what is actually happening today. A base model should be created and calibrated so that it matches the area being studied. The calibrated model should then be verified to ensure that the model is operating as expected based on the inputs. Once the model has been verified, the final step is to validate the model by comparing the outputs to historical data from the study area. This can be done by using statistical techniques and ensuring an adequate R-squared value. Unless these techniques are employed, the simulation model created will produce inaccurate results and not be a useful prediction tool.
2774) 10.058571, Computer speaker - Wikipedia, the free encyclopedia.txt#5, term: computer, content:An unusual design by HiWave Technologies, the DyadUSB USB-powered stereo audio amplifier module[2] used in the SoundScience QSB 30W Portable USB Speakers[3] allows a USB-powered and driven stereo speaker pair to supply 30W of power for short periods with a signal that has short high-power peaks and much lower average power, as most music and speech does. It stores energy from the USB connection during quieter periods, delivering high power for the peaks. (With a constant sine-wave input, power output cannot exceed the 2.5W that any USB speaker can deliver). The module is claimed to require less power most of the time, increasing laptop computer battery endurance, and delivering clean, unclipped sound peaks.
2775) 10.058571, Conventional PCI - Wikipedia, the free encyclopedia.txt#1, term: computer, content:Attached devices can take either the form of an integrated circuit fitted onto the motherboard itself (called a planar device in the PCI specification) or an expansion card that fits into a slot. The PCI Local Bus was first implemented in IBM PC compatibles, where it displaced the combination of several slow ISA slots and one fast VESA Local Bus slot as the bus configuration. It has subsequently been adopted for other computer types. Typical PCI cards used in PCs include: network cards, sound cards, modems, extra ports such as USB or serial, TV tuner cards and disk controllers. PCI video cards replaced ISA and VESA cards until growing bandwidth requirements outgrew the capabilities of PCI. The preferred interface for video cards then became AGP, itself a superset of conventional PCI, before giving way to PCI Express.[4]
2776) 10.058571, Conventional PCI - Wikipedia, the free encyclopedia.txt#49, term: computer, content:There are three card form factors: Type I, Type II, and Type III cards. The card connector used for each type include: Type I and II use a 100-pin stacking connector, while Type III uses a 124-pin edge connector, i.e. the connector for Types I and II differs from that for Type III, where the connector is on the edge of a card, like with a SO-DIMM. The additional 24 pins provide the extra signals required to route I/O back through the system connector (audio, AC-Link, LAN, phone-line interface). Type II cards have RJ11 and RJ45 mounted connectors. These cards must be located at the edge of the computer or docking station so that the RJ11 and RJ45 ports can be mounted for external access.
2777) 10.058571, Cray - Wikipedia, the free encyclopedia.txt#6, term: computer, content:Cray soon left the CEO position to become an independent contractor. He started a new VLSI technology lab for the Cray-2 in Boulder, Colorado, Cray Laboratories, in 1979, which closed in 1982; undaunted, Cray later headed a similar spin-off in 1989, Cray Computer Corporation (CCC) in Colorado Springs, where he worked on the Cray-3 projectthe first attempt at major use of gallium arsenide (GaAs) semiconductors in computing. However, the changing political climate (collapse of the Warsaw Pact and the end of the Cold War) resulted in poor sales (only one Cray-3 was delivered) and the company fell by the wayside, eventually filing for bankruptcy in 1995. CCC's remains then began Cray's final corporation, SRC Computers, Inc. which still exists.
2778) 10.058571, Cray - Wikipedia, the free encyclopedia.txt#7, term: computer, content:Cray Research continued development along a separate line of computers, originally with lead designer Steve Chen and the Cray X-MP. After Chen's departure, the Cray Y-MP, Cray C90 and Cray T90 were developed on the original Cray-1 architecture but achieved much greater performance via multiple additional processors, faster clocks, and wider vector pipes. The uncertainty of the Cray-2 project gave rise to a number of Cray-object-code compatible "Crayette" firms: Scientific Computer Systems (SCS), American Supercomputer, Supertek, and perhaps one other firm. These firms did not mean to compete against Cray and therefore attempted less expensive, slower CMOS versions of the X-MP with the release of the COS operating system (SCS) and the CFT Fortran compiler; they also considered National labs (LANL/LLNL) developed CTSS operating system as well before caving in to the tide of Unixes.
2779) 10.058571, Cray - Wikipedia, the free encyclopedia.txt#20, term: computer, content:In 2002, Cray Inc. announced their first new model, the Cray X1 combined architecture vector / MPP supercomputer. Previously known as the SV2, the X1 is the end result of the earlier SN2 concept originated during the SGI years. In May 2004, Cray was announced to be one of the partners in the U.S. Department of Energy's fastest-computer-in-the-world project to build a 50 teraflops machine for the Oak Ridge National Laboratory. Cray was sued in 2002 by Isothermal Systems Research for patent infringement. The suit claimed that Cray used ISR's patented technology in the development of the Cray X1.[10] The lawsuit was settled in 2003.[11] As of November 2004, the Cray X1 had a maximum measured performance of 5.9 teraflops, being the 29th fastest supercomputer in the world. Since then the X1 has been superseded by the X1E, with faster dual-core processors.
2780) 10.058571, Cryptography - Wikipedia, the free encyclopedia.txt#51, term: computer, content:In the United States, cryptography is legal for domestic use, but there has been much conflict over legal issues related to cryptography. [8] One particularly important issue has been the export of cryptography and cryptographic software and hardware. Probably because of the importance of cryptanalysis in World War II and an expectation that cryptography would continue to be important for national security, many Western governments have, at some point, strictly regulated export of cryptography. After World War II, it was illegal in the US to sell or distribute encryption technology overseas; in fact, encryption was designated as auxiliary military equipment and put on the United States Munitions List.[49] Until the development of the personal computer, asymmetric key algorithms (i.e., public key techniques), and the Internet, this was not especially problematic. However, as the Internet grew and computers became more widely available, high-quality encryption techniques became well known around the globe.
2781) 10.058571, CSIRAC - Wikipedia, the free encyclopedia.txt#2, term: computer, content:The machine was fairly representative of first-generation valve-driven computer designs. It used mercury acoustic delay lines as its primary data storage, with a typical capacity of 768 20-bit words (later doubled), supplemented by a parallel disk-type device with a total 1024-word capacity and an access time of 10 milliseconds. Its memory clock ran at 1000Hz, and the control unit, synchronized to the clock, took two cycles to execute an instruction (later the speed was doubled to one cycle per instruction). The bus (termed the "digit trunk" in their design) is unusual compared to most computers in that it was serialit transferred one bit at a time. The instruction set was minimal,[vague] but supported the basic set of arithmetic and logical operations, as well as conditional and relative jumps (making it possible to write a library of subroutines).
2782) 10.058571, DARPA - Wikipedia, the free encyclopedia.txt#26, term: computer, content:The mission of the Biological Technologies Office (BTO) is to foster, demonstrate, and transition breakthrough fundamental research, discoveries, and applications that integrate biology, engineering, and computer science for national security. BTO seeks to establish and invest in new communities of scientific interest at the intersection of traditional and emerging disciplines. Its investment portfolio goes far beyond life sciences applications in medicine to include areas of research such as human-machine interfaces,[30] microbes as production platforms, and deep exploration of the impact of evolving ecologies and environments on U.S. readiness and capabilities. BTOs programs operate across a wide range of scales, from individual cells to complex biological systems including mammalian and non-mammalian organisms and the macro- and micro-environments in which they operate.
2783) 10.058571, Database - Wikipedia, the free encyclopedia.txt#1, term: computer, content:A database management system (DBMS) is a computer software application that interacts with the user, other applications, and the database itself to capture and analyze data. A general-purpose DBMS is designed to allow the definition, creation, querying, update, and administration of databases. Well-known DBMSs include MySQL, PostgreSQL, Microsoft SQL Server, Oracle, Sybase, SAP HANA, and IBM DB2. A database is not generally portable across different DBMSs, but different DBMS can interoperate by using standards such as SQL and ODBC or JDBC to allow a single application to work with more than one DBMS. Database management systems are often classified according to the database model that they support; the most popular database systems since the 1980s have all supported the relational model as represented by the SQL language.[disputed  discuss] Sometimes a DBMS is loosely referred to as a 'database'.
2784) 10.058571, Database - Wikipedia, the free encyclopedia.txt#40, term: computer, content:The 1980s ushered in the age of desktop computing. The new computers empowered their users with spreadsheets like Lotus 1-2-3 and database software like dBASE. The dBASE product was lightweight and easy for any computer user to understand out of the box. C. Wayne Ratliff the creator of dBASE stated: "dBASE was different from programs like BASIC, C, FORTRAN, and COBOL in that a lot of the dirty work had already been done. The data manipulation is done by dBASE instead of by the user, so the user can concentrate on what he is doing, rather than having to mess with the dirty details of opening, reading, and closing files, and managing space allocation."[19] dBASE was one of the top selling software titles in the 1980s and early 1990s.
2785) 10.058571, DEC Alpha - Wikipedia, the free encyclopedia.txt#47, term: computer, content:To illustrate the comparative performance of Alpha-based systems, some SPEC performance numbers (SPECint95, SPECfp95) are listed below. Note that the SPEC results claim to report the measured performance of a whole computer system (CPU, bus, memory, compiler optimizer), not just the CPU. Also note that the benchmark and scale changed from 1992 to 1995. However, the figures give a rough impression of the performance of the Alpha architecture (64-bit), compared with the contemporary HP (64-bit) and Intel-based offerings (32-bit). Perhaps the most obvious trend is that while Intel could always get reasonably close to Alpha in integer performance, in floating point performance the difference was considerable. On the other side, HP (PA-RISC) is also reasonably close to Alpha, but these CPUs are running at significantly lower clock rates (MHz). The tables lack two important values: the power consumption and the price of a CPU.
2786) 10.058571, Desktop computer - Wikipedia, the free encyclopedia.txt#16, term: computer, content:Desktops have the advantage over laptops, as the spare parts and extensions tend to be standardized, resulting in lower prices and greater availability. For example, the size and mounting of the motherboard is standardized into ATX, microATX, BTX or other form factors. Desktops have several standardized expansion slots, like Conventional PCI or PCI express, while laptops only tend to have one mini-PCI slot and one PC Card slot (or ExpressCard slot). Procedures for assembly and disassembly of desktops tend to be simple and standardized as well. This tends not to be the case for laptops, though adding or replacing some parts, like the optical drive, hard disk, or adding an extra memory module is often quite simple. This means that a desktop computer configuration, usually a tower case, can be customized and upgraded to a greater extent than laptops. This customization has kept tower cases popular among gamers and enthusiasts.
2787) 10.058571, Differential analyser - Wikipedia, the free encyclopedia.txt#7, term: computer, content:In the United States, further differential analysers were built at the Ballistic Research Laboratory in Maryland and in the basement of the Moore School of Electrical Engineering at the University of Pennsylvania during the early 1940s.[18] The latter was used extensively in the computation of artillery firing tables prior to the invention of the ENIAC, which, in many ways, was modelled on the differential analyser.[19] Also in the early 1940s, with Samuel H. Caldwell, one of the initial contributors during the early 1930s, Bush attempted an electrical, rather than mechanical, variation, but the digital computer built elsewhere had much greater promise and the project ceased.[20] In 1947, UCLA installed a differential analyser built for them by General Electric at a cost of $125,000.[21] By 1950, this machine had been joined by three more.[22]
2788) 10.058571, Digital Equipment Corporation - Wikipedia, the free encyclopedia.txt#3, term: computer, content:Initially focusing on the small end of the computer market allowed DEC to grow without its potential competitors making serious efforts to compete with them. Their PDP series of machines became popular in the 1960s, especially the PDP-8, widely considered to be the first successful minicomputer. Looking to simplify and update their line, DEC replaced most of their smaller machines with the PDP-11 in 1970, eventually selling over 600,000 units and cementing DECs position in the industry. Originally designed as a follow-on to the PDP-11, DEC's VAX-11 series was the first widely used 32-bit minicomputer, sometimes referred to as "superminis". These were able to compete in many roles with larger mainframe computers, such as the IBM System/370. The VAX was a best-seller, with over 400,000 sold, and its sales through the 1980s propelled the company into the second largest in the industry. At its peak, DEC was the second largest employer in Massachusetts, second only to the state government.
2789) 10.058571, Digital Equipment Corporation - Wikipedia, the free encyclopedia.txt#25, term: computer, content:Seeing the success of the LINC, in 1963 DEC took the basic logic design but stripped away the extensive A to D systems to produce the PDP-5. The new machine, the first outside the PDP-1 mould, was introduced at WESTCON on 11 August 1963. A 1964 ad expressed the main advantage of the PDP-5, "Now you can own the PDP-5 computer for what a core memory alone used to cost: $27,000"[32] 116 PDP-5s were produced until the lines were shut down in early 1967. Like the PDP-1 before it, the PDP-5 inspired a series of newer models based on the same basic design that would go on to be more famous than its parent.
2790) 10.058571, Digital Equipment Corporation - Wikipedia, the free encyclopedia.txt#42, term: computer, content:The PDP-11 supported several operating systems, including Bell Labs' new Unix operating system as well as DEC's DOS-11, RSX-11, IAS, RT-11, DSM-11, and RSTS/E. Many early PDP-11 applications were developed using standalone paper-tape utilities. DOS-11 was the PDP-11's first disk operating system, but was soon supplanted by more capable systems. RSX provided a general-purpose multitasking environment and supported a wide variety of programming languages. IAS was a time-sharing version of RSX-11D. Both RSTS and Unix were time-sharing systems available to educational institutions at little or no cost, and these PDP-11 systems were destined to be the "sandbox" for a rising generation of engineers and computer scientists. Large numbers of PDP-11/70s were deployed in telecommunications and industrial control applications. AT&T Corporation became DEC's largest customer.
2791) 10.058571, Digital Equipment Corporation - Wikipedia, the free encyclopedia.txt#80, term: computer, content:Eventually, on 26 January 1998, what remained of the company (including Digital's multivendor global services organization and customer support centers) was sold to PC manufacturer Compaq in what was the largest merger up to that time in the computer industry. Several years earlier, Compaq had considered a bid for Digital but became seriously interested only after Digital's major divestments and refocusing on the Internet in 1997. At the time of Compaq's acquisition announcement, Digital had a total of 53,500 employees, down from a peak of 130,000 in the 1980s, but it still employed about 65% more people than Compaq to produce about half the volume of sales revenues. After the merger closed, Compaq moved aggressively to reduce Digitals high selling, general, and administrative (SG&A) costs (equal to 24% of total 1997 revenues) and bring them more in line with Compaqs SG&A expense ratio of 12% of revenues.[78]
2792) 10.058571, DirectX - Wikipedia, the free encyclopedia.txt#7, term: computer, content:DirectX 2.0 became a component of Windows itself with the releases of Windows 95 OSR2 and Windows NT 4.0 in mid-1996. Since Windows 95 was itself still new and few games had been released for it, Microsoft engaged in heavy promotion of DirectX to developers who were generally distrustful of Microsoft's ability to build a gaming platform in Windows. Alex St. John, the evangelist for DirectX, staged an elaborate event at the 1996 Computer Game Developers Conference which game developer Jay Barnson described as a Roman theme, including real lions, togas, and something resembling an indoor carnival.[8] It was at this event that Microsoft first introduced Direct3D and DirectPlay, and demonstrated multiplayer MechWarrior 2 being played over the Internet.
2793) 10.058571, DirectX - Wikipedia, the free encyclopedia.txt#9, term: computer, content:Prior to DirectX, Microsoft had included OpenGL on their Windows NT platform.[9] At the time, OpenGL required "high-end" hardware and was focused on engineering and CAD uses.[citation needed] Direct3D was intended to be a lightweight partner to OpenGL, focused on game use. As 3D gaming grew, OpenGL developed to include better support for programming techniques for interactive multimedia applications like games, giving developers choice between using OpenGL or Direct3D as the 3D graphics API for their applications. At that point a "battle" began between supporters of the cross-platform OpenGL and the Windows-only Direct3D. Incidentally, OpenGL was supported at Microsoft by the DirectX team. If a developer chose to use OpenGL 3D graphics API, the other APIs of DirectX are often combined with OpenGL in computer games because OpenGL does not include all of DirectX's functionality (such as sound or joystick support).
2794) 10.058571, Drum memory - Wikipedia, the free encyclopedia.txt#4, term: computer, content:The performance of a drum with one head per track is determined almost entirely by the rotational latency, whereas in an HDD its performance includes a rotational latency delay plus the time to position the head over the desired track (seek time). In the era when drums were used as main working memory, programmers often did optimum programming -- the programmer[NB 1] positioned code on the drum in such a way as to reduce the amount of time needed for the next instruction to rotate into place under the head. They did this by timing how long it would take after loading an instruction for the computer to be ready to read the next one, then placing that instruction on the drum so that it would arrive under a head just in time. This method of timing-compensation, called the "skip factor" or "interleaving" (interleaving in disk storage), was used for many years in storage memory controllers.
2795) 10.058571, E6B - Wikipedia, the free encyclopedia.txt#21, term: computer, content:So he came up with his now famous wind arc slide, but printed on an endless cloth belt moved inside a square box by a knob. He applied for a patent in 1936 (granted in 1937 as 2,097,116). This was for the Model C, D and G computers widely used in World War II by the British Commonwealth (as the "Dalton Dead Reckoning Computer"), the U.S. Navy, copied by the Japanese, and improved on by the Germans, through Siegfried Knemeyer's invention of the disc-type Dreieckrechner device, somewhat similar to the eventual E6B's backside compass rose dial in general appearance, but having the compass rose on the front instead for real-time calculations of the wind triangle at any time while in flight. These are commonly available on collectible auction web sites.
2796) 10.058571, Electrical engineering - Wikipedia, the free encyclopedia.txt#42, term: computer, content:Electrical engineers typically possess an academic degree with a major in electrical engineering, electronics engineering, electrical engineering technology,[54] or electrical and electronic engineering.[55][56] The same fundamental principles are taught in all programs, though emphasis may vary according to title. The length of study for such a degree is usually four or five years and the completed degree may be designated as a Bachelor of Science in Electrical/Electronics Engineering Technology, Bachelor of Engineering, Bachelor of Science, Bachelor of Technology, or Bachelor of Applied Science depending on the university. The bachelor's degree generally includes units covering physics, mathematics, computer science, project management, and a variety of topics in electrical engineering.[57] Initially such topics cover most, if not all, of the subdisciplines of electrical engineering. At some schools, the students can then choose to emphasize one or more subdisciplines towards the end of their courses of study.
2797) 10.058571, Electrical engineering - Wikipedia, the free encyclopedia.txt#47, term: computer, content:Professional bodies of note for electrical engineers include the Institute of Electrical and Electronics Engineers (IEEE) and the Institution of Engineering and Technology (IET). The IEEE claims to produce 30% of the world's literature in electrical engineering, has over 360,000 members worldwide and holds over 3,000 conferences annually.[64] The IET publishes 21 journals, has a worldwide membership of over 150,000, and claims to be the largest professional engineering society in Europe.[65][66] Obsolescence of technical skills is a serious concern for electrical engineers. Membership and participation in technical societies, regular reviews of periodicals in the field and a habit of continued learning are therefore essential to maintaining proficiency. An MIET(Member of the Institution of Engineering and Technology) is recognised in Europe as an Electrical and computer (technology) engineer.[67]
2798) 10.058571, Electrical engineering - Wikipedia, the free encyclopedia.txt#51, term: computer, content:Although most electrical engineers will understand basic circuit theory (that is the interactions of elements such as resistors, capacitors, diodes, transistors, and inductors in a circuit), the theories employed by engineers generally depend upon the work they do. For example, quantum mechanics and solid state physics might be relevant to an engineer working on VLSI (the design of integrated circuits), but are largely irrelevant to engineers working with macroscopic electrical systems. Even circuit theory may not be relevant to a person designing telecommunication systems that use off-the-shelf components. Perhaps the most important technical skills for electrical engineers are reflected in university programs, which emphasize strong numerical skills, computer literacy, and the ability to understand the technical language and concepts that relate to electrical engineering.[69]
2799) 10.058571, Email client - Wikipedia, the free encyclopedia.txt#2, term: computer, content:Like most client programs, an email client is only active when a user runs it. The most common arrangement is for an email user (the client) to make an arrangement with a remote Mail Transfer Agent (MTA) server for the receipt and storage of the client's emails. The MTA, using a suitable mail delivery agent (MDA), adds email messages to a client's storage as they arrive. The remote mail storage is referred to as the user's mailbox. The default setting on many Unix systems is for the mail server to store formatted messages in mbox, within the user's HOME directory. Of course, users of the system can log-in and run a mail client on the same computer that hosts their mailboxes; in which case, the server is not actually remote, other than in a generic sense.
2800) 10.058571, Embedded system - Wikipedia, the free encyclopedia.txt#23, term: computer, content:Embedded processors can be broken into two broad categories. Ordinary microprocessors (P) use separate integrated circuits for memory and peripherals. Microcontrollers (C) have on-chip peripherals, thus reducing power consumption, size and cost. In contrast to the personal computer market, many different basic CPU architectures are used, since software is custom-developed for an application and is not a commodity product installed by the end user. Both Von Neumann as well as various degrees of Harvard architectures are used. RISC as well as non-RISC processors are found. Word lengths vary from 4-bit to 64-bits and beyond, although the most typical remain 8/16-bit. Most architectures come in a large number of different variants and shapes, many of which are also manufactured by several different companies.
2801) 10.058571, ENIAC - Wikipedia, the free encyclopedia.txt#6, term: computer, content:By the end of its operation in 1955, ENIAC contained 17,468 vacuum tubes, 7200 crystal diodes, 1500 relays, 70,000 resistors, 10,000 capacitors and approximately 5,000,000 hand-soldered joints. It weighed more than 30 short tons (27 t), was roughly 2.4m  0.9m  30m (8  3  100 feet) in size, occupied 167m2 (1800ft2) and consumed 150 kW of electricity.[14][15] This power requirement led to the rumor that whenever the computer was switched on, lights in Philadelphia dimmed.[16] Input was possible from an IBM card reader; an IBM card punch was used for output. These cards could be used to produce printed output offline using an IBM accounting machine, such as the IBM 405. While ENIAC had no system to store memory in its inception, these punch cards could be used for external memory storage.[17] In 1953, a 100-word static magnetic-memory core built by the Burroughs Corporation was added to ENIAC.[18]
2802) 10.058571, ENIAC - Wikipedia, the free encyclopedia.txt#28, term: computer, content:The programming of the stored program for ENIAC was done by Betty Jennings, Clippinger and Adele Goldstine. It was first demonstrated as a stored-program computer on September 16, 1948, running a program by Adele Goldstine for John von Neumann. This modification reduced the speed of ENIAC by a factor of six and eliminated the ability of parallel computation, but as it also reduced the reprogramming time to hours instead of days, it was considered well worth the loss of performance. Also analysis had shown that due to differences between the electronic speed of computation and the electromechanical speed of input/output, almost any real-world problem was completely I/O bound, even without making use of the original machine's parallelism. Most computations would still be I/O bound, even after the speed reduction imposed by this modification.
2803) 10.058571, Firmware - Wikipedia, the free encyclopedia.txt#1, term: computer, content:Firmware is held in non-volatile memory devices such as ROM, EPROM, or flash memory. Changing the firmware of a device may rarely or never be done during its economic lifetime; some firmware memory devices are permanently installed and cannot be changed after manufacture. Common reasons for updating firmware include fixing bugs or adding features to the device. This may require ROM integrated circuits to be physically replaced, or flash memory to be reprogrammed through a special procedure.[3] Firmware such as the ROM BIOS of a personal computer may contain only elementary basic functions of a device and may only provide services to higher-level software. Firmware such as the program of an embedded system may be the only program that will run on the system and provide all of its functions.
2804) 10.058571, Firmware - Wikipedia, the free encyclopedia.txt#17, term: computer, content:Custom firmware hacks have also focused on injecting malware into devices such as smartphones or USB devices. One such smartphone injection was demonstrated on the Symbian OS at MalCon,[13][14] a hacker convention. A USB device firmware hack called BadUSB was presented at Black Hat USA 2014 conference,[15] demonstrating how a USB flash drive microcontroller can be reprogrammed to spoof various other device types in order to take control of a computer, exfiltrate data, or spy on the user.[16][17] Other security researchers have worked further on how to exploit the principles behind BadUSB,[18] releasing at the same time the source code of hacking tools that can be used to modify the behavior of different USB devices.[19]
2805) 10.058571, First-person shooter - Wikipedia, the free encyclopedia.txt#23, term: computer, content:Starsiege: Tribes, also released in 1998, was a multiplayer online shooter allowing more than 32 players in a single match. It featured team-based gameplay with a variety of specialized roles, and an unusual jet pack feature. The game was highly popular and later imitated by games such as the Battlefield series.[1][2] Id's Quake III Arena and Epic's Unreal Tournament, both released in 1999, were popular for their frenetic and accessible online multiplayer modes; both featured very limited single player gameplay.[14] Counter-Strike was also released in 1999, a Half-Life modification with a counter-terrorism theme. The game and later version Counter-Strike: Source (2004) went on to become by far the most popular multiplayer first-person shooter and computer game modification ever, with over 90,000 players competing online at any one time during its peak.[14][54]
2806) 10.058571, Floppy disk - Wikipedia, the free encyclopedia.txt#5, term: computer, content:In 1976, Shugart Associates introduced the first 5-inch FDD. By 1978 there were more than 10 manufacturers producing such FDDs.[5] There were competing floppy disk formats, with hard- and soft-sector versions and encoding schemes such as FM, MFM and GCR. The 5-inch format displaced the 8-inch one for most applications, and the hard-sectored disk format disappeared. In 1984, IBM introduced the 1.2 MB dual-sided floppy disk along with its AT model. IBM started using the 720 KB double-density 3-inch microfloppy disk on its Convertible laptop computer in 1986 and the 1.44 MB high-density version with the PS/2 line in 1987. These disk drives could be added to older PC models. In 1988 IBM introduced a drive for 2.88MB "DSED" diskettes in its top-of-the-line PS/2 models, but this was a commercial failure.
2807) 10.058571, Fortran - Wikipedia, the free encyclopedia.txt#58, term: computer, content:Portability was a problem in the early days because there was no agreed standard  not even IBM's reference manual  and computer companies vied to differentiate their offerings from others by providing incompatible features. Standards have improved portability. The 1966 standard provided a reference syntax and semantics, but vendors continued to provide incompatible extensions. Although careful programmers were coming to realize that use of incompatible extensions caused expensive portability problems, and were therefore using programs such as The PFORT Verifier, it was not until after the 1977 standard, when the National Bureau of Standards (now NIST) published FIPS PUB 69, that processors purchased by the U.S. Government were required to diagnose extensions of the standard. Rather than offer two processors, essentially every compiler eventually had at least an option to diagnose extensions.
2808) 10.058571, Grace Hopper - Wikipedia, the free encyclopedia.txt#16, term: computer, content:Her primary activity in this capacity was as a goodwill ambassador, lecturing widely on the early days of computers, her career, and on efforts that computer vendors could take to make life easier for their users. She visited a large fraction of Digital's engineering facilities, where she generally received a standing ovation at the conclusion of her remarks. Many people such as Admirals and Generals would ask her why satellite communication would take so long. So during many of her lectures, she illustrated a nanosecond using salvaged obsolete Bell System 25 pair telephone cable, cut it to 11.8inch (30cm) lengths, the distance that light travels in one nanosecond, and handed out the individual wires to her listeners. Although no longer a serving officer, she always wore her Navy full dress uniform to these lectures, which is allowed by US Navy uniform regulations.
2809) 10.058571, Graphics processing unit - Wikipedia, the free encyclopedia.txt#21, term: computer, content:Modern GPUs use most of their transistors to do calculations related to 3D computer graphics. They were initially used to accelerate the memory-intensive work of texture mapping and rendering polygons, later adding units to accelerate geometric calculations such as the rotation and translation of vertices into different coordinate systems. Recent developments in GPUs include support for programmable shaders which can manipulate vertices and textures with many of the same operations supported by CPUs, oversampling and interpolation techniques to reduce aliasing, and very high-precision color spaces. Because most of these computations involve matrix and vector operations, engineers and scientists have increasingly studied the use of GPUs for non-graphical calculations; they are especially suited to other embarrassingly parallel problems.
2810) 10.058571, Harvard Mark II - Wikipedia, the free encyclopedia.txt#2, term: computer, content:The Mark II was not a stored-program computer  it read an instruction of the program one at a time from a tape and executed it (like the Mark I). This separation of data and instructions is known as the Harvard architecture. The Mark II had a peculiar programming method that was devised to ensure that the contents of a register were available when needed. The tape containing the program could encode only eight instructions, so what a particular instruction code meant depended on when it was executed. Each second was divided up into several periods, and a coded instruction could mean different things in different periods. An addition could be started in any of eight periods in the second, a multiplication could be started in any of four periods of the second, and a transfer of data could be started in any of twelve periods of the second. Although this system worked, it made the programming complicated, and it reduced the efficiency of the machine somewhat.[1]
2811) 10.058571, Harwell CADET - Wikipedia, the free encyclopedia.txt#3, term: computer, content:The resulting machine was called CADET (Transistor Electronic Digital Automatic Computer  backwards). It first ran a simple test program in February 1955. CADET used 324 point-contact transistors provided by the UK company Standard Telephones and Cables, which were the only ones available in sufficient quantity when the project started; 76 junction transistors were used for the first stage amplifiers for data read from the drum, since point-contact transistors were too noisy. CADET was built from a few standardised designs of circuit boards which never got mounted into the planned desktop unit, so it was left in its breadboard form. From August 1956 CADET was offering a regular computing service, during which it often executed continuous computing runs of 80 hours or more.[4][5]
2812) 10.058571, History of computing hardware - Wikipedia, the free encyclopedia.txt#21, term: computer, content:There was to be a store, or memory, capable of holding 1,000 numbers of 40 decimal digits each (ca. 16.7 kB). An arithmetical unit, called the "mill", would be able to perform all four arithmetic operations, plus comparisons and optionally square roots. Initially it was conceived as a difference engine curved back upon itself, in a generally circular layout,[35] with the long store exiting off to one side. (Later drawings depict a regularized grid layout.)[36] Like the central processing unit (CPU) in a modern computer, the mill would rely upon its own internal procedures, roughly equivalent to microcode in modern CPUs, to be stored in the form of pegs inserted into rotating drums called "barrels", to carry out some of the more complex instructions the user's program might specify.[37]
2813) 10.058571, History of computing hardware - Wikipedia, the free encyclopedia.txt#23, term: computer, content:The machine was about a century ahead of its time. However, the project was slowed by various problems including disputes with the chief machinist building parts for it. All the parts for his machine had to be made by hand - this was a major problem for a machine with thousands of parts. Eventually, the project was dissolved with the decision of the British Government to cease funding. Babbage's failure to complete the analytical engine can be chiefly attributed to difficulties not only of politics and financing, but also to his desire to develop an increasingly sophisticated computer and to move ahead faster than anyone else could follow. Ada Lovelace, Lord Byron's daughter, translated and added notes to the "Sketch of the Analytical Engine" by Luigi Federico Menabrea. This appears to be the first published description of programming.[38]
2814) 10.058571, History of computing hardware - Wikipedia, the free encyclopedia.txt#38, term: computer, content:In 1941, Zuse followed his earlier machine up with the Z3,[52] the world's first working electromechanical programmable, fully automatic digital computer.[53] The Z3 was built with 2000 relays, implementing a 22bit word length that operated at a clock frequency of about 510Hz.[54] Program code and data were stored on punched film. It was quite similar to modern machines in some respects, pioneering numerous advances such as floating point numbers. Replacement of the hard-to-implement decimal system (used in Charles Babbage's earlier design) by the simpler binary system meant that Zuse's machines were easier to build and potentially more reliable, given the technologies available at that time.[55] The Z3 was probably a complete Turing machine. In two 1936 patent applications, Zuse also anticipated that machine instructions could be stored in the same storage used for datathe key insight of what became known as the von Neumann architecture, first implemented in the British SSEM of 1948.[56]
2815) 10.058571, History of computing hardware - Wikipedia, the free encyclopedia.txt#49, term: computer, content:Colossus was the world's first electronic digital programmable computer.[41] It used a large number of valves (vacuum tubes). It had paper-tape input and was capable of being configured to perform a variety of boolean logical operations on its data, but it was not Turing-complete. Nine Mk II Colossi were built (The Mk I was converted to a Mk II making ten machines in total). Colossus Mark I contained 1500 thermionic valves (tubes), but Mark II with 2400 valves, was both 5 times faster and simpler to operate than Mark 1, greatly speeding the decoding process. Mark 2 was designed while Mark 1 was being constructed. Allen Coombs took over leadership of the Colossus Mark 2 project when Tommy Flowers moved on to other projects.[72]
2816) 10.058571, History of computing hardware - Wikipedia, the free encyclopedia.txt#70, term: computer, content:The first commercial computer was the Ferranti Mark 1, built by Ferranti and delivered to the University of Manchester in February 1951. It was based on the Manchester Mark 1. The main improvements over the Manchester Mark 1 were in the size of the primary storage (using random access Williams tubes), secondary storage (using a magnetic drum), a faster multiplier, and additional instructions. The basic cycle time was 1.2 milliseconds, and a multiplication could be completed in about 2.16 milliseconds. The multiplier used almost a quarter of the machine's 4,050 vacuum tubes (valves).[90] A second machine was purchased by the University of Toronto, before the design was revised into the Mark 1 Star. At least seven of these later machines were delivered between 1953 and 1957, one of them to Shell labs in Amsterdam.[91]
2817) 10.058571, History of computing hardware - Wikipedia, the free encyclopedia.txt#96, term: computer, content:During the 1960s there was considerable overlap between second and third generation technologies.[134] IBM implemented its IBM Solid Logic Technology modules in hybrid circuits for the IBM System/360 in 1964. As late as 1975, Sperry Univac continued the manufacture of second-generation machines such as the UNIVAC 494. The Burroughs large systems such as the B5000 were stack machines, which allowed for simpler programming. These pushdown automatons were also implemented in minicomputers and microprocessors later, which influenced programming language design. Minicomputers served as low-cost computer centers for industry, business and universities.[135] It became possible to simulate analog circuits with the simulation program with integrated circuit emphasis, or SPICE (1971) on minicomputers, one of the programs for electronic design automation (EDA). The microprocessor led to the development of the microcomputer, small, low-cost computers that could be owned by individuals and small businesses. Microcomputers, the first of which appeared in the 1970s, became ubiquitous in the 1980s and beyond.
2818) 10.058571, Home computer - Wikipedia, the free encyclopedia.txt#24, term: computer, content:Processor clock rates were typically 12MHz for 6502 based CPU's and 24MHz for Z80 based systems (yielding roughly equal performance), but this aspect was not emphasized by users or manufacturers, as the systems' limited RAM capacity, graphics abilities and storage options had a more perceivable effect on performance than CPU speed. Clock rate was considered a technical detail of interest only to users needing accurate timing for their own programs. To economize on component cost, often the same crystal used to produce color television compatible signals was also divided down and used for the processor clock. This meant processors rarely operated at their full rated speed, and had the side-effect that European and North American versions of the same home computer operated at slightly different speeds and different video resolution due to different television standards.
2819) 10.058571, Home computer - Wikipedia, the free encyclopedia.txt#30, term: computer, content:Although modern operating systems include extensive programming libraries to ease development and promote standardization, home computer operating systems provided little support to application programs. Professionally written software often switched out the ROM based OS anyway to free the address space it occupied and maximize RAM capacity. This gave the program full control of the hardware and allowed the programmer to optimize performance for a specific task. Games would often turn off unused I/O ports, as well as the interrupts that served them. As multitasking was never common on home computers, this practice went largely unnoticed by users. Most software even lacked an exit command, requiring a reboot to use the system for something else.
2820) 10.058571, Home computer - Wikipedia, the free encyclopedia.txt#34, term: computer, content:After the first wave of game consoles and computers landed in American homes, the United States Federal Communications Commission (FCC) began receiving complaints of electromagnetic interference to television reception. By 1979 the FCC demanded that home computer makers submit samples for radio frequency interference testing. It was found that "first generation" home computers emitted too much radio frequency noise for household use. The Atari 400 and 800 were designed with heavy RF shielding to meet the new requirements. Between 1980 and 1982 regulations governing RF emittance from home computers were phased in.[41] Some companies appealed to the FCC to waive the requirements for home computers, while others (with compliant designs) objected to the waiver. Eventually techniques to suppress interference became standardized.[42]
2821) 10.058571, Honeywell, Inc. v. Sperry Rand Corp. - Wikipedia, the free encyclopedia.txt#4, term: computer, content:With 135 days of oral courtroom testimony by 77 witnessesand the presentation of the deposition of an additional 80 witnessesfor a total trial transcript of 20,667 pages, Honeywell v. Sperry Rand was at that time the longest trial in the history of the federal court system. It was preceded by six years of litigation that produced thousands of pages of under-oath depositions. 25,686 exhibits were marked by the court for plaintiff Honeywell; defendants Sperry Rand and its subsidiary Illinois Scientific Developments contributed 6,968 exhibits. The corporations on the two sides spent a combined more than $8 million pursuing the case. The resulting exhibits and testimony constitute a massive evidentiary record describing the invention and development of the electronic digital computer. Materials relevant to the case but not entered into evidence have appeared, but sparsely and infrequently, since the case's conclusion in 1973.
2822) 10.058571, HP 2100 - Wikipedia, the free encyclopedia.txt#15, term: computer, content:The HP 9810, 9820 and 9830 desktop computers used a slow, serialized TTL version of the 2116 CPU, although they did not ultimately use any of the operating system or application software, instead relying on user-friendly ROM-based interpreters such as BASIC which worked when powered up and integrated keyboards and displays rather than disks or standard terminals. In 1975, HP introduced the BPC, the world's first 16-bit microprocessor, using HP's NMOS-II process. The BPC was usually packaged in a ceramic hybrid module with the EMC and IOC chips, which added extended math and I/O instructions. The hybrid was developed as the heart of the new 9825 desktop computer. The later 9845 workstation added an MMU chip. These were the forerunners of personal computers and technical workstations.
2823) 10.058571, IBM AIX - Wikipedia, the free encyclopedia.txt#7, term: computer, content:Among other variants, IBM later produced AIX Version 3 (also known as AIX/6000), based on System V Release 3, for their POWER-based RS/6000 platform. Since 1990, AIX has served as the primary operating system for the RS/6000 series (later renamed IBM eServer pSeries, then IBM System p, and now IBM Power Systems). AIX Version 4, introduced in 1994, added symmetric multiprocessing with the introduction of the first RS/6000 SMP servers and continued to evolve through the 1990s, culminating with AIX 4.3.3 in 1999. Version 4.1, in a slightly modified form, was also the standard operating system for the Apple Network Server systems sold by Apple Computer to complement the Macintosh line.
2824) 10.058571, IBM System i - Wikipedia, the free encyclopedia.txt#28, term: computer, content:The IBM System i, then known as the AS/400, was the continuation of the System/38 database machine architecture (announced by IBM in October 1978 and delivered in August 1979). The AS/400 removed capability-based addressing.[4] The AS/400 added source compatibility with the System/36 combining the two primary computers manufactured by the IBM Rochester plant. The System/36 was IBM's most successful mini-computer but the architecture had reached its limit. The first AS/400 systems (known by the development code names Silverlake and Olympic) were delivered in 1988 under the tag line "Best of Both Worlds" and the product line has been refreshed continually since then. Guy Dehond from Inventive Designers was one of the beta-testers of Silverlake. The programmers who worked on OS/400, the operating system of the AS/400, did not have a UNIX background. Dr Frank Soltis, the chief architect, says that this is the main difference between this and any other operating system.
2825) 10.058571, Information technology - Wikipedia, the free encyclopedia.txt#8, term: computer, content:IBM introduced the first hard disk drive in 1956, as a component of their 305 RAMAC computer system.[20] Most digital data today is still stored magnetically on hard disks, or optically on media such as CD-ROMs.[21] Until 2002 most information was stored on analog devices, but that year digital storage capacity exceeded analog for the first time. As of 2007 almost 94% of the data stored worldwide was held digitally:[22] 52% on hard disks, 28% on optical devices and 11% on digital magnetic tape. It has been estimated that the worldwide capacity to store information on electronic devices grew from less than 3 exabytes in 1986 to 295 exabytes in 2007,[23] doubling roughly every 3 years.[24]
2826) 10.058571, Installation (computer programs) - Wikipedia, the free encyclopedia.txt#10, term: computer, content:Installation that is performed without user interaction during its progress or with no user present at all. One of the reasons to use this approach is to automate the installation of a large number of systems. An unattended installation either does not require the user to supply anything or has received all necessary input prior to the start of installation. Such input may be in the form of command line switches or an answer file, a file that contains all the necessary parameters. Windows XP and most Linux distributions are examples of operating systems that can be installed with an answer file. In unattended installation, it is assumed that there is no user to help mitigate errors. For instance, if the installation medium was faulty, the installer should fail the installation, as there is no user to fix the fault or replace the medium. Unattended installers may record errors in a computer log for later review.
2827) 10.058571, Intel - Wikipedia, the free encyclopedia.txt#1, term: computer, content:Intel Corporation (better known as Intel, stylized as intel) is an American multinational technology company headquartered in Santa Clara, California. Intel is one of the world's largest and highest valued semiconductor chip makers, based on revenue.[3] It is the inventor of the x86 series of microprocessors, the processors found in most personal computers. Intel supplies processors for computer system manufacturers such as Apple, Samsung, HP and Dell. Intel also makes motherboard chipsets, network interface controllers and integrated circuits, flash memory, graphics chips, embedded processors and other devices related to communications and computing. Intel Corporation was founded on July 18, 1968, by semiconductor pioneers Robert Noyce and Gordon Moore and widely associated with the executive leadership and vision of Andrew Grove, Intel combines advanced chip design capability with a leading-edge manufacturing capability.
2828) 10.058571, Intel 80486 - Wikipedia, the free encyclopedia.txt#17, term: computer, content:Early 486 machines were equipped with several ISA slots (using an emulated PC/AT-bus) and sometimes one or two 8-bitonly slots (compatible with the PC/XT-bus).[10] Many motherboards enabled overclocking of these up from the default 6 or 8MHz to perhaps 16.7 or 20MHz (half the i486 bus clock) in a number of steps, often from within the BIOS setup. Especially older peripheral cards normally worked well at such speeds as they often used standard MSI chips instead of slower (at the time) custom VLSI designs. This could give significant performance gains (such as for old video cards moved from a 386 or 286 computer, for example). However, operation beyond 8 or 10MHz could sometimes lead to stability problems, at least in systems equipped with SCSI or sound cards.
2829) 10.058571, Intel 80486 - Wikipedia, the free encyclopedia.txt#25, term: computer, content:In the general purpose desktop computer role, 486-based machines remained in use into the early-2000s, especially as Windows 95, Windows 98, and Windows NT 4.0 were the latest Microsoft operating systems to officially support installation on a 486-based system.[11][12] However, as Windows 95/98 and Windows NT 4.0 were eventually overtaken by newer operating systems, 486 systems likewise fell out of use. Still, a number of 486 machines have remained in use today, mostly for backward compatibility with older programs (most notably games), especially since many of them have problems running on newer operating systems. However, DOSBox is also available for current operating systems and provides emulation of the 486 instruction set, as well as full compatibility with most DOS-based programs.[13]
2830) 10.058571, Intel 8080 - Wikipedia, the free encyclopedia.txt#22, term: computer, content:Shortly after the launch of the 8080, the Motorola 6800 competing design was introduced, and after that, the MOS Technology 6502 variation of the 6800. Zilog introduced the Z80, which had a compatible machine-language instruction set and initially used the same assembly language as the 8080, but for legal reasons, Zilog developed a syntactically-different (but code compatible) alternative assembly language for the Z80. At Intel, the 8080 was followed by the compatible and electrically more elegant 8085, and later by the assembly language compatible 16-bit 8086 and then the 8/16-bit 8088, which was selected by IBM for its new PC to be launched in 1981. Later NEC made a NEC V20 (an 8088 clone with Intel 80186 instruction set compatibility) which also supported an 8080 emulation mode. This was also supported by NEC's V30 (a similarly enhanced 8086 clone). Thus, the 8080, via its ISA, made a lasting impact on computer history.
2831) 10.058571, Internet - Wikipedia, the free encyclopedia.txt#43, term: computer, content:World Wide Web browser software, such as Microsoft's Internet Explorer, Mozilla Firefox, Opera, Apple's Safari, and Google Chrome, lets users navigate from one web page to another via hyperlinks embedded in the documents. These documents may also contain any combination of computer data, including graphics, sounds, text, video, multimedia and interactive content that runs while the user is interacting with the page. Client-side software can include animations, games, office applications and scientific demonstrations. Through keyword-driven Internet research using search engines like Yahoo! and Google, users worldwide have easy, instant access to a vast and diverse amount of online information. Compared to printed media, books, encyclopedias and traditional libraries, the World Wide Web has enabled the decentralization of information on a large scale.
2832) 10.058571, Internet - Wikipedia, the free encyclopedia.txt#49, term: computer, content:Internet telephony is another common communications service made possible by the creation of the Internet. VoIP stands for Voice-over-Internet Protocol, referring to the protocol that underlies all Internet communication. The idea began in the early 1990s with walkie-talkie-like voice applications for personal computers. In recent years many VoIP systems have become as easy to use and as convenient as a normal telephone. The benefit is that, as the Internet carries the voice traffic, VoIP can be free or cost much less than a traditional telephone call, especially over long distances and especially for those with always-on Internet connections such as cable or ADSL. VoIP is maturing into a competitive alternative to traditional telephone service. Interoperability between different providers has improved and the ability to call or receive a call from a traditional telephone is available. Simple, inexpensive VoIP network adapters are available that eliminate the need for a personal computer.
2833) 10.058571, Interrupt - Wikipedia, the free encyclopedia.txt#2, term: computer, content:A software interrupt is caused either by an exceptional condition in the processor itself, or a special instruction in the instruction set which causes an interrupt when it is executed. The former is often called a trap or exception and is used for errors or events occurring during program execution that are exceptional enough that they cannot be handled within the program itself. For example, if the processor's arithmetic logic unit is commanded to divide a number by zero, this impossible demand will cause a divide-by-zero exception, perhaps causing the computer to abandon the calculation or display an error message. Software interrupt instructions function similarly to subroutine calls and are used for a variety of purposes, such as to request services from low-level system software such as device drivers. For example, computers often use software interrupt instructions to communicate with the disk controller to request reading or writing of data from and to the disk.
2834) 10.058571, Interrupt - Wikipedia, the free encyclopedia.txt#18, term: computer, content:Edge-triggered interrupts do not suffer the problems that level-triggered interrupts have with sharing. Service of a low-priority device can be postponed arbitrarily, and interrupts will continue to be received from the high-priority devices that are being serviced. If there is a device that the CPU does not know how to service, it may cause a spurious interrupt, or even periodic spurious interrupts, but it does not interfere with the interrupt signalling of the other devices. However, it is fairly easy for an edge triggered interrupt to be missed - for example if interrupts have to be masked for a period - and unless there is some type of hardware latch that records the event it is impossible to recover. Such problems caused many "lockups" in early computer hardware because the processor did not know it was expected to do something. More modern hardware often has one or more interrupt status registers that latch the interrupt requests; well written edge-driven interrupt software often checks such registers to ensure events are not missed.
2835) 10.058571, J. Presper Eckert - Wikipedia, the free encyclopedia.txt#3, term: computer, content:Dr. John Mauchly, then chairman of the physics department of nearby Ursinus College, was a student in the summer electronics course, and the following fall secured a teaching position at the Moore School. Mauchly's proposal for building an electronic digital computer using vacuum tubes, many times faster and more accurate than the differential analyzer for computing ballistics tables for artillery, caught the interest of the Moore School's Army liaison, Lieutenant Herman Goldstine, and on April 9, 1943 was formally presented in a meeting at Aberdeen Proving Ground to director Colonel Leslie Simon, Oswald Veblen, and others. A contract was awarded for Moore School's construction of the proposed computing machine, which would be named ENIAC, and Eckert was made the project's chief engineer. ENIAC was completed in late 1945 and was unveiled to the public in February, 1946.
2836) 10.058571, Jacquard loom - Wikipedia, the free encyclopedia.txt#6, term: computer, content:The Jacquard head used replaceable punched cards to control a sequence of operations. It is considered an important step in the history of computing hardware.[8] The ability to change the pattern of the loom's weave by simply changing cards was an important conceptual precursor to the development of computer programming and data entry. Charles Babbage knew of Jacquard looms and planned to use cards to store programs in his Analytical engine. In the late 19th century, Herman Hollerith took the idea of using punched cards to store information a step further when he created a punched card tabulating machine which he used to input data for the 1890 U.S. Census. A large, punched-card-based data processing industry developed in the first half of the twentieth century, dominated by the International Business Machine corporation (IBM), with its line of unit record equipment. The cards were used for data, however, with programming done by plugboards.
2837) 10.058571, JSTOR - Wikipedia, the free encyclopedia.txt#11, term: computer, content:In late 2010 and early 2011, Internet activist Aaron Swartz used MIT's data network to bulk-download a substantial portion of JSTOR's collection of academic journal articles.[19][20] When the bulk-download was discovered, JSTOR employees initially placed a video camera in the room to film the mysterious visitor and left the relevant computer untouched. Once video was captured of the visitor, the download was stopped and Swartz identified. Rather than pursue a civil lawsuit against him, in June 2011 they reached a settlement wherein he surrendered the downloaded data.[19][20] The articles that were downloaded by Aaron were from scholarly journals which published scientific papers largely funded by public universities and taxpayer money.
2838) 10.058571, Kermit (protocol) - Wikipedia, the free encyclopedia.txt#1, term: computer, content:The Kermit protocol supports text and binary file transfers on both full-duplex and half-duplex 8 bit and 7-bit serial connections in a system- and medium-independent fashion, and is implemented on hundreds of different computer and operating system platforms. On full-duplex connections, a Sliding Window Protocol is used with selective retransmission which provides excellent performance and error recovery characteristics. On 7-bit connections, locking shifts provide efficient transfer of 8-bit data. When properly implemented, as in the Columbia University Kermit Software collection, its authors claim performance is equal to or better than other protocols such as ZMODEM, YMODEM, and XMODEM, especially on poor connections.[1] On connections over RS-232 Statistical Multiplexers where some control characters cannot be transmitted,[citation needed] Kermit can be configured to work, unlike protocols like XMODEM that require the connection to be transparent (i.e. all 256 possible values of a byte to be transferable).
2839) 10.058571, Kermit (protocol) - Wikipedia, the free encyclopedia.txt#6, term: computer, content:By 1988 Kermit was available on more than 300 computers and operating systems.[9] The protocol became a de facto data communications standard[10] for transferring files between dissimilar computer systems, and by the early 1990s it could convert multilingual character encodings. Kermit software has been used in many countries, for tasks ranging from simple student assignments to solving compatibility problems aboard the International Space Station.[7] It was ported to a wide variety of mainframe, minicomputer and microcomputer systems down to handhelds and electronic pocket calculators. Most versions had a user interface based on the original TOPS-20 Kermit. Later versions of some Kermit implementations also support network as well as serial connections.
2840) 10.058571, Laptop - Wikipedia, the free encyclopedia.txt#1, term: computer, content:A laptop combines the components, inputs, outputs and capabilities of a desktop computer, including the display screen, speakers, a keyboard, pointing devices (such as a touchpad or trackpad), a processor and memory into a single unit. Most 2016-era laptops also have integrated webcams and built-in microphones. The device can be powered either from a rechargeable battery or by mains electricity from an AC adapter. The hardware specifications, such as the processor speed and memory capacity significantly vary between different types, makes, and models. Design elements, form factor, and construction can also vary significantly between models depending on intended use; examples of specialized models of laptops include rugged notebooks for use in construction or military applications, as well as low production cost offerings such as those from the One Laptop per Child organization, which incorporate features like solar charging and semi flexible components not found on most laptop computers.
2841) 10.058571, Laptop - Wikipedia, the free encyclopedia.txt#6, term: computer, content:The first laptops using the flip form factor appeared in the early 1980s. The Dulmont Magnum was released in Australia in 198182, but was not marketed internationally until 198485. The US$8,150 (US$19,980 today) GRiD Compass 1100, released in 1982, was used at NASA and by the military among others. The Gavilan SC, released in 1983, was the first computer described as a "laptop" by its manufacturer.[12] From 1983 onward, several new input techniques were developed and included in laptops, including the touchpad (Gavilan SC, 1983), the pointing stick (IBM ThinkPad 700, 1992) and handwriting recognition (Linus Write-Top,[13] 1987). Some CPUs, such as the 1990 Intel i386SL, were designed to use minimum power to increase battery life of portable computers and were supported by dynamic power management features such as Intel SpeedStep and AMD PowerNow! in some designs.
2842) 10.058571, Laptop - Wikipedia, the free encyclopedia.txt#62, term: computer, content:Because of their portability, laptops are subject to more wear and physical damage than desktops. Components such as screen hinges, latches, power jacks and power cords deteriorate gradually from ordinary use, and may have to be replaced. A liquid spill onto the keyboard, a rather minor mishap with a desktop system (given that a basic keyboard costs about US$20), can damage the internals of a laptop and result destroy the computer or result in a costly repair. One study found that a laptop is three times more likely to break during the first year of use than a desktop.[67] To maintain a laptop, it is recommended to clean a it every three months for dirt, debris, dust, and food particles. Most cleaning kits consist of a lint-free or Microfiber cloth for the LCD screen and keyboard, compressed air for getting dust out of the cooling fan, and cleaning solution. Harsh chemicals such as bleach should not be used to clean a laptop, as they can damage the it.[68]
2843) 10.058571, LEO (computer) - Wikipedia, the free encyclopedia.txt#6, term: computer, content:LEO I's clock speed was 500kHz, with most instructions taking about 1.5 ms to execute.[5][6] To be useful for business applications, the computer had to be able to handle a number of data streams, input and output, simultaneously. Therefore, its chief designer, Dr. John Pinkerton, designed the machine to have multiple input/output buffers. In the first instance, these were linked to fast paper tape readers and punches, fast punched card readers and punches, and a 100 line a minute tabulator. Later, other devices, including magnetic tape, were added. Its ultrasonic delay line memory based on tanks of mercury, with 2K (2048) 35-bit words (i.e., 8 kilobytes), was four times as large as that of EDSAC. The systems analysis was carried out by David Caminer.[7]
2844) 10.058571, LEO (computer) - Wikipedia, the free encyclopedia.txt#7, term: computer, content:Lyons used LEO I initially for valuation jobs, but its role was extended to include payroll, inventory, and so on. One of its early tasks was the elaboration of daily orders which were phoned in every afternoon by the shops and used to calculate the overnight production requirements, assembly instructions, delivery schedules, invoices, costings, and management reports. This was the first instance of an integrated management information system plus a computerised call centre.[citation needed] The LEO project was also a pioneer in outsourcing: in 1956, Lyons started doing the payroll calculations for Ford UK and others on the LEO I machine. The success of this led to the company dedicating one of its LEO II machines to bureau services. Later, the system was used for scientific computations as well. Met Office staff used a LEO I before the Met Office bought its own computer, a Ferranti Mercury, in 1959.[8]
2845) 10.058571, Linux - Wikipedia, the free encyclopedia.txt#11, term: computer, content:Torvalds began the development of the Linux kernel on MINIX and applications written for MINIX were also used on Linux. Later, Linux matured and further Linux kernel development took place on Linux systems.[37] GNU applications also replaced all MINIX components, because it was advantageous to use the freely available code from the GNU Project with the fledgling operating system; code licensed under the GNU GPL can be reused in other computer programs as long as they also are released under the same or a compatible license. Torvalds initiated a switch from his original license, which prohibited commercial redistribution, to the GNU GPL.[38] Developers worked to integrate GNU components with the Linux kernel, making a fully functional and free operating system.[32]
2846) 10.058571, Linux - Wikipedia, the free encyclopedia.txt#44, term: computer, content:The Linux kernel is a widely ported operating system kernel, available for devices ranging from mobile phones to supercomputers; it runs on a highly diverse range of computer architectures, including the hand-held ARM-based iPAQ and the IBM mainframes System z9 or System z10.[73] Specialized distributions and kernel forks exist for less mainstream architectures; for example, the ELKS kernel fork can run on Intel 8086 or Intel 80286 16-bit microprocessors, while the Clinux kernel fork may run on systems without a memory management unit. The kernel also runs on architectures that were only ever intended to use a manufacturer-created operating system, such as Macintosh computers (with both PowerPC and Intel processors), PDAs, video game consoles, portable music players, and mobile phones.
2847) 10.058571, Logarithm - Wikipedia, the free encyclopedia.txt#21, term: computer, content:Thus, log10(x) is related to the number of decimal digits of a positive integer x: the number of digits is the smallest integer strictly bigger than log10(x).[6] For example, log10(1430) is approximately 3.15. The next integer is 4, which is the number of digits of 1430. Both the natural logarithm and the logarithm to base two are used in information theory, corresponding to the use of nats or bits as the fundamental units of information, respectively.[7] Binary logarithms are also used in computer science, where the binary system is ubiquitous, in music theory, where a pitch ratio of two (the octave) is ubiquitous and the cent is the binary logarithm (scaled by 1200) of the ratio between two adjacent equally-tempered pitches, and in photography to measure exposure values.[8]
2848) 10.058571, Logic gate - Wikipedia, the free encyclopedia.txt#32, term: computer, content:Non-electronic implementations are varied, though few of them are used in practical applications. Many early electromechanical digital computers, such as the Harvard Mark I, were built from relay logic gates, using electro-mechanical relays. Logic gates can be made using pneumatic devices, such as the Sorteberg relay or mechanical logic gates, including on a molecular scale.[9] Logic gates have been made out of DNA (see DNA nanotechnology)[10] and used to create a computer called MAYA (see MAYA II). Logic gates can be made from quantum mechanical effects (though quantum computing usually diverges from boolean design). Photonic logic gates use non-linear optical effects.
2849) 10.058571, Logic in computer science - Wikipedia, the free encyclopedia.txt#6, term: computer, content:Another important application of logic to computer technology has been in the area of Frame languages and automatic classifiers. Frame languages such as KL-ONE have a rigid semantics. Definitions in KL-ONE can be directly mapped to set theory and the predicate calculus. This allows specialized theorem provers called classifiers to analyze the various declarations between sets, subsets, and relations in a given model. In this way the model can be validated and any inconsistent definitions flagged. The classifier can also infer new information, for example define new sets based on existing information and change the definition of existing sets based on new data. The level of flexibility is ideal for handling the ever changing world of the Internet. Classifier technology is built on top of languages such as the Web Ontology Language to allow a logical semantic level on to the existing Internet. This layer of is called the Semantic web.[12][13]
2850) 10.058571, Mac OS - Wikipedia, the free encyclopedia.txt#17, term: computer, content:The OS X architectural legacy is the successor to Mac OS 9 and the "classic" Mac OS legacy. It is however a Unix operating system, based on the NeXTSTEP operating system which Apple acquired after purchasing NeXT Computer  with its CEO Steve Jobs returning to Apple at that time. OS X also makes use of the BSD codebase and the XNU kernel.[9] There have been twelve significant releases of OS X, the most recent being OS X 10.11, referred to as "El Capitan". Prior to 10.11 came OS X 10.10 "Yosemite", 10.9 "Mavericks", 10.8 "Mountain Lion", 10.7 "Lion", 10.6 "Snow Leopard", 10.5 "Leopard", 10.4 "Tiger", 10.3 "Panther", 10.2 "Jaguar", 10.1 ("Puma"), and 10.0 ("Cheetah").
2851) 10.058571, Machine learning - Wikipedia, the free encyclopedia.txt#9, term: computer, content:However, an increasing emphasis on the logical, knowledge-based approach caused a rift between AI and machine learning. Probabilistic systems were plagued by theoretical and practical problems of data acquisition and representation.[11]:488 By 1980, expert systems had come to dominate AI, and statistics was out of favor.[12] Work on symbolic/knowledge-based learning did continue within AI, leading to inductive logic programming, but the more statistical line of research was now outside the field of AI proper, in pattern recognition and information retrieval.[11]:708710; 755 Neural networks research had been abandoned by AI and computer science around the same time. This line, too, was continued outside the AI/CS field, as "connectionism", by researchers from other disciplines including Hopfield, Rumelhart and Hinton. Their main success came in the mid-1980s with the reinvention of backpropagation.[11]:25
2852) 10.058571, Magnetic-core memory - Wikipedia, the free encyclopedia.txt#8, term: computer, content:In April 2011, Forrester recalled, "the Wang use of cores did not have any influence on my development of random-access memory. The Wang memory was expensive and complicated. As I recall, which may not be entirely correct, it used two cores per binary bit and was essentially a delay line that moved a bit forward. To the extent that I may have focused on it, the approach was not suitable for our purposes." He describes the invention and associated events, in 1975.[7] Forrester has since observed, "It took us about seven years to convince the industry that random-access magnetic-core memory was the solution to a missing link in computer technology. Then we spent the following seven years in the patent courts convincing them that they had not all thought of it first."[8]
2853) 10.058571, Magnetic-core memory - Wikipedia, the free encyclopedia.txt#14, term: computer, content:Core memory was part of a family of related technologies, now largely forgotten, which exploited the magnetic properties of materials to perform switching and amplification. By the 1950s, vacuum-tube electronics was well developed and very sophisticated, but tubes had a limited lifetime, used much more power, and were much larger than semiconductor or magnetic technology, and their operating characteristics changed over their lifetimes. Magnetic devices had many of the virtues of the discrete and integrated solid-state devices that would replace them, and were extensively used in military applications. A notable example was the portable (truck-based) MOBIDIC computer developed by Sylvania for the United States Army Signal Corps in the late 1950s. The contents of electronic memory were lost when power was disconnected, but core memory was non-volatile and kept its contents.
2854) 10.058571, Magnetic-core memory - Wikipedia, the free encyclopedia.txt#15, term: computer, content:The term "core" comes from conventional transformers whose windings surround a magnetic core. In core memory, the wires pass once through any given corethey are single-turn devices. The properties of materials used for memory cores are dramatically different from those used in power transformers. The magnetic material for a core memory requires a high degree of magnetic remanence, the ability to stay highly magnetized, and a low coercitivity so that less energy is required to change the magnetization direction. The core can take two states, encoding one bit, which can be read when "selected" by a "sense wire". The core memory contents are retained even when the memory system is powered down (non-volatile memory). However, when the core is read, it is reset to a "zero" value. Circuits in the computer memory system then restore the information in an immediate re-write cycle.
2855) 10.058571, Magnetic-core memory - Wikipedia, the free encyclopedia.txt#16, term: computer, content:The most common form of core memory, X/Y line coincident-current, used for the main memory of a computer, consists of a large number of small toroidal ferromagnetic ceramic ferrites (cores) held together in a grid structure (organized as a "stack" of layers called planes), with wires woven through the holes in the cores' centers. In early systems there were four wires: X, Y, Sense, and Inhibit, but later cores combined the latter two wires into one Sense/Inhibit line. Each toroid stored one bit (0 or 1). One bit in each plane could be accessed in one cycle, so each machine word in an array of words was spread over a "stack" of planes. Each plane would manipulate one bit of a word in parallel, allowing the full word to be read or written in one cycle.
2856) 10.058571, Magnetic-core memory - Wikipedia, the free encyclopedia.txt#27, term: computer, content:Another form of core memory called core rope memory provided read-only storage. In this case, the cores, which had more linear magnetic materials, were simply used as transformers; no information was actually stored magnetically within the individual cores. Each bit of the word had one core. Reading the contents of a given memory address generated a pulse of current in a wire corresponding to that address. Each address wire was threaded either through a core to signify a binary [1], or around the outside of that core, to signify a binary [0]. As expected, the cores were much larger physically than those of read-write core memory. This type of memory was exceptionally reliable. An example was the Apollo Guidance Computer used for the moon landings.
2857) 10.058571, Mainframe computer - Wikipedia, the free encyclopedia.txt#4, term: computer, content:Software upgrades usually require setting up the operating system or portions thereof, and are non-disruptive only when using virtualizing facilities such as IBM's z/OS and Parallel Sysplex, or Unisys's XPCL, which support workload sharing so that one system can take over another's application while it is being refreshed. Mainframes are defined by high availability, one of the main reasons for their longevity, since they are typically used in applications where downtime would be costly or catastrophic. The term reliability, availability and serviceability (RAS) is a defining characteristic of mainframe computers. Proper planning and implementation is required to exploit these features, and if improperly implemented, may serve to inhibit the benefits provided. In addition, mainframes are more secure than other computer types: the NIST vulnerabilities database, US-CERT, rates traditional mainframes such as IBM zSeries, Unisys Dorado and Unisys Libra as among the most secure with vulnerabilities in the low single digits as compared with thousands for Windows, Unix, and Linux.[5]
2858) 10.058571, Manchester Mark 1 - Wikipedia, the free encyclopedia.txt#22, term: computer, content:The successful operation of the Manchester Mark 1 and its predecessor, the SSEM, was widely reported in the British press, which used the phrase "electronic brain" to describe the machines.[28] Lord Louis Mountbatten had earlier introduced that term in a speech delivered to the British Institution of Radio Engineers on 31 October 1946, in which he speculated about how the primitive computers then available might evolve.[29] The excitement surrounding the reporting in 1949 of what was the first recognisably modern computer provoked a reaction unexpected by its developers; Sir Geoffrey Jefferson, professor of neurosurgery at the University of Manchester, on being asked to deliver the Lister Oration on 9June 1949 chose "The Mind of Mechanical Man" as his subject. His purpose was to "debunk" the Manchester project.[30] In his address he said:
2859) 10.058571, Manchester Small-Scale Experimental Machine - Wikipedia, the free encyclopedia.txt#6, term: computer, content:The construction of a von Neumann computer depended on the availability of a suitable memory device on which to store the program. During the Second World War researchers working on the problem of removing the clutter from radar signals had developed a form of delay line memory, the first practical application of which was the mercury delay line,[9] developed by J. Presper Eckert. Radar transmitters send out regular brief pulses of radio energy, the reflections from which are displayed on a CRT screen. As operators are usually interested only in moving targets, it was desirable to filter out any distracting reflections from stationary objects. The filtering was achieved by comparing each received pulse with the previous pulse, and rejecting both if they were identical, leaving a signal containing only the images of any moving objects. To store each received pulse for later comparison it was passed through a transmission line, delaying it by exactly the time between transmitted pulses.[10]
2860) 10.058571, Matrix (mathematics) - Wikipedia, the free encyclopedia.txt#93, term: computer, content:An empty matrix is a matrix in which the number of rows or columns (or both) is zero.[69][70] Empty matrices help dealing with maps involving the zero vector space. For example, if A is a 3-by-0 matrix and B is a 0-by-3 matrix, then AB is the 3-by-3 zero matrix corresponding to the null map from a 3-dimensional space V to itself, while BA is a 0-by-0 matrix. There is no common notation for empty matrices, but most computer algebra systems allow creating and computing with them. The determinant of the 0-by-0 matrix is 1 as follows from regarding the empty product occurring in the Leibniz formula for the determinant as 1. This value is also consistent with the fact that the identity map from any finite dimensional space to itself has determinant1, a fact that is often used as a part of the characterization of determinants.
2861) 10.058571, Microcomputer - Wikipedia, the free encyclopedia.txt#7, term: computer, content:Monitors, keyboards and other devices for input and output may be integrated or separate. Computer memory in the form of RAM, and at least one other less volatile, memory storage device are usually combined with the CPU on a system bus in one unit. Other devices that make up a complete microcomputer system include batteries, a power supply unit, a keyboard and various input/output devices used to convey information to and from a human operator (printers, monitors, human interface devices). Microcomputers are designed to serve only one user at a time, although they can often be modified with software or hardware to concurrently serve more than one user. Microcomputers fit well on or under desks or tables, so that they are within easy access of users. Bigger computers like minicomputers, mainframes, and supercomputers take up large cabinets or even dedicated rooms.
2862) 10.058571, Microcomputer - Wikipedia, the free encyclopedia.txt#9, term: computer, content:Although they did not contain any microprocessors, but were built around transistor-transistor logic (TTL), Hewlett-Packard calculators as far back as 1968 had various levels of programmability such that could be called microcomputers. The HP 9100B (1968) had rudimentary conditional (if) statements, statement line numbers, jump statements (go to), registers that could be used as variables, and primitive subroutines. The programming language resembled assembly language in many ways. Later models incrementally added more features, including the BASIC programming language (HP 9830A in 1971). Some models had tape storage and small printers. However, displays were limited to one line at a time. [1] The HP 9100A was referred to as a personal computer in an advertisement in a 1968 Science magazine,[10] but that advertisement was quickly dropped.[11] It is suspected[who?] that HP was reluctant to call them "computers" because it would complicate government procurement and export procedures.[citation needed]
2863) 10.058571, Microprocessor - Wikipedia, the free encyclopedia.txt#17, term: computer, content:Thousands of items that were traditionally not computer-related include microprocessors. These include large and small household appliances, cars (and their accessory equipment units), car keys, tools and test instruments, toys, light switches/dimmers and electrical circuit breakers, smoke alarms, battery packs, and hi-fi audio/visual components (from DVD players to phonograph turntables). Such products as cellular telephones, DVD video system and HDTV broadcast systems fundamentally require consumer devices with powerful, low-cost, microprocessors. Increasingly stringent pollution control standards effectively require automobile manufacturers to use microprocessor engine management systems, to allow optimal control of emissions over widely varying operating conditions of an automobile. Non-programmable controls would require complex, bulky, or costly implementation to achieve the results possible with a microprocessor.
2864) 10.058571, Microprocessor - Wikipedia, the free encyclopedia.txt#50, term: computer, content:Another early single-chip 16-bit microprocessor was TI's TMS 9900, which was also compatible with their TI-990 line of minicomputers. The 9900 was used in the TI 990/4 minicomputer, the TI-99/4A home computer, and the TM990 line of OEM microcomputer boards. The chip was packaged in a large ceramic 64-pin DIP package, while most 8-bit microprocessors such as the Intel 8080 used the more common, smaller, and less expensive plastic 40-pin DIP. A follow-on chip, the TMS 9980, was designed to compete with the Intel 8080, had the full TI 990 16-bit instruction set, used a plastic 40-pin package, moved data 8bits at a time, but could only address 16KB. A third chip, the TMS 9995, was a new design. The family later expanded to include the 99105 and 99110.
2865) 10.058571, Microprocessor - Wikipedia, the free encyclopedia.txt#56, term: computer, content:The world's first single-chip fully 32-bit microprocessor, with 32-bit data paths, 32-bit buses, and 32-bit addresses, was the AT&T Bell Labs BELLMAC-32A, with first samples in 1980, and general production in 1982.[42][43] After the divestiture of AT&T in 1984, it was renamed the WE 32000 (WE for Western Electric), and had two follow-on generations, the WE 32100 and WE 32200. These microprocessors were used in the AT&T 3B5 and 3B15 minicomputers; in the 3B2, the world's first desktop super microcomputer; in the "Companion", the world's first 32-bit laptop computer; and in "Alexander", the world's first book-sized super microcomputer, featuring ROM-pack memory cartridges similar to today's gaming consoles. All these systems ran the UNIX System V operating system.
2866) 10.058571, Microprocessor - Wikipedia, the free encyclopedia.txt#62, term: computer, content:When National Semiconductor decided to leave the Unix market, the chip was redesigned into the Swordfish Embedded processor with a set of on chip peripherals. The chip turned out to be too expensive for the laser printer market and was killed. The design team went to Intel and there designed the Pentium processor, which is very similar to the NS32764 core internally. The big success of the Series 32000 was in the laser printer market, where the NS32CG16 with microcoded BitBlt instructions had very good price/performance and was adopted by large companies like Canon. By the mid-1980s, Sequent introduced the first SMP server-class computer using the NS 32032. This was one of the design's few wins, and it disappeared in the late 1980s. The MIPS R2000 (1984) and R3000 (1989) were highly successful 32-bit RISC microprocessors. They were used in high-end workstations and servers by SGI, among others. Other designs included the Zilog Z80000, which arrived too late to market to stand a chance and disappeared quickly.
2867) 10.058571, Microprocessor - Wikipedia, the free encyclopedia.txt#70, term: computer, content:The first commercial RISC microprocessor design was released in 1984 by MIPS Computer Systems, the 32-bit R2000 (the R1000 was not released). In 1986, HP released its first system with a PA-RISC CPU. In 1987 in the non-Unix Acorn computers' 32-bit, then cache-less, ARM2-based Acorn Archimedes became the first commercial success using the ARM architecture, then known as Acorn RISC Machine (ARM); first silicon ARM1 in 1985. The R3000 made the design truly practical, and the R4000 introduced the world's first commercially available 64-bit RISC microprocessor. Competing projects would result in the IBM POWER and Sun SPARC architectures. Soon every major vendor was releasing a RISC design, including the AT&T CRISP, AMD 29000, Intel i860 and Intel i960, Motorola 88000, DEC Alpha.
2868) 10.058571, Microsoft Windows - Wikipedia, the free encyclopedia.txt#1, term: computer, content:Microsoft introduced an operating environment named Windows on November 20, 1985, as a graphical operating system shell for MS-DOS in response to the growing interest in graphical user interfaces (GUIs).[4] Microsoft Windows came to dominate the world's personal computer market with over 90% market share, overtaking Mac OS, which had been introduced in 1984. However, since 2012, because of the massive growth of smartphones, Windows sells less than Android, which became the most popular operating system in 2014, when counting all of the computing platforms each operating system runs on; in 2014, the number of Windows devices sold were less than 25% of Android devices sold. However, comparisons across different markets are not fully relevant; and for personal computers, Windows is still the most popular operating system.
2869) 10.058571, Microsoft Windows - Wikipedia, the free encyclopedia.txt#6, term: computer, content:The history of Windows dates back to September 1981, when Chase Bishop, a computer scientist, designed the first model of an electronic device and project Interface Manager was started. It was announced in November 1983 (after the Apple Lisa, but before the Macintosh) under the name "Windows", but Windows 1.0 was not released until November 1985.[8] Windows 1.0 was to compete with Apple's operating system, but achieved little popularity. Windows 1.0 is not a complete operating system; rather, it extends MS-DOS. The shell of Windows 1.0 is a program known as the MS-DOS Executive. Components included Calculator, Calendar, Cardfile, Clipboard viewer, Clock, Control Panel, Notepad, Paint, Reversi, Terminal and Write. Windows 1.0 does not allow overlapping windows. Instead all windows are tiled. Only modal dialog boxes may appear over other windows.
2870) 10.058571, Microsoft Windows - Wikipedia, the free encyclopedia.txt#20, term: computer, content:At retail, Windows XP was now marketed in two main editions: the "Home" edition was targeted towards consumers, while the "Professional" edition was targeted towards business environments and power users, and included additional security and networking features. Home and Professional were later accompanied by the "Media Center" edition (designed for home theater PCs, with an emphasis on support for DVD playback, TV tuner cards, DVR functionality, and remote controls), and the "Tablet PC" edition (designed for mobile devices meeting its specifications for a tablet computer, with support for stylus pen input and additional pen-enabled applications).[26][27][28] Mainstream support for Windows XP ended on April 14, 2009. Extended support ended on April 8, 2014.[29]
2871) 10.058571, Microsoft Windows - Wikipedia, the free encyclopedia.txt#31, term: computer, content:Windows NT included support for several different platforms before the x86-based personal computer became dominant in the professional world. Windows NT 4.0 and its predecessors supported PowerPC, DEC Alpha and MIPS R4000. (Although some these platforms implement 64-bit computing, the operating system treated them as 32-bit.) However, Windows 2000, the successor of Windows NT 4.0, dropped support for all platforms except the third generation x86 (known as IA-32) or newer in 32-bit mode. The client line of Window NT family still runs on IA-32, although the Windows Server line has ceased supporting this platform with the release of Windows Server 2008 R2.
2872) 10.058571, Microsoft Windows - Wikipedia, the free encyclopedia.txt#50, term: computer, content:A study conducted by Kevin Mitnick and marketing communications firm Avantgarde in 2004, found that an unprotected and unpatched Windows XP system with Service Pack 1 lasted only 4 minutes on the Internet before it was compromised, and an unprotected and also unpatched Windows Server 2003 system was compromised after being connected to the internet for 8 hours.[66] The computer that was running Windows XP Service Pack 2 was not compromised. The AOL National Cyber Security Alliance Online Safety Study of October 2004, determined that 80% of Windows users were infected by at least one spyware/adware product.[citation needed] Much documentation is available describing how to increase the security of Microsoft Windows products. Typical suggestions include deploying Microsoft Windows behind a hardware or software firewall, running anti-virus and anti-spyware software, and installing patches as they become available through Windows Update.[67]
2873) 10.058571, MIPS instruction set - Wikipedia, the free encyclopedia.txt#51, term: computer, content:In 1984 Hennessy was convinced of the future commercial potential of the design, and left Stanford to form MIPS Computer Systems. They released their first design, the R2000, in 1985, improving the design as the R3000 in 1988. These 32-bit CPUs formed the basis of their company through the 1980s, used primarily in SGI's series of workstations and later Digital Equipment Corporation DECstation workstations and servers. The SGI commercial designs deviated from the Stanford academic research by implementing most of the interlocks in hardware, supplying full multiply and divide instructions (among others). The designs were guided, in part, by software architect Earl Killian who designed the MIPS III 64-bit instruction-set extension, and led the work on the R4000 microarchitecture.[17][18]
2874) 10.058571, MOS Technology 6502 - Wikipedia, the free encyclopedia.txt#22, term: computer, content:The low clock frequency moderated the speed requirement of memory and peripherals attached to the CPU, as only about 50 percent of the clock cycle was available for memory access (due to the asynchronous design, this percentage varied strongly among chip versions). This was critical at a time when affordable memory had access times in the range 250 - 450 ns. The original NMOS 6502 was minimalistically engineered and efficiently manufactured and therefore cheapan important factor in getting design wins in the very price-sensitive game console and home computer markets. Like its precursor, the Motorola 6800, the 6502 has very few registers. At the time the processor was designed, the number of transistors that could be economically put on a chip was very constrained (around a few thousand), so it made sense to rely on RAM instead of allocating expensive NMOS chip area for CPU registers.
2875) 10.058571, Motorola 6800 - Wikipedia, the free encyclopedia.txt#25, term: computer, content:The first working MC6800 chips were produced in February 1974 and engineering samples were given to select customers. Hewlett Packard in Loveland, Colorado wanted the MC6800 for a new desktop calculator and had a prototype system working by June.[45][46] The MC6800 used a new single-voltage N-channel MOS process that proved to be very difficult to implement. The M6800 microcomputer system was finally in production by November 1974. Motorola matched Intel's price for single microprocessor, $360.[47][48] (The IBM System/360 was a well-known computer at this time.) In April 1975 the MEK6800D1 microcomputer design kit was offered for $300. The kit included all six chips in the M6800 family plus application and programming manuals.[49] The price of a single MC6800 microprocessor was $175.
2876) 10.058571, Motorola 6800 - Wikipedia, the free encyclopedia.txt#40, term: computer, content:The MC6801 was a single-chip microcomputer with a 6802 CPU with 128bytes of RAM, a 2KB ROM, a 16-bit timer, 31 programmable parallel I/O lines, and a serial port. It could also use the I/O lines as data and address buses to connect to standard M6800 peripherals. The 6801 would execute 6800 code but it had ten additional instructions and the execution time of key instructions was reduced. The two 8-bit accumulators could act as a single 16-bit accumulator for double precision addition, subtraction and multiplication.[78] It was initially designed for automotive use with General Motors as the lead customer. The first application was a trip computer for the 1978 Cadillac Seville.[79] This 35,000 transistor chip was too expensive for wide-scale adoption in automobiles so a reduced function MC6805 single-chip microcomputer was designed.
2877) 10.058571, Motorola 68000 - Wikipedia, the free encyclopedia.txt#1, term: computer, content:The 68000 grew out of the MACSS (Motorola Advanced Computer System on Silicon) project, begun in 1976 to develop an entirely new architecture without backward compatibility. It would be a higher-power sibling complementing the existing 8-bit 6800 line rather than a compatible successor. In the end, the 68000 did retain a bus protocol compatibility mode for existing 6800 peripheral devices, and a version with an 8-bit data bus was produced. However, the designers mainly focused on the future, or forward compatibility, which gave the 68000 platform a head start against later 32-bit instruction set architectures. For instance, the CPU registers are 32bits wide, though few self-contained structures in the processor itself operate on 32bits at a time. The MACSS team drew heavily on the influence of minicomputer processor design, such as the PDP-11 and VAX systems, which were similarly microcoded.
2878) 10.058571, MS-DOS - Wikipedia, the free encyclopedia.txt#27, term: computer, content:Prior to 1995, Microsoft licensed MS-DOS (and Windows) to computer manufacturers under three types of agreement: per-processor (a fee for each system the company sold), per-system (a fee for each system of a particular model), or per-copy (a fee for each copy of MS-DOS installed). The largest manufacturers used the per-processor arrangement, which had the lowest fee. This arrangement made it expensive for the large manufacturers to migrate to any other operating system, such as DR DOS. In 1991, the U.S. government Federal Trade Commission began investigating Microsoft's licensing procedures, resulting in a 1994 settlement agreement limiting Microsoft to per-copy licensing. Digital Research did not gain by this settlement, and years later its successor in interest, Caldera, sued Microsoft for damages in the Caldera v. Microsoft lawsuit. It was believed that the settlement ran in the order of $150m, but was revealed in November 2009 with the release of the Settlement Agreement to be $280m.[35]
2879) 10.058571, Multiprocessing - Wikipedia, the free encyclopedia.txt#24, term: computer, content:MIMD does raise issues of deadlock and resource contention, however, since threads may collide in their access to resources in an unpredictable way that is difficult to manage efficiently. MIMD requires special coding in the operating system of a computer but does not require application changes unless the programs themselves use multiple threads (MIMD is transparent to single-threaded programs under most operating systems, if the programs do not voluntarily relinquish control to the OS). Both system and user software may need to use software constructs such as semaphores (also called locks or gates) to prevent one thread from interfering with another if they should happen to cross paths in referencing the same data. This gating or locking process increases code complexity, lowers performance, and greatly increases the amount of testing required, although not usually enough to negate the advantages of multiprocessing.
2880) 10.058571, Netbook - Wikipedia, the free encyclopedia.txt#6, term: computer, content:The OLPC project, known for its innovation in producing a durable, cost- and power-efficient netbook for developing countries, is regarded as one of the major factors that led top computer hardware manufacturers to begin creating low-cost netbooks for the consumer market.[20] When the first Asus Eee PC sold over 300,000 units in four months, companies such as Dell and Acer took note and began producing their own inexpensive netbooks. And while the OLPC XO-1 targets a different audience than do the other manufacturers' netbooks, it appears that OLPC is now facing competition. Developing countries now have a large choice of vendors, from which they can choose which low-cost netbook they prefer.[21]
2881) 10.058571, Netbook - Wikipedia, the free encyclopedia.txt#12, term: computer, content:Having peaked at about 20% of the portable computer market, netbooks started to slightly lose market share (within the category) in early 2010, coinciding with the appearance and success of the iPad.[32] Technology commentator Ross Rubin argued two and a half years later in Engadget that "Netbooks never got any respect. While Steve Jobs rebuked the netbook at the iPad's introduction, the iPad owes a bit of debt to the little laptops. The netbook demonstrated the potential of an inexpensive, portable second computing device, with a screen size of about 10 inches, intended primarily for media consumption and light productivity."[33] Although some manufacturers directly blamed competition from the iPad, some analysts pointed out that larger, fully fledged laptops had entered the price range of netbooks at about the same time.[34]
2882) 10.058571, Operating system - Wikipedia, the free encyclopedia.txt#28, term: computer, content:The introduction of the Intel 80386 CPU chip in October 1985,[9] with 32-bit architecture and paging capabilities, provided personal computers with the ability to run multitasking operating systems like those of earlier minicomputers and mainframes. Microsoft responded to this progress by hiring Dave Cutler, who had developed the VMS operating system for Digital Equipment Corporation. He would lead the development of the Windows NT operating system, which continues to serve as the basis for Microsoft's operating systems line. Steve Jobs, a co-founder of Apple Inc., started NeXT Computer Inc., which developed the NEXTSTEP operating system. NEXTSTEP would later be acquired by Apple Inc. and used, along with code from FreeBSD as the core of Mac OS X.
2883) 10.058571, Operating system - Wikipedia, the free encyclopedia.txt#43, term: computer, content:Chrome OS is an operating system based on the Linux kernel and designed by Google. It is developed out in the open in the Chromium OS open source variant and Google makes a proprietary variant of it (similar to the split for the Chrome and Chromium browser). Since Chromium OS targets computer users who spend most of their time on the Internet, it is mainly a web browser with limited ability to run local applications, though it has a built-in file manager and media player (in later versions, (modified) Android apps have also been supported, since the browser has been made to support them). Instead, it relies on Internet applications (or Web apps) used in the web browser to accomplish tasks such as word processing.[16] Chromium OS differs from Chrome OS in that Chromium is open-source and used primarily by developers whereas Chrome OS is the operating system shipped out in Chromebooks.[17]
2884) 10.058571, OS X - Wikipedia, the free encyclopedia.txt#49, term: computer, content:Finder is a file browser allowing quick access to all areas of the computer, which has been modified throughout subsequent releases of Mac OS X.[110][111] Quick Look is part of Mac OS X Leopard's Finder. It allows for dynamic previews of files, including videos and multi-page documents, without opening their parent applications. Spotlight search technology, which is integrated into the Finder since Mac OS X Tiger, allows rapid real-time searches of data files; mail messages; photos; and other information based on item properties (meta data) and/or content.[112][113] Mac OS X makes use of a Dock, which holds file and folder shortcuts as well as minimized windows.
2885) 10.058571, Package manager - Wikipedia, the free encyclopedia.txt#7, term: computer, content:Particularly troublesome with software upgrades are upgrades of configuration files. Since package managers, at least on Unix systems, originated as extensions of file archiving utilities, they can usually only either overwrite or retain configuration files, rather than applying rules to them. There are exceptions to this that usually apply to kernel configuration (which, if broken, will render the computer unusable after a restart). Problems can be caused if the format of configuration files changes. For instance, if the old configuration file does not explicitly disable new options that should be disabled. Some package managers, such as Debian's dpkg, allow configuration during installation. In other situations, it is desirable to install packages with the default configuration and then overwrite this configuration, for instance, in headless installations to a large number of computers. This kind of pre-configured installation is also supported by dpkg.
2886) 10.058571, Pascal's calculator - Wikipedia, the free encyclopedia.txt#72, term: computer, content:This detail is not described in the two surviving Schickard's letters and drawings but as these were merely notes this cannot be taken to mean that he was misleading Kepler when he stated that he had built such a machine and it worked. As Schickard noted "...Arithmeticum organum alias delineabo accuratius, nunc et festinate hoc have" or, in English: "..I will describe the computer more precisely some other time, now I don't have enough time."[51] Amongst the detail omitted could well have been the dtente. The role of a dtente catchment was widely understood by clockmakers, and as Schickard had turned to clockmakers to construct his machine, it is highly likely that this type of approach would have been included before the finalisation of his machine.
2887) 10.058571, Personal computer - Wikipedia, the free encyclopedia.txt#2, term: computer, content:Early computer owners usually had to write their own programs to do anything useful with the machines, which even did not include an operating system. The very earliest microcomputers, equipped with a front panel, required hand-loading of a bootstrap program to load programs from external storage (paper tape, cassettes, or eventually diskettes). Before very long, automatic booting from permanent read-only memory became universal. Today's users have access to a wide range of commercial software, freeware and free and open-source software, which are provided in ready-to-run or ready-to-compile form. Software for personal computers, such as applications and video games, are typically developed and distributed independently from the hardware or OS manufacturers, whereas software for many mobile phones and other portable systems is approved and distributed through a centralized online store.[1][2]
2888) 10.058571, Personal computer - Wikipedia, the free encyclopedia.txt#23, term: computer, content:The global personal computer shipments were 350.9 million units in 2010,[19] 308.3 million units in 2009[20] and 302.2 million units in 2008.[21][22] The shipments were 264 million units in the year 2007, according to iSuppli,[23] up 11.2% from 239 million in 2006.[24] In 2004, the global shipments were 183 million units, an 11.6% increase over 2003.[25] In 2003, 152.6 million computers were shipped, at an estimated value of $175 billion.[26] In 2002, 136.7 million PCs were shipped, at an estimated value of $175 billion.[26] In 2000, 140.2 million personal computers were shipped, at an estimated value of $226 billion.[26] Worldwide shipments of personal computers surpassed the 100-million mark in 1999, growing to 113.5 million units from 93.3 million units in 1998.[27] In 1999, Asia had 14.1 million units shipped.[28]
2889) 10.058571, Personal computer - Wikipedia, the free encyclopedia.txt#42, term: computer, content:One of the drawbacks of laptops is that, due to the size and configuration of components, usually relatively little can be done to upgrade the overall computer from its original design. Internal upgrades are either not manufacturer-recommended, can damage the laptop if done with poor care or knowledge, or in some cases impossible, making the desktop PC more modular. Some internal upgrades, such as memory and hard disk drive upgrades are often easily performed, while a display or keyboard upgrade is usually impossible. Just as desktops, laptops also have the same possibilities for connecting to a wide variety of devices, including external displays, mice, cameras, storage devices and keyboards, which may be attached externally through USB ports and other less common ports such as external video. Laptops are also a little more expensive compared to desktops, as the components for laptops themselves are expensive.
2890) 10.058571, Plan 9 from Bell Labs - Wikipedia, the free encyclopedia.txt#4, term: computer, content:Plan 9 replaced Unix as Bell Labs's primary platform for operating systems research. It explored several changes to the original Unix model that facilitate the use and programming of the system, notably in distributed multi-user environments. After several years of development and internal use, Bell Labs shipped the operating system to universities in 1992. Three years later, in 1995, Plan 9 was made available for commercial parties by AT&T via the book publisher Harcourt Brace. With source licenses costing $350, AT&T targeted the embedded systems market rather than the computer market at large; Ritchie commented that the developers did not expect to do "much displacement" given how established other operating systems had become.[9]
2891) 10.058571, Platform game - Wikipedia, the free encyclopedia.txt#12, term: computer, content:Home consoles of America's early 1980s generally lack hardware support for background scrolling  except for the Atari 2600 (with only vertical scrolling), Atari 5200 and Emerson Arcadia 2001, and notwithstanding Japan's Famicom. This makes it very difficult to produce a smooth scrolling effect on a console. Nevertheless, Sydney Development released B.C.'s Quest For Tires in 1983 on the ColecoVision and several home computer platforms.[15] The game features large, smooth-scrolling levels and simplistic platform gameplay in which players jump over oncoming pitfalls and obstacles, much like Moon Patrol. Not long after this, a scrolling platform game appeared on the Commodore 64 and Atari 800 computers called Snokie. It began to bridge the gap between these earlier scrolling arcade-style games and implements a more mature vision of the genre, with uneven terrain and an emphasis on precision jumping.
2892) 10.058571, Platform game - Wikipedia, the free encyclopedia.txt#30, term: computer, content:The first attempts to bring platform games into 3D used 2D graphics and an isometric perspective. These games are nearly as old as the genre itself, one of the earliest examples being Sega's Congo Bongo in 1983. The first platformers to simulate a 3D perspective and moving camera emerged in the early-mid-1980s. An early example of this was Konami's platform game Antarctic Adventure,[48] where the player controls a penguin in a forward-scrolling third-person perspective while having to jump over pits and obstacles.[48][49][50] Originally released in 1983 for the MSX computer, it was subsequently ported to various platforms the following year,[50] including an arcade version,[48] NES,[50] and ColecoVision.[49] That same year, I, Robot, though not a platformer, featured filled 3D polygonal graphics, flat shading, and camera control options, which were not widely adopted by platformers until the 1990s.
2893) 10.058571, Platform game - Wikipedia, the free encyclopedia.txt#33, term: computer, content:The earliest example of a true 3D platformer is a French computer game called Alpha Waves, created by Christophe de Dinechin and published by Infogrames in 1990 for the Atari ST, Amiga, and PC.[58][59] It featured full-screen 3D graphics, true 3D movement, and a movable camera, all firsts for the genre. The environments were abstract, with simple gameplay focused on hopping from trampoline-like platforms. The game was released in North America by Data East` under the name Continuum. Much like Jump Bug before it, while it is believed to be the first of its kind, it is not widely recognized as especially influential, though it is sometimes regarded as a precursor to Jumping Flash!.[60] Though its appearance was distinct from the popular 2D platformers of the day, it was billed as a platform game on its packaging.[61]
2894) 10.058571, Program counter - Wikipedia, the free encyclopedia.txt#4, term: computer, content:In a typical central processing unit (CPU), the PC is a digital counter (which is the origin of the term "program counter") that may be one of many registers in the CPU hardware. The instruction cycle[4] begins with a fetch, in which the CPU places the value of the PC on the address bus to send it to the memory. The memory responds by sending the contents of that memory location on the data bus. (This is the stored-program computer model, in which executable instructions are stored alongside ordinary data in memory, and handled identically by it[5]). Following the fetch, the CPU proceeds to execution, taking some action based on the memory contents that it obtained. At some point in this cycle, the PC will be modified so that the next instruction executed is a different one (typically, incremented so that the next instruction is the one starting at the memory address immediately following the last memory location of the current instruction).
2895) 10.058571, Random-access memory - Wikipedia, the free encyclopedia.txt#5, term: computer, content:The first practical form of random-access memory was the Williams tube starting in 1947. It stored data as electrically charged spots on the face of a cathode ray tube. Since the electron beam of the CRT could read and write the spots on the tube in any order, memory was random access. The capacity of the Williams tube was a few hundred to around a thousand bits, but it was much smaller, faster, and more power-efficient than using individual vacuum tube latches. Developed at the University of Manchester in England, the Williams tube provided the medium on which the first electronically stored-memory program was implemented in the Manchester Small-Scale Experimental Machine (SSEM) computer, which first successfully ran a program on 21 June 1948.[4] In fact, rather than the Williams tube memory being designed for the SSEM, the SSEM was a testbed to demonstrate the reliability of the memory.[5][6]
2896) 10.058571, Random-access memory - Wikipedia, the free encyclopedia.txt#9, term: computer, content:The two widely used forms of modern RAM are static RAM (SRAM) and dynamic RAM (DRAM). In SRAM, a bit of data is stored using the state of a six transistor memory cell. This form of RAM is more expensive to produce, but is generally faster and requires less dynamic power than DRAM. In modern computers, SRAM is often used as cache memory for the CPU. DRAM stores a bit of data using a transistor and capacitor pair, which together comprise a DRAM memory cell. The capacitor holds a high or low charge (1 or 0, respectively), and the transistor acts as a switch that lets the control circuitry on the chip read the capacitor's state of charge or change it. As this form of memory is less expensive to produce than static RAM, it is the predominant form of computer memory used in modern computers.
2897) 10.058571, Random-access memory - Wikipedia, the free encyclopedia.txt#21, term: computer, content:Most modern operating systems employ a method of extending RAM capacity, known as "virtual memory". A portion of the computer's hard drive is set aside for a paging file or a scratch partition, and the combination of physical RAM and the paging file form the system's total memory. (For example, if a computer has 2 GB of RAM and a 1 GB page file, the operating system has 3 GB total memory available to it.) When the system runs low on physical memory, it can "swap" portions of RAM to the paging file to make room for new data, as well as to read previously swapped information back into RAM. Excessive use of this mechanism results in thrashing and generally hampers overall system performance, mainly because hard drives are far slower than RAM.
2898) 10.058571, Random-access memory - Wikipedia, the free encyclopedia.txt#32, term: computer, content:A different concept is the processor-memory performance gap, which can be addressed by 3D computer chips that reduce the distance between the logic and memory aspects that are further apart in a 2D chip.[13] Memory subsystem design requires a focus on the gap, which is widening over time.[14] The main method of bridging the gap is the use of caches; small amounts of high-speed memory that houses recent operations and instructions nearby the processor, speeding up the execution of those operations or instructions in cases where they are called upon frequently. Multiple levels of caching have been developed in order to deal with the widening of the gap, and the performance of high-speed modern computers are reliant on evolving caching techniques.[15] These can prevent the loss of processor performance, as it takes less time to perform the computation it has been initiated to complete.[16] There can be up to a 53% difference between the growth in speed of processor speeds and the lagging speed of main memory access.[17]
2899) 10.058571, Royal Radar Establishment - Wikipedia, the free encyclopedia.txt#0, term: computer, content:The name Royal Radar Establishment was given to the existing Radar Research Establishment following a visit by Queen Elizabeth II in 1957. Both names were abbreviated to RRE. The establishment had been formed, under its first name, in 1953 by merging the Telecommunications Research Establishment (TRE) and the Radar Research and Development Establishment (RRDE). These had worked on airborne and ground based radar, respectively.[1][2] RRE was located in Malvern, Worcestershire, where both TRE and RRDE had been housed at different sites. The earlier research and development work of TRE and RRDE on radar, solid state physics, electronics, and computer hardware and software was continued in the merged establishment, and the overall scope was extended to include cryogenics and other topics. Infrared detection for guided missiles and heat sensing devices was a major defence application.
2900) 10.058571, RS-232 - Wikipedia, the free encyclopedia.txt#1, term: computer, content:An RS-232 serial port was once a standard feature of a personal computer, used for connections to modems, printers, mice, data storage, uninterruptible power supplies, and other peripheral devices. However, RS-232 is hampered by low transmission speed, large voltage swing, and large standard connectors. In modern personal computers, USB has displaced RS-232 from most of its peripheral interface roles. Many computers do not come equipped with RS-232 ports and must use either an external USB-to-RS-232 converter or an internal expansion card with one or more serial ports to connect to RS-232 peripherals. Nevertheless, RS-232 devices are still used, especially in industrial machines, networking equipment, and scientific instruments.
2901) 10.058571, Scripting language - Wikipedia, the free encyclopedia.txt#21, term: computer, content:Many large application programs include an idiomatic scripting language tailored to the needs of the application user. Likewise, many computer game systems use a custom scripting language to express the programmed actions of non-player characters and the game environment. Languages of this sort are designed for a single application; and, while they may superficially resemble a specific general-purpose language (e.g. QuakeC, modeled after C), they have custom features that distinguish them. Emacs Lisp, while a fully formed and capable dialect of Lisp, contains many special features that make it most useful for extending the editing functions of Emacs. An application-specific scripting language can be viewed as a domain-specific programming language specialized to a single application.
2902) 10.058571, SCSI - Wikipedia, the free encyclopedia.txt#1, term: computer, content:SCSI is derived from "SASI", the "Shugart Associates System Interface", developed circa 1978 and publicly disclosed in 1981.[2] A SASI controller provided a bridge between a hard disk drive's low-level interface and a host computer, which needed to read blocks of data. SASI controller boards were typically the size of a hard disk drive and were usually physically mounted to the drive's chassis. SASI, which was used in mini- and early microcomputers, defined the interface as using a 50-pin flat ribbon connector which was adopted as the first-generation SCSI (SCSI-1) connector. SASI is a fully compliant subset of SCSI-1 so that many, if not all, of the then-existing SASI controllers were SCSI-1 compatible.[3]
2903) 10.058571, Semi-Automatic Ground Environment - Wikipedia, the free encyclopedia.txt#1, term: computer, content:The processing power behind SAGE was supplied by the largest computer ever built, the AN/FSQ-7. Each SAGE Direction Center (DC) housed an FSQ-7 which occupied an entire floor, approximately 22,000 square feet not including supporting equipment. Information was fed to the DC's from a network of radar stations as well as readiness information from various defence sites. The computers, based on the raw radar data, developed "tracks" for the reported targets, and automatically calculated which defences were within range. Operators used light guns to select targets onscreen for further information, select one of the available defences, and issue commands to attack. These commands would then be automatically sent to the defence site via teleprinter.
2904) 10.058571, Semi-Automatic Ground Environment - Wikipedia, the free encyclopedia.txt#36, term: computer, content:A SAGE System ergonomic test at Luke AFB in 1964 "showed conclusively that the wrong timing of human and technical operations was leading to frequent truncation of the flight path tracking system" (Harold Sackman).[47]:9 SAGE software development was "grossly underestimated"[21]:370 (60,000 lines in September 1955):[69] "the biggest mistake [of] the SAGE computer program was [underestimating the] jump from the 35,000 [WWI] instructions  to the more than 100,000 instructions on the" AN/FSQ-8.[70] NORAD conducted a Sage/Missile Master Integration/ECM-ECCM Test in 1963,[71] and although SAGE used AMIS input of air traffic information, the 1959 plan developed by the July 1958 USAF Air Defense Systems Integration Division[6] for SAGE Air Traffic Integration (SATIN) was cancelled by the DoD.[72]
2905) 10.058571, Semi-Automatic Ground Environment - Wikipedia, the free encyclopedia.txt#42, term: computer, content:SAGE histories include a 1983 special issue of the Annals of the History of Computing,[81] and various personal histories were published, e.g., Valley in 1985[82] and Jacobs in 1986.[83] In 1998, the SAGE System was identified as 1 of 4 "Monumental Projects",[84] and a SAGE lecture presented the vintage film In Your Defense followed by anecdotal information from Les Earnest, Jim Wong, and Paul Edwards.[30] In 2013, a copy of a 1950s cover girl image programmed for SAGE display was identified as the "earliest known figurative computer art".[5] Company histories identifying employees' roles in SAGE include the 1981 System Builders: The Story of SDC[85] and the 1998 Architects of Information Advantage: The MITRE Corporation Since 1958.[86]
2906) 10.058571, Server (computing) - Wikipedia, the free encyclopedia.txt#0, term: computer, content:In computing, a server is a computer program or a device that provides functionality for other programs or devices, called "clients". This architecture is called the clientserver model, and a single overall computation is distributed across multiple processes or devices. Servers can provide various functionalities, often called "services", such as sharing data or resources among multiple clients, or performing computation for a client. A single server can serve multiple clients, and a single client can use multiple servers. A client process may run on the same device or may connect over a network to a server on a different device.[1] Typical servers are database servers, file servers, mail servers, print servers, web servers, game servers, and application servers.[2]
2907) 10.058571, Silicon - Wikipedia, the free encyclopedia.txt#48, term: computer, content:Pure monocrystalline silicon is produced silicon wafers used in the semiconductor industry, in electronics, and in some high-cost and high-efficiency photovoltaic applications. Pure silicon is an intrinsic semiconductor, which means that unlike metals, it conducts electron holes and electrons released from atoms by heat; silicon's electrical conductivity increases with higher temperatures. Pure silicon has too low a conductivity (i.e., too high a resistivity) to be used as a circuit element in electronics. In practice, pure silicon is doped with small concentrations of certain other elements, which greatly increase its conductivity and adjust its electrical response by controlling the number and charge (positive or negative) of activated carriers. Such control is necessary for transistors, solar cells, semiconductor detectors, and other semiconductor devices used in the computer industry and other technical applications. In silicon photonics, silicon can be used as a continuous wave Raman laser medium to produce coherent light, though it is ineffective as an everyday light source.
2908) 10.058571, SIMD - Wikipedia, the free encyclopedia.txt#5, term: computer, content:All of these developments have been oriented toward support for real-time graphics, and are therefore oriented toward processing in two, three, or four dimensions, usually with vector lengths of between two and sixteen words, depending on data type and architecture. When new SIMD architectures need to be distinguished from older ones, the newer architectures are then considered "short-vector" architectures, as earlier SIMD and vector supercomputers had vector lengths from 64 to 64,000. A modern supercomputer is almost always a cluster of MIMD machines, each of which implements (short-vector) SIMD instructions. A modern desktop computer is often a multiprocessor MIMD machine where each processor can execute short-vector SIMD instructions.
2909) 10.058571, SIMD - Wikipedia, the free encyclopedia.txt#16, term: computer, content:Apple Computer had somewhat more success, even though they entered the SIMD market later than the rest. AltiVec offered a rich system and can be programmed using increasingly sophisticated compilers from Motorola, IBM and GNU, therefore assembly language programming is rarely needed. Additionally, many of the systems that would benefit from SIMD were supplied by Apple itself, for example iTunes and QuickTime. However, in 2006, Apple computers moved to Intel x86 processors. Apple's APIs and development tools (XCode) were modified to support SSE2 and SSE3 as well as AltiVec. Apple was the dominant purchaser of PowerPC chips from IBM and Freescale Semiconductor and even though they abandoned the platform, further development of AltiVec is continued in several Power Architecture designs from Freescale and IBM. On WWDC '15, Apple announced SIMD Vectors support for version 2.0 of their new Swift programming language Swift.
2910) 10.058571, Slide rule - Wikipedia, the free encyclopedia.txt#67, term: computer, content:The availability of mainframe computing did not however significantly affect the ubiquitous use of the slide rule until cheap hand held electronic calculators for scientific and engineering purposes became available in the mid-1970s, at which point it rapidly declined. The first included the Wang Laboratories LOCI-2,[24][25] introduced in 1965, which used logarithms for multiplication and division and the Hewlett-Packard hp 9100A, introduced in 1968.[26] The hp9100A had trigonometric functions (sin, cos, tan) in addition to exponentials and logarithms. It used the CORDIC (coordinate rotation digital computer) algorithm,[27] which allows for calculation of trigonometric functions using only shift and add operations. This method facilitated the development of ever smaller scientific calculators.
2911) 10.058571, Smartphone - Wikipedia, the free encyclopedia.txt#0, term: computer, content:A smartphone is a mobile phone with an advanced mobile operating system which combines features of a personal computer operating system with other features useful for mobile or handheld use.[1][2][3] Smartphones, which are usually pocket-sized, typically combine the features of a cell phone, such as the ability to receive and make phone calls, with those of other popular digital mobile devices. Other features typically include a personal digital assistant (PDA) for making appointments in a calendar, media player, video games, GPS navigation unit, digital camera and digital video camera. Most smartphones can access the Internet and can run third-party software applications ("apps"). They typically have a color touchscreen user interface that covers 70% or more of the front surface, with LCD, OLED, AMOLED, LED or similar screen.
2912) 10.058571, Software bug - Wikipedia, the free encyclopedia.txt#13, term: computer, content:In software development projects, a "mistake" or "fault" may be introduced at any stage during development. Bugs are a consequence of the nature of human factors in the programming task. They arise from oversights or mutual misunderstandings made by a software team during specification, design, coding, data entry and documentation. For example, in creating a relatively simple program to sort a list of words into alphabetical order, one's design might fail to consider what should happen when a word contains a hyphen. Perhaps, when converting the abstract design into the chosen programming language, one might inadvertently create an off-by-one error and fail to sort the last word in the list. Finally, when typing the resulting program into the computer, one might accidentally type a "<" where a ">" was intended, perhaps resulting in the words being sorted into reverse alphabetical order.
2913) 10.058571, Spreadsheet - Wikipedia, the free encyclopedia.txt#19, term: computer, content:LANPAR was used by Bell Canada, AT&T and the 18 operating telcos nationwide for their local and national budgeting operations. LANPAR was also used by General Motors. Its uniqueness was Pardo's co-invention incorporating forward referencing/natural order calculation (one of the first "non-procedural" computer languages) [12] as opposed to left-to-right, top to bottom sequence for calculating the results in each cell that was used by VisiCalc, Supercalc, and the first version of Multiplan. Without forward referencing/natural order calculation, the user had to manually recalculate the spreadsheet as many times as necessary until the values in all the cells had stopped changing. Forward Referencing/Natural Order Calculation by a compiler was the cornerstone functionality required for any spreadsheet to be practical and successful.
2914) 10.058571, Stan Frankel - Wikipedia, the free encyclopedia.txt#2, term: computer, content:Frankel published a number of scientific papers throughout his career. Some of them explored the use of statistical sampling techniques and machine driven solutions. In a 1947 paper in Physical Review, he and Metropolis predicted the utility of computers in replacing manual integration with iterative summation as a problem solving technique. As head of a new Caltech digital computing group he worked with PhD candidate Berni Alder in 19491950 to develop what is now known as called Monte Carlo analysis. They used techniques that Enrico Fermi had pioneered in the 1930s. Due to a lack of local computing resources, Frankel travelled to England in 1950 to run Alder's project on the Manchester Mark 1 computer. Unfortunately, Alder's thesis advisor was unimpressed, so Alder and Frankel delayed publication of their results until 1955, in the Journal of Chemical Physics. This left the major credit for the technique to a parallel project by a team including Teller and Metropolis who published similar work in the same journal in 1953.
2915) 10.058571, Submarine - Wikipedia, the free encyclopedia.txt#33, term: computer, content:The Royal Navy Submarine Service was used primarily in the classic Axis blockade. Its major operating areas were around Norway, in the Mediterranean (against the Axis supply routes to North Africa), and in the Far East. In that war, British submarines sank 2million tons of enemy shipping and 57major warships, the latter including 35submarines. Among these is the only documented instance of a submarine sinking another submarine while both were submerged. This occurred when HMSVenturer engaged the U864; the Venturer crew manually computed a successful firing solution against a three-dimensionally maneuvering target using techniques which became the basis of modern torpedo computer targeting systems. Seventy-four British submarines were lost,[26] the majority, 42, in the Mediterranean.
2916) 10.058571, Supercomputer - Wikipedia, the free encyclopedia.txt#15, term: computer, content:As the price, performance and energy efficiency of general purpose graphic processors (GPGPUs) have improved,[36] a number of petaflop supercomputers such as Tianhe-I and Nebulae have started to rely on them.[37] However, other systems such as the K computer continue to use conventional processors such as SPARC-based designs and the overall applicability of GPGPUs in general-purpose high-performance computing applications has been the subject of debate, in that while a GPGPU may be tuned to score well on specific benchmarks, its overall applicability to everyday algorithms may be limited unless significant effort is spent to tune the application towards it.[38][39] However, GPUs are gaining ground and in 2012 the Jaguar supercomputer was transformed into Titan by retrofitting CPUs with GPUs.[40][41][42]
2917) 10.058571, Supercomputer - Wikipedia, the free encyclopedia.txt#35, term: computer, content:Quasi-opportunistic supercomputing is a form of distributed computing whereby the super virtual computer of many networked geographically disperse computers performs computing tasks that demand huge processing power.[80] Quasi-opportunistic supercomputing aims to provide a higher quality of service than opportunistic grid computing by achieving more control over the assignment of tasks to distributed resources and the use of intelligence about the availability and reliability of individual systems within the supercomputing network. However, quasi-opportunistic distributed execution of demanding parallel computing software in grids should be achieved through implementation of grid-wise allocation agreements, co-allocation subsystems, communication topology-aware allocation mechanisms, fault tolerant message passing libraries and data pre-conditioning.[80]
2918) 10.058571, Tablet computer - Wikipedia, the free encyclopedia.txt#20, term: computer, content:In 2013, Samsung announced a tablet running Android and Windows 8 operating systems concurrently; switching from one operating system to the other and vice versa does not require restarting the device, and data can be synchronized between the two operating systems.[52] The device, named ATIV Q, was scheduled for release in late 2013 but its release has been indefinitely delayed.[53] [needs update]Acer presented its first tablet computer during its global press conference in New York on 23 November 2010. The family which is called Acer Iconia also includes a big screen smartphone called Iconia Smart. The Iconia series displays utilize Gorilla Glass.[54] Meanwhile, Asus released its Transformer Book Trio, a tablet that is also capable of running the operating systems Windows 8 and Android.[55]
2919) 10.058571, Telecommunication - Wikipedia, the free encyclopedia.txt#60, term: computer, content:At the transport layer, most communication adopts either the Transmission Control Protocol (TCP) or the User Datagram Protocol (UDP). TCP is used when it is essential every message sent is received by the other computer whereas UDP is used when it is merely desirable. With TCP, packets are retransmitted if they are lost and placed in order before they are presented to higher layers. With UDP, packets are not ordered or retransmitted if lost. Both TCP and UDP packets carry port numbers with them to specify what application or process the packet should be handled by.[86] Because certain application-level protocols use certain ports, network administrators can manipulate traffic to suit particular requirements. Examples are to restrict Internet access by blocking the traffic destined for a particular port or to affect the performance of certain applications by assigning priority.
2920) 10.058571, Teleprinter - Wikipedia, the free encyclopedia.txt#43, term: computer, content:In early operating systems such as Digital's RT-11, serial communication lines were often connected to teleprinters and were given device names starting with tt. This and similar conventions were adopted by many other operating systems. Unix and Unix-like operating systems use the prefix tty, for example /dev/tty13, or pty (for pseudo-tty), such as /dev/ptya0. In many computing contexts, "TTY" has become the name for any text terminal, such as an external console device, a user dialing in to the system on a modem on a serial port device, a printing or graphical computer terminal on a computer's serial port or the RS-232 port on a USB-to-RS-232 converter attached to a computer's USB port, or even a terminal emulator application in the window system using a pseudo terminal device.
2921) 10.058571, Texas Instruments - Wikipedia, the free encyclopedia.txt#1, term: computer, content:Texas Instruments was founded in 1951.[8] It emerged after a reorganization of Geophysical Service, a company that manufactured equipment for use in the seismic industry as well as defense electronics. TI began research in transistors in the early 1950s and produced the world's first commercial silicon transistor. In 1954, Texas Instruments designed and manufactured the first transistor radio and Jack Kilby invented the integrated circuit in 1958 while working at TI's Central Research Labs. The company produced the first integrated circuit-based computer for the U.S. Air Force in 1961. TI researched infrared technology in the late 1950s and later made radar systems as well as guidance and control systems for both missiles and bombs. The hand-held calculator was introduced to the world by TI in 1967.
2922) 10.058571, Texas Instruments - Wikipedia, the free encyclopedia.txt#35, term: computer, content:Texas Instruments was active in the 1980s in the area of artificial intelligence. In addition to ongoing developments in speech and signal processing and recognition, it developed and sold the Explorer computer family of LISP machines. For the Explorer a special 32-bit LISP microprocessor was developed, which was used in the Explorer II and the TI MicroExplorer (a LISP Machine on a NuBus board for the Apple Macintosh). AI application software developed by TI for the Explorer included the Gate Assignment system for United Airlines, described as "an artificial intelligence program that captures the combined experience and knowledge of a half-dozen United operations experts." In software for the PC, they introduced "Personal Consultant" a rule-based expert system development tool and runtime engine, followed by "Personal Consultant Plus" written in the LISP-like language from MIT known as Scheme, and the natural language menu system NLMenu.
2923) 10.058571, Text editor - Wikipedia, the free encyclopedia.txt#7, term: computer, content:When computer terminals with video screens became available, screen-based text editors (sometimes called just "screen editors") became common. One of the earliest full-screen editors was O26, which was written for the operator console of the CDC 6000 series computers in 1967. Another early full-screen editor was vi. Written in the 1970s, it is still a standard editor[5] on Unix and Linux operating systems. Emacs, one of the first open source and free software projects, is another early full-screen or real-time editor, one that was ported to many systems.[6] A full-screen editor's ease-of-use and speed (compared to the line-based editors) motivated many early purchases of video terminals.[7]
2924) 10.058571, Tom Kilburn - Wikipedia, the free encyclopedia.txt#10, term: computer, content:Kilburn's next project, known as Atlas, aimed to create a fast computer by making maximum use of existing and new technologies. The project was backed by Ferranti and a 300,000 grant from the National Research Development Corporation.[2] It incorporated numerous technologies and techniques such as "multiprogramming, job scheduling, spooling, interrupts, pipelining, interleaved storage, autonomous transfer units, virtual storage and paging  though none of these techniques had been invented when the project started in 1956."[2] Other innovations included read only memory and a compiler-compiler.[12] The greatest innovation was virtual memory, which allowed the drum storage to be treated as if it were core.[13][14] Three of them were built, and installed at Manchester University, the University of London and the Rutherford Laboratory.[2]
2925) 10.058571, Touchscreen - Wikipedia, the free encyclopedia.txt#13, term: computer, content:Until 1988 touchscreens had the bad reputation of being imprecise. Most user interface books would state that touchscreens selections were limited to targets larger than the average finger. At the time, selections were done in such a way that a target was selected as soon as the finger came over it, and the corresponding action was performed immediately. Errors were common, due to parallax or calibration problems, leading to frustration. A new strategy called "lift-off strategy"[21] was introduced by researchers at the University of Maryland Human  Computer Interaction Lab and is still used today. As users touch the screen, feedback is provided as to what will be selected, users can adjust the position of the finger, and the action takes place only when the finger is lifted off the screen. This allowed the selection of small targets, down to a single pixel on a VGA screen (standard best of the time).
2926) 10.058571, Tuncer ?ren - Wikipedia, the free encyclopedia.txt#2, term: computer, content:ren started his working life in the industry in 1963 as Systems Engineer for IBM Trk in Istanbul, where he coordinated effort in the textile industry, and in education. In 1970, he started his academic career as assistant Professor at the Computer Science Department of the University of Ottawa, where in 1981 he became Full Professor. he has been visiting professor at National Space Activities Commission of Brazil, Sao Jose dos Campos, So Paulo, Brazil in 1971; at the Institute of Systems Sciences, Johannes Kepler University, Linz, Austria in 1983; at the Middle East Technical University, Ankara, Turkey in 1983 and 1991; at the University of Vienna, Austria in 1984-85; and at the Universit Paul Czanne  Aix Marseille 3, Marseille since 2004.[2]
2927) 10.058571, Turing completeness - Wikipedia, the free encyclopedia.txt#8, term: computer, content:In the late 19th century, Leopold Kronecker formulated notions of computability, defining primitive recursive functions. These functions can be calculated by rote computation, but they are not enough to make a universal computer, because the instructions which compute them do not allow for an infinite loop. In the early 20th century, David Hilbert led a program to axiomatize all of mathematics with precise axioms and precise logical rules of deduction which could be performed by a machine. Soon, it became clear that a small set of deduction rules are enough to produce the consequences of any set of axioms. These rules were proved by Kurt Gdel in 1930 to be enough to produce every theorem. However, they will always prove some theorems as both true and false, for an axiomatization not simpler than Peano arithmetic.
2928) 10.058571, Turing completeness - Wikipedia, the free encyclopedia.txt#10, term: computer, content:The first result of computability theory is that it is impossible in general to predict what a Turing-complete program will do over an arbitrarily long time. For example, it is impossible to determine for every program-input pair whether the program, operating on the input, will eventually stop or will continue forever (see halting problem). It is impossible to determine whether the program will return "true" or whether it will return "false". For any characteristic of the program's eventual output, it is impossible to determine whether this characteristic will hold. This can cause problems in practice when analyzing real-world computer programs. One way to avoid this is to cause programs to stop executing after a fixed period of time (timeout), or to limit the power of flow control instructions. Such systems are not Turing complete by design.
2929) 10.058571, University of Manchester - Wikipedia, the free encyclopedia.txt#11, term: computer, content:Before the merger, Victoria University of Manchester and UMIST counted 23 Nobel Prize winners amongst their former staff and students. Manchester has traditionally been strong in the sciences; it is where the nuclear nature of the atom was discovered by Rutherford, and the world's first stored-program computer was built at the university. Famous scientists associated with the university include physicists Osborne Reynolds, Niels Bohr, Ernest Rutherford, James Chadwick, Arthur Schuster, Hans Geiger, Ernest Marsden and Balfour Stewart. The university has contributed in other fields, such as by the work of mathematicians Paul Erds, Horace Lamb and Alan Turing; author Anthony Burgess; philosophers Samuel Alexander, Ludwig Wittgenstein and Alasdair MacIntyre; the Pritzker Prize and RIBA Stirling Prize-winning architect Norman Foster and composer Peter Maxwell Davies all attended, or worked in, Manchester.
2930) 10.058571, University of Pennsylvania - Wikipedia, the free encyclopedia.txt#44, term: computer, content:Penn's arts and science programs are all well regarded, with many departments ranked among the nation's top 10. At the undergraduate level, Wharton, Penn's business school, and Penn's nursing school have maintained their No. 1, 2 or 3 rankings since U.S. News began reviewing such programs.[citation needed] The College of Arts and Sciences' English Department is also consistently ranked in the top five humanities programs in the country, ranking 4th in the most current US News report. In the School of Engineering, top departments are bioengineering (typically ranked in the top 5 by U.S. News), mechanical engineering, chemical engineering and nanotechnology.[citation needed] The school is also strong in some areas of computer science and artificial intelligence.
2931) 10.058571, Unix - Wikipedia, the free encyclopedia.txt#9, term: computer, content:The microkernel concept was introduced in an effort to reverse the trend towards larger kernels and return to a system in which most tasks were completed by smaller utilities. In an era when a standard computer consisted of a hard disk for storage and a data terminal for input and output (I/O), the Unix file model worked quite well, as most I/O was linear. However, modern systems include networking and other new devices. As graphical user interfaces developed, the file model proved inadequate to the task of handling asynchronous events such as those generated by a mouse. In the 1980s, non-blocking I/O and the set of inter-process communication mechanisms were augmented with Unix domain sockets, shared memory, message queues, and semaphores. In microkernel implementations, functions such as network protocols could be moved out of the kernel, while conventional (monolithic) Unix implementations have network protocol stacks as part of the kernel.
2932) 10.058571, USB - Wikipedia, the free encyclopedia.txt#39, term: computer, content:Media Transfer Protocol (MTP) was designed by Microsoft to give higher-level access to a device's filesystem than USB mass storage, at the level of files rather than disk blocks. It also has optional DRM features. MTP was designed for use with portable media players, but it has since been adopted as the primary storage access protocol of the Android operating system from the version 4.1 Jelly Bean as well as Windows Phone 8 (Windows Phone 7 devices had used the Zune protocol which was an evolution of MTP). The primary reason for this is that MTP does not require exclusive access to the storage device the way UMS does, alleviating potential problems should an Android program request the storage while it is attached to a computer. The main drawback is that MTP is not as well supported outside of Windows operating systems.
2933) 10.058571, USB - Wikipedia, the free encyclopedia.txt#46, term: computer, content:By design, it is difficult to insert a USB plug into its receptacle incorrectly. The USB specification states that the required USB icon must be embossed on the "topside" of the USB plug, which "...provides easy user recognition and facilitates alignment during the mating process." The specification also shows that the "recommended" "Manufacturer's logo" ("engraved" on the diagram but not specified in the text) is on the opposite side of the USB icon. The specification further states, "The USB Icon is also located adjacent to each receptacle. Receptacles should be oriented to allow the icon on the plug to be visible during the mating process." However, the specification does not consider the height of the device compared to the eye level height of the user, so the side of the cable that is "visible" when mated to a computer on a desk can depend on whether the user is standing or kneeling.[56]
2934) 10.058571, Vacuum tube - Wikipedia, the free encyclopedia.txt#101, term: computer, content:The cathode ray tube (CRT) is a vacuum tube used particularly for display purposes. Although there are still many televisions and computer monitors using cathode ray tubes, they are rapidly being replaced by flat panel displays whose quality has greatly improved even as their prices drop. This is also true of digital oscilloscopes (based on internal computers and analog to digital converters), although traditional analog scopes (dependent upon CRTs) continue to be produced, are economical, and preferred by many technicians. At one time many radios used "magic eye tubes", a specialized sort of CRT used in place of a meter movement to indicate signal strength, or input level in a tape recorder. A modern indicator device, the vacuum fluorescent display (VFD) is also a sort of cathode ray tube.
2935) 10.058571, Vannevar Bush - Wikipedia, the free encyclopedia.txt#0, term: computer, content:Vannevar Bush (/vnivr/ van-NEE-var; March 11, 1890 June 28, 1974) was an American engineer, inventor and science administrator, who during World War II headed the U.S. Office of Scientific Research and Development (OSRD), through which almost all wartime military R&D was carried out, including initiation and early administration of the Manhattan Project. He is also known in engineering for his work on analog computers, for founding Raytheon, and for the memex, a hypothetical adjustable microfilm viewer with a structure analogous to that of hypertext. In 1945, Bush published the essay "As We May Think" in which he predicted that "wholly new forms of encyclopedias will appear, ready made with a mesh of associative trails running through them, ready to be dropped into the memex and there amplified".[2] The memex influenced generations of computer scientists, who drew inspiration from its vision of the future. He was chiefly responsible for the movement that led to the creation of the National Science Foundation.
2936) 10.058571, VAX - Wikipedia, the free encyclopedia.txt#12, term: computer, content:Further VLSI VAX processors followed in the form of the V-11, CVAX, CVAX SOC ("System On Chip", a single-chip CVAX), Rigel, Mariah and NVAX implementations. The VAX microprocessors extended the architecture to inexpensive workstations and later also supplanted the high-end VAX models. This wide range of platforms (mainframe to workstation) using one architecture was unique in the computer industry at that time. Sundry graphics were etched onto the CVAX microprocessor die. The phrase CVAX... when you care enough to steal the very best was etched in broken Russian as a play on a Hallmark Cards slogan, intended as a message to Soviet engineers who were known to be both purloining DEC computers for military applications and reverse engineering their chip design.[10][11]
2937) 10.058571, Vector processor - Wikipedia, the free encyclopedia.txt#6, term: computer, content:The first successful implementation of vector processing appears to be the Control Data Corporation STAR-100 and the Texas Instruments Advanced Scientific Computer (ASC). The basic ASC (i.e., "one pipe") ALU used a pipeline architecture that supported both scalar and vector computations, with peak performance reaching approximately 20 MFLOPS, readily achieved when processing long vectors. Expanded ALU configurations supported "two pipes" or "four pipes" with a corresponding 2X or 4X performance gain. Memory bandwidth was sufficient to support these expanded modes. The STAR was otherwise slower than CDC's own supercomputers like the CDC 7600, but at data related tasks they could keep up while being much smaller and less expensive. However the machine also took considerable time decoding the vector instructions and getting ready to run the process, so it required very specific data sets to work on before it actually sped anything up.
2938) 10.058571, Video editing - Wikipedia, the free encyclopedia.txt#5, term: computer, content:Like many other technologies, the cost of video editing has declined by an order of magnitude or more. The 2" Quadruplex system cost so much that many television production facilities could only afford a single unit and editing was a highly involved process requiring special training. In contrast, nearly any home computer sold since the year 2000 has the speed and storage capacity to digitize and edit standard-definition television (SDTV). The two major retail operating systems include basic video editing software - Apple's iMovie and Microsoft's Windows Movie Maker. Additional options exist such as more advanced commercial products, as well as there are free opensource[2] video-editing programs.[3] Automatic video editing products have also emerged, opening up video editing to a broader commercial audience of amateurs and reducing the time it takes to edit videos.
2939) 10.058571, Video game - Wikipedia, the free encyclopedia.txt#26, term: computer, content:Although departments of computer science have been studying the technical aspects of video games for years, theories that examine games as an artistic medium are a relatively recent development in the humanities. The two most visible schools in this emerging field are ludology and narratology. Narrativists approach video games in the context of what Janet Murray calls "Cyberdrama". That is to say, their major concern is with video games as a storytelling medium, one that arises out of interactive fiction. Murray puts video games in the context of the Holodeck, a fictional piece of technology from Star Trek, arguing for the video game as a medium in which we get to become another person, and to act out in another world.[50] This image of video games received early widespread popular support, and forms the basis of films such as Tron, eXistenZ and The Last Starfighter.
2940) 10.058571, Video game console - Wikipedia, the free encyclopedia.txt#14, term: computer, content:In 1983, the video game business suffered a much more severe crash. A flood of consoles, low-quality video games by smaller companies (especially for the 2600), industry leader Atari hyping games such as E.T and a 2600 version of Pac-Man that were poorly received, and a growing number of home computer users caused consumers and retailers to lose faith in video game consoles. Most video game companies filed for bankruptcy, or moved into other industries, abandoning their game consoles. A group of employees from Mattel Electronics formed the INTV Corporation and bought the rights for the Intellivision. INTV alone continued to manufacture the Intellivision in small quantities and release new Intellivision games until 1991. All other North American game consoles were discontinued by 1984.
2941) 10.058571, Video game console - Wikipedia, the free encyclopedia.txt#18, term: computer, content:Jack Tramiel, after buying Atari, downsizing its staff, and settling its legal disputes, attempted to bring Atari back into the home console market. Atari released a smaller, sleeker, cheaper version of their popular Atari 2600. They also released the Atari 7800, a console technologically comparable with the NES and backwards compatible with the 2600. Finally Atari repackaged its 8-bit XE home computer as the XEGS game console. The new consoles helped Atari claw its way out of debt, but failed to gain much market share from Nintendo. Atari's lack of funds meant that its consoles saw fewer releases, lower production values (both the manuals and the game labels were frequently black and white), and limited distribution.
2942) 10.058571, Video game industry - Wikipedia, the free encyclopedia.txt#9, term: computer, content:The early part of the decade saw the rise of home computing, and home-made games, especially in Europe (with the ZX Spectrum) and Asia (with the NEC PC-88 and MSX). This time also saw the rise of video game journalism, which was later expanded to include covermounted cassettes and CDs. In 1983, the North American industry crashed due to the production of too many badly developed games (quantity over quality), resulting in the fall of the North American industry. The industry would eventually be revitalized by the release of the Nintendo Entertainment System, which resulted in the home console market being dominated by Japanese companies such as Nintendo,[4] while a professional European computer game industry also began taking shape with companies such as Ocean Software.[23] The latter part of the decade saw the rise of the Game Boy handheld system. In 1987, Nintendo lost a legal challenge against Blockbuster Entertainment, which enabled games rentals in the same way as movies.
2943) 10.058571, Von Neumann architecture - Wikipedia, the free encyclopedia.txt#3, term: computer, content:The earliest computing machines had fixed programs. Some very simple computers still use this design, either for simplicity or training purposes. For example, a desk calculator (in principle) is a fixed program computer. It can do basic mathematics, but it cannot be used as a word processor or a gaming console. Changing the program of a fixed-program machine requires rewiring, restructuring, or redesigning the machine. The earliest computers were not so much "programmed" as they were "designed". "Reprogramming", when it was possible at all, was a laborious process, starting with flowcharts and paper notes, followed by detailed engineering designs, and then the often-arduous process of physically rewiring and rebuilding the machine. It could take three weeks to set up a program on ENIAC and get it working.[4]
2944) 10.058571, Wayback Machine - Wikipedia, the free encyclopedia.txt#24, term: computer, content:In 2003, Harding Earley Follmer & Frailey defended a client from a trademark dispute using the Archive's Wayback Machine. The attorneys were able to demonstrate that the claims made by the plaintiff were invalid, based on the content of their web site from several years prior. The plaintiff, Healthcare Advocates, then amended their complaint to include the Internet Archive, accusing the organization of copyright infringement as well as violations of the DMCA and the Computer Fraud and Abuse Act. Healthcare Advocates claimed that, since they had installed a robots.txt file on their web site, even if after the initial lawsuit was filed, the Archive should have removed all previous copies of the plaintiff web site from the Wayback Machine.[47] The lawsuit was settled out of court.[48]
2945) 10.058571, Wearable computer - Wikipedia, the free encyclopedia.txt#30, term: computer, content:Also in 1994, DARPA started the Smart Modules Program to develop a modular, humionic approach to wearable and carryable computers, with the goal of producing a variety of products including computers, radios, navigation systems and human-computer interfaces that have both military and commercial use. In July 1996, DARPA went on to host the "Wearables in 2005" workshop, bringing together industrial, university, and military visionaries to work on the common theme of delivering computing to the individual.[33] A follow-up conference was hosted by Boeing in August 1996, where plans were finalized to create a new academic conference on wearable computing. In October 1997, Carnegie Mellon University, MIT, and Georgia Tech co-hosted the IEEE International Symposium on Wearables Computers (ISWC) in Cambridge, Massachusetts. The symposium was a full academic conference with published proceedings and papers ranging from sensors and new hardware to new applications for wearable computers, with 382 people registered for the event.
2946) 10.058571, Webcam - Wikipedia, the free encyclopedia.txt#21, term: computer, content:The first commercial webcam, the black-and-white QuickCam, entered the marketplace in 1994, created by the U.S. computer company Connectix (which sold its product line to Logitech in 1998). QuickCam was available in August 1994 for the Apple Macintosh, connecting via a serial port, at a cost of $100. Jon Garber, the designer of the device, had wanted to call it the "Mac-camera", but was overruled by Connectix's marketing department; a version with a PC-compatible serial port and software for Microsoft Windows was launched in October 1995. The original QuickCam provided 320x240-pixel resolution with a grayscale depth of 16 shades at 60 frames per second, or 256 shades at 15 frames per second.[12] These cam were tested on several Delta II launch using a variety of communication protocols including CDMA, TDMA, GSM and HF.
2947) 10.058571, Windows 2000 - Wikipedia, the free encyclopedia.txt#21, term: computer, content:The right pane of Windows 2000 Explorer, which usually just lists files and folders, can also be customized. For example, the contents of the system folders aren't displayed by default, instead showing in the right pane a warning to the user that modifying the contents of the system folders could harm their computer. It's possible to define additional Explorer panes by using DIV elements in folder template files.[64] This degree of customizability is new to Windows 2000; neither Windows 98 nor the Desktop Update could provide it.[72] The new DHTML-based search pane is integrated into Windows 2000 Explorer, unlike the separate search dialog found in all previous Explorer versions. The Indexing Service has also been integrated into the operating system and the search pane built into Explorer allows searching files indexed by its database.[73]
2948) 10.058571, Windows 2000 - Wikipedia, the free encyclopedia.txt#47, term: computer, content:A new way of organizing Windows network domains, or groups of resources, called Active Directory, is introduced with Windows 2000 to replace Windows NT's earlier domain model. Active Directory's hierarchical nature allowed administrators a built-in way to manage user and computer policies and user accounts, and to automatically deploy programs and updates with a greater degree of scalability and centralization than provided in previous Windows versions. User information stored in Active Directory also provided a convenient phone book-like function to end users. Active Directory domains can vary from small installations with a few hundred objects, to large installations with millions. Active Directory can organise and link groups of domains into a contiguous domain name space to form trees. Groups of trees outside of the same namespace can be linked together to form forests.
2949) 10.058571, Windows 95 - Wikipedia, the free encyclopedia.txt#6, term: computer, content:Prior to the official release, the American public was given a chance to preview Windows 95 in the Windows 95 Preview Program. For US$19.95, users were sent a set of 3.5-inch floppy diskettes that would install Windows 95 either as an upgrade to Windows 3.1x or as a fresh install on a clean computer. Users who bought into the program were also given a free preview of The Microsoft Network (MSN), the online service that Microsoft launched with Windows 95. During the preview period Microsoft established various electronic distribution points for promotional and technical documentation on Chicago[10] including a detailed document for media reviewers describing the new system highlights.[10][11] The preview versions expired in November 1995, after which the user would have to purchase their own copy of the final version of Windows 95.
2950) 10.058571, Windows Vista - Wikipedia, the free encyclopedia.txt#0, term: computer, content:Windows Vista (codenamed Longhorn[7]) is an operating system by Microsoft for use on personal computers, including home and business desktops, laptops, tablet PCs and media center PCs. Development was completed on 8 November 2006, and over the following three months, it was released in stages to computer hardware and software manufacturers, business customers and retail channels. On 30 January 2007, it was released worldwide[8] and was made available for purchase and download from Microsoft's website.[9] The release of Windows Vista came more than five years after the introduction of its predecessor, Windows XP, the longest time span between successive releases of Microsoft Windows desktop operating systems. It was succeeded by Windows 7, which was released to manufacturing on 22 July 2009 and released worldwide for retail on 22 October 2009.
2951) 10.058571, Windows XP - Wikipedia, the free encyclopedia.txt#19, term: computer, content:Windows XP was originally bundled with Internet Explorer 6, Outlook Express 6, Windows Messenger, and MSN Explorer. New networking features were also added, including Internet Connection Firewall, Internet Connection Sharing integration with UPnP, NAT traversal APIs, Quality of Service features, IPv6 and Teredo tunneling, Background Intelligent Transfer Service, extended fax features, network bridging, peer to peer networking, support for most DSL modems, IEEE 802.11 (Wi-Fi) connections with auto configuration and roaming, TAPI 3.1, and networking over FireWire.[43] Remote Assistance and Remote Desktop were also added, which allow users to connect to a computer running Windows XP from across a network or the Internet and access their applications, files, printers, and devices or request help.[44] Improvements were also made to IntelliMirror features such as Offline Files, Roaming user profiles and Folder redirection.
2952) 10.058571, Windows XP - Wikipedia, the free encyclopedia.txt#24, term: computer, content:Microsoft also targeted emerging markets with the 2004 introduction of Windows XP Starter Edition, a special variant of Home Edition intended for low-cost PC's. The OS is primarily aimed at first-time computer owners (particularly in developing countries); containing heavy localization (including wallpapers and screen savers incorporating images of local landmarks), and a "My Support" area which contains video tutorials on basic computing tasks. It also removes certain "complex" features, and does not allow users to run more than three applications at a time. After a pilot program in India and Thailand, Starter was released in other emerging markets throughout 2005.[53] In 2006, Microsoft also unveiled the FlexGo initiative, which would also target emerging markets with subsidized PCs on a pre-paid, subscription basis.[54]
2953) 10.058571, Windows XP - Wikipedia, the free encyclopedia.txt#43, term: computer, content:Microsoft continued to provide Security Essentials virus definitions and updates for its Malicious Software Removal Tool (MSRT) for XP until July 14, 2015.[124] As the end of extended support approached, Microsoft began to increasingly urge XP customers to migrate to newer versions such as Windows 7 or 8 in the interest of security, suggesting that attackers could reverse engineer security patches for newer versions of Windows and use them to target equivalent vulnerabilities in XP.[125][126][127] On March 8, 2014, Microsoft deployed an update for XP that, on the 8th of each month, displays a pop-up notification to remind users about the end of supportthese notifications may be disabled by the user.[128] Microsoft also partnered with Laplink to provide a special "express" version of its PCmover software to help users migrate files and settings from XP to a computer with a newer version of Windows.[128][129]
2954) 10.058571, Wire - Wikipedia, the free encyclopedia.txt#15, term: computer, content:Stranded wire is composed of a number of small wires bundled or wrapped together to form a larger conductor. Stranded wire is more flexible than solid wire of the same total cross-sectional area. Stranded wire tends to be a better conductor than solid wire because the individual wires collectively comprise a greater surface area. Stranded wire is used when higher resistance to metal fatigue is required. Such situations include connections between circuit boards in multi-printed-circuit-board devices, where the rigidity of solid wire would produce too much stress as a result of movement during assembly or servicing; A.C. line cords for appliances; musical instrument cables; computer mouse cables; welding electrode cables; control cables connecting moving machine parts; mining machine cables; trailing machine cables; and numerous others.
2955) 10.058571, Word processor - Wikipedia, the free encyclopedia.txt#7, term: computer, content:Text editors offer facilities for typing, storing, replaying, and usually printing text (strings of characters). Text editors do not format lines or documents; in fact they lack those concepts. (There are extensions of text editors which perform format lines and pages: batch document processing systems, starting with TJ-2 and RUNOFF and still available in such systems as LaTeX, as well as programs that implement the paged-media extensions to HTML and CSS). Text editors are now used mainly by programmers, website designers, computer system administrators, and, in the case of LaTeX, by mathematicians and scientists (for complex formulas and for citations in rare languages). They are also useful when fast startup times, small file sizes, editing speed, and simplicity of operation are valued, and when formatting is unimportant. Due to their use in managing complex software projects, text editors can sometimes provide better facilities for managing large writing projects than a word processor.[7]
2956) 10.058571, World Wide Web - Wikipedia, the free encyclopedia.txt#23, term: computer, content:The following example demonstrates the functioning of a web browser when accessing a page at the URL http://www.example.org/home.html. The browser resolves the server name of the URL (www.example.org) into an Internet Protocol address using the globally distributed Domain Name System (DNS). This lookup returns an IP address such as 203.0.113.4 or 2001:db8:2e::7334. The browser then requests the resource by sending an HTTP request across the Internet to the computer at that address. It requests service from a specific TCP port number that is well known for the HTTP service, so that the receiving host can distinguish an HTTP request from other network protocols it may be servicing. The HTTP protocol normally uses port number 80. The content of the HTTP request can be as simple as two lines of text:
2957) 10.058571, x86 assembly language - Wikipedia, the free encyclopedia.txt#0, term: computer, content:x86 assembly language is a family of backward-compatible assembly languages, which provide some level of compatibility all the way back to the Intel 8008. x86 assembly languages are used to produce object code for the x86 class of processors. Like all assembly languages, it uses short mnemonics to represent the fundamental instructions that the CPU in a computer can understand and follow. Compilers sometimes produce assembly code as an intermediate step when translating a high level program into machine code. Regarded as a programming language, assembly coding is machine-specific and low level. Assembly languages are more typically used for detailed and time critical applications such as small real-time embedded systems or operating system kernels and device drivers.
2958) 10.058571, Zilog Z80 - Wikipedia, the free encyclopedia.txt#46, term: computer, content:In the U.S., the Radio Shack TRS-80, introduced in 1977, as well as the Models II, III, IV, and the proposed Model V, used the Z80. In the United Kingdom, Sinclair Research used the Z80 and Z80A in its ZX80, ZX81 and, ZX Spectrum home computers. Amstrad used them in their Amstrad CPC range and an early UK computer, the Nascom 1 and 2 also used the Z80. The Commodore 128 featured a Z80 processor alongside its MOS Technology 8502 processor for CP/M compatibility.[64] Other 6502 architecture computers on the market at the time, such as the BBC Micro, Apple II,[65] and the 6510 based Commodore 64,[66] could make use of the Z80 with an external unit, a plug-in card, or an expansion ROM cartridge. The Microsoft Z-80 SoftCard for the Apple II was a particularly successful add-on card and one of Microsoft's few hardware products of the era.
2959) 9.483311, 64-bit computing - Wikipedia, the free encyclopedia.txt#8, term: computer, content:Some supercomputer architectures of the 1970s and 1980s, such as the Cray-1,[5] used registers up to 64 bits wide, and supported 64-bit integer arithmetic, although they did not support 64-bit addressing. In the mid-1980s, Intel i860[6] development began culminating in a (too late[7] for Windows NT) 1989 release; the i860 had 32-bit integer registers and 32-bit addressing, so it was not a fully 64-bit processor, although its graphics unit supported 64-bit integer arithmetic.[8] However, 32 bits remained the norm until the early 1990s, when the continual reductions in the cost of memory led to installations with quantities of RAM approaching 4GB, and the use of virtual memory spaces exceeding the 4GB ceiling became desirable for handling certain types of problems. In response, MIPS and DEC developed 64-bit microprocessor architectures, initially for high-end workstation and server machines. By the mid-1990s, HAL Computer Systems, Sun Microsystems, IBM, Silicon Graphics, and Hewlett Packard had developed 64-bit architectures for their workstation and server systems. A notable exception to this trend were mainframes from IBM, which then used 32-bit data and 31-bit address sizes; the IBM mainframes did not include 64-bit processors until 2000. During the 1990s, several low-cost 64-bit microprocessors were used in consumer electronics and embedded applications. Notably, the Nintendo 64[9] and the PlayStation 2 had 64-bit microprocessors before their introduction in personal computers. High-end printers and network equipment, as well as industrial computers, also used 64-bit microprocessors, such as the Quantum Effect Devices R5000. 64-bit computing started to drift down to the personal computer desktop from 2003 onwards, when some models in Apple's Macintosh lines switched to PowerPC 970 processors (termed "G5" by Apple), and AMD released its first 64-bit x86-64 processor.
2960) 9.483311, Artificial intelligence - Wikipedia, the free encyclopedia.txt#77, term: computer, content:The field of machine ethics is concerned with giving machines ethical principles, or a procedure for discovering a way to resolve the ethical dilemmas they might encounter, enabling them to function in an ethically responsible manner through their own ethical decision making.[216] The field was delineated in the AAAI Fall 2005 Symposium on Machine Ethics: "Past research concerning the relationship between technology and ethics has largely focused on responsible and irresponsible use of technology by human beings, with a few people being interested in how human beings ought to treat machines. In all cases, only human beings have engaged in ethical reasoning. The time has come for adding an ethical dimension to at least some machines. Recognition of the ethical ramifications of behavior involving machines, as well as recent and potential developments in machine autonomy, necessitate this. In contrast to computer hacking, software property issues, privacy issues and other topics normally ascribed to computer ethics, machine ethics is concerned with the behavior of machines towards human users and other machines. Research in machine ethics is key to alleviating concerns with autonomous systemsit could be argued that the notion of autonomous machines without such a dimension is at the root of all fear concerning machine intelligence. Further, investigation of machine ethics could enable the discovery of problems with current ethical theories, advancing our thinking about Ethics."[217] Machine ethics is sometimes referred to as machine morality, computational ethics or computational morality. A variety of perspectives of this nascent field can be found in the collected edition "Machine Ethics"[216] that stems from the AAAI Fall 2005 Symposium on Machine Ethics.[217]
2961) 9.483311, Internet - Wikipedia, the free encyclopedia.txt#66, term: computer, content:The Internet allows computer users to remotely access other computers and information stores easily from any access point. Access may be with computer security, i.e. authentication and encryption technologies, depending on the requirements. This is encouraging new ways of working from home, collaboration and information sharing in many industries. An accountant sitting at home can audit the books of a company based in another country, on a server situated in a third country that is remotely maintained by IT specialists in a fourth. These accounts could have been created by home-working bookkeepers, in other remote locations, based on information emailed to them from offices all over the world. Some of these things were possible before the widespread use of the Internet, but the cost of private leased lines would have made many of them infeasible in practice. An office worker away from their desk, perhaps on the other side of the world on a business trip or a holiday, can access their emails, access their data using cloud computing, or open a remote desktop session into their office PC using a secure virtual private network (VPN) connection on the Internet. This can give the worker complete access to all of their normal files and data, including email and other applications, while away from the office. It has been referred to among system administrators as the Virtual Private Nightmare,[86] because it extends the secure perimeter of a corporate network into remote locations and its employees' homes.
2962) 9.483311, Konrad Zuse - Wikipedia, the free encyclopedia.txt#16, term: computer, content:While working on his Z4 computer, Zuse realised that programming in machine code was too complicated. He started working on a PhD thesis[26] containing groundbreaking research years ahead of its time, mainly the first high-level programming language, Plankalkl ("Plan Calculus") and, as an elaborate example program, the first real computer chess engine.[27] After the 1945 Luisenstadt bombing, he flew from Berlin for the rural Allgu, and, unable to do any hardware development, he continued working on the Plankalkl, eventually publishing some brief excerpts of his thesis in 1948 and 1959; the work in its entirety, however, remained unpublished until 1972.[27] The PhD thesis was submitted at University of Augsburg, but rejected for formal reasons, because Zuse forgot to pay the 400 Mark university enrollment fee. (The rejection did not bother him.[28]) Plankalkl slightly influenced the design of ALGOL 58[29] but was itself only implemented in 1975 in a dissertation by Joachim Hohmann.[30] Heinz Rutishauser, one of the inventors of ALGOL, wrote: "The very first attempt to devise an algorithmic language was undertaken in 1948 by K. Zuse. His notation was quite general, but the proposal never attained the consideration it deserved". Further implementations followed in 1998 and then in 2000 by a team from the Free University of Berlin. Donald Knuth suggested a thought experiment: What might have happened, had the bombing not taken place, and had the PhD thesis accordingly been published as planned?[27]
2963) 9.483311, Microprocessor - Wikipedia, the free encyclopedia.txt#39, term: computer, content:The Intel 4004 was followed in 1972 by the Intel 8008, the world's first 8-bit microprocessor. The 8008 was not, however, an extension of the 4004 design, but instead the culmination of a separate design project at Intel, arising from a contract with Computer Terminals Corporation, of San Antonio TX, for a chip for a terminal they were designing,[35] the Datapoint 2200fundamental aspects of the design came not from Intel but from CTC. In 1968, CTC's Vic Poor and Harry Pyle developed the original design for the instruction set and operation of the processor. In 1969, CTC contracted two companies, Intel and Texas Instruments, to make a single-chip implementation, known as the CTC 1201.[36] In late 1970 or early 1971, TI dropped out being unable to make a reliable part. In 1970, with Intel yet to deliver the part, CTC opted to use their own implementation in the Datapoint 2200, using traditional TTL logic instead (thus the first machine to run "8008 code" was not in fact a microprocessor at all and was delivered a year earlier). Intel's version of the 1201 microprocessor arrived in late 1971, but was too late, slow, and required a number of additional support chips. CTC had no interest in using it. CTC had originally contracted Intel for the chip, and would have owed them US$50,000 (equivalent to $292,153 in 2015) for their design work.[36] To avoid paying for a chip they did not want (and could not use), CTC released Intel from their contract and allowed them free use of the design.[36] Intel marketed it as the 8008 in April, 1972, as the world's first 8-bit microprocessor. It was the basis for the famous "Mark-8" computer kit advertised in the magazine Radio-Electronics in 1974. This processor had an 8-bit data bus and a 14-bit address bus.[37]
2964) 8.710978, Computer keyboard - Wikipedia, the free encyclopedia.txt#38, term: computer, content:The SysRq and Print screen commands often share the same key. SysRq was used in earlier computers as a "panic" button to recover from crashes (and it is still used in this sense to some extent by the Linux kernel; see Magic SysRq key). The Print screen command used to capture the entire screen and send it to the printer, but in the present it usually puts a screenshot in the clipboard. The Break key/Pause key no longer has a well-defined purpose. Its origins go back to teleprinter users, who wanted a key that would temporarily interrupt the communications line. The Break key can be used by software in several different ways, such as to switch between multiple login sessions, to terminate a program, or to interrupt a modem connection. In programming, especially old DOS-style BASIC, Pascal and C, Break is used (in conjunction with Ctrl) to stop program execution. In addition to this, Linux and variants, as well as many DOS programs, treat this combination the same as Ctrl+C. On modern keyboards, the break key is usually labeled Pause/Break. In most Windows environments, the key combination Windows key+Pause brings up the system properties. The Escape key (often abbreviated Esc) is used to initiate an escape sequence. As most computer users no longer are concerned with the details of controlling their computer's peripherals, the task for which the escape sequences were originally designed, the escape key was appropriated by application programmers, most often to "escape" or back out of a mistaken command. This use continues today in Microsoft Windows's use of escape as a shortcut in dialog boxes for No, Quit, Exit, Cancel, or Abort. A common application today of the Esc key is as a shortcut key for the Stop button in many web browsers. On machines running Microsoft Windows, prior to the implementation of the Windows key on keyboards, the typical practice for invoking the "start" button was to hold down the control key and press escape. This process still works in Windows 2000, XP, Vista, 7, 8, and 10. The Enter key is located: One in the alphanumeric keys and the other one is in the numeric keys. When one worked something on their computer and wanted to do something with their work, pressing the enter key would do the command they ordered. Another function is to create a space for next paragraph. When one typed and finished typing a paragraph and they wanted to have a second paragraph, they could press enter and it would do spacing. Shift key: when one presses shift and a letter, it will capitalize the letter pressed with the shift key. Another use is to type more symbols than appear to be available, for instance the apostrophe key is accompanied with a quotation mark on the top. If one wants to type the quotation mark but pressed that key alone, the symbol that would appear would be the apostrophe. The quotation mark will only appear if both the required key and the Shift key are pressed. The Menu key or Application key is a key found on Windows-oriented computer keyboards. It is used to launch a context menu with the keyboard rather than with the usual right mouse button. The key's symbol is usually a small icon depicting a cursor hovering above a menu. On some Samsung keyboards the cursor in the icon is not present, showing the menu only. This key was created at the same time as the Windows key. This key is normally used when the right mouse button is not present on the mouse. Some Windows public terminals do not have a Menu key on their keyboard to prevent users from right-clicking (however, in many Windows applications, a similar functionality can be invoked with the Shift+F10 keyboard shortcut).
2965) 8.382142, Alan Turing - Wikipedia, the free encyclopedia.txt#41, term: computer, content:From 1945 to 1947, Turing lived in Hampton, London,[85] while he worked on the design of the ACE (Automatic Computing Engine) at the National Physical Laboratory (NPL). He presented a paper on 19 February 1946, which was the first detailed design of a stored-program computer.[86] Von Neumann's incomplete First Draft of a Report on the EDVAC had predated Turing's paper, but it was much less detailed and, according to John R. Womersley, Superintendent of the NPL Mathematics Division, it "contains a number of ideas which are Dr. Turing's own".[87] Although ACE was a feasible design, the secrecy surrounding the wartime work at Bletchley Park led to delays in starting the project and he became disillusioned. In late 1947 he returned to Cambridge for a sabbatical year during which he produced a seminal work on Intelligent Machinery that was not published in his lifetime.[88] While he was at Cambridge, the Pilot ACE was being built in his absence. It executed its first program on 10 May 1950, and a number of later computers around the world owe much to it, including the English Electric DEUCE and the American Bendix G-15. The full version of Turing's ACE was not built until after his death.[89]
2966) 8.382142, Analytical Engine - Wikipedia, the free encyclopedia.txt#25, term: computer, content:Despite this ground work, Babbage's work fell into historical obscurity, and the Analytical Engine was unknown to builders of electro-mechanical and electronic computing machines in the 1930s and 1940s when they began their work, resulting in the need to re-invent many of the architectural innovations Babbage had proposed. Howard Aiken, who built the quickly-obsoleted electromechanical calculator, the Harvard Mark I, between 1937 and 1945, praised Babbage's work likely as a way of enhancing his own stature, but knew nothing of the Analytical Engine's architecture during the construction of the Mark I, and considered his visit to the constructed portion of the Analytical Engine "the greatest disappointment of my life".[32] The Mark I showed no influence from the Analytical Engine and lacked the Analytical Engine's most prescient architectural feature, conditional branching.[32] J. Presper Eckert and John W. Mauchly similarly were not aware of the details of Babbage's Analytical Engine work prior to the completion of their design for the first electronic general-purpose computer, the ENIAC.[33][34]
2967) 8.382142, Artificial intelligence - Wikipedia, the free encyclopedia.txt#10, term: computer, content:A set of advanced statistical techniques (loosely known as deep learning), access to large amounts of data and faster computers produced enormous advances in machine learning and perception.[35] By the mid 2010s, successful machine learning applications were being used widely throughout the world. Programs that incorporated these techniques began to accomplish things that had seemed impossible in the mid-80s.[36] In a Jeopardy! quiz show exhibition match, IBM's question answering system, Watson, defeated the two greatest Jeopardy champions, Brad Rutter and Ken Jennings, by a significant margin.[37] The Kinect, which provides a 3D bodymotion interface for the Xbox 360 and the Xbox One, uses algorithms that emerged from lengthy AI research[38] as do intelligent personal assistants in smartphones.[39] In March 2016, AlphaGo won 4 out of 5 games of Go in a match with Go champion Lee Sedol, becoming the first computer Go-playing system to beat a professional human Go player without handicaps.[40]
2968) 8.382142, Artificial intelligence - Wikipedia, the free encyclopedia.txt#56, term: computer, content:According to a survey,[164] the expression "Deep Learning" was introduced to the Machine Learning community by Rina Dechter in 1986[165] and gained traction after Igor Aizenberg and colleagues introduced it to Artificial Neural Networks in 2000.[166] The first functional Deep Learning networks were published by Alexey Grigorevich Ivakhnenko and V. G. Lapa in 1965.[167] These networks are trained one layer at a time. Ivakhnenko's 1971 paper[168] describes the learning of a deep feedforward multilayer perceptron with eight layers, already much deeper than many later networks. In 2006, a publication by Geoffrey Hinton and Ruslan Salakhutdinov introduced another way of pre-training many-layered feedforward neural networks (FNNs) one layer at a time, treating each layer in turn as an unsupervised restricted Boltzmann machine, then using supervised backpropagation for fine-tuning.[169] Similar to shallow artificial neural networks, deep neural networks can model complex non-linear relationships. Over the last few years, advances in both machine learning algorithms and computer hardware have led to more efficient methods for training deep neural networks that contain many layers of non-linear hidden units and a very large output layer.[170]
2969) 8.382142, Ball-and-disk integrator - Wikipedia, the free encyclopedia.txt#7, term: computer, content:Ball disk integrators were used in the analog guidance computers of ballistic missile weapon systems as late as the mid 1970s. The Pershing 1 missile system, utilized the Bendix ST-120 inertial guidance platform, combined with a mechanical analog computer, to achieve accurate guidance. The ST-120 provided accelerometer information for all three axes. The accelerometer for forward movement transmitted its position to the ball position radial arm, causing the ball fixture to move away from the disk center as acceleration increased. The disk itself represents time and rotates at a constant rate. As the ball fixture moves further out from the center of the disk, the ball spins faster. The ball speed represents the missile speed, the number of ball rotations represent distance traveled. These mechanical positions were used to determine staging events, thrust termination, and warhead separation, as well as "good guidance" signals used to complete the arming chain for the warhead. The first known use of this general concept was in the V-2 missile developed by the Von Braun group at Peenemunde. See PIGA Accelerometer. It was later refined at Redstone Arsenal and applied to the Redstone rocket and subsequently Pershing 1.
2970) 8.382142, BIOS - Wikipedia, the free encyclopedia.txt#8, term: computer, content:The BIOS of the original IBM PC XT had no interactive user interface. Error codes or messages were displayed on the screen, or coded series of sounds were generated to signal errors when the power-on self-test (POST) had not proceeded to the point of successfully initializing a video display adapter. Options on the IBM PC and XT were set by switches and jumpers on the main board and on peripheral cards. Starting around the mid-1990s, it became typical for the BIOS ROM to include a "BIOS configuration utility" (BCU[8]) or "BIOS setup utility", accessed at system power-up by a particular key sequence. This program allowed the user to set system configuration options, of the type formerly set using DIP switches, through an interactive menu system controlled through the keyboard. In the interim period, IBM-compatible PCsincluding the IBM ATheld configuration settings in battery-backed RAM and used a bootable configuration program on disk, not in the ROM, to set the configuration options contained in this memory. The disk was supplied with the computer, and if it was lost the system settings could not be changed.
2971) 8.382142, BIOS - Wikipedia, the free encyclopedia.txt#30, term: computer, content:Peripheral cards such as some hard disk drive controllers and some video display adapters have their own BIOS extension option ROMs, which provide additional functionality to BIOS. Code in these extensions runs before the BIOS boots the system from mass storage. These ROMs typically test and initialize hardware, add new BIOS services, and augment or replace existing BIOS services with their own versions of those services. For example, a SCSI controller usually has a BIOS extension ROM that adds support for hard drives connected through that controller. Some video cards have extension ROMs that replace the video services of the motherboard BIOS with their own video services. BIOS extension ROMs gain total control of the machine, so they can in fact do anything, and they may never return control to the BIOS that invoked them. An extension ROM could in principle contain an entire operating system or an application program, or it could implement an entirely different boot process such as booting from a network. Operation of an IBM-compatible computer system can be completely changed by removing or inserting an adapter card (or a ROM chip) that contains a BIOS extension ROM.
2972) 8.382142, Bletchley Park - Wikipedia, the free encyclopedia.txt#34, term: computer, content:The Lorenz messages were codenamed Tunny at Bletchley Park. They were only sent in quantity from mid-1942. The Tunny networks were used for high-level messages between German High Command and field commanders. With the help of German operator errors, the cryptanalysts in the Testery (named after Ralph Tester, its head) worked out the logical structure of the machine despite not knowing its physical form. They devised automatic machinery to help with decryption, which culminated in Colossus, the world's first programmable digital electronic computer. This was designed and built by Tommy Flowers and his team at the Post Office Research Station at Dollis Hill. The prototype first worked in December 1943, was delivered to Bletchley Park in January and first worked operationally on 5 February 1944. Enhancements were developed for the Mark 2 Colossus, the first of which was working at Bletchley Park on the morning of 1 June in time for D-day. Flowers then produced one Colossus a month for the rest of the war, making a total of ten with an eleventh part-built. The machines were operated mainly by Wrens in a section named the Newmanry after its head Max Newman.
2973) 8.382142, Boolean algebra - Wikipedia, the free encyclopedia.txt#61, term: computer, content:A subset Y of X can be identified with an indexed family of bits with index set X, with the bit indexed by x  X being 1 or 0 according to whether or not x  Y. (This is the so-called characteristic function notion of a subset.) For example, a 32-bit computer word consists of 32 bits indexed by the set {0,1,2,,31}, with 0 and 31 indexing the low and high order bits respectively. For a smaller example, if X = {a,b,c} where a, b, c are viewed as bit positions in that order from left to right, the eight subsets {}, {c}, {b}, {b,c}, {a}, {a,c}, {a,b}, and {a,b,c} of X can be identified with the respective bit vectors 000, 001, 010, 011, 100, 101, 110, and 111. Bit vectors indexed by the set of natural numbers are infinite sequences of bits, while those indexed by the reals in the unit interval [0,1] are packed too densely to be able to write conventionally but nonetheless form well-defined indexed families (imagine coloring every point of the interval [0,1] either black or white independently; the black points then form an arbitrary subset of [0,1]).
2974) 8.382142, Boolean algebra - Wikipedia, the free encyclopedia.txt#94, term: computer, content:Computers use two-value Boolean circuits for the above reasons. The most common computer architectures use ordered sequences of Boolean values, called bits, of 32 or 64 values, e.g. 01101000110101100101010101001011. When programming in machine code, assembly language, and certain other programming languages, programmers work with the low-level digital structure of the data registers. These registers operate on voltages, where zero volts represents Boolean 0, and a reference voltage (often +5V or +3.3V[20]) represents Boolean 1. Such languages support both numeric operations and logical operations. These computers can add, subtract, multiply, divide, two sequences of bits which are interpreted as integers. These computers can perform the Boolean logical operations of disjunction, conjunction, and negation again on two sequences of bits. Programmers therefore have the option of working in and applying the laws of either numeric algebra or Boolean algebra as needed. A core differentiating feature between algebra and logic, is the carry operation with algebra but not with logic.
2975) 8.382142, Boolean algebra - Wikipedia, the free encyclopedia.txt#103, term: computer, content:The 256-element free Boolean algebra on three generators is deployed in computer displays based on raster graphics, which use bit blit to manipulate whole regions consisting of pixels, relying on Boolean operations to specify how the source region should be combined with the destination, typically with the help of a third region called the mask. Modern video cards offer all 223=256 ternary operations for this purpose, with the choice of operation being a one-byte (8-bit) parameter. The constants SRC = 0xaa or 10101010, DST = 0xcc or 11001100, and MSK = 0xf0 or 11110000 allow Boolean operations such as (SRC^DST)&MSK (meaning XOR the source and destination and then AND the result with the mask) to be written directly as a constant denoting a byte calculated at compile time, 0x60 in the (SRC^DST)&MSK example, 0x66 if just SRC^DST, etc. At run time the video card interprets the byte as the raster operation indicated by the original expression in a uniform way that requires remarkably little hardware and which takes time completely independent of the complexity of the expression.
2976) 8.382142, Booting - Wikipedia, the free encyclopedia.txt#14, term: computer, content:IBM coined this term for the 7030 (Stretch),[6] revived it for the design of the System/360, and continues to use it in those environments today.[13] In the System/360 processors, an IPL is initiated by the computer operator by selecting the three hexadecimal digit device address (CUU; C=I/O Channel address, UU=Control unit and Device address[NB 1]) followed by pressing the LOAD button. On most[NB 2] System/370 and some later systems, the functions of the switches and the LOAD button are simulated using selectable areas on the screen of a graphics console, often an IBM 2250-like device or an IBM 3270-like device. For example, on the System/370 Model 158, the keyboard sequence 0-7-X (zero, seven and X, in that order) results in an IPL from the device address which was keyed into the input area. Amdahl 470V/6 and related CPUs supported four hexadecimal digits on those CPUs which had the optional second channel unit installed, for a total of 32 channels. Later, IBM would also support more than 16 channels.
2977) 8.382142, Calculator - Wikipedia, the free encyclopedia.txt#27, term: computer, content:In October 1961, the world's first all-electronic desktop calculator, the British Bell Punch/Sumlock Comptometer ANITA (A New Inspiration To Arithmetic/Accounting) was announced.[13][14] This machine used vacuum tubes, cold-cathode tubes and Dekatrons in its circuits, with 12 cold-cathode "Nixie" tubes for its display. Two models were displayed, the Mk VII for continental Europe and the Mk VIII for Britain and the rest of the world, both for delivery from early 1962. The Mk VII was a slightly earlier design with a more complicated mode of multiplication, and was soon dropped in favour of the simpler Mark VIII. The ANITA had a full keyboard, similar to mechanical comptometers of the time, a feature that was unique to it and the later Sharp CS-10A among electronic calculators. The ANITA weighed roughly 33 pounds (15kg) due to its large tube system.[15] Bell Punch had been producing key-driven mechanical calculators of the comptometer type under the names "Plus" and "Sumlock", and had realised in the mid-1950s that the future of calculators lay in electronics. They employed the young graduate Norbert Kitz, who had worked on the early British Pilot ACE computer project, to lead the development. The ANITA sold well since it was the only electronic desktop calculator available, and was silent and quick.
2978) 8.382142, Calculator - Wikipedia, the free encyclopedia.txt#51, term: computer, content:The first Soviet programmable desktop calculator ISKRA 123, powered by the power grid, was released at the beginning of the 1970s. The first Soviet pocket battery-powered programmable calculator, Elektronika "B3-21", was developed by the end of 1977 and released at the beginning of 1978. The successor of B3-21, the Elektronika B3-34 wasn't backward compatible with B3-21, even if it kept the reverse Polish notation (RPN). Thus B3-34 defined a new command set, which later was used in a series of later programmable Soviet calculators. Despite very limited capabilities (98 bytes of instruction memory and about 19 stack and addressable registers), people managed to write all kinds of programs for them, including adventure games and libraries of calculus-related functions for engineers. Hundreds, perhaps thousands, of programs were written for these machines, from practical scientific and business software, which were used in real-life offices and labs, to fun games for children. The Elektronika MK-52 calculator (using the extended B3-34 command set, and featuring internal EEPROM memory for storing programs and external interface for EEPROM cards and other periphery) was used in Soviet spacecraft program (for Soyuz TM-7 flight) as a backup of the board computer.
2979) 8.382142, Cellular automaton - Wikipedia, the free encyclopedia.txt#12, term: computer, content:Also in 1969 computer scientist Alvy Ray Smith completed a Stanford PhD dissertation on Cellular Automata Theory, the first mathematical treatment of CA as a general class of computers. Many papers came from this dissertation: He showed the equivalence of neighborhoods of various shapes, how to reduce a Moore to a von Neumann neighborhood or how to reduce any neighborhood to a von Neumann neighborhood.[20] He proved that two-dimensional CA are computation universal, introduced 1-dimensional CA, and showed that they too are computation universal, even with simple neighborhoods.[21] He showed how to subsume the complex von Neumann proof of construction universality (and hence self-reproducing machines) into a consequence of computation universality in a 1-dimensional CA.[22] Intended as the introduction to the German edition of von Neumann's book on CA, he wrote a survey of the field with dozens of references to papers, by many authors in many countries over a decade or so of work, often overlooked by modern CA researchers.[23]
2980) 8.382142, Church–Turing thesis - Wikipedia, the free encyclopedia.txt#24, term: computer, content:Other formalisms (besides recursion, the -calculus, and the Turing machine) have been proposed for describing effective calculability/computability. Stephen Kleene (1952) adds to the list the functions "reckonable in the system S1" of Kurt Gdel 1936, and Emil Post's (1943, 1946) "canonical [also called normal] systems".[41] In the 1950s Hao Wang and Martin Davis greatly simplified the one-tape Turing-machine model (see PostTuring machine). Marvin Minsky expanded the model to two or more tapes and greatly simplified the tapes into "up-down counters", which Melzak and Lambek further evolved into what is now known as the counter machine model. In the late 1960s and early 1970s researchers expanded the counter machine model into the register machine, a close cousin to the modern notion of the computer. Other models include combinatory logic and Markov algorithms. Gurevich adds the pointer machine model of Kolmogorov and Uspensky (1953, 1958): "...they just wanted to... convince themselves that there is no way to extend the notion of computable function."[42]
2981) 8.382142, Clock rate - Wikipedia, the free encyclopedia.txt#5, term: computer, content:The first commercial PC, the Altair 8800 (by MITS), used an Intel 8080 CPU with a clock rate of 2MHz (2 million cycles per second). The original IBM PC (c. 1981) had a clock rate of 4.77MHz (4,772,727 cycles per second). In 1992, both Hewlett-Packard and Digital Equipment Corporation broke the difficult 100 MHz limit with RISC techniques in the PA-7100 and AXP 21064 DEC Alpha respectively. In 1995, Intel's P5 Pentium chip ran at 100MHz (100 million cycles per second). On March 6, 2000, AMD reached the 1 GHz milestone a few months ahead of Intel. In 2002, an Intel Pentium 4 model was introduced as the first CPU with a clock rate of 3GHz (three billion cycles per second corresponding to ~3.31010seconds or 0.33 nanoseconds per cycle). Since then, the clock rate of production processors has increased much more slowly, with performance improvements coming from other design changes. 0.33 nanoseconds is the time for electric signal to travel a distance of about 10 cm, coming close to the distances a signal travels in the computer.
2982) 8.382142, COBOL - Wikipedia, the free encyclopedia.txt#115, term: computer, content:Later, COBOL suffered from a shortage of material covering it; it took until 1963 for introductory books to appear (with Richard D. Irwin publishing a college textbook on COBOL in 1966).[161] By 1985, there were twice as many books on Fortran and four times as many on BASIC as on COBOL in the Library of Congress.[162] University professors taught more modern, state-of-the-art languages and techniques instead of COBOL which was said to have a "trade school" nature.[163] Donald Nelson, chair of the CODASYL COBOL committee, said in 1984 that "academics ... hate COBOL" and that computer science graduates "had 'hate COBOL' drilled into them".[164] A 2013 poll by Micro Focus found that 20% of university academics thought COBOL was outdated or dead and that 55% believed their students thought COBOL was outdated or dead. The same poll also found that only 25% of academics had COBOL programming on their curriculum even though 60% thought they should teach it.[165] In contrast, in 2003, COBOL featured in 80% of information systems curricula in the United States, the same proportion as C++ and Java.[166]
2983) 8.382142, Computer keyboard - Wikipedia, the free encyclopedia.txt#35, term: computer, content:Alphabetical, numeric, and punctuation keys are used in the same fashion as a typewriter keyboard to enter their respective symbol into a word processing program, text editor, data spreadsheet, or other program. Many of these keys will produce different symbols when modifier keys or shift keys are pressed. The alphabetic characters become uppercase when the shift key or Caps Lock key is depressed. The numeric characters become symbols or punctuation marks when the shift key is depressed. The alphabetical, numeric, and punctuation keys can also have other functions when they are pressed at the same time as some modifier keys. The Space bar is a horizontal bar in the lowermost row, which is significantly wider than other keys. Like the alphanumeric characters, it is also descended from the mechanical typewriter. Its main purpose is to enter the space between words during typing. It is large enough so that a thumb from either hand can use it easily. Depending on the operating system, when the space bar is used with a modifier key such as the control key, it may have functions such as resizing or closing the current window, half-spacing, or backspacing. In computer games and other applications the key has myriad uses in addition to its normal purpose in typing, such as jumping and adding marks to check boxes. In certain programs for playback of digital video, the space bar is used for pausing and resuming the playback.
2984) 8.382142, Computer keyboard - Wikipedia, the free encyclopedia.txt#36, term: computer, content:Modifier keys are special keys that modify the normal action of another key, when the two are pressed in combination. For example, <Alt> + <F4> in Microsoft Windows will close the program in an active window. In contrast, pressing just <F4> will probably do nothing, unless assigned a specific function in a particular program. By themselves, modifier keys usually do nothing. The most widely used modifier keys include the Control key, Shift key and the Alt key. The AltGr key is used to access additional symbols for keys that have three symbols printed on them. On the Macintosh and Apple keyboards, the modifier keys are the Option key and Command key, respectively. On MIT computer keyboards, the Meta key is used as a modifier and for Windows keyboards, there is a Windows key. Compact keyboard layouts often use a Fn key. "Dead keys" allow placement of a diacritic mark, such as an accent, on the following letter (e.g., the Compose key). The Enter/Return key typically causes a command line, window form or dialog box to operate its default function, which is typically to finish an "entry" and begin the desired process. In word processing applications, pressing the enter key ends a paragraph and starts a new one.
2985) 8.382142, Computer mouse - Wikipedia, the free encyclopedia.txt#43, term: computer, content:These mice are specifically designed for use in computer games. They typically employ a wide array of controls and buttons and have designs that differ radically from traditional mice. It is also common for gaming mice, especially those designed for use in real-time strategy games such as StarCraft, or in multiplayer online battle arena games such as Dota 2 to have a relatively high sensitivity, measured in dots per inch (DPI).[49] Some advanced mice from gaming manufacturers also allow users to customize the weight of the mouse by adding or subtracting weights to allow for easier control.[50] Ergonomic quality is also an important factor in gaming mice, as extended gameplay times may render further use of the mouse to be uncomfortable. Some mice have been designed to have adjustable features such as removable and/or elongated palm rests, horizontally adjustable thumb rests and pinky rests. Some mice may include several different rests with their products to ensure comfort for a wider range of target consumers.[51] Gaming mice are held by gamers in three styles of grip:[52][53]
2986) 8.382142, Computer mouse - Wikipedia, the free encyclopedia.txt#68, term: computer, content:The computer industry often measures mouse sensitivity in terms of counts per inch (CPI), commonly expressed as dots per inch (DPI) the number of steps the mouse will report when it moves one inch. In early mice, this specification was called pulses per inch (ppi).[28] The Mickey originally referred to one of these counts, or one resolvable step of motion. If the default mouse-tracking condition involves moving the cursor by one screen-pixel or dot on-screen per reported step, then the CPI does equate to DPI: dots of cursor motion per inch of mouse motion. The CPI or DPI as reported by manufacturers depends on how they make the mouse; the higher the CPI, the faster the cursor moves with mouse movement. However, software can adjust the mouse sensitivity, making the cursor move faster or slower than its CPI. Current[update] software can change the speed of the cursor dynamically, taking into account the mouse's absolute speed and the movement from the last stop-point. In most software, an example being the Windows platforms, this setting is named "speed" referring to "cursor precision". However, some operating systems name this setting "acceleration", the typical Apple OS designation. This term is in fact incorrect. The mouse acceleration, in the majority of mouse software, refers to the setting allowing the user to modify the cursor acceleration: the change in speed of the cursor over time while the mouse movement is constant.
2987) 8.382142, Computer programming - Wikipedia, the free encyclopedia.txt#11, term: computer, content:As time has progressed, computers have made giant leaps in processing power, which have allowed the development of programming languages that are more abstracted from the underlying hardware. Popular programming languages of the modern era include ActionScript, C, C++, C#, Haskell, Java, JavaScript, Objective-C, Perl, PHP, Python, Ruby, Smalltalk, SQL, Visual Basic, and dozens more.[14] Although these high-level languages usually incur greater overhead, the increase in speed of modern computers has made the use of these languages much more practical than in the past. These increasingly abstracted languages are typically easier to learn and allow the programmer to develop applications much more efficiently and with less source code. However, high-level languages are still impractical for a few programs, such as those where low-level hardware control is necessary or where maximum processing speed is vital. Computer programming has become a popular career in the developed world, particularly in the United States, Europe, and Japan. Due to the high labor cost of programmers in these countries, some forms of programming have been increasingly subject to outsourcing (importing software and services from other countries, usually at a lower wage), making programming career decisions in developed countries more complicated, while increasing economic opportunities for programmers in less developed areas, particularly China and India.
2988) 8.382142, Computer science - Wikipedia, the free encyclopedia.txt#22, term: computer, content:Formal methods are a particular kind of mathematically based technique for the specification, development and verification of software and hardware systems. The use of formal methods for software and hardware design is motivated by the expectation that, as in other engineering disciplines, performing appropriate mathematical analysis can contribute to the reliability and robustness of a design. They form an important theoretical underpinning for software engineering, especially where safety or security is involved. Formal methods are a useful adjunct to software testing since they help avoid errors and can also give a framework for testing. For industrial use, tool support is required. However, the high cost of using formal methods means that they are usually only used in the development of high-integrity and life-critical systems, where safety or security is of utmost importance. Formal methods are best described as the application of a fairly broad variety of theoretical computer science fundamentals, in particular logic calculi, formal languages, automata theory, and program semantics, but also type systems and algebraic data types to problems in software and hardware specification and verification.
2989) 8.382142, Control unit - Wikipedia, the free encyclopedia.txt#5, term: computer, content:Other more advanced forms of Control Units manage the translation of instructions (but not the data containing portion) into several micro-instructions and the CU manages the scheduling of the micro-instructions between the selected execution units to which the data is then channeled and changed according to the execution unit's function (i.e., ALU contains several functions).[citation needed] On some processors, the Control Unit may be further broken down into additional units, such as an instruction unit or scheduling unit to handle scheduling, or a retirement unit to deal with results coming from the instruction pipeline.[citation needed] Again, the Control Unit orchestrates the main functions of the CPU: carrying out stored instructions in the software program then directing the flow of data throughout the computer based upon these instructions (roughly likened to how traffic lights will systematically control the flow of cars [containing data] to different locations within the traffic grid [CPU] until it parks at the desired parking spot [memory address/register].[citation needed] The car occupants [data] then go into the building [execution unit] and comes back changed in some way then get back into the car and returns to another location via the controlled traffic grid).[citation needed]
2990) 8.382142, Digital Equipment Corporation - Wikipedia, the free encyclopedia.txt#52, term: computer, content:The way the DEC standard RX50[52] floppy disk drive supported DEC's initial offerings seemed to encapsulate their approach to the personal computer market. Although the mechanical drive hardware was nearly identical to other 5" floppy disk drives available on competing systems,[53] DEC sought to differentiate their product by using a proprietary disk format for the data written on the disk. The DEC format had a higher capacity for data, but the RX50 drives were incompatible with other PC floppy drives. This required DEC owners to buy higher-priced, specially formatted floppy media, which was harder to obtain through standard distribution channels. DEC attempted to enforce exclusive control over its floppy media sales by copyrighting its proprietary disk format, and requiring a negotiated license agreement and royalty payments from anybody selling compatible media. The proprietary data format meant that RX50 floppies were not interchangeable with other PC floppies, further isolating DEC products from the developing de facto standard PC market. Hardware hackers and DEC enthusiasts eventually reverse-engineered the RX50 format,[52][54] but the damage had already been done, in terms of market confusion and isolation.
2991) 8.382142, DNA computing - Wikipedia, the free encyclopedia.txt#23, term: computer, content:Benenson, Shapiro and colleagues have demonstrated a DNA computer using the FokI enzyme[28] and expanded on their work by going on to show automata that diagnose and react to prostate cancer: under expression of the genes PPAP2B and GSTP1 and an over expression of PIM1 and HPN.[9] Their automata evaluated the expression of each gene, one gene at a time, and on positive diagnosis then released a single strand DNA molecule (ssDNA) that is an antisense for MDM2. MDM2 is a repressor of protein 53, which itself is a tumor suppressor.[29] On negative diagnosis it was decided to release a suppressor of the positive diagnosis drug instead of doing nothing. A limitation of this implementation is that two separate automata are required, one to administer each drug. The entire process of evaluation until drug release took around an hour to complete. This method also requires transition molecules as well as the FokI enzyme to be present. The requirement for the FokI enzyme limits application in vivo, at least for use in cells of higher organisms.[30] It should also be pointed out that the 'software' molecules can be reused in this case.
2992) 8.382142, Electronic literature - Wikipedia, the free encyclopedia.txt#6, term: computer, content:In 1975-76, Will Crowther programmed a text game named Colossal Cave Adventure (also known as Adventure). Considered one of the earlier computer adventure games, it possessed a story that had the reader make choices on which way to go. These choices could lead the reader to the end, or to his or her untimely death. This non-linear format was later mimicked by the text adventure game, Zork, created by a group of MIT students in 1977-79. These two games are considered to be the first examples of interactive fiction as well as some of the earliest video games. The earliest pieces of electronic literature as presently defined were created using Storyspace, software developed by Jay David Bolter and Michael Joyce in the 1980s.[7] They sold the software in 1990 to Eastgate Systems, a small software company that has maintained and updated the code in Storyspace up to the present.[6] Storyspace and other similar programs use hypertext to create links within text. Literature using hypertext is frequently referred to as hypertext fiction. Originally, these stories were often disseminated on discs and later on CD.[5] Hypertext fiction is still being created today using not only Storyspace, but other programs such as Twine.
2993) 8.382142, ENIAC - Wikipedia, the free encyclopedia.txt#26, term: computer, content:ENIAC was a one-of-a-kind design and was never repeated. The freeze on design in 1943 meant that the computer design would lack some innovations that soon became well-developed, notably the ability to store a program. Eckert and Mauchly started work on a new design, to be later called the EDVAC, which would be both simpler and more powerful. In particular, in 1944 Eckert wrote his description of a memory unit (the mercury delay line) which would hold both the data and the program. John von Neumann, who was consulting for the Moore School on the EDVAC, sat in on the Moore School meetings at which the stored program concept was elaborated. Von Neumann wrote up an incomplete set of notes (First Draft of a Report on the EDVAC) which were intended to be used as an internal memorandum describing, elaborating, and couching in formal logical language the ideas developed in the meetings. ENIAC administrator and security officer Herman Goldstine distributed copies of this First Draft to a number of government and educational institutions, spurring widespread interest in the construction of a new generation of electronic computing machines, including Electronic Delay Storage Automatic Calculator (EDSAC) at Cambridge University, England and SEAC at the U.S. Bureau of Standards.
2994) 8.382142, Ethernet - Wikipedia, the free encyclopedia.txt#6, term: computer, content:Ethernet initially competed with two largely proprietary systems, Token Ring and Token Bus. Because Ethernet was able to adapt to market realities and shift to inexpensive and ubiquitous twisted pair wiring, these proprietary protocols soon found themselves competing in a market inundated by Ethernet products, and, by the end of the 1980s, Ethernet was clearly the dominant network technology.[7] In the process, 3Com became a major company. 3Com shipped its first 10Mbit/s Ethernet 3C100 NIC in March 1981, and that year started selling adapters for PDP-11s and VAXes, as well as Multibus-based Intel and Sun Microsystems computers.[18]:9 This was followed quickly by DEC's Unibus to Ethernet adapter, which DEC sold and used internally to build its own corporate network, which reached over 10,000 nodes by 1986, making it one of the largest computer networks in the world at that time.[19] An Ethernet adapter card for the IBM PC was released in 1982, and, by 1985, 3Com had sold 100,000.[15] By the early 1990s, Ethernet became so prevalent that it was a must-have feature for modern computers, and Ethernet ports began to appear on some PCs and most workstations. This process was greatly sped up with the introduction of 10BASE-T and its relatively small modular connector, at which point Ethernet ports appeared even on low-end motherboards.
2995) 8.382142, Flash memory - Wikipedia, the free encyclopedia.txt#83, term: computer, content:With the increasing speed of modern CPUs, parallel flash devices are often much slower than the memory bus of the computer they are connected to. Conversely, modern SRAM offers access times below 10ns, while DDR2 SDRAM offers access times below 20ns. Because of this, it is often desirable to shadow code stored in flash into RAM; that is, the code is copied from flash into RAM before execution, so that the CPU may access it at full speed. Device firmware may be stored in a serial flash device, and then copied into SDRAM or SRAM when the device is powered-up.[61] Using an external serial flash device rather than on-chip flash removes the need for significant process compromise (a process that is good for high-speed logic is generally not good for flash and vice versa). Once it is decided to read the firmware in as one big block it is common to add compression to allow a smaller flash chip to be used. Typical applications for serial flash include storing firmware for hard drives, Ethernet controllers, DSL modems, wireless network devices, etc.
2996) 8.382142, Floppy disk - Wikipedia, the free encyclopedia.txt#27, term: computer, content:"A simple example of a good design is the 3-inch magnetic diskette for computers, a small circle of "floppy" magnetic material encased in hard plastic. Earlier types of floppy disks did not have this plastic case, which protects the magnetic material from abuse and damage. A sliding metal cover protects the delicate magnetic surface when the diskette is not in use and automatically opens when the diskette is inserted into the computer. The diskette has a square shape: there are apparently eight possible ways to insert it into the machine, only one of which is correct. What happens if I do it wrong? I try inserting the disk sideways. Ah, the designer thought of that. A little study shows that the case really isn't square: it's rectangular, so you can't insert a longer side. I try backward. The diskette goes in only part of the way. Small protrusions, indentations, and cutouts, prevent the diskette from being inserted backward or upside down: of the eight ways one might try to insert the diskette, only one is correct, and only that one will fit. An excellent design."
2997) 8.382142, Floppy disk - Wikipedia, the free encyclopedia.txt#28, term: computer, content:A spindle motor in the drive rotates the magnetic medium at a certain speed, while a stepper motor-operated mechanism moves the magnetic read/write head(s) radially along the surface of the disk. Both read and write operations require the media to be rotating and the head to contact the disk media, an action originally accomplished by a "disk load" solenoid.[18] Later drives held the heads out of contact until a front-panel lever was rotated (5") or disk insertion was complete (3"). To write data, current is sent through a coil in the head as the media rotates. The head's magnetic field aligns the magnetic particles directly below the head on the media. When the current is reversed the particles align in the opposite direction encoding the data digitally. To read data, the magnetic particles in the media induce a tiny voltage in the head coil as they pass under it. This small signal is amplified and sent to the floppy disk controller, which converts the streams of pulses from the media into data, checks it for errors, and sends it to the host computer system.
2998) 8.382142, Floppy disk - Wikipedia, the free encyclopedia.txt#33, term: computer, content:Different sizes of floppy disks are mechanically incompatible, and disks can fit only one size of drive. Drive assemblies with both 3-inch and 5-inch slots were available during the transition period between the sizes, but they contained two separate drive mechanisms. In addition, there are many subtle, usually software-driven incompatibilities between the two. 5-inch disks formatted for use with Apple II computers would be unreadable and treated as unformatted on a Commodore. As computer platforms began to form, attempts were made at interchangeability. For example, the "Superdrive" included from the Macintosh SE to the Power Macintosh G3 could read, write and format IBM PC format 3-inch disks, but few IBM-compatible computers had drives that did the reverse. 8-inch, 5-inch and 3-inch drives were manufactured in a variety of sizes, most to fit standardized drive bays. Alongside the common disk sizes were non-classical sizes for specialized systems.
2999) 8.382142, Floppy disk - Wikipedia, the free encyclopedia.txt#49, term: computer, content:Disk formatting is usually done by a utility program supplied by the computer OS manufacturer; generally, it sets up a file storage directory system on the disk, and initializes its sectors and tracks. Areas of the disk unusable for storage due to flaws can be locked (marked as "bad sectors") so that the operating system does not attempt to use them. This was time consuming so many environments had quick formatting which skipped the error checking process. When floppy disks were often used, disks pre-formatted for popular computers were sold. A formatted floppy disk does not include the sector and track headings of an unformatted disk; the difference in storage between them depends on the drive's application. Floppy disk drive and media manufacturers specify the unformatted capacity (for example, 2 MB for a standard 3-inch HD floppy). It is implied that this should not be exceeded, since doing so will most likely result in performance problems. DMF was introduced permitting 1.68 MB to fit onto an otherwise standard 3-inch disk; utilities then appeared allowing disks to be formatted as such.
3000) 8.382142, Fortran - Wikipedia, the free encyclopedia.txt#55, term: computer, content:Although a 1968 journal article by the authors of BASIC already described Fortran as "old-fashioned",[31] since Fortran has been in use for many decades, there is a vast body of Fortran software in daily use throughout the scientific and engineering communities.[32] Jay Pasachoff wrote in 1984 that "physics and astronomy students simply have to learn Fortran. So much exists in Fortran that it seems unlikely that scientists will change to Pascal, Modula-2, or whatever."[33] In 1993, Cecil E. Leith called Fortran the "mother tongue of scientific computing" adding that its replacement by any other possible language "may remain a forlorn hope."[34] It is the primary language for some of the most intensive supercomputing tasks, such as astronomy, weather and climate modeling, numerical linear algebra (LAPACK), numerical libraries (IMSL and NAG), structural engineering, hydrological modeling, optimization, satellite simulation and data analysis, computational fluid dynamics, computational chemistry, computational economics and computational physics. Many of the floating-point benchmarks to gauge the performance of new computer processors  such as CFP2006, the floating-point component of the SPEC CPU2006 benchmarks  are written in Fortran.
3001) 8.382142, Free software - Wikipedia, the free encyclopedia.txt#4, term: computer, content:In 1983, Richard Stallman, one of the original authors of the popular Emacs program and a longtime member of the hacker community at the MIT Artificial Intelligence Laboratory, announced the GNU project, the purpose of which was to produce a completely non-proprietary Unix-compatible operating system, saying that he had become frustrated with the shift in climate surrounding the computer world and its users. In his initial declaration of the project and its purpose, he specifically cited as a motivation his opposition to being asked to agree to non-disclosure agreements and restrictive licenses which prohibited the free sharing of potentially profitable in-development software, a prohibition directly contrary to the traditional hacker ethic. Software development for the GNU operating system began in January 1984, and the Free Software Foundation (FSF) was founded in October 1985. He developed a free software definition and the concept of "copyleft", designed to ensure software freedom for all. Some non-software industries are beginning to use techniques similar to those used in free software development for their research and development process; scientists, for example, are looking towards more open development processes, and hardware such as microchips are beginning to be developed with specifications released under copyleft licenses (see the OpenCores project, for instance). Creative Commons and the free culture movement have also been largely influenced by the free software movement.
3002) 8.382142, Grace Hopper - Wikipedia, the free encyclopedia.txt#6, term: computer, content:In 1943, during World War II, Hopper obtained a leave of absence from Vassar and was sworn into the United States Navy Reserve, one of many women to volunteer to serve in the WAVES. She had to get an exemption to enlist; she was 15 pounds (6.8kg) below the Navy minimum weight of 120 pounds (54kg). She reported in December and trained at the Naval Reserve Midshipmen's School at Smith College in Northampton, Massachusetts. Hopper graduated first in her class in 1944, and was assigned to the Bureau of Ships Computation Project at Harvard University as a lieutenant, junior grade. She served on the Mark I computer programming staff headed by Howard H. Aiken. Hopper and Aiken coauthored three papers on the Mark I, also known as the Automatic Sequence Controlled Calculator. Hopper's request to transfer to the regular Navy at the end of the war was declined due to her age (38). She continued to serve in the Navy Reserve. Hopper remained at the Harvard Computation Lab until 1949, turning down a full professorship at Vassar in favor of working as a research fellow under a Navy contract at Harvard.[19]
3003) 8.382142, High-level programming language - Wikipedia, the free encyclopedia.txt#2, term: computer, content:The first high-level programming language designed for computers was Plankalkl, created by Konrad Zuse.[4] However, it was not implemented in his time, and his original contributions were (due to World War II) largely isolated from other developments, although it influenced Heinz Rutishauser's language "Superplan" (and to some degree also Algol). The first really widespread high-level language was Fortran, a machine independent development of IBM's earlier Autocode systems. Algol, defined in 1958 and 1960, by committees of European and American computer scientists, introduced recursion as well as nested functions under lexical scope. It was also the first language with a clear distinction between value and name-parameters and their corresponding semantics.[5] Algol also introduced several structured programming concepts, such as the while-do and if-then-else constructs and its syntax was the first to be described by a formal method, BackusNaur Form (BNF). During roughly the same period Cobol introduced records (also called structs) and Lisp introduced a fully general lambda abstraction in a programming language for the first time.
3004) 8.382142, History of computing hardware - Wikipedia, the free encyclopedia.txt#73, term: computer, content:IBM introduced a smaller, more affordable computer in 1954 that proved very popular.[95] The IBM 650 weighed over 900kg, the attached power supply weighed around 1350kg and both were held in separate cabinets of roughly 1.5 meters by 0.9 meters by 1.8 meters. It cost US$500,000[96] ($4.41million as of 2016) or could be leased for US$3,500 a month ($30thousand as of 2016).[94] Its drum memory was originally 2,000 ten-digit words, later expanded to 4,000 words. Memory limitations such as this were to dominate programming for decades afterward. The program instructions were fetched from the spinning drum as the code ran. Efficient execution using drum memory was provided by a combination of hardware architecture: the instruction format included the address of the next instruction; and software: the Symbolic Optimal Assembly Program, SOAP,[97] assigned instructions to the optimal addresses (to the extent possible by static analysis of the source program). Thus many instructions were, when needed, located in the next row of the drum to be read and additional wait time for drum rotation was not required.
3005) 8.382142, History of computing hardware - Wikipedia, the free encyclopedia.txt#101, term: computer, content:Systems as complicated as computers require very high reliability. ENIAC remained on, in continuous operation from 1947 to 1955, for eight years before being shut down. Although a vacuum tube might fail, it would be replaced without bringing down the system. By the simple strategy of never shutting down ENIAC, the failures were dramatically reduced. The vacuum-tube SAGE air-defense computers became remarkably reliable  installed in pairs, one off-line, tubes likely to fail did so when the computer was intentionally run at reduced power to find them. Hot-pluggable hard disks, like the hot-pluggable vacuum tubes of yesteryear, continue the tradition of repair during continuous operation. Semiconductor memories routinely have no errors when they operate, although operating systems like Unix have employed memory tests on start-up to detect failing hardware. Today, the requirement of reliable performance is made even more stringent when server farms are the delivery platform.[136] Google has managed this by using fault-tolerant software to recover from hardware failures, and is even working on the concept of replacing entire server farms on-the-fly, during a service event.[137][138]
3006) 8.382142, Home computer - Wikipedia, the free encyclopedia.txt#15, term: computer, content:Introduced in August 1981, the IBM Personal Computer would eventually supplant CP/M as the standard platform used in business. This was largely due to the IBM name and the system's 16 bit open architecture, which expanded maximum memory tenfold, and also encouraged production of third-party clones. In the late 1970s, the 6502-based Apple II series had carved out a niche for itself in business, thanks to the industry's first killer app, VisiCalc, released in 1979. However the Apple II would quickly be displaced for office use by IBM PC compatibles running Lotus 1-2-3.[24] Apple Computer's 1980 Apple III was underwhelming, and although the 1984 release of the Apple Macintosh introduced the modern GUI to the market, it wasn't common until IBM-compatible computers adopted it.[25] Throughout the 1980s, businesses large and small adopted the PC platform, leading, by the end of the decade, to sub-US$1000 IBM PC XT-class white box machines, usually built in Asia and sold by US companies like PCs Limited.
3007) 8.382142, Home computer - Wikipedia, the free encyclopedia.txt#33, term: computer, content:From about 1985, the high end of the home computer market began to be dominated by "next generation" home computers using the 16-bit Motorola 68000 chip, which enabled the greatly increased abilities of the Amiga and Atari ST series. Graphics resolutions approximately doubled to give roughly NTSC-class resolution, and color palettes increased from dozens to hundreds or thousands of colors available. Stereo sound became standard for the first time. Clock rates on the 68000-based systems were approximately 8MHz with RAM capacities of 256KB (for the base Amiga 1000[40]) up to 1024KB (1megabyte, a milestone, first seen on the Atari 1040ST). These systems used 3.5" floppy disks from the beginning but 5.25" drives were made available to facilitate data exchange with IBM PC compatibles. The Amiga and ST both had GUIs inspired by the Apple Macintosh, but at a list price of US$2495 (over $5000 in 2007 dollars), the Macintosh itself was too expensive for most households.
3008) 8.382142, Home computer - Wikipedia, the free encyclopedia.txt#36, term: computer, content:All this was predicted to be commonplace by the end of the 1980s, but by 1987 Dan Gutman wrote that the predicted revolution was "in shambles", with only 15% of American homes owning a computer.[54] Virtually every aspect that was foreseen would be delayed to later years or would be entirely surpassed by later technological developments. The home computers of the early 1980s could not multitask,[55] which meant that using one as a home automation or entertainment appliance would require it be kept powered on at all times and dedicated exclusively for this use. Even if the computers could be used for multiple purposes simultaneously as today, other technical limitations predominated; memory capacities were too small to hold entire encyclopedias or databases of financial records;[56] floppy disk-based storage was inadequate in both capacity and speed for multimedia work;[57] and the home computers' graphics chips could only display blocky, unrealistic images and blurry, jagged text that would be difficult to read a newspaper from.[58] Although CD-ROM technology was introduced in 1985 with much promise for its future use, the drives were prohibitively expensive and only interfaced with IBM PCs and compatibles.[59][60][61]
3009) 8.382142, HP 2100 - Wikipedia, the free encyclopedia.txt#19, term: computer, content:Czechoslovakia produced its own HP 1000 compatible clones, designated ADT4000 (4300, 4500, 4700, 4900). More than 1000 units were delivered by the vendors Aritma Prague (development), ZPA akovice and ZPA Trutnov between 1973 and 1990. Those computers served in power plants, including nuclear ones, other industry, military, at universities, etc., for their high reliability and real-time features. Operating systems were DOS/ADT (several versions) and Unix. The oldest hybrid ADT7000 (1974) was composed of digital ADT4000 and analog ADT3000 parts, but only the digital part was interesting for customers. ADT4316 (1976) had 16K words of ferrite core memory, the ADT4500 (1978) up to 4M words of semiconductor RAM. The ADT 4900 was designed as a single-board computer, but its mass production did not start. Czechoslovak People's Army used ADT based MOMI 1 and MOMI 2 mobile minicomputers, built into a container carried by the Tatra 148 truck.
3010) 8.382142, IBM 7090 - Wikipedia, the free encyclopedia.txt#11, term: computer, content:The 7090 series featured a data channel architecture for input and output, a forerunner of modern direct memory access I/O. Up to eight data channels could be attached, with up to ten IBM 729 tape drives attached to each channel. The data channels had their own very limited set of operations called commands. These were used with tape (and later, disk) storage as well as card units and printers, and offered high performance for the time. Printing and punched card I/O, however, employed the same modified unit record equipment introduced with the 704 and was slow. It became common to use a less expensive IBM 1401 computer to read cards onto magnetic tape for transfer to the 7090/94. Output would be spooled onto tape and transferred to the 1401 for printing or card punching using its much faster peripherals, notably the IBM 1403 line printer. Later IBM introduced the 7094/7044 Direct Coupled System, using data channel to data channel communication, with the 7094 primarily performing computations and the 7044 performing I/O operations using its fast 1400-series peripherals.
3011) 8.382142, IBM PC compatible - Wikipedia, the free encyclopedia.txt#4, term: computer, content:IBM at first asked developers to avoid writing software that addressed the computer's hardware directly, and to instead make standard calls to BIOS functions that carried out hardware-dependent operations.[1] This software would run on any machine using MS-DOS or PC-DOS. Software that directly addressed the hardware instead of making standard calls was faster, however; this was particularly relevant to games. Software addressing IBM PC hardware in this way would not run on MS-DOS machines with different hardware. The IBM PC was sold in high enough volumes to justify writing software specifically for it, and this encouraged other manufacturers to produce machines which could use the same programs, expansion cards, and peripherals as the PC. The 808x computer marketplace rapidly excluded all machines which were not hardware- and software-compatible with the PC. The 640 kB barrier on "conventional" system memory available to MS-DOS is a legacy of that period; other non-clone machines, while subject to a limit, could exceed 640kB.
3012) 8.382142, IBM PC DOS - Wikipedia, the free encyclopedia.txt#11, term: computer, content:In August 1984, IBM introduced the Intel 80286-derived IBM PC/AT, its next-generation machine. Along with this was DOS 3.00. Despite jumping a whole version number, it again proved little more than an incremental upgrade, adding nothing more substantial than support for the AT's new 1.2 megabyte (MB) floppy disks. Planned networking capabilities in DOS 3.00 were judged too buggy to be usable and Microsoft disabled them prior to the OS's release. In any case, IBM's original plans for the AT had been to equip it with a proper next-generation OS that would use its extended features, but this never materialized.[1] PC DOS 3.1 (released March 1985) fixed the bugs in DOS 3.00 and supported IBM's Network Adapter card on the IBM PC Network. PC DOS 3.2 added support for 3-inch double-density 720 kB floppy disk drives, supporting the IBM PC Convertible, IBM's first computer to use 3-inch floppy disks, released April 1986.
3013) 8.382142, IBM SSEC - Wikipedia, the free encyclopedia.txt#8, term: computer, content:Seeber had carefully designed the SSEC to treat instructions as data, so they could be modified and stored under program control. IBM filed a patent based on the SSEC on January 19, 1949, which was later upheld as supporting the machine's stored program ability.[6]:136[17] Each instruction could take input from any source (electronic or mechanical registers or tape readers) store the result in any destination (electronic or mechanical registers, tape or card punch or printer), and gave the address of the next instruction, which could also be any source. This made it powerful in theory.[14] However, in practice instructions were stored usually on paper tape, resulting in an overall rate of only about 50 instructions per second. The serial nature of the paper tape memory made programming the SSEC more like the World War II era calculators. For example, "loops" were usually literal loops of paper tape glued together. For each new program, tapes and card decks were literally "loaded" on the readers, and a plugboard changed in the printer to modify output formatting. For these reasons, the SSEC is usually classified as the last of the "programmable calculator" machines instead of the first stored-program computer.[18]
3014) 8.382142, Integral - Wikipedia, the free encyclopedia.txt#91, term: computer, content:A major mathematical difficulty in symbolic integration is that in many cases, a closed formula for the antiderivative of a rather simple-looking function does not exist. For instance, it is known that the antiderivatives of the functions exp(x2), xx and (sin x)/x cannot be expressed in the closed form involving only rational and exponential functions, logarithm, trigonometric and inverse trigonometric functions, and the operations of multiplication and composition; in other words, none of the three given functions is integrable in elementary functions, which are the functions which may be built from rational functions, roots of a polynomial, logarithm, and exponential functions. The Risch algorithm provides a general criterion to determine whether the antiderivative of an elementary function is elementary, and, if it is, to compute it. Unfortunately, it turns out that functions with closed expressions of antiderivatives are the exception rather than the rule. Consequently, computerized algebra systems have no hope of being able to find an antiderivative for a randomly constructed elementary function. On the positive side, if the 'building blocks' for antiderivatives are fixed in advance, it may be still be possible to decide whether the antiderivative of a given function can be expressed using these blocks and operations of multiplication and composition, and to find the symbolic answer whenever it exists. The Risch algorithm, implemented in Mathematica and other computer algebra systems, does just that for functions and antiderivatives built from rational functions, radicals, logarithm, and exponential functions.
3015) 8.382142, Integrated circuit - Wikipedia, the free encyclopedia.txt#15, term: computer, content:SSI circuits were crucial to early aerospace projects, and aerospace projects helped inspire development of the technology. Both the Minuteman missile and Apollo program needed lightweight digital computers for their inertial guidance systems. Although the Apollo guidance computer led and motivated integrated-circuit technology,[21] it was the Minuteman missile that forced it into mass-production. The Minuteman missile program and various other Navy programs accounted for the total $4 million integrated circuit market in 1962, and by 1968, U.S. Government space and defense spending still accounted for 37% of the $312 million total production. The demand by the U.S. Government supported the nascent integrated circuit market until costs fell enough to allow firms to penetrate the industrial, and eventually, the consumer markets. The average price per integrated circuit dropped from $50.00 in 1962 to $2.33 in 1968.[22] Integrated circuits began to appear in consumer products by the turn of the decade, a typical application being FM inter-carrier sound processing in television receivers.
3016) 8.382142, Intel - Wikipedia, the free encyclopedia.txt#126, term: computer, content:In February 2008, Intel stated that its office in Munich had been raided by European Union regulators. Intel reported that it was cooperating with investigators.[257] Intel faced a fine of up to 10% of its annual revenue, if found guilty of stifling competition.[258] AMD subsequently launched a website promoting these allegations.[259][260] In June 2008, the EU filed new charges against Intel.[261] In May 2009, the EU found that Intel had engaged in anti-competitive practices and subsequently fined Intel 1.06billion (US$1.44billion), a record amount. Intel was found to have paid companies, including Acer, Dell, HP, Lenovo and NEC,[262] to exclusively use Intel chips in their products, and therefore harmed other companies including AMD.[262][263][264] The European Commission said that Intel had deliberately acted to keep competitors out of the computer chip market and in doing so had made a "serious and sustained violation of the EU's antitrust rules".[262] In addition to the fine, Intel was ordered by the Commission to immediately cease all illegal practices.[262] Intel has stated that they will appeal against the Commission's verdict. In June 2014, the General Court, which sits below the European Court of Justice, rejected the appeal.[262]
3017) 8.382142, Intel 8080 - Wikipedia, the free encyclopedia.txt#4, term: computer, content:The Intel 8080 was the successor to the 8008. It used the same basic instruction set and register model as the 8008 (developed by Computer Terminal Corporation), even though it was not source code compatible nor binary compatible with its predecessor. Every instruction in the 8008 has an equivalent instruction in the 8080 (even though the actual opcodes differ between the two CPUs). The 8080 also added a few 16-bit operations to its instruction set as well. Whereas the 8008 required the use of the HL register pair to indirectly access its 14-bit memory space, the 8080 added addressing modes to allow direct access to its full 16-bit memory space. In addition, the internal 7-level push-down call stack of the 8008 was replaced by a dedicated 16-bit stack pointer (SP) register. The 8080's large 40-pin DIP packaging permitted it to provide a 16-bit address bus and an 8-bit data bus, allowing easy access to 64KB of memory.
3018) 8.382142, Interactive fiction - Wikipedia, the free encyclopedia.txt#49, term: computer, content:Infocom's games of 1979-88, such as Zork, were written using a LISP-like programming language called ZIL (Zork Implementation Language or Zork Interactive Language, it was referred to as both) that compiled into a byte code able to run on a standardized virtual machine called the Z-machine. As the games were text based and used variants of the same Z-machine interpreter, the interpreter only had to be ported to a computer once, rather than once each game. Each game file included a sophisticated parser which allowed the user to type complex instructions to the game. Unlike earlier works of interactive fiction which only understood commands of the form 'verb noun', Infocom's parser could understand a wider variety of sentences. For instance one might type "open the large door, then go west", or "go to the hall". With the Z-machine, Infocom was able to release most of their games for most popular home computers of the time simultaneously, including Apple II family, Atari 800, IBM PC compatibles, Amstrad CPC/PCW (one disc worked on both machines), Commodore 64, Commodore Plus/4, Commodore 128, Kaypro CP/M, Texas Instruments TI-99/4A, the Mac, Atari ST, the Commodore Amiga and the Radio Shack TRS-80. Infocom was also known for shipping creative props, or "feelies" (and even "smellies"), with its games.
3019) 8.382142, Interactive fiction - Wikipedia, the free encyclopedia.txt#53, term: computer, content:The Z-machine was designed by the founders of Infocom, in 1979. They were influenced by the then-new idea of a virtual Pascal computer, but replaced P with Z for Zork, the celebrated adventure game of 1977-79. The Z-machine evolved during the 1980s but over 30 years later, it remains in use essentially unchanged. Glulx was designed by Andrew Plotkin in the late 1990s as a new-generation IF virtual machine. It overcomes the technical constraint on the Z-machine by being a 32-bit rather than 16-bit processor. Frotz is a modern Z-machine interpreter originally written in C (programming language) by Stefan Jokisch in 1995 for DOS. Over time it was ported to other platforms, such as Unix, RISC OS, Mac OS and most recently iOS. Modern Glulx interpreters are based on "Glulxe", by Andrew Plotkin, and "Git", by Iain Merrick. Other interpreters include Zoom for Mac OS X, or for Unix or Linux, maintained by Andrew Hunter, and Spatterlight for Mac OS X, maintained by Tor Andersson.
3020) 8.382142, International Federation for Information Processing - Wikipedia, the free encyclopedia.txt#1, term: computer, content:IFIP actively promotes the principle of open access and proceedings for which IFIP holds the copyright are made available electronically via IFIPs Open Access Digital Library.[1] Downloading articles from IFIPs Open DL is not only free of charge, but unlike commercial publishers and other professional organisations, IFIP does not charge authors of open access articles to publish in its Open DL. Conference and workshop organizers who prefer printed proceedings can take advantage of the agreement between IFIP and Springer and publish their proceedings as part of IFIP's Advances in Information and Communication Technology (AICT) series or in the Lecture Notes in Computer Science (LNCS) series. Proceedings published by Springer in IFIPs LNCS and AICT series will be included within IFIPs Open DL after an embargo period of three years. An important activity of the IFIP Technical Committees is to organise and sponsor high quality conferences and workshops in the field of ICT. Sponsoring is generally in the form of Best Paper Awards (BPA) and/or Student Travel Grants (STG). To assist conference and workshop organisers, IFIP has facilities to host conference websites and supports conference management systems such as JEMS, which include export functions that seamlessly integrate with IFIPs Open DL.
3021) 8.382142, Internet - Wikipedia, the free encyclopedia.txt#99, term: computer, content:Packet capture is the monitoring of data traffic on a computer network. Computers communicate over the Internet by breaking up messages (emails, images, videos, web pages, files, etc.) into small chunks called "packets", which are routed through a network of computers, until they reach their destination, where they are assembled back into a complete "message" again. Packet Capture Appliance intercepts these packets as they are traveling through the network, in order to examine their contents using other programs. A packet capture is an information gathering tool, but not an analysis tool. That is it gathers "messages" but it does not analyze them and figure out what they mean. Other programs are needed to perform traffic analysis and sift through intercepted data looking for important/useful information. Under the Communications Assistance For Law Enforcement Act all U.S. telecommunications providers are required to install packet sniffing technology to allow Federal law enforcement and intelligence agencies to intercept all of their customers' broadband Internet and voice over Internet protocol (VoIP) traffic.[120]
3022) 8.382142, Jack Dongarra - Wikipedia, the free encyclopedia.txt#2, term: computer, content:He specializes in numerical algorithms in linear algebra, parallel computing, the use of advanced-computer architectures, programming methodology, and tools for parallel computers. His research includes the development, testing and documentation of high quality mathematical software. He has contributed to the design and implementation of the following open source software packages and systems: EISPACK, LINPACK, the BLAS, LAPACK, ScaLAPACK,[3][4] Netlib, PVM, MPI,[5] NetSolve,[6] TOP500, ATLAS,[7] and PAPI.[8] With Eric Grosse, he pioneered the open source distribution of numeric source code via email with netlib. He has published approximately 300 articles, papers, reports and technical memoranda and he is coauthor of several books. He was awarded the IEEE Sid Fernbach Award in 2004 for his contributions in the application of high performance computers using innovative approaches; in 2008 he was the recipient of the first IEEE Medal of Excellence in Scalable Computing; in 2010 he was the first recipient of the SIAM Special Interest Group on Supercomputing's award for Career Achievement; in 2011 he was the recipient of the IEEE IPDPS Charles Babbage Award; and in 2013 he was the recipient of the ACM/IEEE Ken Kennedy Award for his leadership in designing and promoting standards for mathematical software used to solve numerical problems common to high performance computing.He is a Fellow of the AAAS, ACM, SIAM, and the IEEE and a member of the National Academy of Engineering.
3023) 8.382142, John von Neumann - Wikipedia, the free encyclopedia.txt#62, term: computer, content:Von Neumann is credited with the equilibrium strategy of mutual assured destruction, providing the deliberately humorous acronym, MAD. (Other humorous acronyms coined by von Neumann include his computer, the Mathematical Analyzer, Numerical Integrator, and Computeror MANIAC). He also "moved heaven and earth" to bring MAD about. His goal was to quickly develop ICBMs and the compact hydrogen bombs that they could deliver to the USSR, and he knew the Soviets were doing similar work because the CIA interviewed German rocket scientists who were allowed to return to Germany, and von Neumann had planted a dozen technical people in the CIA. The Russians believed that bombers would soon be vulnerable, and they shared von Neumann's view that an H-bomb in an ICBM was the ne plus ultra of weapons, and they believed that whoever had superiority in these weapons would take over the world, without necessarily using them.[115] He was afraid of a "missile gap" and took several more steps to achieve his goal of keeping up with the Soviets:
3024) 8.382142, JPEG - Wikipedia, the free encyclopedia.txt#104, term: computer, content:The JPEG committee investigated the patent claims in 2002 and were of the opinion that they were invalidated by prior art.[36] Others also concluded that Forgent did not have a patent that covered JPEG.[37] Nevertheless, between 2002 and 2004 Forgent was able to obtain about US$105 million by licensing their patent to some 30 companies. In April 2004, Forgent sued 31 other companies to enforce further license payments. In July of the same year, a consortium of 21 large computer companies filed a countersuit, with the goal of invalidating the patent. In addition, Microsoft launched a separate lawsuit against Forgent in April 2005.[38] In February 2006, the United States Patent and Trademark Office agreed to re-examine Forgent's JPEG patent at the request of the Public Patent Foundation.[39] On May 26, 2006 the USPTO found the patent invalid based on prior art. The USPTO also found that Forgent knew about the prior art, and did not tell the Patent Office, making any appeal to reinstate the patent highly unlikely to succeed.[40]
3025) 8.382142, LINC - Wikipedia, the free encyclopedia.txt#18, term: computer, content:While Bell in his book says designing the LINC provided the ideas for DEC's second and third machines, the 18-bit inexpensive follow-on to its first, the PDP-4 and the company's first 12-bit design of its own, the PDP-5, Digital Equipment Corporation would launch the extremely successful PDP-8 before it manufactured the first next-generation LINC-compatible computer, the LINC-8 and a combination of the 7400-series chip-based PDP-8/I and a redesigned LINC, combined as the PDP-12. DEC's final 12-bit lab machine, the Lab-8/E, did away with the LINC entirely. [1]. The first follow-on, the LINC-8, booted (slowly) to a PDP-8 program called PROGOFOP (PROGram OF OPeration) which interfaced to the separate LINC hardware. The PDP-12 was the last and most popular follow-on to the LINC. It was a capable and improved machine, and was more stable than the LINC-8, but architecturally was still an imperfect hybrid of a LINC and a PDP-8, full of many small technical glitches. (For example, the LINC had an overflow bit which was a small but important part of the LINC's machine state; the PDP-12 had no provision for saving and restoring the state of this bit across PDP-8 interrupts.)
3026) 8.382142, Linux - Wikipedia, the free encyclopedia.txt#1, term: computer, content:Linux was originally developed as a free operating system for personal computers based on the Intel x86 architecture, but has since been ported to more computer hardware platforms than any other operating system.[18] Because of the dominance of Android on smartphones, Linux has the largest installed base of all general-purpose operating systems.[19] Linux, in its original form, is also the leading operating system on servers and other big iron systems such as mainframe computers and virtually all fastest supercomputers,[20][21] but is used on only around 1.6% of desktop computers[22][23] when not including ChromeOS, which has about 5% of the overall and nearly 20% of the sub-$300 notebook sales.[24] Linux also runs on embedded systems, which are devices whose operating system is typically built into the firmware and is highly tailored to the system; this includes smartphones and tablet computers running Android and other Linux derivatives,[25] TiVo and similar DVR devices, network routers, facility automation controls, televisions,[26][27] video game consoles and smartwatches.[28]
3027) 8.382142, Lisp (programming language) - Wikipedia, the free encyclopedia.txt#94, term: computer, content:Programmers in the Scheme dialect often express loops using tail recursion. Scheme's commonality in academic computer science has led some students to believe that tail recursion is the only, or the most common, way to write iterations in Lisp, but this is incorrect. All oft-seen Lisp dialects have imperative-style iteration constructs, from Scheme's do loop to Common Lisp's complex loop expressions. Moreover, the key issue that makes this an objective rather than subjective matter is that Scheme makes specific requirements for the handling of tail calls, and thus the reason that the use of tail recursion is generally encouraged for Scheme is that the practice is expressly supported by the language definition. By contrast, ANSI Common Lisp does not require[43] the optimization commonly termed a tail call elimination. Thus, the fact that tail recursive style as a casual replacement for the use of more traditional iteration constructs (such as do, dolist or loop) is discouraged[44] in Common Lisp is not just a matter of stylistic preference, but potentially one of efficiency (since an apparent tail call in Common Lisp may not compile as a simple jump) and program correctness (since tail recursion may increase stack use in Common Lisp, risking stack overflow).
3028) 8.382142, Magnetic-core memory - Wikipedia, the free encyclopedia.txt#32, term: computer, content:Diagnosing hardware problems in core memory required time-consuming diagnostic programs to be run. While a quick test checked if every bit could contain a one and a zero, these diagnostics tested the core memory with worst-case patterns and had to run for several hours. As most computers had just a single core memory board, these diagnostics also moved themselves around in memory, making it possible to test every bit. An advanced test was called a "Schmoo test" in which the half-select currents were modified along with the time at which the sense line was tested ("strobed"). The data plot of this test seemed to resemble a cartoon character called "Schmoo," and the name stuck. In many occasions, errors could be resolved by gently tapping the printed circuit board with the core array on a table. This slightly changed the positions of the cores along the wires running through them, and could fix the problem. The procedure was seldom needed, as core memory proved to be very reliable compared to other computer components of the day.
3029) 8.382142, Mainframe computer - Wikipedia, the free encyclopedia.txt#15, term: computer, content:Several manufacturers produced mainframe computers from the late 1950s through the 1970s. The group of manufacturers was first known as "IBM and the Seven Dwarfs":[18]:p.83 usually Burroughs, UNIVAC, NCR, Control Data, Honeywell, General Electric and RCA, although some lists varied. Later, with the departure of General Electric and RCA, it was referred to as IBM and the BUNCH. IBM's dominance grew out of their 700/7000 series and, later, the development of the 360 series mainframes. The latter architecture has continued to evolve into their current zSeries mainframes which, along with the then Burroughs and Sperry (now Unisys) MCP-based and OS1100 mainframes, are among the few mainframe architectures still extant that can trace their roots to this early period. While IBM's zSeries can still run 24-bit System/360 code, the 64-bit zSeries and System z9 CMOS servers have nothing physically in common with the older systems. Notable manufacturers outside the USA were Siemens and Telefunken in Germany, ICL in the United Kingdom, Olivetti in Italy, and Fujitsu, Hitachi, Oki, and NEC in Japan. The Soviet Union and Warsaw Pact countries manufactured close copies of IBM mainframes during the Cold War; the BESM series and Strela are examples of an independently designed Soviet computer.
3030) 8.382142, Manchester Mark 1 - Wikipedia, the free encyclopedia.txt#7, term: computer, content:The SSEM had been designed by the team of Frederic C. Williams, Tom Kilburn and Geoff Tootill. To develop the Mark 1 they were joined by two research students, D.B.G.Edwards and G. E. Thomas; work began in earnest in August 1948. The project soon had the dual purpose of supplying Ferranti with a working design on which they could base a commercial machine, the Ferranti Mark 1, and of building a computer that would allow researchers to gain experience of how such a machine could be used in practice. The first of the two versions of the Manchester Mark 1 known as the Intermediary Version was operational by April 1949.[10] However, this first version lacked features such as the instructions necessary to programmatically transfer data between the main store and its newly developed magnetic backing store, which had to be done by halting the machine and manually initiating the transfer. These missing features were incorporated in the Final Specification version, which was fully working by October 1949.[13] The machine contained 4,050 valves and had a power consumption of 25 kilowatts.[14] To increase reliability, purpose-built CRTs made by GEC were used in the machine instead of the standard devices used in the SSEM.[2]
3031) 8.382142, Manchester Small-Scale Experimental Machine - Wikipedia, the free encyclopedia.txt#10, term: computer, content:Although early computers such as EDSAC made successful use of mercury delay line memory,[15] the technology had several drawbacks; it was heavy, it was expensive, and it did not allow data to be accessed randomly. In addition, because data was stored as a sequence of acoustic waves propagated through a mercury column, the device's temperature had to be very carefully controlled, as the velocity of sound through a medium varies with its temperature. Williams had seen an experiment at Bell Labs demonstrating the effectiveness of cathode ray tubes (CRT) as an alternative to the delay line for removing ground echoes from radar signals. While working at the TRE, shortly before he joined the University of Manchester in December 1946, he and Tom Kilburn had developed a form of electronic memory known as the Williams or WilliamsKilburn tube based on a standard CRT, the first random-access digital storage device.[16] The Manchester Small-Scale Experimental Machine (SSEM) was designed to show that the system was a practical storage device, by testing that data held within it could be read and written at the speed necessary for use in a computer.[17]
3032) 8.382142, Massachusetts Institute of Technology - Wikipedia, the free encyclopedia.txt#13, term: computer, content:MIT's involvement in military research surged during World War II. In 1941, Vannevar Bush was appointed head of the federal Office of Scientific Research and Development and directed funding to only a select group of universities, including MIT.[52] Engineers and scientists from across the country gathered at MIT's Radiation Laboratory, established in 1940 to assist the British military in developing microwave radar. The work done there significantly affected both the war and subsequent research in the area.[53] Other defense projects included gyroscope-based and other complex control systems for gunsight, bombsight, and inertial navigation under Charles Stark Draper's Instrumentation Laboratory;[54][55] the development of a digital computer for flight simulations under Project Whirlwind;[56] and high-speed and high-altitude photography under Harold Edgerton.[57][58] By the end of the war, MIT became the nation's largest wartime R&D contractor (attracting some criticism of Bush),[52] employing nearly 4000 in the Radiation Laboratory alone[53] and receiving in excess of $100 million ($1.2 billion in 2015 dollars) before 1946.[44] Work on defense projects continued even after then. Post-war government-sponsored research at MIT included SAGE and guidance systems for ballistic missiles and Project Apollo.[59]
3033) 8.382142, Microsoft Windows - Wikipedia, the free encyclopedia.txt#14, term: computer, content:The next major consumer-oriented release of Windows, Windows 95, was released on August 24, 1995. While still remaining MS-DOS-based, Windows 95 introduced support for native 32-bit applications, plug and play hardware, preemptive multitasking, long file names of up to 255 characters, and provided increased stability over its predecessors. Windows 95 also introduced a redesigned, object oriented user interface, replacing the previous Program Manager with the Start menu, taskbar, and Windows Explorer shell. Windows 95 was a major commercial success for Microsoft; Ina Fried of CNET remarked that "by the time Windows 95 was finally ushered off the market in 2001, it had become a fixture on computer desktops around the world."[17] Microsoft published four OEM Service Releases (OSR) of Windows 95, each of which was roughly equivalent to a service pack. The first OSR of Windows 95 was also the first version of Windows to be bundled with Microsoft's web browser, Internet Explorer.[18] Mainstream support for Windows 95 ended on December 31, 2000, and extended support for Windows 95 ended on December 31, 2001.[19]
3034) 8.382142, MOS Technology 6502 - Wikipedia, the free encyclopedia.txt#2, term: computer, content:The 6502 was designed by many of the same engineers that had designed the Motorola 6800 microprocessor family.[4] Motorola started the 6800 microprocessor project in 1971 with Tom Bennett as the main architect. The chip layout began in late 1972, the first 6800 chips were fabricated in February 1974 and the full family was officially released in November 1974.[5][6] John Buchanan was the designer of the 6800 chip[7][8] and Rod Orgill, who later did the 6501, assisted Buchanan with circuit analyses and chip layout.[9] Bill Mensch joined Motorola in June 1971 after graduating from the University of Arizona (at age 26).[10] His first assignment was helping define the peripheral ICs for the 6800 family and later he was the principal designer of the 6820 Peripheral Interface Adapter (PIA).[11] Motorola's engineers could run analog and digital simulations on an IBM 370-165 mainframe computer.[12] Bennett hired Chuck Peddle in 1973 to do architectural support work on the 6800 family products already in progress.[13] He contributed in many areas including the design of the 6850 ACIA (serial interface).[14]
3035) 8.382142, MS-DOS - Wikipedia, the free encyclopedia.txt#6, term: computer, content:Microsoft omitted multi-user support from MS-DOS because Microsoft's Unix-based operating system, Xenix, was fully multi-user.[15] The company planned to over time improve MS-DOS so it would be almost indistinguishable from single-user Xenix, or XEDOS, which would also run on the Motorola 68000, Zilog Z8000, and the LSI-11; they would be upwardly compatible with Xenix, which Byte in 1983 described as "the multi-user MS-DOS of the future".[16][17] Microsoft advertised MS-DOS and Xenix together, listing the shared features of its "single-user OS" and "the multi-user, multi-tasking, UNIX-derived operating system", and promising easy porting between them.[18] After the breakup of the Bell System, however, AT&T Computer Systems started selling UNIX System V. Believing that it could not compete with AT&T in the Unix market, Microsoft abandoned Xenix, and in 1987 transferred ownership of Xenix to the Santa Cruz Operation (SCO).
3036) 8.382142, MS-DOS - Wikipedia, the free encyclopedia.txt#16, term: computer, content:In the emerging world of home users, a variety of other computers based on various other processors were in serious competition with the IBM PC: the Apple II, early Apple Macintosh, the Commodore 64 and others did not use the 808x processor; many 808x machines of different architectures used custom versions of MS-DOS. At first all these machines were in competition. In time the IBM PC hardware configuration became dominant in the 808x market as software written to communicate directly with the PC hardware without using standard operating system calls ran much faster, but on true PC-compatibles only. Non-PC-compatible 808x machines were too small a market to have fast software written for them alone, and the market remained open only for IBM PCs and machines that closely imitated their architecture, all running either a single version of MS-DOS compatible only with PCs, or the equivalent IBM PC DOS. Most clones cost much less than IBM-branded machines of similar performance, and became widely used by home users, while IBM PCs had a large share of the business computer market.
3037) 8.382142, Multi-core processor - Wikipedia, the free encyclopedia.txt#27, term: computer, content:An outdated version of an anti-virus application may create a new thread for a scan process, while its GUI thread waits for commands from the user (e.g. cancel the scan). In such cases, a multi-core architecture is of little benefit for the application itself due to the single thread doing all the heavy lifting and the inability to balance the work evenly across multiple cores. Programming truly multithreaded code often requires complex co-ordination of threads and can easily introduce subtle and difficult-to-find bugs due to the interweaving of processing on data shared between threads (see thread-safety). Consequently, such code is much more difficult to debug than single-threaded code when it breaks. There has been a perceived lack of motivation for writing consumer-level threaded applications because of the relative rarity of consumer-level demand for maximum use of computer hardware. Although threaded applications incur little additional performance penalty on single-processor machines, the extra overhead of development has been difficult to justify due to the preponderance of single-processor machines. Also, serial tasks like decoding the entropy encoding algorithms used in video codecs are impossible to parallelize because each result generated is used to help create the next result of the entropy decoding algorithm.
3038) 8.382142, Oberon (operating system) - Wikipedia, the free encyclopedia.txt#1, term: computer, content:The Oberon operating system was originally developed as part of the NS32032-based Ceres workstation project. It is written almost entirely in the Oberon programming language .[5] The basic system was designed and implemented by Niklaus Wirth and Jrg Gutknecht and its design and implementation is fully documented in their book "Project Oberon".[6] The user Interface and programmers reference is found in Martin Reiser's book "The Oberon System".[7] It was later extended and ported to other hardware[8] [9] [10] [11] [12] by a team at ETH-Zrich and there was recognition in popular magazines.[13] [14] [15] [16] [17] [18] Wirth and Gutknecht (although being active Computer Science professors) referred to themselves as 'part-time programmers' in the book 'Project Oberon'.[6] In late 2013, a couple of months before his 80th birthday, Niklaus Wirth published a second edition of Project Oberon.[19] It details the implementation of the Oberon System using a RISC CPU of his own design realized on a Xilinx FPGA board. It was presented at the symposium[20] organized for him at ETHZ.
3039) 8.382142, Operating system - Wikipedia, the free encyclopedia.txt#29, term: computer, content:The GNU Project was started by activist and programmer Richard Stallman with the goal of creating a complete free software replacement to the proprietary UNIX operating system. While the project was highly successful in duplicating the functionality of various parts of UNIX, development of the GNU Hurd kernel proved to be unproductive. In 1991, Finnish computer science student Linus Torvalds, with cooperation from volunteers collaborating over the Internet, released the first version of the Linux kernel. It was soon merged with the GNU user space components and system software to form a complete operating system. Since then, the combination of the two major components has usually been referred to as simply "Linux" by the software industry, a naming convention that Stallman and the Free Software Foundation remain opposed to, preferring the name GNU/Linux. The Berkeley Software Distribution, known as BSD, is the UNIX derivative distributed by the University of California, Berkeley, starting in the 1970s. Freely distributed and ported to many minicomputers, it eventually also gained a following for use on PCs, mainly as FreeBSD, NetBSD and OpenBSD.
3040) 8.382142, Optical disc drive - Wikipedia, the free encyclopedia.txt#20, term: computer, content:Current optical drives use either a tray-loading mechanism, where the disc is loaded onto a motorized or manually operated tray, or a slot-loading mechanism, where the disc is slid into a slot and drawn in by motorized rollers. With both types of mechanism, if a CD or DVD is left in the drive after the computer is turned off, the disc cannot be ejected using the normal eject mechanism of the drive. However, tray-loading drives account for this situation by providing a small hole where one can insert a straightened paperclip to manually open the drive tray to retrieve the disc. Slot-loading optical disc drives have the disadvantages that they cannot usually accept the smaller 80 mm discs (unless 80mm optical disc adapter is used) or any non-standard sizes, usually have no emergency eject hole or eject button, and therefore have to be disassembled if the optical disc cannot be ejected normally. However, the Nintendo Wii, because of backwards compatibility with Nintendo GameCube games,[8] and PlayStation 3[9] video game consoles are able to load standard size DVDs and 80mm discs in the same slot-loading drive.
3041) 8.382142, OS X - Wikipedia, the free encyclopedia.txt#30, term: computer, content:The Cocoa API was created as the result of a 1993 collaboration between NeXT Computer and Sun Microsystems. This heritage is highly visible for Cocoa developers, since the "NS" prefix is ubiquitous in the framework, standing variously for Nextstep or NeXT/Sun. The official OpenStep API, published in September 1994, was the first to split the API between Foundation and Application Kit and the first to use the "NS" prefix.[50] Traditionally, Cocoa programs have been mostly written in Objective-C, with Java as an alternative. However, on July 11, 2005, Apple announced that "features added to Cocoa in Mac OS X versions later than 10.4 will not be added to the Cocoa-Java programming interface."[66] OS X also used to support the Java Platform as a "preferred software package"in practice this means that applications written in Java fit as neatly into the operating system as possible while still being cross-platform compatible, and that graphical user interfaces written in Swing look almost exactly like native Cocoa interfaces. Since 2014, Apple has promoted its new programming language Swift as the preferred language for software development on Apple platforms.
3042) 8.382142, OS X - Wikipedia, the free encyclopedia.txt#52, term: computer, content:With the exception of Mac OS X Server 1.0 and the original public beta, OS X versions were named after big cats until version 10.9, when Apple switched to using California locations. Prior to its release, version 10.0 was code named "Cheetah" internally at Apple, and version 10.1 was code named internally as "Puma". After the immense buzz surrounding version 10.2, codenamed "Jaguar", Apple's product marketing began openly using the code names to promote the operating system. 10.3 was marketed as "Panther", 10.4 as "Tiger", 10.5 as "Leopard", 10.6 as "Snow Leopard", 10.7 as "Lion", 10.8 as "Mountain Lion", and 10.9 as "Mavericks". "Panther", "Tiger" and "Leopard" are registered as trademarks of Apple, but "Cheetah", "Puma" and "Jaguar" have never been registered. Apple has also registered "Lynx" and "Cougar" as trademarks, though these were allowed to lapse.[140] Computer retailer Tiger Direct sued Apple for its use of the name "Tiger". On May 16, 2005 a US federal court in the Southern District of Florida ruled that Apple's use did not infringe on Tiger Direct's trademark.[141]
3043) 8.382142, OS X - Wikipedia, the free encyclopedia.txt#73, term: computer, content:Software Update is a software tool by Apple Inc. that installs the latest version of Apple software on computers running OS X. It was originally introduced to Mac users in Mac OS 9. A Windows version has been available since the introduction of iTunes 7, under the name Apple Software Update. Software Update automatically informs users of new updates. The program is part of the CoreServices in OS X. Software Update can be set to check for updates daily, weekly, monthly, or not at all; in addition, it can download and store the associated .pkg file (the same type used by Installer) to be installed at a later date and maintains a history of installed updates. Software Updates consist of incremental updates of the Mac OS and its applications, Security Updates, device drivers and firmware updates. All software updates require the user to enter their administrative password, as with all consequential system changes. Some updates require a system restart. Starting with OS X 10.5, updates that require a reboot log out the user prior to installation and automatically restart the computer when complete; in earlier versions, the updates are installed, but critical files are not replaced until the next system startup.
3044) 8.382142, Oxford University Press - Wikipedia, the free encyclopedia.txt#1, term: computer, content:The university became involved in the print trade around 1480, and grew into a major printer of Bibles, prayer books, and scholarly works.[3] Its Press took on the project that became the Oxford English Dictionary in the late 19th century, and expanded to meet the ever-rising costs of the work.[4] As a result, the last hundred years has seen Oxford publish children's books, school text books, music, journals, the World's Classics series, and a best-selling range of English Language Teaching texts to match its academic and religious titles. Moves into international markets led to the Press opening its own offices outside the United Kingdom, beginning with New York City, United States in 1896.[5] With the advent of computer technology and increasingly harsh trading conditions, the Press's printing house at Oxford was closed in 1989, and its former paper mill at Wolvercote was demolished in 2004. By contracting out its printing and binding operations, the modern Press publishes some 6,000 new titles around the world each year. As part of a charitable organization, OUP is committed to major financial support of its parent university, and furthers the university's aims of excellence in scholarship, research, and education through its publishing activities.
3045) 8.382142, Personal computer - Wikipedia, the free encyclopedia.txt#56, term: computer, content:The motherboard connects all processor, memory and peripheral devices together. The RAM, graphics card and processor are in most cases mounted directly onto the motherboard. The central processing unit (microprocessor chip) plugs into a CPU socket, while the memory modules plug into corresponding memory sockets. Some motherboards have the video display adapter, sound and other peripherals integrated onto the motherboard, while others use expansion slots for graphics cards, network cards, or other I/O devices. The graphics card or sound card may employ a break out box to keep the analog parts away from the electromagnetic radiation inside the computer case. Disk drives, which provide mass storage, are connected to the motherboard with one cable, and to the power supply through another cable. Usually, disk drives are mounted in the same case as the motherboard; expansion chassis are also made for additional disk storage. For extended amounts of data, a tape drive can be used or extra hard disks can be put together in an external case.
3046) 8.382142, Personal digital assistant - Wikipedia, the free encyclopedia.txt#2, term: computer, content:The first PDA was released in 1984 by Psion, the Organizer. Followed by Psion's Series 3, in 1991, which began to resemble the more familiar PDA style. It also had a full keyboard.[4][5] The term PDA was first used on January 7, 1992 by Apple Computer CEO John Sculley at the Consumer Electronics Show in Las Vegas, Nevada, referring to the Apple Newton.[6] In 1994, IBM introduced the first PDA with full mobile phone functionality, the IBM Simon, which can also be considered the first smartphone. Then in 1996, Nokia introduced a PDA with full mobile phone functionality, the 9000 Communicator, which became the world's best-selling PDA. The Communicator spawned a new category of PDAs: the "PDA phone", now called "smartphone". Another early entrant in this market was Palm, with a line of PDA products which began in March 1996. The terms "personal digital assistant" and "PDA" apply to smartphones but are not used in marketing, media, or general conversation to refer to devices such as the BlackBerry, iPad, iPhone or Android devices.
3047) 8.382142, Plan 9 from Bell Labs - Wikipedia, the free encyclopedia.txt#44, term: computer, content:Plan 9 demonstrated that an integral concept of Unixthat every system interface could be represented as a set of filescould be successfully implemented in a modern distributed system.[31] Some features from Plan 9, like the UTF-8 character encoding of Unicode, have been implemented in other operating systems. Unix-like operating systems such as Linux have implemented 9P, Plan 9's file system, and have adopted features of rfork, Plan 9's process creation mechanism.[43] Additionally, in Plan 9 from User Space, several of Plan 9's applications and tools, including the sam and acme editors, have been ported to Unix and Linux systems and have achieved some level of popularity. Several projects seek to replace the GNU operating system programs surrounding the Linux kernel with the Plan 9 operating system programs.[44][45] The 9wm window manager was inspired by 8, the older windowing system of Plan 9;[46] wmii is also heavily influenced by Plan 9.[42] In computer science research, Plan 9 has been used as a grid computing platform[47][41] and as a vehicle for research into ubiquitous computing without middleware.[48]
3048) 8.382142, Printer (computing) - Wikipedia, the free encyclopedia.txt#27, term: computer, content:In the 1970s & 80s, dot matrix printers were one of the more common types of printers used for general use, such as for home and small office use. Such printers normally had either 9 or 24 pins on the print head (early 7 pin printers also existed, which did not print descenders). There was a period during the early home computer era when a range of printers were manufactured under many brands such as the Commodore VIC-1525 using the Seikosha Uni-Hammer system. This used a single solenoid with an oblique striker that would be actuated 7 times for each column of 7 vertical pixels while the head was moving at a constant speed. The angle of the striker would align the dots vertically even though the head had moved one dot spacing in the time. The vertical dot position was controlled by a synchronised longitudinally ribbed platen behind the paper that rotated rapidly with a rib moving vertically seven dot spacings in the time it took to print one pixel column.[7][8] 24-pin print heads were able to print at a higher quality and started to offer additional type styles and were marketed as Near Letter Quality by some vendors. Once the price of inkjet printers dropped to the point where they were competitive with dot matrix printers, dot matrix printers began to fall out of favour for general use.
3049) 8.382142, Programmer - Wikipedia, the free encyclopedia.txt#8, term: computer, content:Programmers write programs according to the specifications determined primarily by more senior programmers and by systems analysts. After the design process is complete, it is the job of the programmer to convert that design into a logical series of instructions that the computer can follow. The programmer codes these instructions in one of many programming languages. Different programming languages are used depending on the purpose of the program. COBOL, for example, is commonly used for business applications that typically run on mainframe and midrange computers, whereas Fortran is used in science and engineering. C++ is widely used for both scientific and business applications. Java, C#, VB and PHP are popular programming languages for Web and business applications. Programmers generally know more than one programming language and, because many languages are similar, they often can learn new languages relatively easily. In practice, programmers often are referred to by the language they know, e.g. as Java programmers, or by the type of function they perform or environment in which they work: for example, database programmers, mainframe programmers, or Web developers.
3050) 8.382142, Quantum computing - Wikipedia, the free encyclopedia.txt#39, term: computer, content:In 2011, D-Wave Systems announced the first commercial quantum annealer, the D-Wave One, claiming a 128 qubit processor.[53] On May 25, 2011 Lockheed Martin agreed to purchase a D-Wave One system.[54] Lockheed and the University of Southern California (USC) will house the D-Wave One at the newly formed USC Lockheed Martin Quantum Computing Center.[55] D-Wave's engineers designed the chips with an empirical approach, focusing on solving particular problems. Investors liked this more than academics, who said D-Wave had not demonstrated they really had a quantum computer. Criticism softened after a D-Wave paper in Nature, that proved the chips have some quantum properties.[56][57] Two published papers have suggested that the D-Wave machine's operation can be explained classically, rather than requiring quantum models.[58][59] Later work showed that classical models are insufficient when all available data is considered.[60] Experts remain divided on the ultimate classification of the D-Wave systems though their quantum behavior was established concretely with a demonstration of entanglement.[61]
3051) 8.382142, Random-access memory - Wikipedia, the free encyclopedia.txt#18, term: computer, content:One can read and over-write data in RAM. Many computer systems have a memory hierarchy consisting of processor registers, on-die SRAM caches, external caches, DRAM, paging systems and virtual memory or swap space on a hard drive. This entire pool of memory may be referred to as "RAM" by many developers, even though the various subsystems can have very different access times, violating the original concept behind the random access term in RAM. Even within a hierarchy level such as DRAM, the specific row, column, bank, rank, channel, or interleave organization of the components make the access time variable, although not to the extent that access time to rotating storage media or a tape is variable. The overall goal of using a memory hierarchy is to obtain the higher possible average access performance while minimizing the total cost of the entire memory system (generally, the memory hierarchy follows the access time with the fast CPU registers at the top and the slow hard drive at the bottom).
3052) 8.382142, Read-only memory - Wikipedia, the free encyclopedia.txt#6, term: computer, content:Subsequent developments have addressed these shortcomings. PROM, invented in 1956, allowed users to program its contents exactly once by physically altering its structure with the application of high-voltage pulses. This addressed problems 1 and 2 above, since a company can simply order a large batch of fresh PROM chips and program them with the desired contents at its designers' convenience. The 1971 invention of EPROM essentially solved problem 3, since EPROM (unlike PROM) can be repeatedly reset to its unprogrammed state by exposure to strong ultraviolet light. EEPROM, invented in 1983, went a long way to solving problem 4, since an EEPROM can be programmed in-place if the containing device provides a means to receive the program contents from an external source (for example, a personal computer via a serial cable). Flash memory, invented at Toshiba in the mid-1980s, and commercialized in the early 1990s, is a form of EEPROM that makes very efficient use of chip area and can be erased and reprogrammed thousands of times without damage.
3053) 8.382142, Read-only memory - Wikipedia, the free encyclopedia.txt#10, term: computer, content:Forms of read-only memory were employed as non-volatile storage for programs in most early stored-program computers, such as ENIAC after 1948. (Until then it was not a stored-program computer as every program had to be manually wired into the machine, which could take days to weeks.) Read-only memory was simpler to implement since it needed only a mechanism to read stored values, and not to change them in-place, and thus could be implemented with very crude electromechanical devices (see historical examples below). With the advent of integrated circuits in the 1960s, both ROM and its mutable counterpart static RAM were implemented as arrays of transistors in silicon chips; however, a ROM memory cell could be implemented using fewer transistors than an SRAM memory cell, since the latter needs a latch (comprising 5-20 transistors) to retain its contents, while a ROM cell might consist of the absence (logical 0) or presence (logical 1) of one transistor connecting a bit line to a word line.[4] Consequently, ROM could be implemented at a lower cost-per-bit than RAM for many years.
3054) 8.382142, Register machine - Wikipedia, the free encyclopedia.txt#41, term: computer, content:The papers: The papers begin with Wang (1957) and his dramatic simplification of the Turing machine. Turing (1936), Kleene (1952), Davis (1958) and in particular Post (1936) are cited in Wang (1957); in turn, Wang is referenced by Melzak (1961), Minsky (1961) and Shepherdson-Sturgis (1961-3) as they independently reduce the Turing tapes to "counters". Melzak (1961) provides his pebble-in-holes counter machine model with indirection but doesn't carry the treatment further. The work of Elgot-Robinson (1964) define the RASPthe computer-like random access stored program machinesand appear to be the first to investigate the failure of the bounded counter machine to calculate the mu-recursive functions. This failureexcept with the draconian use of Gdel numbers in the manner of Minsky (1961))leads to their definition of "indexed" instructions (i.e. indirect addressing) for their RASP model. Elgot-Robinson (1964) and more so Hartmanis (1971) investigate RASPs with self-modifying programs. Hartmanis (1971) specifies an instruction set with indirection, citing lecture notes of Cook (1970). For use in investigations of computational complexity Cook and his graduate student Reckhow (1973) provide the definition of a RAM (their model and mnemonic convention are similar to Melzak's, but offer him no reference in the paper). The pointer machines are an offshoot of Knuth (1968, 1973) and independently Schnhage (1980).
3055) 8.382142, Semi-Automatic Ground Environment - Wikipedia, the free encyclopedia.txt#25, term: computer, content:On May 3, 1956, General Partridge presented CINCNORADs Operational Concept for Control of Air Defense Weapons to the Armed Forces Policy Council,[20] and a June 1956 symposium presentation identified advanced programming methods of SAGE code.[31] For SAGE consulting Western Electric and Bell Telephone Laboratories formed the Air Defense Engineering Service (ADES),[32] which was contracted in January 1954.[21] IBM delivered the FSQ-7 computer's prototype in June 1956,[8] and Kingston's XD-2 with dual computers[30] guided a Cape Canaveral BOMARC to a successful aircraft intercept on August 7, 1958.[4]:197 Initially contracted to RCA, the AN/FSQ-7 production units were started by IBM in 1958[citation needed] (32 DCs were planned[4]:207 for networking NORAD regions.)[33] IBM's production contract developed 56 SAGE computers for $ billion (~$18 million per computer pair in each FSQ-7)[30]cf. the $2 billion WWII Manhattan Project.
3056) 8.382142, Smartphone - Wikipedia, the free encyclopedia.txt#3, term: computer, content:The first mobile phone to incorporate PDA features was an IBM prototype developed in 1992 and demonstrated that year at the COMDEX computer industry trade show. The prototype demonstrated PDA features as well as other visionary apps like maps, stocks and news incorporated with a cellular phone. A refined version of the product was marketed to consumers in 1994 by BellSouth under the name Simon Personal Communicator. The Simon was the first cellular device that can be properly referred to as a "smartphone", although it was not called that in 1994.[9][10][11] In addition to its ability to make and receive cellular phone calls, Simon was able to send and receive faxes and emails and included several other apps like address book, calendar, appointment scheduler, calculator, world time clock, and note pad through its touch screen display. Simon is the first smartphone to be incorporated with the features of a PDA.[12] The term "smart phone" first appeared in print in 1995, for describing AT&T's "PhoneWriter Communicator" as a "smart phone".[13]
3057) 8.382142, Software engineering - Wikipedia, the free encyclopedia.txt#23, term: computer, content:The initial impact of outsourcing, and the relatively lower cost of international human resources in developing third world countries led to a massive migration of software development activities from corporations in North America and Europe to India and later: China, Russia, and other developing countries. This approach had some flaws, mainly the distance / timezone difference that prevented human interaction between clients and developers and the massive job transfer. This had a negative impact on many aspects of the software engineering profession. For example, some students in the developed world avoid education related to software engineering because of the fear of offshore outsourcing (importing software products or services from other countries) and of being displaced by foreign visa workers.[44] Although statistics do not currently show a threat to software engineering itself; a related career, computer programming does appear to have been affected.[45][46] Nevertheless, the ability to smartly leverage offshore and near-shore resources via the follow-the-sun workflow has improved the overall operational capability of many organizations.[47] When North Americans are leaving work, Asians are just arriving to work. When Asians are leaving work, Europeans are arriving to work. This provides a continuous ability to have human oversight on business-critical processes 24 hours per day, without paying overtime compensation or disrupting a key human resource, sleep patterns.
3058) 8.382142, Sound card - Wikipedia, the free encyclopedia.txt#13, term: computer, content:Sound cards for computers compatible with the IBM PC were very uncommon until 1988, which left the single internal PC speaker as the only way early PC software could produce sound and music.[3] The speaker hardware was typically limited to square waves, which fit the common nickname of "beeper". The resulting sound was generally described as "beeps and boops". Several companies, most notably Access Software, developed techniques for digital sound reproduction over the PC speaker (see RealSound); the resulting audio, while barely functional, suffered from distorted output and low volume, and usually required all other processing to be stopped while sounds were played. Other home computer models of the 1980s included hardware support for digital sound playback, or music synthesis (or both), leaving the IBM PC at a disadvantage to them when it came to multimedia applications such as music composition or gaming. The initial design and marketing focuses of sound cards for the IBM PC platform were not based on gaming, but rather on specific audio applications such as music composition (AdLib Personal Music System, IBM Music Feature Card, Creative Music System), or on speech synthesis (Digispeech DS201, Covox Speech Thing, Street Electronics Echo).
3059) 8.382142, Sound card - Wikipedia, the free encyclopedia.txt#19, term: computer, content:By 1992 one sound card vendor advertised that its product was "Sound Blaster, AdLib, Disney Sound Source and Covox Speech Thing Compatible!".[7] The Sound Blaster line of cards, together with the first inexpensive CD-ROM drives and evolving video technology, ushered in a new era of multimedia computer applications that could play back CD audio, add recorded dialogue to video games, or even reproduce full motion video (albeit at much lower resolutions and quality in early days). The widespread decision to support the Sound Blaster design in multimedia and entertainment titles meant that future sound cards such as Media Vision's Pro Audio Spectrum and the Gravis Ultrasound had to be Sound Blaster compatible if they were to sell well. Until the early 2000s (by which the AC'97 audio standard became more widespread and eventually usurped the SoundBlaster as a standard due to its low cost and integration into many motherboards), Sound Blaster compatibility is a standard that many other sound cards still support to maintain compatibility with many games and applications released.
3060) 8.382142, Teleprinter - Wikipedia, the free encyclopedia.txt#21, term: computer, content:"Speed", intended to be roughly comparable to words per minute, is the standard term introduced by Western Union for a mechanical teleprinter data transmission rate using the 5-bit baudot code that was popular in the 1940s and for several decades thereafter. Such a machine would send 1 start bit, 5 data bits, and 1.42 stop bits. This unusual stop bit time is actually a rest period to allow the mechanical printing mechanism to synchronize in the event that a garbled signal is received.[17] This is true especially on High frequency radio circuits where selective fading is present. Selective fading causes the mark signal amplitude to be randomly different from the space signal amplitude. Selective fading, or Rayleigh fading can cause two carriers to randomly and independently fade to different depths.[18] Since modern computer equipment cannot easily generate 1.42 bits for the stop period, common practice is to either approximate this with 1.5 bits, or to send 2.0 bits while accepting 1.0 bits receiving.
3061) 8.382142, Tommy Flowers - Wikipedia, the free encyclopedia.txt#7, term: computer, content:A Mark 2 redesign utilizing 2,400 valves had begun before the first computer was finished, in anticipation of a need for additional computers. The first Mark 2 Colossus went into service at Bletchley Park on 1 June 1944, and immediately produced vital information for the imminent D-Day landings planned for Monday 5 June (postponed 24 hours by bad weather). Flowers later described a crucial meeting between Dwight D. Eisenhower and his staff on 5 June, during which a courier entered and handed Eisenhower a note summarizing a Colossus decrypt. This confirmed that Hitler wanted no additional troops moved to Normandy, as he was still convinced that the preparations for the Normandy Landings were a diversionary feint. Handing back the decrypt, Eisenhower announced to his staff, "We go tomorrow."[6] Earlier, a report from Field Marshal Rommel on the western defences was decoded by Colossus and revealed that one of the sites chosen as the drop site for an US parachute division was the base for a German tank division. The site was changed.[7]
3062) 8.382142, Touchscreen - Wikipedia, the free encyclopedia.txt#9, term: computer, content:In the early 1980s, General Motors tasked its Delco Electronics division with a project aimed at replacing an automobile's non essential functions (i.e. other than throttle, transmission, braking and steering) from mechanical or electro-mechanical systems with solid state alternatives wherever possible. The finished device was dubbed the ECC for "Electronic Control Center", a digital computer and software control system hardwired to various peripheral sensors, servos, solenoids, antenna and a monochrome CRT touchscreen that functioned both as display and sole method of input.[16] The ECC replaced the traditional mechanical stereo, fan, heater and air conditioner controls and displays, and was capable of providing very detailed and specific information about the vehicle's cumulative and current operating status in real time. The ECC was standard equipment on the 198589 Buick Riviera and later the 198889 Buick Reatta, but was unpopular with consumers partly due to the technophobia of some traditional Buick customers, but mostly because of costly to repair technical problems suffered by the ECC's touchscreen which being the sole access method, would render climate control or stereo operation impossible.[17]
3063) 8.382142, Trackball - Wikipedia, the free encyclopedia.txt#13, term: computer, content:Trackballs have appeared in computer and video games, particularly early arcade games (see a List of trackball arcade games) notably Atari's Centipede and Missile Command  though Atari spelled it "trak-ball". Football, by Atari, released in 1978, is commonly misunderstood to be the first arcade game to use a trackball, but in The Ultimate History of Video Games by Steven L. Kent the designer of Football, Dave Stubben, claims they copied the design from a Japanese soccer game by Taito. Console trackballs, now fairly rare, were common in the early 1980s: the Atari 2600 and 5200 consoles, as well as the competing ColecoVision console, though using a joystick as their standard controller, each had one as an optional peripheral. The Apple Pippin, a console introduced in 1995 had a trackball built into its gamepad as standard. Trackballs were occasionally used in e-sports prior to the mainstreaming of optical mice in the early 2000s because they were more reliable than ball mice, but now they are extremely rare because optical mice offer superior speed and precision.[citation needed] A trackball requires no mousepad and enables the player to aim swiftly (in first person shooters).[citation needed] Trackballs remain in use in pub golf machines (such as Golden Tee) to simulate swinging the club.
3064) 8.382142, UNIVAC I - Wikipedia, the free encyclopedia.txt#2, term: computer, content:The UNIVAC I was the first American computer designed at the outset for business and administrative use with fast execution of relatively simple arithmetic and data transport operations, as opposed to the complex numerical calculations required of scientific computers. As such the UNIVAC competed directly against punch-card machines, though the UNIVAC originally could neither read nor punch cards. That shortcoming hindered sales to companies concerned about the high cost of manually converting large quantities of existing data stored on cards. This was corrected by adding offline card processing equipment, the UNIVAC Card to Tape converter and the UNIVAC Tape to Card converter, to transfer data between cards and UNIVAC magnetic tapes.[6] However, the early market share of the UNIVAC I was lower than the Remington Rand Company wished. To promote sales, the company joined with CBS to have UNIVAC I predict the result of the 1952 Presidential election. UNIVAC I predicted Eisenhower would have a landslide victory over Adlai Stevenson whom the pollsters favored. The result was a greater public awareness of computing technology.[7]
3065) 8.382142, University of Manchester - Wikipedia, the free encyclopedia.txt#44, term: computer, content:Historically, Manchester has been linked with high scientific achievement: the university and its constituent former institutions combined had 25 Nobel laureates among their students and staff, the third largest number of any single university in the United Kingdom (after Oxford and Cambridge) and the ninth largest of any university in Europe. Furthermore, according to an academic poll two of the top ten discoveries by university academics and researchers were made at the university (namely the first working computer and the contraceptive pill).[74] The university currently employs four Nobel Prize winners amongst its staff, more than any other in the UK.[75] The Langworthy Professorship, an endowed chair at the University's School of Physics and Astronomy, has been historically given to a long line of academic luminaries, including Ernest Rutherford (190719), Lawrence Bragg (191937), Patrick Blackett (193753) and more recently Konstantin Novoselov, all of whom have won the Nobel Prize. In 2013 Manchester was given the Regius Professorship in Physics, the only one of its kind in the UK; the current holder is Andre Geim.
3066) 8.382142, University of Pennsylvania - Wikipedia, the free encyclopedia.txt#31, term: computer, content:Dual-degree programs which lead to the same multiple degrees without participation in the specific above programs are also available. Unlike CDD programs, "dual degree" students fulfill requirements of both programs independently without involvement of another program. Specialized dual-degree programs include Liberal Studies and Technology as well as a Computer and Cognitive Science Program. Both programs award a degree from the College of Arts and Sciences and a degree from the School of Engineering and Applied Sciences. In addition, the Vagelos Scholars Program in Molecular Life Sciences allows its students to either double major in the sciences or submatriculate and earn both a B.A. and a M.S. in four years. The most recent Vagelos Integrated Program in Energy Research (VIPER) will be first offered for the class of 2016. A joint program of Penns School of Arts and Sciences and the School of Engineering and Applied Science, VIPER leads to dual Bachelor of Arts and Bachelor of Science in Engineering degrees by combining majors from each school.
3067) 8.382142, Vannevar Bush - Wikipedia, the free encyclopedia.txt#8, term: computer, content:Starting in 1927, Bush constructed a differential analyzer, an analog computer that could solve differential equations with as many as 18 independent variables. This invention arose from previous work performed by Herbert R. Stewart, one of Bush's masters students, who at Bush's suggestion created the integraph, a device for solving first-order differential equations, in 1925. Another student, Harold Hazen, proposed extending the device to handle second-order differential equations. Bush immediately realized the potential of such an invention, for these were much more difficult to solve, but also quite common in physics. Under Bush's supervision, Hazen was able to construct the differential analyzer, a table-like array of shafts and pens that mechanically simulated and plotted the desired equation. Unlike earlier designs that were purely mechanical, the differential analyzer had both electrical and mechanical components.[16] Among the engineers who made use of the differential analyzer was General Electric's Edith Clarke, who used it to solve problems relating to electric power transmission.[17] For developing the differential analyzer, Bush was awarded the Franklin Institute's Louis E. Levy Medal in 1928.[18]
3068) 8.382142, Video card - Wikipedia, the free encyclopedia.txt#4, term: computer, content:As an alternative to the use of a video card, video hardware can be integrated into the motherboard or the CPU. Both approaches can be called integrated graphics. Motherboard-based implementations are sometimes called "on-board video" while CPU-based implementations are known as accelerated processing units (APUs). Almost all desktop computer motherboards with integrated graphics allow the disabling of the integrated graphics chip in BIOS, and have a PCI, or PCI Express (PCI-E) slot for adding a higher-performance graphics card in place of the integrated graphics. The ability to disable the integrated graphics sometimes also allows the continued use of a motherboard on which the on-board video has failed. Sometimes both the integrated graphics and a dedicated graphics card can be used simultaneously to feed separate displays. The main advantages of integrated graphics include cost, compactness, simplicity and low energy consumption. The performance disadvantage of integrated graphics arises because the graphics processor shares system resources with the CPU. A dedicated graphics card has its own random access memory (RAM), its own cooling system, and dedicated power regulators, with all components designed specifically for processing video images. Upgrading to a dedicated graphics card offloads work from the CPU and system RAM, so not only will graphics processing be faster, but the computer's overall performance may also improve.
3069) 8.382142, Video card - Wikipedia, the free encyclopedia.txt#7, term: computer, content:As the processing power of video cards has increased, so has their demand for electrical power. Current high-performance video cards tend to consume a great deal of power. For example, the thermal design power (TDP) for the GeForce GTX TITAN is 250 Watts.[5] While CPU and power supply makers have recently moved toward higher efficiency, power demands of GPUs have continued to rise, so the video card may be the biggest electricity user in a computer.[6][7] Although power supplies are increasing their power too, the bottleneck is due to the PCI-Express connection, which is limited to supplying 75 Watts.[8] Modern video cards with a power consumption over 75 Watts usually include a combination of six-pin (75 W) or eight-pin (150 W) sockets that connect directly to the power supply. Providing adequate cooling becomes a challenge in such computers. Computers with multiple video cards may need power supplies in the 1000 W - 1500 W range. Heat extraction becomes a major design consideration for computers with two or more high end video cards.
3070) 8.382142, Video game industry - Wikipedia, the free encyclopedia.txt#6, term: computer, content:In 1971 the arcade game, Computer Space was released.[5] The following year, Atari, Inc. released the first commercially successful video game, Pong, the original arcade version of which sold over 19,000 arcade cabinets.[6] That same year saw the introduction of video games to the home market with the release of the early video game console, the Magnavox Odyssey. However, both the arcade and home markets would be dominated by Pong clones, which flooded the market and led to the video game crash of 1977. The crash eventually came to an end with the success of Taito's Space Invaders, released in 1978, sparking a renaissance for the video game industry and paving the way for the golden age of video arcade games.[7] The game's success inspired arcade machines to become prevalent in mainstream locations such as shopping malls, traditional storefronts, restaurants and convenience stores during the golden age.[8] Space Invaders would go on to sell over 360,000 arcade cabinets worldwide,[9] and by 1982, generate a revenue of $2billion in quarters,[10][11] equivalent to $4.6 billion in 2011.[12]
3071) 8.382142, Windows 2000 - Wikipedia, the free encyclopedia.txt#34, term: computer, content:Windows 2000 introduced the Microsoft Management Console (MMC), which is used to create, save, and open administrative tools.[43] Each of these is called a console, and most allow an administrator to administer other Windows 2000 computers from one centralised computer. Each console can contain one or many specific administrative tools, called snap-ins.[43] These can be either standalone (with one function), or an extension (adding functions to an existing snap-in). In order to provide the ability to control what snap-ins can be seen in a console, the MMC allows consoles to be created in author mode or user mode.[43] Author mode allows snap-ins to be added, new windows to be created, all portions of the console tree to be displayed and consoles to be saved. User mode allows consoles to be distributed with restrictions applied. User mode consoles can grant full access to the user for any change, or they can grant limited access, preventing users from adding snapins to the console though they can view multiple windows in a console. Alternatively users can be granted limited access, preventing them from adding to the console and stopping them from viewing multiple windows in a single console.[84]
3072) 8.382142, Windows 2000 - Wikipedia, the free encyclopedia.txt#58, term: computer, content: Windows 2000 Server shares the same user interface with Windows 2000 Professional, but contains additional components for the computer to perform server roles and run infrastructure and application software. A significant new component introduced in the server versions is Active Directory, which is an enterprise-wide directory service based on LDAP (Lightweight Directory Access Protocol). Additionally, Microsoft integrated Kerberos network authentication, replacing the often-criticised NTLM (NT LAN Manager) authentication system used in previous versions. This also provided a purely transitive-trust relationship between Windows 2000 domains in a forest (a collection of one or more Windows 2000 domains that share a common schema, configuration, and global catalog, being linked with two-way transitive trusts). Furthermore, Windows 2000 introduced a Domain Name Server which allows dynamic registration of IP addresses. Windows 2000 Server supports up to 4 processors and 4GB of RAM, with a minimum requirement of 128 MB of RAM and 1GB hard disk space, however requirements may be higher depending on installed components.[103]
3073) 8.382142, Windows 8 - Wikipedia, the free encyclopedia.txt#64, term: computer, content:In June 2014, state broadcaster China Central Television (CCTV) broadcast a news story further characterizing Windows 8 as a threat to national security. The story featured an interview with Ni Guangnan, who stated that operating systems could aggregate "sensitive user information" that could be used to "understand the conditions and activities of our national economy and society", and alleged that per documents leaked by Edward Snowden, the U.S. government had worked with Microsoft to retrieve encrypted information. Yang Min, a computer scientist at Fudan University, also stated that "the security features of Windows 8 are basically to the benefit of Microsoft, allowing them control of the users' data, and that poses a big challenge to the national strategy for information security." Microsoft denied the claims in a number of posts on the Chinese social network Sina Weibo, which stated that the company had never "assisted any government in an attack of another government or clients" or provided client data to the U.S. government, never "provided any government the authority to directly visit" or placed any backdoors in its products and services, and that it had never concealed government requests for client data.[190][191][192]
3074) 8.382142, Windows Vista - Wikipedia, the free encyclopedia.txt#30, term: computer, content:Whereas prior releases of Windows supported per-file encryption using Encrypting File System, the Enterprise and Ultimate editions of Vista include BitLocker Drive Encryption, which can protect entire volumes, notably the operating system volume. However, BitLocker requires approximately a 1.5-gigabyte partition to be permanently not encrypted and to contain system files in order for Windows to boot. In normal circumstances, the only time this partition is accessed is when the computer is booting, or when there is a Windows update that changes files in this area, which is a legitimate reason to access this section of the drive. The area can be a potential security issue, because a hexadecimal editor (such as dskprobe.exe), or malicious software running with administrator and/or kernel level privileges would be able to write to this "Ghost Partition" and allow a piece of malicious software to compromise the system, or disable the encryption. BitLocker can work in conjunction with a Trusted Platform Module (TPM) cryptoprocessor (version 1.2) embedded in a computer's motherboard, or with a USB key.[86] However, as with other full disk encryption technologies, BitLocker is vulnerable to a cold boot attack, especially where TPM is used as a key protector without a boot PIN being required too.[87]
3075) 8.382142, World Wide Web - Wikipedia, the free encyclopedia.txt#38, term: computer, content:For criminals, the web has become the preferred way to spread malware. Cybercrime on the web can include identity theft, fraud, espionage and intelligence gathering.[45] Web-based vulnerabilities now outnumber traditional computer security concerns,[46][47] and as measured by Google, about one in ten web pages may contain malicious code.[48] Most web-based attacks take place on legitimate websites, and most, as measured by Sophos, are hosted in the United States, China and Russia.[49] The most common of all malware threats is SQL injection attacks against websites.[50] Through HTML and URIs, the Web was vulnerable to attacks like cross-site scripting (XSS) that came with the introduction of JavaScript[51] and were exacerbated to some degree by Web 2.0 and Ajax web design that favors the use of scripts.[52] Today by one estimate, 70% of all websites are open to XSS attacks on their users.[53] Phishing is another common threat to the Web. "SA, the Security Division of EMC, today announced the findings of its January 2013 Fraud Report, estimating the global losses from phishing at $1.5 Billion in 2012".[54] Two of the well-known phishing methods are Covert Redirect and Open Redirect.
3076) 8.297897, Computer-aided design - Wikipedia, the free encyclopedia.txt#27, term: computer, content:Designers have long used computers for their calculations.[13][14][15][16] Digital computers were used in power system analysis or optimization as early as proto-"Whirlwind" in 1949. Circuit[17] design theory, or power network methodology would be algebraic, symbolic, and often vector-based. Examples of problems being solved in the mid-1940s to 50s include, Servo motors controlled by generated pulse (1949), The digital computer with built-in compute operations to automatically co-ordinate transforms to compute radar related vectors (1951) and the essentially graphic mathematical process of forming a shape with a digital machine tool (1952).[18] These were accomplished with the use of computer software. The man credited with coining the term CAD.[19] Douglas T. Ross stated "As soon as I saw the interactive display equipment, [being used by radar operators 1953]. The designers of these very early computers built utility programs so that programmers could debug programs using flow charts on a display scope with logical switches that could be opened and closed during the debugging session. They found that they could create electronic symbols and geometric figures to be used to create simple circuit diagrams and flow charts.[20] They made the pleasant discovery that an object once drawn could be reproduced at will, its orientation, Linkage [ flux, mechanical, lexical scoping ] or scale changed. This suggested numerous possibilities to them. It took ten years of interdisciplinary development[21] work before SKETCHPAD sitting on evolving math libraries emerged from MIT`s labs. Additional developments were carried out in the 1960s within the aircraft, automotive, industrial control and electronics industries in the area of 3D surface construction, NC programming and design analysis, most of it independent of one another and often not publicly published until much later. Some of the mathematical description work on curves was developed in the early 1940s by Robert Issac Newton from Pawtucket, Rhode Island. Robert A. Heinlein in his 1957 novel The Door into Summer suggested the possibility of a robotic Drafting Dan. However, probably the most important work on polynomial curves and sculptured surface was done by Pierre Bzier, Paul de Casteljau (Citroen), Steven Anson Coons (MIT, Ford), James Ferguson (Boeing), Carl de Boor (GM), Birkhoff (GM) and Garibedian (GM) in the 1960s and W. Gordon (GM) and R. Riesenfeld in the 1970s.
3077) 8.297897, Control unit - Wikipedia, the free encyclopedia.txt#4, term: computer, content:More precisely, the Control Unit (CU) is generally a sizable collection of complex digital circuitry interconnecting and controlling the many execution units (i.e. ALU, data buffers, registers) contained within a CPU.[citation needed] The CU is normally the first CPU unit to accept from an externally stored computer program, a single instruction (based on the CPU's instruction set). The CU then decodes this individual instruction into several sequential steps (fetching addresses/data from registers/memory, managing execution [i.e. data sent to the ALU or I/O], and storing the resulting data back into registers/memory) that controls and coordinates the CPU's inner works to properly manipulate the data. The design of these sequential steps are based on the needs of each instruction and can range in number of steps, the order of execution, and which units are enabled. Thus by only using a program of set instructions in memory, the CU will configure all the CPU's data flows as needed to manipulate the data correctly between instructions. This results in a computer that could run a complete program and requiring no human intervention to make hardware changes between instructions (as had to be done when using only punch cards for computations before stored programmed computers with CUs where invented). These detailed steps from the CU dictate which of the CPU's interconnecting hardware control signals to enable/disable or which CPU units are selected/de-selected and the unit's proper order of execution as required by the instruction's operation to produce the desired manipulated data.[citation needed] Additionally, the CU's orderly hardware coordination properly sequences these control signals then configures the many hardware units comprising the CPU, directing how data should also be moved, changed, and stored outside the CPU (i.e. memory) according to the instruction's objective.[citation needed] Depending on the type of instruction entering the CU, the order and number of sequential steps produced by the CU could vary the selection and configuration of which parts of the CPU's hardware are utilized to achieve the instruction's objective (mainly moving, storing, and modifying data within the CPU).[citation needed] This one feature, that efficiently uses just software instructions to control/select/configure a computer's CPU hardware (via the CU) and eventually manipulates a program's data, is a significant reason most modern computers are flexible and universal when running various programs.[citation needed] As compared to some 1930s or 1940s computers without a proper CU, they often required rewiring their hardware when changing programs.[citation needed] This CU instruction decode process is then repeated when the Program Counter is incremented to the next stored program address and the new instruction enters the CU from that address, and so on till the programs end.[citation needed]
3078) 6.7057137, Booting - Wikipedia, the free encyclopedia.txt#15, term: computer, content:The IPL function in the System/360 and its successors, and its compatibles such as Amdahl's, reads 24 bytes from an operator-specified device into main storage starting at real address zero. The second and third groups of eight bytes are treated as Channel Command Words (CCWs) to continue loading the startup program (the first CCW is always simulated by the CPU and consists of a Read IPL command, 02h, with command chaining and suppress incorrect length indication being enforced). When the I/O channel commands are complete, the first group of eight bytes is then loaded into the processor's Program Status Word (PSW) and the startup program begins execution at the location designated by that PSW.[13] The IPL device is usually a disk drive, hence the special significance of the 02h read-type command, but exactly the same procedure is also used to IPL from other input-type devices, such as tape drives, or even card readers, in a device-independent manner, allowing, for example, the installation of an operating system on a brand-new computer from an OS initial distribution magnetic tape. For disk controllers, the 02h command also causes the selected device to seek to cylinder 0000h, head 0000h, simulating a Seek cylinder and head command, 07h, and to search for record 01h, simulating a Search ID Equal command, 31h; seeks and searches are not simulated by tape and card controllers, as for these device classes an 02h command is simply a sequential read command, not a Read IPL command.
3079) 6.7057137, Computer keyboard - Wikipedia, the free encyclopedia.txt#37, term: computer, content:Navigation keys or cursor keys include a variety of keys which move the cursor to different positions on the screen. Arrow keys are programmed to move the cursor in a specified direction; page scroll keys, such as the Page Up and Page Down keys, scroll the page up and down. The Home key is used to return the cursor to the beginning of the line where the cursor is located; the End key puts the cursor at the end of the line. The Tab key advances the cursor to the next tab stop. The Insert key is mainly used to switch between overtype mode, in which the cursor overwrites any text that is present on and after its current location, and insert mode, where the cursor inserts a character at its current position, forcing all characters past it one position further. The Delete key discards the character ahead of the cursor's position, moving all following characters one position "back" towards the freed place. On many notebook computer keyboards the key labeled Delete (sometimes Delete and Backspace are printed on the same key) serves the same purpose as a Backspace key. The Backspace key deletes the preceding character. Lock keys lock part of a keyboard, depending on the settings selected. The lock keys are scattered around the keyboard. Most styles of keyboards have three LEDs indicating which locks are enabled, in the upper right corner above the numeric pad. The lock keys include Scroll lock, Num lock (which allows the use of the numeric keypad), and Caps lock.
3080) 6.7057137, Computer Technology Limited - Wikipedia, the free encyclopedia.txt#3, term: computer, content:The Modular One was a 16-bit computer built with Emitter Coupled Logic (ECL) and was competitive with other first generation minicomputers. A key feature, from which it derived its name, was that it was composed of separate processor, memory and peripheral modules sharing a common interface and physical form factor, so allowing them to be put together in any combination, housed one or two high in modular racking. Standard modules were roughly 50cm wide and deep, 70cm tall, and complete with power supply, typically weighed in excess of 25kg. Modules were interconnected using a single type of interface, comprising two identical cards to be plugged into two modules to be connected, these cards themselves linked by a flat ribbon cable either one or two metres long. Thus, memory was just another peripheral (such as a printer) but was both input and output. Every interaction over these interfaces comprised a 3-way handshake, which in the case of a processor accessing a memory module, consisted of send address, receive data, and send new data, a scheme well suited to the destructive read followed by rewrite required by magnetic-core memory of the time. These three phases were mediated by voltage edges rather than pulses, as this was thought to be faster. Furthermore, the input and output impedances of Emitter Coupled Logic were comparable to the characteristic impedance of ribbon cable. This, together with the small voltage swings between the "0" and "1" states made for low noise, reflection-free communication.
3081) 6.7057137, Exploit (computer security) - Wikipedia, the free encyclopedia.txt#1, term: computer, content:There are several methods of classifying exploits. The most common is by how the exploit contacts the vulnerable software. A remote exploit works over a network and exploits the security vulnerability without any prior access to the vulnerable system. A local exploit requires prior access to the vulnerable system and usually increases the privileges of the person running the exploit past those granted by the system administrator. Exploits against client applications also exist, usually consisting of modified servers that send an exploit if accessed with a client application. Exploits against client applications may also require some interaction with the user and thus may be used in combination with the social engineering method. Another classification is by the action against the vulnerable system; unauthorized data access, arbitrary code execution, and denial of service are examples. Many exploits are designed to provide superuser-level access to a computer system. However, it is also possible to use several exploits, first to gain low-level access, then to escalate privileges repeatedly until one reaches root. Normally a single exploit can only take advantage of a specific software vulnerability. Often, when an exploit is published, the vulnerability is fixed through a patch and the exploit becomes obsolete until newer versions of the software become available. This is the reason why some black hat hackers do not publish their exploits but keep them private to themselves or other hackers. Such exploits are referred to as zero day exploits and to obtain access to such exploits is the primary desire of unskilled attackers, often nicknamed script kiddies.[1]
3082) 6.7057137, File format - Wikipedia, the free encyclopedia.txt#10, term: computer, content:Hiding extensions can also pose a security risk.[1] For example, a malicious user could create an executable program with an innocent name such as "Holiday photo.jpg.exe". The ".exe" would be hidden and a user would see "Holiday photo.jpg", which would appear to be a JPEG image, unable to harm the machine save for bugs in the application used to view it. However, the operating system would still see the ".exe" extension and thus run the program, which would then be able to cause harm to the computer. The same is true with files with only one extension: as it is not shown to the user, no information about the file can be deduced without explicitly investigating the file. Extensions can be spoofed. Some Word macro viruses create a Word file in template format and save it with a .DOC extension. Since Word generally ignores extensions and looks at the format of the file these would open as templates, execute, and spread the virus. To further trick users, it is possible to store an icon inside the program, in which case some operating systems' icon assignment for the executable file (.exe) would be overridden with an icon commonly used to represent JPEG images, making the program look like an image. This issue requires users with extensions hidden to be vigilant and never let the operating system choose with what program to open a file not known to be trustworthy (which counteracts the idea of making things easier for the user). This presents a practical problem for Windows systems where extension-hiding is turned on by default.
3083) 6.7057137, Graphics processing unit - Wikipedia, the free encyclopedia.txt#12, term: computer, content:In the early- and mid-1990s, CPU-assisted real-time 3D graphics were becoming increasingly common in arcade, computer and console games, which led to an increasing public demand for hardware-accelerated 3D graphics. Early examples of mass-market 3D graphics hardware can be found in arcade system boards such as the Sega Model 1, Namco System 22, and Sega Model 2, and the fifth-generation video game consoles such as the Saturn, PlayStation and Nintendo 64. Arcade systems such as the Sega Model 2 and Namco Magic Edge Hornet Simulator in 1993 were capable of hardware T&L (transform, clipping, and lighting) years before appearing in consumer graphics cards.[26][27] Other systems used DSPs to accelerate transformations.Fujitsu, which worked on the Sega Model 2 arcade system,[28] began working on integrating T&L into a single LSI solution for use in home computers in 1995;[29][30] the Fujitsu Pinolite, the first 3D geometry processor for personal computers, released in 1997.[31] The first hardware T&L GPU on home video game consoles was the Nintendo 64's Reality Coprocessor, released in 1996.[32] In 1997, Mitsubishi released the 3Dpro/2MP, a fully featured GPU capable of transformation and lighting, for workstations and Windows NT desktops;[33] AMD utilized it for their FireGL 4000 graphics card, released in 1997.[34]
3084) 6.7057137, Kermit (protocol) - Wikipedia, the free encyclopedia.txt#2, term: computer, content:Kermit can be used as a means to boot other software, even itself. To distribute Kermit through non 8-bit clean networks Columbia developed .boo, a binary-to-text encoding system similar to BinHex. After a mainframe computer received MS-DOS Kermit in .boo format, users could type in a "baby Kermit" in BASIC on their personal computers that would download Kermit and convert it into binary.[2][3] Similarly, CP/M machines used many different floppy disk formats, which meant that one machine could not normally read disks from another CP/M machine, and Kermit was used as part of a process to enable the transfer of applications and data between CP/M machines and other machines with different operating systems. The CP/M file-copy program PIP could usually access a computer's serial (RS-232) port, and if configured to use a very low baud rate (because it had no built-in error correction) could be used to transfer a small simple version of Kermit from one machine to another over a null modem cable, or failing that, a very simple version of the Kermit protocol could be hand coded in binary in less than 2K using DDT, the CP/M Dynamic Debugging Tool. Once that was done the simple version of Kermit could be used to download a fully functional version. That version could then be used to transfer any CP/M application or data.[4]
3085) 6.7057137, Magnetic-core memory - Wikipedia, the free encyclopedia.txt#29, term: computer, content:Core memory is non-volatile storageit can retain its contents indefinitely without power. It is also relatively unaffected by EMP and radiation. These were important advantages for some applications like first-generation industrial programmable controllers, military installations and vehicles like fighter aircraft, as well as spacecraft, and led to core being used for a number of years after availability of semiconductor MOS memory (see also MOSFET). For example, the Space Shuttle flight computers initially used core memory, which preserved the contents of memory even through the Challenger's disintegration and subsequent plunge into the sea in 1986.[14] Another characteristic of early core was that the coercive force was very temperature-sensitive; the proper half-select current at one temperature is not the proper half-select current at another temperature. So a memory controller would include a temperature sensor (typically a thermistor) to adjust the current levels correctly for temperature changes. An example of this is the core memory used by Digital Equipment Corporation for their PDP-1 computer; this strategy continued through all of the follow-on core memory systems built by DEC for their PDP line of air-cooled computers. Another method of handling the temperature sensitivity was to enclose the magnetic core "stack" in a temperature controlled oven. Examples of this are the heated-air core memory of the IBM 1620 (which could take up to 30 minutes to reach operating temperature, about 106F (41C) and the heated-oil-bath core memory of the IBM 7090, early IBM 7094s, and IBM 7030.
3086) 6.7057137, Media player (software) - Wikipedia, the free encyclopedia.txt#3, term: computer, content:3D video players are used to play 2D video in 3D format. A high-quality three-dimensional video presentation requires that each frame of a motion picture be embedded with information on the depth of objects present in the scene. This process involves shooting the video with special equipment from two distinct perspectives or modelling and rendering each frame as a collection of objects composed of 3D vertices and textures, much like in any modern video game, to achieve special effects. Tedious and costly, this method is only used in a small fraction of movies produced worldwide, while most movies remain in the form of traditional 2D images. It is, however, possible to give an otherwise two-dimensional picture the appearance of depth. Using a technique known as anaglyph processing a "flat" picture can be transformed so as to give an illusion of depth when viewed through anaglyph glasses (usually red-cyan). An image viewed through anaglyph glasses appears to have both protruding and deeply embedded objects in it, at the expense of somewhat distorted colours. The method itself is old enough, dating back to mid-19th century, but it is only with recent advances in computer technology that it has become possible to apply this kind of transformation to a series of frames in a motion picture reasonably fast or even in real time, i.e. as the video is being played back. Several implementations exist in the form of 3D video players,[1][2] that render conventional 2D video in anaglyph 3D, as well as in the form of 3D video converters,[3][4][5] that transform video into stereoscopic anaglyph and transcode it for playback with regular software or hardware video players.
3087) 6.7057137, Metastability in electronics - Wikipedia, the free encyclopedia.txt#6, term: computer, content:Serious computer and digital hardware bugs caused by metastability have a fascinating social history. Many engineers have refused to believe that a bistable device can enter into a state that is neither true nor false and has a positive probability that it will remain indefinite for any given period of time, albeit with exponentially decreasing probability over time. However, metastability is an inevitable result of any attempt to map a continuous domain to a discrete one. There will always be points in the continuous domain which are equidistant (or nearly so) from the points of the discrete domain, making a decision as to which discrete point to select a difficult and potentially lengthy process.[7] If the inputs to an arbiter or flip-flop arrive almost simultaneously, the circuit most likely will traverse a point of metastability. Metastability remains poorly understood in some circles, and various engineers have proposed their own circuits said to solve or filter out the metastability; typically these circuits simply shift the occurrence of metastability from one place to another.[8] Chips using multiple clock sources are often tested with tester clocks that have fixed phase relationships, not the independent clocks drifting past each other that will be experienced during operation. This usually explicitly prevents the metastable failure mode that will occur in the field from being seen or reported. Current engineering solutions to this problem are often the well-characterized, multi-stage common-clock shift registers discussed in the links below.
3088) 6.7057137, PowerPC - Wikipedia, the free encyclopedia.txt#14, term: computer, content:However, toward the close of the decade, manufacturing issues began plaguing the AIM alliance in much the same way they did Motorola, which consistently pushed back deployments of new processors for Apple and other vendors: first from Motorola in the 1990s with the PowerPC 7xx and 74xx processors, and IBM with the 64-bit PowerPC 970 processor in 2003. In 2004, Motorola exited the chip manufacturing business by spinning off its semiconductor business as an independent company called Freescale Semiconductor. Around the same time, IBM exited the 32-bit embedded processor market by selling its line of PowerPC products to Applied Micro Circuits Corporation (AMCC) and focusing on 64-bit chip designs, while maintaining its commitment of PowerPC CPUs toward game machine makers such as Nintendo's GameCube and Wii, Sony's PlayStation 3 and Microsoft's Xbox 360, of which the latter two both use 64-bit processors. In 2005 Apple announced they would no longer use PowerPC processors in their Apple Macintosh computers, favoring Intel-produced processors instead, citing the performance limitations of the chip for future personal computer hardware specifically related to heat generation and energy usage, as well as the inability of IBM to move the 970 processor to the 3GHz range. The IBM-Freescale alliance was replaced by an open standards body called Power.org. Power.org operates under the governance of the IEEE with IBM continuing to use and evolve the PowerPC processor on game consoles and Freescale Semiconductor focusing solely on embedded devices.
3089) 6.7057137, Punched card - Wikipedia, the free encyclopedia.txt#20, term: computer, content:For some computer applications, binary formats were used, where each hole represented a single binary digit (or "bit"), every column (or row) is treated as a simple bitfield, and every combination of holes is permitted. For example, the IBM 711 card reader used with the 704/709/7090/7094 series scientific computers treated every row as two 36-bit words, ignoring 8 columns. (The specific 72 columns used were selectable using a plugboard control panel, which is almost always wired to select columns 172.) Sometimes the ignored columns (usually 7380) were used to contain a sequence number for each card, so the card deck could be sorted to the correct order in case it was dropped. An alternative format, used by the IBM 704's IBM 714 native card reader, is referred to as Column Binary or Chinese Binary, and used 3 columns for each 36-bit word.[37] Later computers, such as the IBM 1130 or System/360, used every column. The IBM 1401's card reader could be used in Column Binary mode, which stored two characters in every column, or one 36-bit word in three columns when used as input device for other computers. However, most of the older card punches were not intended to punch more than 3 holes in a column, so they could not be used to produce binary cards.[citation needed]
3090) 6.7057137, Semi-Automatic Ground Environment - Wikipedia, the free encyclopedia.txt#29, term: computer, content:Project Wild Goose teams of Air Material Command personnel installed c.1960 the Ground Air Transmit Receive stations for the SAGE TDDL (in April 1961, Sault Ste Marie was the first operational sector with TDDL.)[42]  By the middle of 1960, AMC had determined that about 800,000 man-hours (involving 130 changes) would be required to bring the F-106 fleet to the point where it would be a valuable adjunct to the air defense system. Part of the work (Project Broad Jump) was accomplished by Sacramento Air Materiel Area. The remainder (Project Wild Goose) was done at ADC bases by roving AMC field assistance teams supported by ADC maintenance personnel. (cited by Volume I p.271 & Schaffel p.325) After a September 1959 experimental ATABE test between an "abbreviated" AN/FSQ-7 staged at Fort Banks and the Lexington XD-1, the 1961 "SAGE/Missile Master test program" conducted large-scale field testing of the ATABE "mathematical model" using radar tracks of actual SAC and ADC aircraft flying mock penetrations into defense sectors.[43] Similarly conducted was the joint SAC-NORAD Sky Shield II exercise followed by Sky Shield III on 2 September 1962[44] On July 15, 1963, ESD's CMC Management Office assumed "responsibilities in connection with BMEWS, Space Track, SAGE, and BUIC."[45] The Chidlaw Building's computerized[specify] NORAD/ADC Combined Operations Center in 1963 became the highest echelon of the SAGE computer network when operations moved from Ent AFB's 1954 manual Command Center to the partially underground[45] "war room".[46] Also in 1963, radar stations were renumbered (e.g., Cambria AFS was redesignated from P-2 to Z-2 on July 31) and the vacuum-tube SAGE System was completed (and obsolete).[47]:9
3091) 6.7057137, UNIVAC I - Wikipedia, the free encyclopedia.txt#13, term: computer, content:Besides the operator's console, the only I/O devices connected to the UNIVAC I were up to 10 UNISERVO tape drives, a Remington Standard electric typewriter[citation needed] and a Tektronix oscilloscope. The UNISERVO was the first commercial computer tape drive commercially sold. It used data density 128 bits per inch[citation needed] (with real transfer rate 7,200 characters per second) on magnetically plated phosphor bronze tapes. The UNISERVO could also read and write UNITYPER created tapes at 20 bits per inch. The UNITYPER was an offline typewriter to tape device, used by programmers and for minor data editing. Backward and forward tape read and write operations were possible on the UNIVAC and were fully overlapped with instruction execution, permitting high system throughput in typical sort/merge data processing applications. Large volumes of data could be inputted via magnetic tapes created on offline card to tape system and outputted via a separate offline tape to printer system. The operators console had three columns of decimal coded switches that allowed any of the 1000 memory locations to be displayed on the oscilloscope. Since the mercury delay line memory stored bits in a serial format, a programmer or operator could monitor any memory location continuously and with sufficient patience, decode its contents as displayed on the scope. The on-line typewriter was typically used for announcing program breakpoints, checkpoints, and for memory dumps.
3092) 6.7057137, Vacuum tube - Wikipedia, the free encyclopedia.txt#45, term: computer, content:Vacuum tubes used as switches made electronic computing possible for the first time, but the cost and relatively short mean time to failure of tubes were limiting factors.[23] "The common wisdom was that valveswhich, like light bulbs, contained a hot glowing filamentcould never be used satisfactorily in large numbers, for they were unreliable, and in a large installation too many would fail in too short a time".[24] Tommy Flowers, who later designed Colossus, "discovered that, so long as valves were switched on and left on, they could operate reliably for very long periods, especially if their 'heaters' were run on a reduced current".[24] In 1934 Flowers built a successful experimental installation using over 3,000 tubes in small independent modules; when a tube failed, it was possible to switch off one module and keep the others going, thereby reducing the risk of another tube failure being caused; this installation was accepted by the Post Office (who operated telephone exchanges). Flowers was also a pioneer of using tubes as very fast (compared to electromechanical devices) electronic switches. Later work confirmed that tube unreliability was not as serious an issue as generally believed; the 1946 ENIAC, with over 17,000 tubes, had a tube failure (which took 15 minutes to locate) on average every two days. The quality of the tubes was a factor, and unfortunately the diversion of skilled people during the Second World War lowered the general quality of tubes.[25] During the war Colossus was instrumental in breaking German codes. After the war, development continued with tube-based computers including, military computers ENIAC and Whirlwind, the Ferranti Mark 1 (the first commercially available electronic computer), and UNIVAC I, also available commercially.
3093) 6.7057137, Windows 2000 - Wikipedia, the free encyclopedia.txt#5, term: computer, content:Windows 2000 is a continuation of the Microsoft Windows NT family of operating systems, replacing Windows NT 4.0. The original name for the operating system was Windows NT 5.0. Beta 1 of NT 5.0 was released in September 1997, followed by Beta 2 in August 1998.[20] On October 27, 1998, Microsoft announced that the name of the final version of the operating system would be Windows 2000, a name which referred to its projected release date.[21] Windows 2000 Beta 3 was released in January 1999.[20] NT 5.0 Beta 1 was similar to NT 4.0, including a very similar themed logo. NT 5.0 Beta 2 introduced a new 'mini' boot screen, and removed the 'dark space' theme in the logo. The NT 5.0 betas had very long startup and shutdown sounds, though these were changed in the early Windows 2000 beta, but during Beta 3, a new piano-made startup and shutdown sounds were made, featured in the final version as well as in Windows ME. The new login prompt from the final version made its first appearance in Beta 3 build 1946 (the first build of Beta 3). The new, updated icons (for My Computer, Recycle Bin etc.) first appeared in Beta 3 build 1976. The Windows 2000 boot screen in the final version first appeared in Beta 3 build 1994. Windows 2000 did not have a codename because, according to Dave Thompson of Windows NT team, "Jim Allchin didn't like codenames".[22]
3094) 6.7057137, Windows 2000 - Wikipedia, the free encyclopedia.txt#6, term: computer, content:Windows 2000 Service Pack 1 was codenamed "Asteroid"[23] and Windows 2000 64-bit was codenamed "Janus."[24][25] During development, there was a build for the Alpha which was abandoned some time after RC1[26] after Compaq announced they had dropped support for Windows NT on Alpha. From here, Microsoft issued three release candidates between July and November 1999, and finally released the operating system to partners on December 12, 1999.[27] The public could buy the full version of Windows 2000 on February 17, 2000. Three days before this event, which Microsoft advertised as "a standard in reliability," a leaked memo from Microsoft reported on by Mary Jo Foley revealed that Windows 2000 had "over 63,000 potential known defects."[28] After Foley's article was published, she claimed that Microsoft blacklisted her for a considerable time.[29] However, Abraham Silberschatz et al. claim in their computer science textbook that "Windows 2000 was the most reliable, stable operating system Microsoft had ever shipped to that point. Much of this reliability came from maturity in the source code, extensive stress testing of the system, and automatic detection of many serious errors in drivers."[30] InformationWeek summarized the release "our tests show the successor to NT 4.0 is everything we hoped it would be. Of course, it isn't perfect either."[31] Wired News later described the results of the February launch as "lackluster."[32] Novell criticized Microsoft's Active Directory, the new directory service architecture, as less scalable or reliable than its own Novell Directory Services (NDS) alternative.[33]
3095) 6.7057137, Windows 7 - Wikipedia, the free encyclopedia.txt#8, term: computer, content:Among Windows 7's new features are advances in touch and handwriting recognition,[43] support for virtual hard disks,[44] improved performance on multi-core processors,[45][46][47][48] improved boot performance, DirectAccess, and kernel improvements. Windows 7 adds support for systems using multiple heterogeneous graphics cards from different vendors (Heterogeneous Multi-adapter),[49] a new version of Windows Media Center,[50] a Gadget for Windows Media Center, improved media features, XPS Essentials Pack[51] and Windows PowerShell[52] being included, and a redesigned Calculator with multiline capabilities including Programmer and Statistics modes along with unit conversion for length, weight, temperature, and several others.[53] Many new items have been added to the Control Panel, including ClearType Text Tuner[54] Display Color Calibration Wizard,[55] Gadgets, Recovery, Troubleshooting, Workspaces Center, Location and Other Sensors, Credential Manager, Biometric Devices, System Icons, and Display.[56] Windows Security Center has been renamed to Windows Action Center (Windows Health Center and Windows Solution Center in earlier builds), which encompasses both security and maintenance of the computer. ReadyBoost on 32-bit editions now supports up to 256gigabytes of extra allocation. Windows 7 also supports images in RAW image format through the addition of Windows Imaging Component-enabled image decoders, which enables raw image thumbnails, previewing and metadata display in Windows Explorer, plus full-size viewing and slideshows in Windows Photo Viewer and Windows Media Center.[57] Windows 7 also has a native TFTP client with the ability to transfer files to or from a TFTP server.[58]
3096) 6.7057137, Windows 8 - Wikipedia, the free encyclopedia.txt#51, term: computer, content:Reviews of the various editions of Windows 8 have been mixed. Tom Warren of The Verge said that although Windows 8's emphasis on touch computing was significant and risked alienating desktop users, a "tablet PC with Windows 8 makes an iPad feel immediately out of date" due to the capabilities of the operating system's hybrid model and increased focus on cloud services.[159] David Pierce of The Verge described Windows 8 as "the first desktop operating system that understands what a computer is supposed to do in 2012" and praised Microsoft's "no compromise" approach and the operating system's emphasis on Internet connectivity and cloud services. Pierce also considered the Start Screen to be a "brilliant innovation for desktop computers" when compared with "folder-littered desktops on every other OS" because it allows users to interact with dynamic information.[160] In contrast, an ExtremeTech article said it was Microsoft "flailing"[161] and a review in PC Magazine condemned the Metro-style user interface.[162] Some of the included apps in Windows 8 were considered to be basic and lacking in functionality, but the Xbox apps were praised for their promotion of a multi-platform entertainment experience. Other improvements and features (such as File History, Storage Spaces, and the updated Task Manager) were also regarded as positive changes.[159] Peter Bright of Ars Technica wrote that while its user interface changes may overshadow them, Windows 8's improved performance, updated file manager, new storage functionality, expanded security features, and updated Task Manager were still positive improvements for the operating system. Bright also said that Windows 8's duality towards tablets and traditional PCs was an "extremely ambitious" aspect of the platform as well, but criticized Microsoft for emulating Apple's model of a closed distribution platform when implementing the Windows Store.[163]
3097) 5.8674994, Intel 4004 - Wikipedia, the free encyclopedia.txt#0, term: computer, content:The Intel 4004 is a 4-bit central processing unit (CPU) released by Intel Corporation in 1971. It was the first commercially available microprocessor by Intel.[2] The chip design started in April 1970, when Federico Faggin joined Intel, and it was completed under his leadership in January 1971. The first commercial sale of the fully operational 4004 occurred in March 1971 to Busicom Corp. of Japan for which it was originally designed and built as a custom chip.[3] In mid-November of the same year, with the prophetic ad "Announcing a new era in integrated electronics", the 4004 was made commercially available to the general market. The 4004 is historys first monolithic CPU, fully integrated in one small chip. Such a feat of integration was made possible by the use of the then-new silicon gate technology which allowed twice the number of random-logic transistors and an increase in speed by a factor of five compared to the incumbent technology. The 4004 microprocessor is one of 4 chips constituting the MCS-4 chip-set, which includes the 4001 ROM, 4002 RAM, and 4003 Shift Register. With these components, small computers with varying amounts of memory and I/O facilities can be built. Three other CPU chip designs were done at about the same time: the Four-Phase System AL1, done in 1969; the MP944, completed in 1970 and used in the F-14 Tomcat fighter jet; and the Texas Instruments TMS0100 chip, announced in September 17, 1971. Both the AL1 and the MP944 use several chips for the implementation of the CPU function. The TMS0100 chip was presented as a calculator on a chip with the original designation TMS1802NC.[4] This chip contains a very primitive CPU and can only be used to implement various simple 4-function calculators. It is the precursor of the TMS1000, introduced in 1974, which is considered the first microcontroller i.e., a computer on a chip containing not only the CPU, but also ROM, RAM, and I/O functions.[5] The MCS-4 family of 4 chips developed by Intel, of which the 4004 is the CPU or microprocessor, is far more versatile and powerful than the single chip TMS1000, allowing the creation of a variety of small computers for various applications. The MCS-4 was eventually superseded by powerful microcontrollers like the Intel 8048 and the Zilog Z8 in 1978-1979. The architecture of this processor formed the basis for later models of microprocessors.[citation needed]
3098) 5.8674994, University of Pennsylvania - Wikipedia, the free encyclopedia.txt#39, term: computer, content:Several major scientific discoveries have also taken place at Penn. The university is probably best well known as the place where the first general-purpose electronic computer (ENIAC) was born in 1946 at the Moore School of Electrical Engineering.[68] It was here also where the world's first spelling and grammar checkers were created, as well as the popular COBOL programming language.[68] Penn can also boast some of the most important discoveries in the field of medicine. The dialysis machine used as an artificial replacement for lost kidney function was conceived and devised out of a pressure cooker by William Inouye while he was still a student at Penn Med;[69] the Rubella and Hepatitis B vaccines were developed at Penn;[69][70] the discovery of cancer's link with genes, cognitive therapy, Retin-A (the cream used to treat acne), Resistin, the Philadelphia gene (linked to chronic myelogenous leukemia), and the technology behind PET Scans were all discovered by Penn Med researchers.[69] More recent gene research has led to the discovery of the genes for fragile X syndrome, the most common form of inherited mental retardation, spinal and bulbar muscular atrophy, a disorder marked by progressive muscle wasting, and CharcotMarieTooth disease, a progressive neurodegenerative disease that affects the hands, feet, and limbs.[69] Conductive polymer was also developed at Penn by Alan J. Heeger, Alan MacDiarmid and Hideki Shirakawa, an invention that earned them the Nobel Prize in Chemistry. Ralph L. Brinster, on faculty since 1965, developed the scientific basis for in vitro fertilization and the transgenic mouse at Penn. The theory of superconductivity was also partly developed at Penn, by then faculty member John Robert Schrieffer (along with John Bardeen and Leon Cooper). The university has also contributed major advancements in the fields of economics and management. Among the many discoveries are conjoint analysis, widely used as a predictive tool especially in market research, Simon Kuznets's method of measuring Gross National Product,[71] the Penn effect (the observation that consumer price levels in richer countries are systematically higher than in poorer ones), and the "Wharton Model"[72] developed by Nobel-laureate Lawrence Klein to measure and forecast economic activity. The idea behind Health Maintenance Organizations also belonged to Penn professor Robert Eilers, who put it into practice during then President Nixon's health reform in the 1970s.[71]
3099) 5.8674994, Video card - Wikipedia, the free encyclopedia.txt#12, term: computer, content:Video card shipments totaled 14.5 million units in the third quarter of 2013, a 17% fall from Q3 2012 levels.[16] The traditional PC market is shrinking as tablet computers and smartphones gain share. Years ago, the move to integrated graphics on the motherboard greatly reduced the market for low end video cards. Now, AMD and Intel's accelerated processing units, which combine graphics processing with CPU functions on the CPU die itself, are putting further pressure on video card sales.[17] AMD introduced a line of combined processors which it calls the AMD A-Series APU Processors (A4, A6, A8, A10) while Intel, rather than marketing an exclusive line of APUs, introduced its "4th Generation Intel Core Processors", some of which are APUs. Those processors are described as offering "Superb visuals and graphics performancewithout the cost of a separate graphics card."[18] They are branded as having Intel HD Graphics or Intel Iris Pro Graphics. As an example, the Intel Core i7 4750HQ with Iris Pro Graphics 5200, an accelerated processing unit for notebook computers, allows users with mid-range graphics requirements to use a notebook computer without a video card. In a September, 2013 review of the Intel Core i7 4750HQ accelerated processing unit (which is closely related to the Intel processor with HD Graphics 5000 used in the MacBook Air,) the website hardware.info stated: "With its latest generation of integrated graphics, Intel set out to rival the performance of the mid-range mobile Nvidia GeForce GT 650M graphics card. And the tests leave no doubt about it, both 3DMark and the gaming benchmarks confirm that the [Intel] Iris Pro Graphics 5200 is on the same level of or slightly below that of the GT 650M."[3] (The GeForce GT 650M is not sold through retail channels, but an EVGA desktop GTX 650 was selling for around $120 in late 2013.[19]) Although the review notes that Intel's accelerated processing unit is not yet cost competitive, the technology is approaching competitiveness, at least with mid-range mobile dedicated video. (A video benchmarking website that tabulates user-submitted benchmarks shows Intel Iris Pro Graphics 5200, based on a very small sample of 8 submissions, scoring a G3D Mark of 912,[20] versus 1296 for the Nvidia GeForce GT 650M,[21] with higher scores being better. If the benchmark is linear, that puts the Iris Pro Graphics 5200's performance at about 70% of the GeForce GT 650M Intel was targeting. AMD's A10-5750M mobile APU with Radeon HD 8650G graphics scores 858 on this graphics benchmark.[22]) With anticipated price reductions, it is predicted that APUs will eventually replace low to mid-range dedicated video implementations. That will leave only the high-end enthusiast and professional market segments for video card vendors.

